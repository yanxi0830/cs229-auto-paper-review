{
  "name" : "1007.1282.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "vpest283@uottawa.ca" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n00 7.\n12 82\nv1 [\ncs .L\nG ]\n8 J\nul 2\n01 0\nKeywords-PAC learnability, fixed distribution learning, sample complexity, infinite VC dimension, witness of irregularity, Sontag’s ANN, precompactness.\nI. INTRODUCTION\nWe begin with a quote of the first part of the open problem 12.6 from Vidyasagar’s book [11] (this problem appears already in the original 1997 version).\n“How can one reconcile the fact that in distribution-free learning, every learnable concept class is also “polynomially” learnable, whereas this might not be so in fixeddistribution learning?\nIn the case of distribution-free learning of concept classes (...) there are only two possibilities: 1. C has infinite VC-dimension, in which case C is not PAC learnable at all. 2. C has finite VC-dimension, in which case C is not only PAC learnable, but the sample complexity m0(ε, δ) is O(1/ε + log(1/δ)). Let us call such a concept class “polynomially learnable”.\nIn other words, there is no “intermediate” possibility of a concept class being learnable, but having a sample complexity that is superpolynomial in 1/ε.\nIn the case of fixed-distribution learning, the situation is not so clear. (...) Is there a concept class for which every algorithm would require a superpolynomial number of samples? The only known way of consructing such a concept class would be to (...) attempt to construct a concept class whose ε-covering number grows faster than any exponential in 1/ε. It would be interesting to know whether such a concept class exists.”\nIn fact, the existence of a concept class whose sample complexity grows exponentially in 1/ε under a given fixed input distribution was already shown in 1991 by Benedek and Itai [2] (Theorem 3.5). Their example consisted of all finite subsets of a domain. Later and independently, a rather more natural concept class with such properties (generated by a neural network) was constructed by Barbara Hammer in her Ph.D. thesis [5] (Example 4.4.3 on page 77), cf. also [6].\nHere we somewhat strengthen the above results and at the same time show that the phenomenon is quite common. Suppose that a concept class C satisfies a slightly stronger property than having an infinite VC dimension, namely: C shatters every finite subset of an infinite set. Fix a sequence εk of desired values of learning precision, converning to zero, and let f be an increasing real function on [0,+∞). Then one can find a probability measure µ on the domain Ω of C with the property that C is PAC learnable under µ, but the sample complexity of learning to precision εk, k = 1, 2, 3, . . ., is growing as Ω(f(ε−1k )). The prescribed rate of growth can be ridiculouly high, for instance, a non-recursive function. The bound is essentially tight. For example, a wellknown sigmoidal feed-forward neural network of infinite VC dimension constructed by Sontag [8] has this property.\nThis naturally brings up a question of behaviour of Sontag’s network N under non-atomic input distributions. It follows from Talagrand’s theory of witness of irregularity [9], [10] that N is not Glivenko–Cantelli with regard to any measure having a non-atomic part. We do not know if a similar property holds for PAC learnability, although it is easy to see non-learnability of N for some common measures (the uniform distribution on the interval, the gaussian measure). While discussing a relationship between Glivenko–Cantelli property, PAC learnability, and precompactness, we give an answer to another (minor) question of Vidyasagar.\nNote that we find it instructive to present the above observations in the reverse order. In Conclusion, we suggest a few open problems and a conjecture supported by the results of this note which might shed light on Vidyasagar’s problem."
    }, {
      "heading" : "II. GLIVENKO–CANTELLI CLASSES AND LEARNABILITY",
      "text" : "A. PAC learnability and total boundedness\nBenedek and Itai [2] had proved that a concept class C is PAC learnable under a single probability distribution µ if and only if C is totally bounded in the L1(µ)-distance. Here we remind their results.\nTheorem 2.1 (Theorem 4.8 in [2]; Theorem 6.3 in [11]): Suppose C is a concept class, ε > 0, and that B1, . . . , Bk is an ε/2-cover for C . Then the minimal empirical risk algorithm is PAC to accuracy ε. In particular, the sample complexity of PAC learning C to accuracy ε with confidence 1− δ is\nm ≤ 32 ε log k δ .\nRecall that a subset A of a metric space X is ε-separated, or ε-discrete, if, whenever a, b ∈ A and a 6= b, one has d(a, b) ≥ ε > 0. The largest cardinality of an ε-discrete subset of X is the ε-packing number of X . For example, the following lemma estimates from below the packing number of the Hamming cube.\nLemma 2.2 ([11], Lemma 7.2 on p. 279): Let 0 < ε ≤ 1/4. The Hamming cube {0, 1}n, equipped with the normalized Hamming distance\ndh(x, y) = 1\nn |{i : xi 6= yi}| ,\nadmits a family of elements which are pairwise at a distance of at least 2ε from each other of cardinality at least exp[2(0.5− 2ε)2n].\nThe following is a source of lower bounds on the sample complexity.\nTheorem 2.3 (Lemma 4.8 in [2]; Theorem 6.6 in [11]): Suppose C is a given concept class, and let ε > 0 be specified. Then any algorithm that is PAC to accuracy ε requires at least lgM(2ε,C , L1(µ)) samples, where M(2ε,C , L1(µ)) denotes the 2ε-packing number of the concept class C with regard to the L1(µ)-distance.\nFor the most comprehensive presentation of PAC learnability under a single distribution, see [11], Ch. 6.\nB. Glivenko–Cantelli classes\nA function class F on a domain (a standard Borel space) Ω is Glivenko–Cantelli with regard to a probability distribution µ ([3], Ch. 3), or else has the property of uniform convergence of empirical means (UCEM property) [11], if for each ε > 0\nsup µ∈P\nµ⊗n\n{\nsup f∈F\n|Eµ(f)− Eµn(f)| ≥ ε }\n→ 0 as n → ∞. (1)\nHere µ⊗n is the product measure on Ωn, and µn stands for the empirical (uniform) measure on n points, sampled from the domain in an i.i.d. fashion. We assume F to assume values in an interval (i.e., to be uniformly bounded). The\nnotion applies to neural networks as well, if F denotes the family of output functions corresponding to all possible values of learning parameters.\nEvery Glivenko–Cantelli class F is PAC learnable, which explains the important role of this notion. In fact, every consistent learning rule L will learn F . We find it instructive to give a different proof, replying in passing to a remark of Vidyasagar [11], p. 241. After proving that every Glivenko– Cantelli concept class C with regard to a fixed measure µ is precompact with regard to the L1(µ)-distance, the author remarks that his proof is both indirect (Glivenko–Cantelli ⇒ PAC learnable ⇒ precompact), and does not extend to function classes, so it is not known to the author whether the result holds if C is replaced with a function class F .\nThe answer is yes, as is (implicitely) stated in [10] (p. 379, the beginning of the proof of Proposition 2.5), but a deduction is also rather roundabout (proving first the absence of a witness of irregularity). In fact, the result is really very simple.\nObservation 2.4: Every (uniformly bounded) Glivenko– Cantelli function class F with regard to a fixed probabillty measure µ is precompact in the L1(µ)-distance.\nProof: If F is not precompact, then for some ε0 > 0 it contains an infinite ε0-discrete subfamily F ′. For every finite sample σ ∈ Ωn there is a further infinite subfamily F ′′ ⊆ F ′ of functions whose restrictions to σ are at a pairwise L1(µn)-distance < ε0/2 from each other (the pigeonhole principle coupled with the fact that the restriction of F to σ is L1(µn)-precompact). This means that µ- and µn-expectations of some function of the form |f1 − f2|, fi ∈ F , i = 1, 2, differ between themselves by at least ε0/2, and for at least one of i ∈ {1, 2},\n|Eµ(fi)− Eµn(fi)| ≥ ε0/4 (an application of the triangle inequality in R). Since the latter is true for every sample, no matter the size, F is not Glivenko–Cantelli.\nIn fact, the same proof works in a slightly more general case when F is uniformly bounded by a single function (not necessarily integrable).\nThis gives an alternative deduction of the implication Glivenko–Cantelli ⇒ PAC learnability. Admittedly, the result obtained is somewhat weaker, as this way we do not get consistent learnability.\nC. Talagrand’s witness of irregularity\nTalagrand [9], [10] had characterized uniform Glivenko– Cantelli function classes with regard to a single distribution in terms of shattering. We will remind his main result for concept classes only. Let Ω be a measurable space, let C be a concept class on Ω, and let µ be a probability measure on Ω. A measurable subset A ⊆ Ω is a witness of irregularity of C , if µ(A) > 0 and for every n the set of all n-tuples of elements of A shattered by C has full measure in An.\nIn other words, µ-almost all n-tuples of elements of A are shattered by C .\nTheorem 2.5 (Talagrand [9], Th. 2): A concept class C is Glivenko–Cantelli with regard to the probability measure µ if and only if C admits no witness of irregulaity.\nLet µ be a probability measure on Ω. Recall that a set A is an atom if for every measurable B ⊆ A one has either µ(B) = 0 or µ(B) = µ(A). The measure µ is non-atomic if it contains no atoms, and purely atomic if the measures of atoms add up to one. The restriction of µ to the union of atoms is the atomic part of µ.\nSince a witness of irregularity can contain no atoms, the following is an immediate corollary of Talagrand’s 1987 result.\nCorollary 2.6: If a measure µ is purely atomic, then every concept class C is uniform Glivenko–Cantelli with regard to µ, and in particular PAC learnable.\nThe corollary is easy to prove directly, without using subtle results of Talagrand, and the result was observed (independently) in 1991 and investigated in detail by Benedek and Itai ([2], Theorem 3.2). Notice that the result does not assert polynomial PAC learnability of C , and we will see shortly that the required sample complexity of C can grow arbitrarily fast.\nD. The neural network of Sontag\nFigure 1 recalls a well-known example of a sigmoidal neural network N constructed by Sontag [8], pp. 34–36. (Cf. also [11], page 389, where the top diagram in Figure 1 is borrowed from.) The activation sigmoid is of the form\nφ(x) = 1\nπ tan−1 x+\ncosx\nα(1 + x2) +\n1 2 ,\nwhere α ≥ 2π is fixed, e.g. α = 100. and the outputlayer perceptron has both input weights equal to one and a threshold of one. The input-output function of the network is given by\ny = η[ρ(x)],\nwhere\nρ(x) = 2 coswx\nα(1 + w2x2) .\nThe input space of N is the space R of real numbers. Recall that a collection x1, x2, . . . , xn of real numbers is rationally independent if no non-trivial linear combination of 1, x1, x2, . . . , xn with rational coefficients vanishes.\nTheorem 2.7 ([8], pp. 42-43): The Sontag network N shatters every rationally independent n-tuple of real inputs x1, x2, . . . , xn.\nIn particular, the VC dimension of Sontag’s network is infinite. Besides, it is easy to find an infinite rationally independent set, and so every finite subset of such a set is shattered by N . We will need this fact later.\nHere is another extreme property of Sontag’s network.\nTheorem 2.8: The neural network of Sontag N is Glivenko–Cantelli under a probability distribution µ on the inputs if and only if µ is purely atomic.\nProof: Sufficiency (⇐) follows from Corollary 2.6. Let us prove necessity (⇒). By splitting µ into a purely atomic part µa and a continuous part µc, in view of Theorems 2.5 of Talagrand and 2.7 of Sontag, it suffices to prove that for every non-atomic probability measure ν on R the set of rationally independent n-tuples has a full ν⊗n measure in R\nn: the support of µc will then be a witness of irregularity. In its turn, this reduces to a proof that for a fixed collection (λ1, . . . , λn+1) of rationals not all of which are zero, the affine hyperplane\nHλ = {x ∈ Rn : 〈x, λ〉 = λn+1}, where λ = (λ1, . . . , λn), has ν⊗n-measure null. This is a consequence of Eggleston’s theorem [4]: If A is a measurable, Lebesgue-positive subset of the unit square, then there is a measurable positive set B and a perfect set C such\nthat B × C is included in A. “Lebesgue measure on the unit square” here is not a loss of generality, as every two non-atomic standard Borel probability measure spaces are isomorphic, and we obtain by induction that if A ⊆ Rn and ν⊗n(A) > 0, then A contains a product of n sets one of which is ν⊗n-measure positive and all the rest are perfect (contain no isolated points). Clearly, no (n− 1)-hyperplane in Rn can have this property.\nExample 2.9: Sontag’s ANN is not PAC learnable under the uniform distribution on an interval.\nIndeed, for the sequence of learning parameters wk = 2k the corresponding output binary functions are at a pairwise L1(λ)-distance 1/2 from each other, where λ is a uniform distribution on some interval.\nA similar argument works for the gaussian distribution on the inputs.\nHowever, we do not know if there exists a non-atomic measure under which Sontag’s ANN is PAC learnable.\nE. Glivenko–Cantelli versus learnability\nNot every PAC learnable function, or even concept, class is Glivenko–Cantelli. Examples of such concept classes exist trivially, e.g. the concept class consisting of all finite and all cofinite subsets of the unit intervals is PAC learnable under every non-atomic distribution, yet clearly not uniform Glivenko–Cantelli, cf. [2], p. 385, note (2), or [11], p. 230, Example 6.4. A more interesting example, though based on the same idea, is Example 6.6 in [11], p. 232. Here we present such an example of a countable concept class.\nExample 2.10: For n ∈ N, say that intervals [i/n, (i + 1)/n], i = 0, 1, . . . , n − 1, are of order n. Let Cn consist of all unions of less than √ n intervals of order n, and set C = ∪∞i=1Cn. If now k ∈ N is any and x1 < x2 < . . . < xk are points of the unit interval, choose n > k2 so that 1/n is smaller than any of the half-distances between neighbouring points (xi+1 − xi)/2, i = 1, 2, . . . , n. Clearly, elements of Cn shatter the sample {x1, x2, . . . , xk}, and so the entire interval is a witness of irregularity for the concept class C . By Talagrand’s result, the class C is not Glivenko–Cantelli. At the same time, for every n, Cn forms an n−1/2-net for C with regard to the L1(λ)-distance, and so C is PAC learnable under the Lebesgue measure λ (the uniform measure on the interval).\nObserve that, in fact, C fails the Glivenko–Cantelli property with regard to every measure having a non-atomic part. As we have seen, there exist non-atomic measures under which C is PAC learnable. There are also measures under which C is not PAC learnable. for example the Haar measure ν on the Cantor set.\nRecall the construction of the Cantor “middle third” set C (Figure 3). This is the set left of the closed unit interval [0, 1] after first deleting the middle third (1/3, 2/3), then deleting the middle thirds of the two remaining intervals, (1/9, 2/9) and (7/9, 8/9), and continuing to delete the middle thirds ad infimum. The elements of the Cantor set are exactly those real numbers between 0 and 1 admitting a ternary expansion not containing 1. Sometimes C is called Cantor dust. The complement to the Cantor set is a union of countably many open intervals, all the middle thirds left out. The set Cn left after the first n steps of removing the middle thirds is the union of 2n closed intervals of equal length 3−n each. The Haar measure of every such interval is set to be equal to 2−n, and this condition defines a non-atomic measure ν supported on C in a unique way.\nIt is easy to see now that the closed intervals I1, I2, . . . , I2n at the level n are shattered with concept classes from CN if N is large enough (≥ 22n), in the following sense: for every set of indices J ⊆ {1, 2, . . . , 2n} there is a C ∈ CN which contains every interval Ij , j ∈ J , and is disjoint from every interval Ik , where k /∈ J . Now\nsecond step\ntwo middle thirds removed at the\none can modify the proof of Lemma 2.2 exactly as it was done in [7], proof of Theorem 3, in order to conclude that C is not totally bounded in the L1(ν)-distance."
    }, {
      "heading" : "III. ALL RATES OF SAMPLE COMPLEXITY ARE POSSIBLE",
      "text" : "Theorem 3.1: Let C be a concept class which shatters every finite subset of some infinite set. Let (εk), εk ↓ 0 be a sequence of positive reals converging to zero, and let f : R+ → R+ be a non-decreasing function growing at least linearly: f(x) = Ω(x). Then there is a probability measure µ = µ((εk), f) on the input domain Ω with the property that for every δ > 0 and k ∈ N the class C is PAC learnable under the distribution µ to accuracy εk, and the rate of required sample complexity is at least\nn(εk, δ) = Ω\n(\nf\n(\n1\nεk\n))\n. (2)\nMoreover, the above estimate is essentially tight in the sense that the sample complexity\nn(εk, δ) = O\n(\nf\n(\n1\nεk\n)\n+ log\n(\n1\nδ\n))\n. (3)\nsuffices to learn C to accuracy 4εk with confidence 1− δ. Proof: We can assume without loss in generality that ε1 = 1/5. For every k, set mk = 5(εk+1 − εk). Then mk form a sequence of non-negative reals which sums up to one. Denote, for simplicity, fk = f(ε −1\nk ). Further, choose pairwise disjoint finite sets Fk of cardinality |Fk| = fk − fk−1 (where f0 = 0) in a way that every union of finitely many of Fk’s is shattered by C (this is possible due to the assumption on the class C ). Let µk denote a uniform measure supported on Fk of total mass mk. Now set µ = ∑∞\ni=1 µk. Since ∑∞\ni=1 mk = 1, µ is a probability Borel measure.\nLet k be arbitrary. Select any subset of C shattering ∪ki=1Fi and containing\nk ∏\ni=1\n|Fi| = 2fk\nelements. This set forms a finite εk-net in C with regard to the L1(µ)-distance. Since εk ↓ 0, we use Theorem 2.1 to conclude: the class C is PAC learnable under µ, and the sample complexity of learning C to accuracy εk and confidence 1− δ, δ > 0 is\nm ≥ 8 ε2 log 2fk δ = 8\nε2k\n(\nfk + log(δ −1)\n)\n.\nFor every k, Lemma 2.2, applied with ε = 0.2, guarantees the existence of a subset Φk of C every two elements of which are at a L1(µi)-distance ≥ 0.42mi from each other, and containing ≥ exp[0.0128(fk − fk−1)] elements. Let N be so large that\n∑N k=1 mk ≥ (1.05)−1. Fix k. Since\n∪Nk=1Fk is shattered by C , one can find elements of C which correspond to elements of the product\n∏N i=k Φi, and every\ntwo of which are at a distance ≥ 0.42∑Nk=1 mkεk ≥ 0.4εk from each other. According to Theorem 2.3, this means that the computational complexity of learning C under µ to accuracy εk with confidence 1 − δ is at least 0.0128fk samples.\nRemark 3.2: The measure µ constructed in the proof is purely atomic. However, by replacing the domain Ω with Ω × [0, 1], every concept C ∈ C with C × [0, 1], and µ with the product µ⊗ λ, where λ is the uniform (Lebesgue) measure on the interval, one can “translate” every example as above into an example of learning under a non-atomic probability distribution.\nCorollary 3.3: Let ν be a probability distribution on a domain Ω having infinite support. Then there exist concept classes C which are PAC learnable under ν and whose required sample complexity is arbitrarily high.\nProof: The measure space (Ω, ν) admits a measurepreserving map φ to the measure space constructed in the proof of Theorem 3.1 in such a way that νφ−1 = µ (here one uses the fact that µ is purely atomic). Now the concept class Cφ−1, consisting of all sets φ−1(C), has the same learning properties under the distribution ν as the class C has under µ.\nCorollary 3.4: Let εk ↓ 0 be a sequence of positive values converging to zero, and let fk be a real function on [0,+∞) growing at least linearly. Then there is a probability distribution µ on the real numbers under which Sontag’s network N is PAC learnable to accuracy εk with confidence 1−δ, requiring the sample of size Ω(f(ε−1k )). This estimate is essentially tight, because the sample size\nn(εk, δ) = O\n(\nf\n(\n1\nεk\n)\n+ log\n(\n1\nδ\n))\n. (4)\nalready suffices to train N to accuracy 4εk with confidence 1− δ.\nRemark 3.5: It is easy to construct concept classes which are PAC learnable under every input distribution, and yet exhibit all possible rates of learning sample complexity. These are the classes C which, speaking informally, cannot\ntell a difference between a given probability distribution µ and some purely atomic measure ν. More precisely, if the sigma-algebra of sets generated by C is purely atomic and C shatters every finite subset of an infinite set, then C will have the above property.\nAn example is a class C that consists of all finite unions of middle thirds of the Cantor set C. The atoms of the sigma-algebra of sets generated by this class are precisely the middle thirds, and so C has the desired property."
    }, {
      "heading" : "IV. CONCLUSION",
      "text" : "Stimulated by a question embedded into the Problem 12.6 of Vidyasagar [11], we have shown that all rates of sample compleixity growth are possible for distributiondependent learning, in particular all are realized by binary output feed-forward sigmoidal neural network of Sontag. Now Vidyasagar continues thus:\n“I would like to have an “intrinsic” explanation as to why in distribution-free learning, every learnable concept class is also forced to be polynomially learnable. Next, how far can one “push” this line of argument? Suppose P is a family of probabilities that contains a ball in the total variation metric ρ. From Theorem 8.8 it follows that every concept class that is learnable with respect to P must also be polynomially learnable (because C must have finite VCdimension). Is it possible to identify other such classes of probabilities?”\nWe suggest the following conjecture, which, in our view, is the right framework in which to address Vidyasagar’s question.\nConjecture (“the sample complexity alternative”). Let P be a family of probability distributions on the domain Ω. Then either every class learnable under P is learnable with sample complexity O(ε−1), or else there exist PAC learnable classes under P whose required sample complexity grows arbitrarily fast.\nThe classical VC theory tells that the conjecture is true if P is the family of all probability measures: namely, the first alternative holds always. In view of Corollary 3.3, the conjecture is also true in the other extreme case, where P = {µ} contains a single distribution: unless µ is finitelysupported, we have the second alternative.\nProblem 1. Does the above alternative hold for every family P of probability distributions on the inputs?\nProblem 2. Does there exist a non-atomic probability measure on R under which the Sontag ANN is PAC learnable?\nProblem 3. Give a criterion for a concept class to be PAC learnable under a fixed probability distribution in terms of shattering.\nSome sufficient conditions can be found in [2], [1], but none of them is also necessary. The “right” condition will be strictly intermediate between the witness of irregularity [9], [10] and the VC dimension modulo countable sets [7]."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "The author is grateful to the anonymous referees, in particular for pointing out the references [5], [6], and to Ilijas Farah for pointing out the reference [4]."
    } ],
    "references" : [ {
      "title" : "A sufficient condition for polynomial distribution-dependent learnability",
      "author" : [ "M. Anthony", "J. Shawe-Taylor" ],
      "venue" : "Discrete Applied Math. 77 ",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Learnability with respect to fixed distributions",
      "author" : [ "G.M. Benedek", "A. Itai" ],
      "venue" : "Theor. Comp. Sci. 86 ",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 1991
    }, {
      "title" : "Uniform Central Limit Theorems",
      "author" : [ "R.M. Dudley" ],
      "venue" : "Cambridge Studies in Advanced Mathematics, 63, Cambridge University Press, Cambridge ",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Two measure properties of Cartesian product sets",
      "author" : [ "H.G. Eggleston" ],
      "venue" : "Quart. J. Math. Oxford (2) 5 ",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1954
    }, {
      "title" : "Learning with Recurrent Neural Networks",
      "author" : [ "B. Hammer" ],
      "venue" : "Dissertation, Universität Osnabrück",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "On the learnability of recursive data",
      "author" : [ "B. Hammer" ],
      "venue" : "Mathematics of Control Signals and Systems, 12 ",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "PAC learnability of a concept class under nonatomic measures: a problem by Vidyasagar",
      "author" : [ "V. Pestov" ],
      "venue" : "to appear in Proc. 21st Conf. on Algorithmic Learning Theory ",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Feedforward nets for interpolation and classification",
      "author" : [ "E.D. Sontag" ],
      "venue" : "J. Comp. Systems Sci 45(1) ",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "The Glivenko–Cantelli problem",
      "author" : [ "M. Talagrand" ],
      "venue" : "Ann. Probab. 15 ",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 1987
    }, {
      "title" : "The Glivenko-Cantelli problem",
      "author" : [ "M. Talagrand" ],
      "venue" : "ten years later, J. Theoret. Probab. 9 ",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Learning and Generalization",
      "author" : [ "M. Vidyasagar" ],
      "venue" : "with Applications to Neural Networks, 2nd Ed., Springer-Verlag",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2003
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "6 from Vidyasagar’s book [11] (this problem appears already in the original 1997 version).",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 1,
      "context" : "” In fact, the existence of a concept class whose sample complexity grows exponentially in 1/ε under a given fixed input distribution was already shown in 1991 by Benedek and Itai [2] (Theorem 3.",
      "startOffset" : 180,
      "endOffset" : 183
    }, {
      "referenceID" : 4,
      "context" : "thesis [5] (Example 4.",
      "startOffset" : 7,
      "endOffset" : 10
    }, {
      "referenceID" : 5,
      "context" : "also [6].",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 7,
      "context" : "For example, a wellknown sigmoidal feed-forward neural network of infinite VC dimension constructed by Sontag [8] has this property.",
      "startOffset" : 110,
      "endOffset" : 113
    }, {
      "referenceID" : 8,
      "context" : "It follows from Talagrand’s theory of witness of irregularity [9], [10] that N is not Glivenko–Cantelli with regard to any measure having a non-atomic part.",
      "startOffset" : 62,
      "endOffset" : 65
    }, {
      "referenceID" : 9,
      "context" : "It follows from Talagrand’s theory of witness of irregularity [9], [10] that N is not Glivenko–Cantelli with regard to any measure having a non-atomic part.",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 1,
      "context" : "Benedek and Itai [2] had proved that a concept class C is PAC learnable under a single probability distribution μ if and only if C is totally bounded in the L(μ)-distance.",
      "startOffset" : 17,
      "endOffset" : 20
    }, {
      "referenceID" : 1,
      "context" : "8 in [2]; Theorem 6.",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 10,
      "context" : "3 in [11]): Suppose C is a concept class, ε > 0, and that B1, .",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 10,
      "context" : "2 ([11], Lemma 7.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 1,
      "context" : "8 in [2]; Theorem 6.",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 10,
      "context" : "6 in [11]): Suppose C is a given concept class, and let ε > 0 be specified.",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 10,
      "context" : "For the most comprehensive presentation of PAC learnability under a single distribution, see [11], Ch.",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 2,
      "context" : "A function class F on a domain (a standard Borel space) Ω is Glivenko–Cantelli with regard to a probability distribution μ ([3], Ch.",
      "startOffset" : 124,
      "endOffset" : 127
    }, {
      "referenceID" : 10,
      "context" : "3), or else has the property of uniform convergence of empirical means (UCEM property) [11], if for each ε > 0",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 10,
      "context" : "We find it instructive to give a different proof, replying in passing to a remark of Vidyasagar [11], p.",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 9,
      "context" : "The answer is yes, as is (implicitely) stated in [10] (p.",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 8,
      "context" : "Talagrand [9], [10] had characterized uniform Glivenko– Cantelli function classes with regard to a single distribution in terms of shattering.",
      "startOffset" : 10,
      "endOffset" : 13
    }, {
      "referenceID" : 9,
      "context" : "Talagrand [9], [10] had characterized uniform Glivenko– Cantelli function classes with regard to a single distribution in terms of shattering.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 8,
      "context" : "5 (Talagrand [9], Th.",
      "startOffset" : 13,
      "endOffset" : 16
    }, {
      "referenceID" : 1,
      "context" : "The corollary is easy to prove directly, without using subtle results of Talagrand, and the result was observed (independently) in 1991 and investigated in detail by Benedek and Itai ([2], Theorem 3.",
      "startOffset" : 184,
      "endOffset" : 187
    }, {
      "referenceID" : 7,
      "context" : "Figure 1 recalls a well-known example of a sigmoidal neural network N constructed by Sontag [8], pp.",
      "startOffset" : 92,
      "endOffset" : 95
    }, {
      "referenceID" : 10,
      "context" : "also [11], page 389, where the top diagram in Figure 1 is borrowed from.",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 7,
      "context" : "7 ([8], pp.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 3,
      "context" : "This is a consequence of Eggleston’s theorem [4]: If A is a measurable, Lebesgue-positive subset of the unit square, then there is a measurable positive set B and a perfect set C such",
      "startOffset" : 45,
      "endOffset" : 48
    }, {
      "referenceID" : 1,
      "context" : "[2], p.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 10,
      "context" : "385, note (2), or [11], p.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 10,
      "context" : "6 in [11], p.",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 0,
      "context" : "This is the set left of the closed unit interval [0, 1] after first deleting the middle third (1/3, 2/3), then deleting the middle thirds of the two remaining intervals, (1/9, 2/9) and (7/9, 8/9), and continuing to delete the middle thirds ad infimum.",
      "startOffset" : 49,
      "endOffset" : 55
    }, {
      "referenceID" : 6,
      "context" : "2 exactly as it was done in [7], proof of Theorem 3, in order to conclude that C is not totally bounded in the L(ν)-distance.",
      "startOffset" : 28,
      "endOffset" : 31
    }, {
      "referenceID" : 0,
      "context" : "However, by replacing the domain Ω with Ω × [0, 1], every concept C ∈ C with C × [0, 1], and μ with the product μ⊗ λ, where λ is the uniform (Lebesgue) measure on the interval, one can “translate” every example as above into an example of learning under a non-atomic probability distribution.",
      "startOffset" : 44,
      "endOffset" : 50
    }, {
      "referenceID" : 0,
      "context" : "However, by replacing the domain Ω with Ω × [0, 1], every concept C ∈ C with C × [0, 1], and μ with the product μ⊗ λ, where λ is the uniform (Lebesgue) measure on the interval, one can “translate” every example as above into an example of learning under a non-atomic probability distribution.",
      "startOffset" : 81,
      "endOffset" : 87
    }, {
      "referenceID" : 10,
      "context" : "6 of Vidyasagar [11], we have shown that all rates of sample compleixity growth are possible for distributiondependent learning, in particular all are realized by binary output feed-forward sigmoidal neural network of Sontag.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 1,
      "context" : "Some sufficient conditions can be found in [2], [1], but none of them is also necessary.",
      "startOffset" : 43,
      "endOffset" : 46
    }, {
      "referenceID" : 0,
      "context" : "Some sufficient conditions can be found in [2], [1], but none of them is also necessary.",
      "startOffset" : 48,
      "endOffset" : 51
    }, {
      "referenceID" : 8,
      "context" : "The “right” condition will be strictly intermediate between the witness of irregularity [9], [10] and the VC dimension modulo countable sets [7].",
      "startOffset" : 88,
      "endOffset" : 91
    }, {
      "referenceID" : 9,
      "context" : "The “right” condition will be strictly intermediate between the witness of irregularity [9], [10] and the VC dimension modulo countable sets [7].",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 6,
      "context" : "The “right” condition will be strictly intermediate between the witness of irregularity [9], [10] and the VC dimension modulo countable sets [7].",
      "startOffset" : 141,
      "endOffset" : 144
    }, {
      "referenceID" : 4,
      "context" : "The author is grateful to the anonymous referees, in particular for pointing out the references [5], [6], and to Ilijas Farah for pointing out the reference [4].",
      "startOffset" : 96,
      "endOffset" : 99
    }, {
      "referenceID" : 5,
      "context" : "The author is grateful to the anonymous referees, in particular for pointing out the references [5], [6], and to Ilijas Farah for pointing out the reference [4].",
      "startOffset" : 101,
      "endOffset" : 104
    }, {
      "referenceID" : 3,
      "context" : "The author is grateful to the anonymous referees, in particular for pointing out the references [5], [6], and to Ilijas Farah for pointing out the reference [4].",
      "startOffset" : 157,
      "endOffset" : 160
    } ],
    "year" : 2013,
    "abstractText" : "We show that the learning sample complexity of a sigmoidal neural network constructed by Sontag (1992) required to achieve a given misclassification error under a fixed purely atomic distribution can grow arbitrarily fast: for any prescribed rate of growth there is an input distribution having this rate as the sample complexity, and the bound is asymptotically tight. The rate can be superexponential, a nonrecursive function, etc. We further observe that Sontag’s ANN is not Glivenko–Cantelli under any input distribution having a non-atomic part. Keywords-PAC learnability, fixed distribution learning, sample complexity, infinite VC dimension, witness of irregularity, Sontag’s ANN, precompactness.",
    "creator" : "LaTeX with hyperref package"
  }
}