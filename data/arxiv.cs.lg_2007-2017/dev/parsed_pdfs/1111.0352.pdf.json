{
  "name" : "1111.0352.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Revisiting k-means: New Algorithms via Bayesian Nonparametrics",
    "authors" : [ "Brian Kulis", "Michael I. Jordan" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "A ubiquitous question encountered when applying parametric clustering algorithms—including k-means, mixtures of Gaussians, and graph cut-based approaches—is how many clusters exist in the data. Most clustering methods assume a fixed, known value for k which, in many cases, is simply not known a priori. Some work has gone into the development of techniques to overcome this problem in the context of k-means, such as [2, 7, 8, 12, 14, 19, 21], which utilize various model selection criteria.\nIn the case of mixture models, one possible solution is to employ ideas from Bayesian nonparametrics to extend mixture models so that they “automatically” discover the number of clusters in a given data set; a canonical example is the Dirichlet process (DP) mixture model. Loosely speaking, the DP mixture model arises by placing a Dirichlet prior over the mixing weights of a standard mixture model, and then appropriately taking a limit where the number of clusters goes to infinity. The resulting (infinite) mixing weights follow the so-called stick-breaking process, which decay exponentially; as a result, the number of components effectively grows logarithmically in the number of data points. The resulting scheme is nonparametric in the sense that the number of parameters grows with the number of data points. Inference algorithms for DP mixture models are more complex than standard EM for finite mixture models, but several effective posterior inference techniques have been developed, including Gibbs sampling algorithms [16, 22], MetropolisHastings algorithms [16], split-merge proposals [11], variational inference [3], and permutation-augmented samplers [13].\nar X\niv :1\n11 1.\n03 52\nv1 [\ncs .L\nG ]\n2 N\nov 2\nDespite the success of DP mixture models, similar ideas have not been explored for hard clustering algorithms (i.e., methods that always assign points to individual clusters) such as k-means. The goal of this paper is to derive and analyze extensions of k-means based on an asymptotic hard clustering limit of the DP mixture model. To convert the DP mixture into a hard clustering problem, we will utilize a technique that is analogous to how mixtures of Gaussians and k-means are related, namely that the k-means algorithm may be viewed as a limit of the EM algorithm—if all of the covariance matrices corresponding to the clusters in a Gaussian mixture model are equal to I and we let go to zero, the EM steps approach the k-means steps in the limit. In the case of a DP mixture model, we can perform a similar limiting argument on the covariances in the context of a simple Gibbs sampling algorithm, and this leads to an algorithm involving hard cluster assignments. This resulting algorithm is very similar to the classical k-means algorithm, except that a new cluster is formed whenever a point is sufficiently far away from all existing cluster centroids. We further show that this algorithm monotonically converges to a local optimum of an elegant k-means-like objective function. This objective is nearly identical to the original k-means objective, except that the number of clusters is part of the optimization, and the objective includes a penalty term based on how many clusters are formed. While this objective appears in the k-means literature, as an extension motivated by the Akaike Information Criterion (AIC) [15], it does not appear that any algorithms have been developed for optimization of the objective, nor are there any existing connections made to Bayesian nonparametrics.\nWe can take a step further and extend our analysis to the hierarchical Dirichlet process (HDP), a model for clustering multiple related data sets [20]. In the HDP mixture model, each data set is modeled as a DP mixture, but the mixture models are all tied together by an additional Dirichlet process. When we take an asymptotic argument on the covariances of a standard Gibbs sampler for the HDP mixture, we obtain a novel k-means-like algorithm that clusters multiple data sets with shared cluster structure. In particular, the resulting algorithm forms both local and global clusters—local clusters are clusters from each individual data set, and global clusters are formed by combining local clusters. As in the case of the DP mixture, we can determine the underlying objective function optimized by the resulting hard clustering algorithm; in the case of the HDP, we obtain the global k-means objective function over all the data with two additional penalties, one for the total number of local clusters and one for the total number of global clusters. In the literature on kmeans and other hard clustering algorithms, we are unaware of other existing flexible methods for clustering multiple data sets, so our analysis yields an algorithm that could potentially be useful in a variety of contexts.\nTo further boost the practicality of our approach, we demonstrate two additional extensions of our analysis. First, we show that there exists another method for optimization of the penalized k-means objective arising from the DP mixture, based on a spectral relaxation of the discrete optimization problem. This spectral relaxation involves thresholding eigenvectors appropriately, and highlights an interesting connection between spectral methods and Bayesian nonparametrics. Second, one of the most practical benefits to the standard k-means objective function arises from its connections to other hard clustering objectives, and we explore one such connection. In particular, it is known that a general weighted k-means objective is mathematically equivalent to several graph clustering objectives, including normalized cut and ratio cut. This connection leads to kernel k-means algorithms for monotonically decreasing the cut values, and which scale efficiently to large graphs; the work in [6] developed a technique for graph clustering based on this connection that outperforms spectral clustering approaches to graph clustering, and scales to significantly larger graphs. In a similar manner, we focus on the normalized cut objective, and show that our derived clustering algorithm can be adapted to the graph setting. The resulting objective to optimize is the normalized cut with a penalty over the number of clusters in the graph, and the resulting algorithm takes time O(|E|) to update the cluster assignments to all nodes, where |E| is the number of edges in the graph; note that this is independent of the number of clusters, which will vary throughout the algorithm.\nWe present some preliminary experiments demonstrating the flexibility of our approach, as well as some remaining open problems. Ultimately, we hope that this line of work will inspire additional results on the integration of Bayesian nonparametrics and hard clustering methods."
    }, {
      "heading" : "2 Background",
      "text" : "We begin with a short discussion of the relevant models considered in this work. In particular, we will overview mixtures of Gaussians, k-means, graph cuts, and DP mixtures. We will introduce the models as well as briefly discuss some algorithmic details and known connections among the models."
    }, {
      "heading" : "2.1 Gaussian Mixture Models",
      "text" : "In a (finite) Gaussian mixture model, we assume that data arises from the following distribution:\np(x) = k∑ c=1 πcN (x | µc,Σc),\nwhere k is the fixed number of components, πc are the mixing coefficients, and µc and Σc are the means and covariances, respectively, of the k Gaussian distributions. In the non-Bayesian setting, we can use the expectation-maximization algorithm to perform maximum likelihood given a set of observations x1, ...,xn. Briefly, we initialize the means µc, covariances Σc, and mixing coefficients πc. Then we alternate between the E-step and M-step. In the E-step, using the current parameter values, we compute the following quantities for all i = 1, ..., n and for all c = 1, ..., k:\nγ(zic) = πcN (xi | µc,Σc)∑c j=1 πjN (xi | µj ,Σj) .\nIn the M-step, we re-estimate the parameters using the values of γ(zic):\nµnewc = 1\nnc n∑ i=1 γ(zic)xi\nΣnewc = 1\nnc n∑ i=1 γ(zic)(xi − µnewc )(xi − µnewc )T\nπnewc = nc n ,\nwhere nc = ∑n i=1 γ(zic). One can show that the EM algorithm converges to a local optimum of the log likelihood function. Note that the values γ(zic) are the probabilities of assigning point xi to cluster c, and so the resulting clustering is a soft clustering of the data."
    }, {
      "heading" : "2.2 k-Means",
      "text" : "A related model for clustering is provided by the k-means objective function, an objective for discovering a hard clustering of the data. Given a set of data points x1, ...,xn, the k-means objective function attempts to find clusters `1, ..., `k to minimize the following objective function:\nmin {`c}kc=1\n∑k c=1 ∑ x∈`c ‖x− µc‖ 2 2\nwhere µc = 1|`c| ∑ x∈µc x.\nThe most popular method for minimizing this objective function is simply called the k-means algorithm. One initializes the algorithm with some hard clustering of the data along with the cluster means of the corresponding clusters. Then the algorithm alternates between reassigning points to clusters and recomputing the means.\nIn particular, for the reassignment step one computes the squared Euclidean distance from each point to each cluster mean, and finds the minimum:\n`∗(i) = argminc‖xi − µc‖22.\nEach point is then reassigned to the cluster indexed by `∗(i). The centroid update step of the algorithm then recomputes the mean of each cluster, updating µc for all c.\nThe EM algorithm for mixtures of Gaussians is quite similar to the k-means algorithm. Indeed, one can show a precise connection between the two algorithms. Suppose in the mixture of Gaussians model that all Gaussians have the same fixed covariance equal to I . Because they are fixed, the covariances need not be re-estimated during the M-step. In this case, the γ(zic) updates take the following form:\nγ(zic) = exp ( − 12 ‖xi − µc‖ 2 2 )∑k j=1 exp ( − 12 ‖xi − µj‖ 2 2\n) . It is straightforward to show that, in the limit as → 0, the value of γ(zic) approaches 0 for all c except for the one corresponding to the smallest distance ‖xi − µc‖22. In this case, the E-step is equivalent to the reassignment step of k-means, and the M-step exactly recomputes the means of the new clusters, establishing the equivalence of the updates. One can further show that an expectation of the complete-data log likelihood approaches the k-means objective in the limit as → 0."
    }, {
      "heading" : "2.3 Graph Cuts",
      "text" : "The k-means objective and algorithm can be generalized in various ways. For instance, it is straightforward to apply k-means in kernel space. Suppose the data has been mapped via some function φ so that the data points in feature space are φ(x1), φ(x2), ..., φ(xn). Further suppose we can compute a kernel function κ(xi,xj) = φ(xi)\nTφ(xj) without explicitly computing the function φ. Note that the computation between a data point and any cluster mean can be expressed in kernel space by expanding the squared Euclidean distance:\n‖φ(x)− µc‖22 = (φ(x)− µc)T (φ(x)− µc) = φ(x)Tφ(x)− 2φ(x)Tµc + µTc µc\n= φ(x)Tφ(x)− 2 ∑ φ(xi)∈`c φ(x) Tφ(xi)\n|`c| +\n∑ φ(xi),φ(xj)∈`c φ(xi) Tφ(xj)\n|`c|2 = κ(x,x)− 2 ∑ φ(xi)∈`c κ(x,xi)\n|`c| +\n∑ φ(xi),φ(xj)∈`c κ(xi,xj)\n|`c|2\nWhen computing distances via the above method, it is unnecessary to explicitly compute the means of the clusters. Therefore, the resulting kernel k-means algorithm does not have an explicit mean re-estimation step; instead, the distances to each implicit cluster mean are computed in kernel space and then each point is reassigned to the cluster corresponding to the nearest implicit cluster mean. This is repeated until convergence.\nAnother extension of the k-means objective is to introduce a weight wi for each point, and to minimize a weighted form of the k-means objective function:\nmin {`c}kc=1\n∑k c=1 ∑ x∈`c wi‖xi − µc‖ 2 2\nwhere µc = ∑ xi∈`c wixi∑\nxi∈`c wi\n.\nA third, less obvious extension to the k-means objective function arises in the context of graph clustering [4, 18, 23]. Given a graph G = (V, E , A), where V is a set of vertices, E a set of edges, and A is the underlying\nadjacency matrix for the graph, various methods have been proposed to cluster the vertices in the graph into a disjoint collection of clusters. Two popular criteria for the graph clustering problem are the ratio cut and normalized cut. In the ratio cut, one seeks a clustering V1, ...,Vk of the vertices to minimize the following objective:\nmin V1,...,Vk k∑ c=1 links(Vc,V \\ Vc) |Vc| , (Ratio Cut)\nwhere links(B, C) = ∑ i∈B,j∈C Aij . Thus, the ratio cut criterion attempts to find clusters of vertices such that the “cut” from clusters to remaining nodes in the graph (normalized by the size of the clusters), is minimized. The related normalized cut problem minimizes the following:\nmin V1,...,Vk k∑ c=1 links(Vc,V \\ Vc) deg(Vc) , (Normalized Cut)\nwhere deg(B) = links(B,V) (or equivalently, the sum of the degrees of the vertices in B). A surprising fact is that both the normalized cut and ratio cut objective functions can be expressed exactly as the weighted kernel k-means function for appropriate choice of weights and kernels [6]. In the case of ratio cut, we set all weights to one and form a kernel matrixK = σI+L, where L is the unweighted graph Laplacian; for normalized cut, we let the weights be the degrees of the vertices, and form the kernel matrix K = σD−1 + D−1/2AD−1/2, whereD is the diagonal degree matrix. In both cases, σ is chosen so that the resulting kernel matrix is positive semi-definite. With this choice of kernel matrix and weights, the weighted kernel k-means objective function is equivalent to either the ratio cut or normalized cut objective; as a result, one can apply the weighted kernel k-means algorithm directly on the kernel matrix K to minimize the graph cut objectives monotonically."
    }, {
      "heading" : "2.4 Dirichlet Process Mixture Models",
      "text" : "Finally, we briefly review DP mixture models [9]. We can equivalently write the standard Gaussian mixture as a generative model where one chooses a cluster with probability πc and then generates an observation from the Gaussian corresponding to the chosen cluster. The distribution over the cluster indicators follows a discrete distribution, so a Bayesian extension to the mixture model arises by first placing a symmetric Dirichlet prior on the mixing coefficients. If we further assume that the covariances of the Gaussians are fixed to I and that the means are drawn from some prior distribution G0, we obtain the following Bayesian model:\nµ1, ...,µk ∼ G0 π ∼ Dir(k,π0)\nz1, ...,zn ∼ Discrete(π) x1, ...,xn ∼ N (µzi , I).\nWe denote π = π1, ..., πk above. One way to view the DP mixture model is to take a limit of the above model as k →∞ when choosing π0 = (α/k)e. In this case, there are an infinite number of mixing coefficients πc, so the resulting likelihood has the form:\np(x) = ∞∑ c=1 πcN (x | µc, I).\nThe distribution of the mixing coefficients is drawn from a so-called stick-breaking process. One imagines a stick of length one, and the mixing coefficients are obtained by appropriately breaking off pieces of the\nstick. In particular, we define a set of random variables {βc}∞c=1, each chosen independently from a beta distribution with parameters 1 and α. Then the corresponding mixing coefficients are defined as\nπc = βc · c−1∏ i=1 (1− βi).\nThe resulting mixing coefficients drop off exponentially, and one can show that the expected number of clusters in a set of n data points will grow logarithmic in n. Furthermore, the cluster indicators z1, ..,zn can be shown to follow the Chinese restaurant process, which can be utilized to develop sampling algorithms for inferring clusters in the data (in particular, the Gibbs sampler described below).\nWe will describe one of the simplest algorithms for inference in a DP mixture based on Gibbs sampling. This was utilized by West et al. [22] and further discussed by Neal [16], Algorithm 2. The state of the underlying Markov chain consists of the set of all cluster indicators and the set of all cluster means. The algorithm proceeds by first looping repeatedly through each of the data points and performing Gibbs moves on the cluster indicators for each point. For i = 1, ..., n, we reassign xi to existing cluster c with probability\nn−i,c Z(n− 1 + α) N (xi | µc, I),\nwhere n−i,c is the number of data points (excluding xi) that are assigned to cluster c. With probability\nα\nZ(n− 1 + α)\n∫ N (xi | µ, I)dG0(µ)\nwe start a new cluster. Z is an appropriate normalizing constant. If we end up choosing to start a new cluster, we select its mean from the posterior distribution obtained from the prior G0 and the single sample xi. After resampling all clusters, we perform Gibbs moves on the means by sampling µc given all points currently assigned to cluster c, for all c.\nWe note that one typically writes the DP mixture model (adapted to our Gaussian mixture scenario) as follows:\nG ∼ DP(G0, α) φi ∼ G for all i xi ∼ N (φi, I) for all i.\nThus, each G is a draw from the Dirichlet process DP(G0, α), whose base measure G0 is a prior over means of the Gaussians. We can think of a draw from G as choosing one of the infinite means µc drawn from G0, with the property that the means are chosen with probability equal to the corresponding mixing weights. As a result, each φi is equal to µc for some c."
    }, {
      "heading" : "3 Hard Clustering via Dirichlet Processes",
      "text" : "In the following sections, we derive hard clustering algorithms based on DP mixture models. We will analyze properties of the resulting algorithms and show connections to existing hard clustering algorithms, particularly k-means."
    }, {
      "heading" : "3.1 The Infinite Limit of the Gibbs Sampler",
      "text" : "Let us first defineG0. It is a prior distribution over the means, which we will take to be a zero-mean Gaussian with ρI covariance, i.e., µ ∼ N (0, ρI). Given this prior, we can compute the following integral from the Gibbs sampler: ∫\nN (xi | µ, I)dG0(µ). (1)\nA straightforward calculation reveals that this is equal to\n1\n(2π)d/2(ρ+ )d/2 · exp\n( − 1\n2(ρ+ ) ‖xi‖2\n) .\nTherefore, the probability of starting a new cluster is equal to\nα\nCZ(n− 1 + α) · exp\n( − 1\n2(ρ+ ) ‖xi‖2\n) ,\nwith C = (2π)d/2(ρ + )d/2. We now would like to see what happens to these probabilities as → 0. However, in order to do this, we must additionally let α be a function of and ρ as well. In particular, we will write α = C · exp(− λ2 ) for some λ. Now, let γ̂(zic) correspond to the posterior probability of point i being assigned to cluster c, given all other cluster assignments, the observed data, and α. After cancelling all the n− 1 + α terms, we obtain the following probabilities to be used during Gibbs sampling:\nγ̂(zic) = n−i,c · exp(− 12 ‖xi − µc‖ 2) exp ( − λ2 − ‖xi‖2 2(ρ+ ) ) + ∑k j=1 n−i,j · exp(− 1 2 ‖xi − µj‖2)\nfor existing clusters and γ̂(zi,new) = exp ( − λ2 − ‖xi‖2 2(ρ+ ) ) exp ( − λ2 − ‖xi‖2 2(ρ+ ) ) + ∑k j=1 n−i,j · exp(− 1 2 ‖xi − µj‖2)\nfor generating a new cluster. The denominator in the above expressions is equal to Z, which normalizes the probabilities to sum to 1. Now we consider the asymptotic behavior of the above probabilities. The numerator for γ̂(zi,new) can be written as\nexp ( − 1\n2\n[ λ+\nρ+ ‖xi‖2\n]) .\nIt is straightforward to see that, as → 0 with a fixed ρ, the λ term dominates this numerator. Furthermore, all of the above probabilities will become binary; in particular, the values of γ̂(zi,c) and γ̂(zi,new) will be increasingly dominated by the largest value of {‖xi−µ1‖2, ..., ‖xi−µk‖2, λ}. In the limit, only the largest of these values will receive a non-zero γ̂ value. The resulting update, therefore, takes a simple form that is analogous to the k-means cluster reassignment step. We reassign a point to the cluster corresponding to the closest mean, unless the closest cluster has squared Euclidean distance greater than λ. In this case, we start a new cluster.\nIf we choose to start a new cluster, the final step is to sample a new mean from the posterior based on the prior G0 and the single observation xi. Similarly, once we have performed Gibbs moves on the cluster assignments, we must perform Gibbs moves on all the means, which amounts to sampling from the posterior based on G0 and all observations in a cluster. Since the prior and likelihood are Gaussian, the posterior will be Gaussian as well. If we let x̄c be the mean of the points currently assigned to cluster c and nc be the number of points assigned to cluster c, then the posterior is a Gaussian with mean µ̃c and covariance Σ̃c, where\nµ̃c =\n( 1 +\nρnc\n)−1 x̄c, Σ̃c =\nρ\n+ ρnc I.\nAs before, we consider the asymptotic behavior of the above Gaussian distribution as → 0. The mean of the Gaussian approaches x̄c and the covariance goes to 0, meaning that the mass of the distribution becomes concentrated at x̄c. Thus, in the limit we simply choose x̄c as the mean.\nPutting everything together, we obtain a hard clustering algorithm that behaves similarly to k-means with the exception that a new cluster is formed whenever a point is farther than λ away from every existing cluster centroid. We choose to initialize the algorithm with a single cluster whose mean is simply the global centroid; the resulting algorithm is specified as Algorithm 1, which we denote as the DP-k-means algorithm.\nAlgorithm 1 DP-k-means Input: x1, ...,xn: input data, λ : cluster penalty parameter Output: Clustering `1, ..., `k and number of clusters k\n1. Initialize k = 1, `1 = {x1, ...,xn} and µ1 the global mean. 2. Initialize cluster indicators zi = 1 for all i = 1, ..., n. 3. Repeat until convergence\n• For each point xi\n– Compute dic = ‖xi − µc‖2 for c = 1, ..., k – If minc dic > λ, set k = k + 1, zi = k, and µk = xi.\n– Otherwise, set zi = argmincdic.\n• Generate clusters `1, ..., `k based on z1, ..., zk:\n`j = {xi | zi = j}\n• For each cluster `j , compute µj = 1 |`j | ∑ x∈`j x"
    }, {
      "heading" : "3.2 Underlying Objective and the AIC",
      "text" : "With the procedure from the previous section in hand, we can now analyze its properties. A first question to ask is what the underlying clustering objective corresponding to this k-means-like algorithm is. In this section, we show that the algorithm monotonically decreases the following objective at each iteration, where an iteration is defined as a complete loop through all data points to update all cluster assignments and means:\nmin {`c}kc=1\n∑k c=1 ∑ x∈`c ‖x− µc‖ 2 + λk\nwhere µc = 1|`c| ∑ x∈`c x. (2)\nThis objective is simply the k-means objective function with an additional penalty based on the number of clusters. The threshold λ controls the tradeoff between the traditional k-means term and the cluster penalty term. We can prove the following:\nTheorem 3.1. Algorithm 1 monotonically decreases the objective given in (2) until local convergence.\nProof. Denote J t to be the objective after step t of the algorithm:\nJ t = k(t)∑ c=1 ∑ xi∈`(t)c ‖xi − µc‖2 + λk(t),\nwhere k(t) is the number of clusters after step t and `(t)c is cluster c at step t. Also denote z (t) i as the cluster assignment variables obtained during step t and µ(t)c to be the cluster mean for cluster c after step t.\nAnalogous to the convergence proof for standard k-means, we can prove the following inequalities:\nJ t ≥ k(t+1)∑ c=1 ∑ xi∈`(t)c ‖xi − µ(t) z (t) i ‖2 + λk(t+1)\n= k(t+1)∑ c=1 ∑ xi∈`(t+1)c ‖xi − µ(t)c ‖2 + λk(t+1) ≥ k(t+1)∑ c=1 ∑ xi∈`(t+1)c ‖xi − µ(t+1)c ‖2 + λk(t+1) = J t+1.\nThe first inequality follows from the reassignment step. Each point is reassigned to its closest mean, so the distances to the reassigned means will be less than or equal to the distances to the current means. For distances greater than λ, we can generate a new cluster and pay a penalty of λ for the cluster and still decrease the objective. The second line follows from the fact that we are simply rearranging the sums based on the updated clusters. The final inequality follows from the fact that, given a set of points, the mean is the best representative in terms of the squared Euclidean distance; thus, the recomputation of the cluster means will only lower the objective function.\nThe fact that the algorithm will converge follows from the fact that the objective function cannot increase, and that there are only a finite number of possible clusterings of the data.\nPerhaps unsurprisingly, this objective has been studied in the past in conjunction with the Akaike Information Criterion (AIC). For instance, Manning et al. [15] describe the above penalized k-means objective function with a motivation arising from the AIC. Interestingly, it does not appear that algorithms have been studied for this particular objective function, so our analysis seemingly provides the first constructive algorithm for monotonic local convergence as well as highlighting the connections to the DP mixture model.\nIn the case of k-means, one can show that the complete-data log likelihood approaches the k-means objective in the limit as → 0. We conjecture that a similar result holds for the DP mixture model, which would indicate that our result is not specific to the particular choice of the Gibbs sampler."
    }, {
      "heading" : "4 Clustering with Multiple Data Sets",
      "text" : "One of the most useful extensions to the standard DP mixture model arises when we introduce another DP layer on top of the base measure. In particular, we let the base measure G0 itself be distributed according to a Dirichlet process with base measure H . The result of this is that, given a collection of data sets, each of which is a DP mixture whose base measure is the common Dirichlet process with base measure H , we can cluster each data set while ensuring that the clusters across the data sets share some structure. We will not describe the sampling algorithm for the hierarchical Dirichlet process (HDP) in detail, but refer the reader to [20] for a detailed introduction to the HDP model and a description of inference techniques. We will see that the limiting process described earlier for the standard DP can be straightforwardly extended to the HDP; we will outline the algorithm below, and Figure 1 gives an overview of the approach."
    }, {
      "heading" : "4.1 Overview of the HDP",
      "text" : "To set the stage, let us assume that we have D data sets, 1, ..., j, ..., D. Denote xij to be data point i from data set j, and let there be nj data points from each data set j. The basic idea is that we will locally cluster the data points from each data set, but that some clusters will be shared across data sets. Each data set j has a set of local cluster indicators given by zij such that zij = c if data point i in data set j is assigned to local\ncluster Sjc. Each local cluster Sjc is associated to a global cluster mean µp. Local clusters across multiple data sets may be associated with a single global cluster mean.\nRecall the standard DP mixture model:\nG ∼ DP(G0, α) φi ∼ G for all i xi ∼ N (φi, I) for all i.\nFor the HDP, we have a set of data sets indexed by j, each of which is a DP mixture. However, instead of defining the base measure of each DP mixture using G0, the prior over the means, we instead let G0 itself be a Dirichlet process whose base measure is a prior over the means. This yields the following:\nG0 ∼ DP(H, γ) Gj ∼ DP(G0, α) for all j φij ∼ Gj for all i, j xij ∼ N (φij , I) for all i, j.\nAnalogous to the standard DP mixture, the φij chooses some global mean µc, now based on both the local and global Dirichlet processes Gj and G0, respectively. The prior over the means of the Gaussian is now specified by H ."
    }, {
      "heading" : "4.2 The Hard HDP",
      "text" : "We can now extend the asymptotic argument that we employed for the hard DP algorithm to the HDP. We will summarize the resulting algorithm; the derivation is analogous to the derivation for the single DP mixture case. As with the hard DP algorithm, we will have a threshold that determines when to introduce a new cluster. For the hard HDP, we will require two parameters (corresponding to α and γ in the above HDP model): let λ` be the “local” threshold parameter, and λg be the “global” threshold parameter. The algorithm works as follows: for each data point xij , we compute the distance to every global cluster µp. For any global cluster p for which there is no current association in data set j, we add a penalty of λ` to the distance (intuitively, this penalty captures the fact that if we end up assigning xij to a global cluster that is not currently in use\nAlgorithm 2 Hard HDP Input: {xij}: input data, λ` : local cluster penalty parameter, λg: global cluster penalty parameter Output: Global clustering `1, ..., `g and number of clusters kj for all data sets j\n1. Initialize g = 1, kj = 1 for all j and µ1 to be the global mean across all data sets. 2. Initialize local cluster indicators zij = 1 for all i and j, and global cluster associations vj1 = 1 for all j. 3. Repeat until convergence\n• For each point xij :\n– Compute dijp = ‖xij − µp‖2 for p = 1, ..., g. – For all p such that vjc 6= p for all c = 1, ..., kj , set dijp = dijp + λ`. – If minp dijp > λ` + λg ,\n∗ Set kj = kj + 1, zij = kj , g = g + 1, µg = xij , and vjkj = g.\n– Else let p̂ = argminpdijp.\n∗ If vjc = p̂ for some c, set zij = c and vjc = p̂. ∗ Otherwise, set kj = kj + 1, zij = kj , and vjkj = p̂.\n• For all local clusters:\n– Let Sjc = {xij |zij = c}. – Compute d̄jcp = ∑ x∈Sjc ‖x− µp‖ 2 for p = 1, ..., g.\n– If minp d̄jcp > λg , set g = g + 1, vjc = g, and\nµg = 1 |Sjp| ∑\nx∈Sjp\nx.\n– Else set vjc = argminpd̄icp.\n• For each global cluster p = 1, .., g, re-compute means:\n– Let `p = {xij |zij = c and vjc = p}. – Compute\nµp = 1 |`p| ∑ x∈`p x.\nby data set j, we will incur a penalty of λ` to create a new local cluster, which we only want to do if the cluster if sufficiently close to xij). We reassign each data point xij to its nearest cluster, unless the closest distance is greater than λ`+λg , in which case we start a new global cluster (in this case we are starting a new local cluster and a new global cluster, hence the sum of the two penalties). Then, for each local cluster, we consider whether to reassign it to a different global mean: for each local cluster Sjc, we compute the sum of distances of the points to every µp. We reassign the association of Sjc to the corresponding closest µp; if the closest is farther than λg , then we start a new global cluster whose mean is the mean of the points assigned to Sjc. Finally, we recompute all means µp by computing the mean of all points (over all data sets) associated to each µp. See Algorithm 2 for the full specification of the procedure; the algorithm is derived directly as an asymptotic hard clustering algorithm over the Gibbs sampler for the HDP.\nAs with the DP-k-means algorithm, we can determine the underlying objective function, and use it to determine convergence. Let k = ∑D j=1 kj be the total number of local clusters, and g be the total number of\nglobal clusters. Then we can show that the objective optimized is the following:\nmin {`p}gp=1\n∑g p=1 ∑ xij∈`p ‖xij − µp‖ 2 2 + λ`k + λgg,\nwhere µp = 1|`p| ∑ xij∈`p xij (3)\nThis objective is pleasantly simple and intuitive: we minimize the global k-means objective function, but we incorporate a penalty whenever either a new local cluster or a new global cluster is created. With appropriately chosen λ` and λg , the result is that we obtain sharing of cluster structure across data sets. We can prove that the hard HDP clustering algorithm monotonically minimizes this objective.\nTheorem 4.1. Algorithm 2 monotonically minimizes the objective (3) until local convergence.\nThe proof follows along similar lines to the proof of the basic DP-k-means algorithm. A possible next step would be to consider a spectral relaxation of the above objective, but the introduction of the local cluster penalties makes it difficult to apply standard spectral relaxations. We leave as an open question the potentially fruitful direction of finding spectral or semidefinite relaxations of the above objective."
    }, {
      "heading" : "5 Further Extensions",
      "text" : "We now discuss two additional extensions of the proposed objective: a spectral relaxation for the proposed hard clustering method and a normalized cut algorithm that takes time O(|E|) per iteration but does not fix the number of clusters in the graph."
    }, {
      "heading" : "5.1 Spectral Meets Nonparametric",
      "text" : "In this section we will show that the DP-k-means objective developed earlier admits a natural spectral relaxation, which suggests another potential optimization algorithm. Recall that spectral clustering algorithms for k-means are based on the observation that the k-means objective can be relaxed to a problem where the globally optimal solution may be computed via eigenvectors. In particular, for the k-means objective, one computes the eigenvectors corresponding to the k largest eigenvalues of the kernel matrix over the data; these eigenvectors form the globally optimal “relaxed” cluster indicator matrix [24].\nIn a similar manner, in this section we will show the following extension: the globally optimal solution to the DP-k-means objective function is obtained by computing the eigenvectors of the kernel matrix corresponding to all eigenvalues greater than λ, and stacking these into a matrix. This suggests that the optimal number of clusters for a given λ is related to the number of eigenvalues larger than λ. To prove the correctness of this relaxation, let us denote Z as the n× k cluster indicator matrix whose rows correspond to the cluster indicator variables zic. Let Y = Z(ZTZ)−1/2 be a normalized indicator matrix, and notice that Y TY = I . We can prove the following lemma.\nLemma 5.1. The DP-k-means objective function can equivalently be written as:\nmax Y\ntr(Y T (K − λI)Y ),\nwhere the optimization is performed over the space of all normalized indicator matrices Y .\nProof. The proof follows by expansion of the objective function:\nk∑ c=1 ∑ x∈`c ‖x− µc‖22 + λk\n= k∑ c=1 ∑ x∈`c ( xTx− 2 |`c| ∑ xi∈`c xTxi + 1 |`c|2 ∑ xi,xj∈`c xTi xj ) + λk.\nThe main sum can be broken up into three pieces. The first simplifies to tr(K), where K is the kernel matrix, and is a constant. The second and third terms are nearly identical: we write the second term with a change of notation (x to xi):\n−2 k∑ c=1 ∑ xi,xj∈`c xTi xj |`c| .\nIn the third term, the sum over x simplifies to |`c|, leading to\nk∑ c=1 ∑ xi,xj∈`c xTi xj |`c| ,\nand the terms can be combined so that the objective is now to maximize\nk∑ c=1 ∑ xi,xj∈`c xTi xj |`c| − λk.\nIf zc is an indicator vector for cluster c, then we can equivalently write the expression as:\nk∑ c=1 zTc Kzc zTc zc − λk.\nFinally, introducing yc = zc/‖zc‖2, we obtain\nk∑ c=1 yTc Kyc − λk,\nor equivalently tr(Y TKY )− λtr(Y TY ) = tr(Y T (K − λI)Y ).\nNow we perform a standard spectral relaxation: we relax the optimization to be over all orthonormal matrices Y :\nmax {Y | Y TY=I}\ntr(Y T (K − λI)Y ). (4)\nThe standard relaxation for k-means states that, for a fixed number of clusters k, the optimal relaxed objective is given by the sum of the k largest eigenvalues, and the optimal Y is the matrix of corresponding eigenvectors. In our case, the number of clusters is not fixed, which complicates the argument slightly. However, it is straightforward to see that the optimal relaxed solution will still contain a matrix of top eigenvectors (suppose not—if the optimal matrix has k columns, then the top-k eigenvector matrix will not have a smaller trace value). It suffices to consider matrices Y of top-k eigenvectors, for increasing k, and choose the one with the largest value of tr(Y T (K − λI)Y ). Once the eigenvalues of K − λI become negative, then the trace objective will start to decrease. Therefore, we only want to take eigenvectors corresponding to non-negative eigenvalues ofK−λI or, equivalently, eigenvalues ofK that are larger than λ. We have proven the following:\nTheorem 5.2. By relaxing the cluster indicator matrix Y to be any orthonormal matrix, the optimal Y in the relaxed clustering objective (4) is obtained by forming a matrix of all eigenvectors ofK whose corresponding eigenvalues are greater than λ.\nFrom this, one can design a simple spectral algorithm that computes the relaxed cluster indicator matrix Y , and then clusters the rows of Y , as is common for spectral clustering methods. Thus, the main difference between a standard spectral relaxation for k-means and the DP-k-means is that, for the former, we take the top-k eigenvectors, while for the latter, we take all eigenvectors corresponding to eigenvalues greater than λ."
    }, {
      "heading" : "5.2 Graph Clustering",
      "text" : "It is also possible to develop extensions to the DP-k-means algorithm for graph cut problems such as normalized and ratio cut. In this section, we will illustrate a practical application of our techniques by detailing the resulting normalized cut algorithm.\nWe state a result proven in [6] for standard normalized cut.\nTheorem 5.3. Let J(K,W ) be the weighted kernel k-means objective with kernel matrix K and (diagonal) weight matrix W , and let NC(A) be the normalized cut objective with adjacency matrix A. Let D be the diagonal degree matrix corresponding to A (D = diag(Ae)). Then the following relationship holds:\nJ(K,W ) = σn+ tr(D−1/2AD−1/2)− (σ + 1)k +NC(A),\nwhen we define K = σD−1 +D−1AD−1, W = D, and σ is large enough that K is positive semi-definite.\nIn the standard normalized cut formulation, the first three terms are constants, so minimizing the weighted kernel k-means objective is equivalent to minimizing the normalized cut for the particular choice of kernel and weights given above. When we consider the DP extensions to both weighted kernel k-means and normalized cut, however, the third term −(σ + 1)k is no longer a constant. Let the DP-k-means objective be given by J(K,W ) + λk, and the analogous penalized normalized cut objective be given by NC(A) + λ′k. Letting σn+ tr(D−1/2AD−1/2) = C, a constant, we have:\nJ(K,W ) + λk = C +NC(A)− (σ + 1)k + λk = C +NC(A) + λ′k,\nwhere λ′ = λ−σ−1. Thus, optimizing the hard DP weighted kernel k-means objective with model parameter λ is equivalent to optimizing the penalized normalized cut objective with model parameter λ′ = λ − σ − 1, and with the construction of K and W as in the above theorem.\nNow it remains to show that a full iteration of weighted kernel k-means with the above kernel may be performed in time O(|E|). We write the weighted distance wi‖φ(xi) − µc‖22 between a point and cluster centroid c in terms of the kernel matrix as\nwiKii − 2 ∑\nxj∈`c wiwjKij∑ xj∈`c wj\n+ wi ∑ xj ,xm∈`c wjwmKjm\n( ∑\nxj∈`c wj) 2\n.\nNow, since K = σD−1 + D−1AD−1, we note that, for i 6= j, we have Kij = Aij/(DiiDjj). Further, the weights wi are equal to Dii. In the σ = 0 case, the distance simplifies to\nAii deg(vi) − 2links({vi},Vc) deg(Vc) + deg(vi)links(Vc,Vc) deg(Vc)2 .\nWhen σ 6= 0, then the third term contributes an additional (deg(vi)σ)/deg(Vc), the second term contributes an additional −2σdeg(vi)/deg(Vc) if vertex i is currently assigned to cluster c, and the first term contributes σ. Putting it all together, we have that the distance between a vertex i and cluster c is\nσ + Aii deg(vi) − 2links({vi},Vc) deg(Vc) + deg(vi)links(Vc,Vc) deg(Vc)2 + σdeg(vi) deg(Vc) − 2σdeg(vi) deg(Vc) I(vi ∈ `c), (5)\nwhere I is an indicator function returning 1 when vertex i is in cluster c. We must compute this distance between all vertices and all clusters. Note that, at the beginning of every iteration, we can pre-compute links(Vc,Vc) and deg(Vc) for all c, and this may be performed in O(|E|) total time; computing these quantities is analogous to computing the means of the clusters in the vector case. Then, for a given vertex vi, we must compute links({vi},Vc) for all c. Notice that these quantities can be computed by simply passing\nthrough all neighbors of vertex i and updating the link quantities appropriately. Therefore, to compute such quantities over all vertices takes O(|E|) total time.\nPutting everything together, we apply Algorithm 1 where the distances dic are computed using (5), and for a given model threshold λ′ for the penalized normalized cut problem, we utilize λ′+σ+1 in place of λ in Algorithm 1. We do not explicitly compute cluster means but rather work with them implicitly, as is standard for kernel k-means. Note that the additive σ from (5) cancels with the σ from the λ′ + σ + 1 term."
    }, {
      "heading" : "6 A Preliminary Experiment",
      "text" : "We provide some preliminary results over a simple synthetic data set to explore some of the properties of our basic algorithm. We generated a data set of 100 points from three Gaussian distributions, as shown in the leftmost plot in Figure 2. The remainder of that figure shows different clusterings, obtained via different choices of λ. In Figure 3, we plot the average number of clusters obtained for various choices of λ; for this plot, 100 runs of the algorithm (with different permutations of the data indices) are performed for each choice of λ. Interestingly, the algorithm shows noticable robustness in terms of finding the “true” number of clusters in this simple data set: for λ between .1575 and .22, the algorithm virtually always returns 3 clusters."
    }, {
      "heading" : "7 Conclusions and Open Problems",
      "text" : "This paper outlines connections arising between DP mixture models and hard clustering algorithms, and develops new algorithms for hard clustering in the case of an unknown number of clusters. Our analysis is only a first step, and we believe that there are several avenues of future work:\n• Can we generalize the asymptotic analysis beyond the use of the Gibbs sampler? One can derive the k-means objective from the complete-data log likelihood of the Gaussian mixture model; we suspect that an analogous result holds for the DP mixture.\n• In the case of standard k-means (as well as normalized cut, etc), there has been significant work in improving the basic global algorithm. For instance, [5] looked at incorporating local moves, which finds points to move from one cluster to another such that the k-means objective is lowered. One can imagine applying similar ideas here, but perhaps another class of moves—merging and splitting clusters—could also be considered. Such moves have been considered for DP mixtures as well; while it would be interesting if one could apply limiting arguments to these moves in order to apply them to the hard clustering case, it does not appear that such limiting yields interesting algorithms.\n• We examined one particular spectral relaxation for the DP-k-means objective. What other relaxations are possible? Can we derive scalable semidefinite relaxations? What relaxations are possible for the hard HDP?\n• One initial motivation for studying Bayesian nonparametric extensions for hard clustering was to develop a scalable graph clustering algorithm that could be applied on large real-world networks, particularly those with power-law structure. An example is in image segmentation, where natural scene statistics yield clusters whose sizes follow power laws. Pitman-Yor processes [10, 17] are a definite candidate for developing the appropriate extensions, but further work is needed to determine if they can be applied asymptotically to the hard clustering case.\n• The work of [1] considers a more general connection between mixture models and k-means-type algorithms. In particular, the authors show that one can generalize the k-means algorithm to utilize any Bregman divergence. Furthermore, there is a bijection between exponential family distributions and Bregman divergences, so a limiting argument on an exponential family mixture model yields a particular Bregman clustering algorithm. Does a similar result hold in the case of the DP mixtures?\nFinally, while this paper is focused on an asymptotic analysis for the DP mixture, a significant amount of empirical work is still required to determine the practical impact of these algorithms. We are currently testing the derived algorithms on a number of real-world problems. A practical consideration concerns the selection of λ: whereas the choice of k in k-means provides the user with exact knowledge of the number of outputted clusters, it is difficult to ascertain roughly how many clusters corresponds to a particular choice of λ. Deriving heuristics for this choice could have a significant practical impact on results, and is a further issue under consideration."
    } ],
    "references" : [ {
      "title" : "Clustering with Bregman divergences",
      "author" : [ "A. Banerjee", "S. Merugu", "I.S. Dhillon", "J. Ghosh" ],
      "venue" : "Journal of Machine Learning Research, 6:1705–1749",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "MDL principle for robust vector quantisation",
      "author" : [ "H. Bischof", "A. Leonardis", "A. Selb" ],
      "venue" : "Pattern Analysis and Applications, 2(1):59–72",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Variational inference for Dirichlet process mixtures",
      "author" : [ "D. Blei", "M. Jordan" ],
      "venue" : "Journal of Bayesian Analysis, 1(1):121–144",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Spectral k-way ratio cut partitioning",
      "author" : [ "P. Chan", "M. Schlag", "J. Zien" ],
      "venue" : "IEEE Trans. CAD-Integrated Circuits and Systems, 13:1088–1096",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Iterative clustering of high dimensional text data augmented by local search",
      "author" : [ "I.S. Dhillon", "Y. Guan", "J. Kogan" ],
      "venue" : "Proceedings of the 2nd IEEE International Conference on Data Mining, pages 131–138",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Weighted graph cuts without eigenvectors: A multilevel approach",
      "author" : [ "I.S. Dhillon", "Y. Guan", "B. Kulis" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence, 29(11):1944–1957",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "How many clusters? Which clustering method? Answers via model-based cluster analysis",
      "author" : [ "C. Fraley", "A.E. Raftery" ],
      "venue" : "Computer Journal, 41(8):578–588",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Learning the k in k-means",
      "author" : [ "G. Hamerly", "C. Elkan" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Bayesian Nonparametrics: Principles and Practice",
      "author" : [ "N. Hjort", "C. Holmes", "P. Mueller", "S. Walker" ],
      "venue" : "Cambridge University Press, Cambridge, UK",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Gibbs sampling methods for stick-breaking priors",
      "author" : [ "H. Ishwaran", "L.F. James" ],
      "venue" : "Journal of the American Statistical Association, 96(453):161–173",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "A split-merge Markov chain Monte Carlo procedure for the Dirichlet process mixture model",
      "author" : [ "S. Jain", "R.M. Neal" ],
      "venue" : "Journal of Computational and Graphical Statistics, 13(1):158–182",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "The application of cluster analysis in strategic management research: an analysis and critique",
      "author" : [ "D.J. Ketchen", "C . L. Shook" ],
      "venue" : "Strategic Management Journal,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1996
    }, {
      "title" : "A permutation-augmented sampler for DP mixture models",
      "author" : [ "P. Liang", "M. Jordan", "B. Taskar" ],
      "venue" : "International Conference on Machine Learning",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Selecting variables for k-means cluster analysis by using a genetic algorithm that optimizes the silhouettes",
      "author" : [ "R. Lleti", "M.C. Ortiz", "L.A. Sarabia", "M.S. Sanchez" ],
      "venue" : "Analytica Chimica Acta, 515:87–100",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Introduction to Information Retrieval",
      "author" : [ "C.D. Manning", "P. Raghavan", "H. Schütze" ],
      "venue" : "Cambridge University Press",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Markov chain sampling methods for Dirichlet process mixture models",
      "author" : [ "R.M. Neal" ],
      "venue" : "Journal of Computational and Graphical Statistics, 9:249–265",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Combinatorial Stochastic Processes",
      "author" : [ "J. Pitman" ],
      "venue" : "Springer-Verlag",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Normalized cuts and image segmentation",
      "author" : [ "J. Shi", "J. Malik" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence, 22(8):888–905",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Finding the number of clusters in a data set: An information theoretic approach",
      "author" : [ "C.A. Sugar", "G.M. James" ],
      "venue" : "Journal of the American Statistical Association, 98:750–763",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Hierarchical Dirichlet processes",
      "author" : [ "Y.W. Teh", "M.I. Jordan", "M.J. Beal", "D.M. Blei" ],
      "venue" : "Journal of the American Statistical Association, 101(476):1566–1581",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Estimating the number of clusters in a data set via the gap statistic",
      "author" : [ "R. Tibshirani", "G. Walther", "T. Hastie" ],
      "venue" : "Journal of the Royal Statistical Society Series B, 63:411–423",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Hierarchical priors and mixture models",
      "author" : [ "M. West", "P. Müller", "M.D. Escobar" ],
      "venue" : "with application in regression and density estimation. In P. R. Freeman and A. F. M. Smith, editors, Aspects of Uncertainty, pages 363–386. John Wiley",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Multiclass spectral clustering",
      "author" : [ "S.X. Yu", "J. Shi" ],
      "venue" : "Proceedings of the International Conference on Computer Vision",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Spectral relaxation for k-means clustering",
      "author" : [ "H. Zha", "X. He", "C. Ding", "H. Simon", "M. Gu" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2001
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "Some work has gone into the development of techniques to overcome this problem in the context of k-means, such as [2, 7, 8, 12, 14, 19, 21], which utilize various model selection criteria.",
      "startOffset" : 114,
      "endOffset" : 139
    }, {
      "referenceID" : 6,
      "context" : "Some work has gone into the development of techniques to overcome this problem in the context of k-means, such as [2, 7, 8, 12, 14, 19, 21], which utilize various model selection criteria.",
      "startOffset" : 114,
      "endOffset" : 139
    }, {
      "referenceID" : 7,
      "context" : "Some work has gone into the development of techniques to overcome this problem in the context of k-means, such as [2, 7, 8, 12, 14, 19, 21], which utilize various model selection criteria.",
      "startOffset" : 114,
      "endOffset" : 139
    }, {
      "referenceID" : 11,
      "context" : "Some work has gone into the development of techniques to overcome this problem in the context of k-means, such as [2, 7, 8, 12, 14, 19, 21], which utilize various model selection criteria.",
      "startOffset" : 114,
      "endOffset" : 139
    }, {
      "referenceID" : 13,
      "context" : "Some work has gone into the development of techniques to overcome this problem in the context of k-means, such as [2, 7, 8, 12, 14, 19, 21], which utilize various model selection criteria.",
      "startOffset" : 114,
      "endOffset" : 139
    }, {
      "referenceID" : 18,
      "context" : "Some work has gone into the development of techniques to overcome this problem in the context of k-means, such as [2, 7, 8, 12, 14, 19, 21], which utilize various model selection criteria.",
      "startOffset" : 114,
      "endOffset" : 139
    }, {
      "referenceID" : 20,
      "context" : "Some work has gone into the development of techniques to overcome this problem in the context of k-means, such as [2, 7, 8, 12, 14, 19, 21], which utilize various model selection criteria.",
      "startOffset" : 114,
      "endOffset" : 139
    }, {
      "referenceID" : 15,
      "context" : "Inference algorithms for DP mixture models are more complex than standard EM for finite mixture models, but several effective posterior inference techniques have been developed, including Gibbs sampling algorithms [16, 22], MetropolisHastings algorithms [16], split-merge proposals [11], variational inference [3], and permutation-augmented samplers [13].",
      "startOffset" : 214,
      "endOffset" : 222
    }, {
      "referenceID" : 21,
      "context" : "Inference algorithms for DP mixture models are more complex than standard EM for finite mixture models, but several effective posterior inference techniques have been developed, including Gibbs sampling algorithms [16, 22], MetropolisHastings algorithms [16], split-merge proposals [11], variational inference [3], and permutation-augmented samplers [13].",
      "startOffset" : 214,
      "endOffset" : 222
    }, {
      "referenceID" : 15,
      "context" : "Inference algorithms for DP mixture models are more complex than standard EM for finite mixture models, but several effective posterior inference techniques have been developed, including Gibbs sampling algorithms [16, 22], MetropolisHastings algorithms [16], split-merge proposals [11], variational inference [3], and permutation-augmented samplers [13].",
      "startOffset" : 254,
      "endOffset" : 258
    }, {
      "referenceID" : 10,
      "context" : "Inference algorithms for DP mixture models are more complex than standard EM for finite mixture models, but several effective posterior inference techniques have been developed, including Gibbs sampling algorithms [16, 22], MetropolisHastings algorithms [16], split-merge proposals [11], variational inference [3], and permutation-augmented samplers [13].",
      "startOffset" : 282,
      "endOffset" : 286
    }, {
      "referenceID" : 2,
      "context" : "Inference algorithms for DP mixture models are more complex than standard EM for finite mixture models, but several effective posterior inference techniques have been developed, including Gibbs sampling algorithms [16, 22], MetropolisHastings algorithms [16], split-merge proposals [11], variational inference [3], and permutation-augmented samplers [13].",
      "startOffset" : 310,
      "endOffset" : 313
    }, {
      "referenceID" : 12,
      "context" : "Inference algorithms for DP mixture models are more complex than standard EM for finite mixture models, but several effective posterior inference techniques have been developed, including Gibbs sampling algorithms [16, 22], MetropolisHastings algorithms [16], split-merge proposals [11], variational inference [3], and permutation-augmented samplers [13].",
      "startOffset" : 350,
      "endOffset" : 354
    }, {
      "referenceID" : 14,
      "context" : "While this objective appears in the k-means literature, as an extension motivated by the Akaike Information Criterion (AIC) [15], it does not appear that any algorithms have been developed for optimization of the objective, nor are there any existing connections made to Bayesian nonparametrics.",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 19,
      "context" : "We can take a step further and extend our analysis to the hierarchical Dirichlet process (HDP), a model for clustering multiple related data sets [20].",
      "startOffset" : 146,
      "endOffset" : 150
    }, {
      "referenceID" : 5,
      "context" : "This connection leads to kernel k-means algorithms for monotonically decreasing the cut values, and which scale efficiently to large graphs; the work in [6] developed a technique for graph clustering based on this connection that outperforms spectral clustering approaches to graph clustering, and scales to significantly larger graphs.",
      "startOffset" : 153,
      "endOffset" : 156
    }, {
      "referenceID" : 3,
      "context" : "A third, less obvious extension to the k-means objective function arises in the context of graph clustering [4, 18, 23].",
      "startOffset" : 108,
      "endOffset" : 119
    }, {
      "referenceID" : 17,
      "context" : "A third, less obvious extension to the k-means objective function arises in the context of graph clustering [4, 18, 23].",
      "startOffset" : 108,
      "endOffset" : 119
    }, {
      "referenceID" : 22,
      "context" : "A third, less obvious extension to the k-means objective function arises in the context of graph clustering [4, 18, 23].",
      "startOffset" : 108,
      "endOffset" : 119
    }, {
      "referenceID" : 5,
      "context" : "A surprising fact is that both the normalized cut and ratio cut objective functions can be expressed exactly as the weighted kernel k-means function for appropriate choice of weights and kernels [6].",
      "startOffset" : 195,
      "endOffset" : 198
    }, {
      "referenceID" : 8,
      "context" : "4 Dirichlet Process Mixture Models Finally, we briefly review DP mixture models [9].",
      "startOffset" : 80,
      "endOffset" : 83
    }, {
      "referenceID" : 21,
      "context" : "[22] and further discussed by Neal [16], Algorithm 2.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "[22] and further discussed by Neal [16], Algorithm 2.",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 14,
      "context" : "[15] describe the above penalized k-means objective function with a motivation arising from the AIC.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 19,
      "context" : "We will not describe the sampling algorithm for the hierarchical Dirichlet process (HDP) in detail, but refer the reader to [20] for a detailed introduction to the HDP model and a description of inference techniques.",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 23,
      "context" : "In particular, for the k-means objective, one computes the eigenvectors corresponding to the k largest eigenvalues of the kernel matrix over the data; these eigenvectors form the globally optimal “relaxed” cluster indicator matrix [24].",
      "startOffset" : 231,
      "endOffset" : 235
    }, {
      "referenceID" : 5,
      "context" : "We state a result proven in [6] for standard normalized cut.",
      "startOffset" : 28,
      "endOffset" : 31
    }, {
      "referenceID" : 4,
      "context" : "For instance, [5] looked at incorporating local moves, which finds points to move from one cluster to another such that the k-means objective is lowered.",
      "startOffset" : 14,
      "endOffset" : 17
    }, {
      "referenceID" : 9,
      "context" : "Pitman-Yor processes [10, 17] are a definite candidate for developing the appropriate extensions, but further work is needed to determine if they can be applied asymptotically to the hard clustering case.",
      "startOffset" : 21,
      "endOffset" : 29
    }, {
      "referenceID" : 16,
      "context" : "Pitman-Yor processes [10, 17] are a definite candidate for developing the appropriate extensions, but further work is needed to determine if they can be applied asymptotically to the hard clustering case.",
      "startOffset" : 21,
      "endOffset" : 29
    }, {
      "referenceID" : 0,
      "context" : "• The work of [1] considers a more general connection between mixture models and k-means-type algorithms.",
      "startOffset" : 14,
      "endOffset" : 17
    } ],
    "year" : 2017,
    "abstractText" : "One of the many benefits of Bayesian nonparametric processes such as the Dirichlet process is that they can be used for modeling infinite mixture models, thus providing a flexible answer to the question of how many clusters exist in a data set. For the most part, such flexibility is currently lacking in techniques based on hard clustering, such as k-means, graph cuts, and Bregman hard clustering. For finite mixture models, there is a precise connection between k-means and mixtures of Gaussians, obtained by an appropriate limiting argument. In this paper, we apply a similar technique to an infinite mixture arising from the Dirichlet process (DP). We show that a Gibbs sampling algorithm for DP mixtures approaches a hard clustering algorithm in the limit, and further that the resulting algorithm monotonically minimizes an elegant underlying k-means-like objective that includes a penalty term based on the number of clusters. We generalize our analysis to the case of clustering multiple related data sets through a similar asymptotic argument with the hierarchical Dirichlet process. We discuss additional extensions that further highlight the benefits of our analysis: i) a spectral relaxation involving thresholded eigenvectors, and ii) a normalized cut graph clustering algorithm that requires O(|E|) time per iteration and automatically determines the number of clusters in a graph.",
    "creator" : "LaTeX with hyperref package"
  }
}