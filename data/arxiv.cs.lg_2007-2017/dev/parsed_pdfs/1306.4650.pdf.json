{
  "name" : "1306.4650.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Stochastic Majorization-Minimization Algorithms for Large-Scale Optimization",
    "authors" : [ "Julien Mairal" ],
    "emails" : [ "julien.mairal@inria.fr" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n30 6.\n46 50\nv2 [\nst at\n.M L\n] 1\n0 Se\n√ n) after n iterations, and of O(1/n) for strongly convex functions.\nEqually important, our scheme almost surely converges to stationary points for a large class of non-convex problems. We develop several efficient algorithms based on our framework. First, we propose a new stochastic proximal gradient method, which experimentally matches state-of-the-art solvers for large-scale ℓ1logistic regression. Second, we develop an online DC programming algorithm for non-convex sparse estimation. Finally, we demonstrate the effectiveness of our approach for solving large-scale structured matrix factorization problems."
    }, {
      "heading" : "1 Introduction",
      "text" : "Majorization-minimization [15] is a simple optimization principle for minimizing an objective function. It consists of iteratively minimizing a surrogate that upper-bounds the objective, thus monotonically driving the objective function value downhill. This idea is used in many existing procedures. For instance, the expectation-maximization (EM) algorithm (see [5, 21]) builds a surrogate for a likelihood model by using Jensen’s inequality. Other approaches can also be interpreted under the majorization-minimization point of view, such as DC programming [8], where “DC” stands for difference of convex functions, variational Bayes techniques [28], or proximal algorithms [1, 23, 29].\nIn this paper, we propose a stochastic majorization-minimization algorithm, which is is suitable for solving large-scale problems arising in machine learning and signal processing. More precisely, we address the minimization of an expected cost—that is, an objective function that can be represented by an expectation over a data distribution. For such objectives, online techniques based on stochastic approximations have proven to be particularly efficient, and have drawn a lot of attraction in machine learning, statistics, and optimization [3–6, 9–12, 14, 16, 17, 19, 22, 24–26, 30].\nOur scheme follows this line of research. It consists of iteratively building a surrogate of the expected cost when only a single data point is observed at each iteration; this data point is used to update the surrogate, which in turn is minimized to obtain a new estimate. Some previous works are closely related to this scheme: the online EM algorithm for latent data models [5, 21] and the online matrix factorization technique of [19] involve for instance surrogate functions updated in a similar fashion. Compared to these two approaches, our method is targeted to more general optimization problems.\nAnother related work is the incremental majorization-minimization algorithm of [18] for finite training sets; it was indeed shown to be efficient for solving machine learning problems where storing\ndense information about the past iterates can be afforded. Concretely, this incremental scheme requires to store O(pn) values, where p is the variable size, and n is the size of the training set.1 This issue was the main motivation for us for proposing a stochastic scheme with a memory load independent of n, thus allowing us to possibly deal with infinite data sets, or a huge variable size p.\nWe study the convergence properties of our algorithm when the surrogates are strongly convex and chosen among the class of first-order surrogate functions introduced in [18], which consist of approximating the possibly non-smooth objective up to a smooth error. When the objective is convex, we obtain expected convergence rates that are asymptotically optimal, or close to optimal [14, 22]. More precisely, the convergence rate is of order O(1/ √ n) in a finite horizon setting, and O(1/n) for a strongly convex objective in an infinite horizon setting. Our second analysis shows that for nonconvex problems, our method almost surely converges to a set of stationary points under suitable assumptions. We believe that this result is equally valuable as convergence rates for convex optimization. To the best of our knowledge, the literature on stochastic non-convex optimization is rather scarce, and we are only aware of convergence results in more restricted settings than ours—see for instance [3] for the stochastic gradient descent algorithm, [5] for online EM, [19] for online matrix factorization, or [9], which provides stronger guarantees, but for unconstrained smooth problems.\nWe develop several efficient algorithms based on our framework. The first one is a new stochastic proximal gradient method for composite or constrained optimization. This algorithm is related to a long series of work in the convex optimization literature [6,10,12,14,16,22,25,30], and we demonstrate that it performs as well as state-of-the-art solvers for large-scale ℓ1-logistic regression [7]. The second one is an online DC programming technique, which we demonstrate to be better than batch alternatives for large-scale non-convex sparse estimation [8]. Finally, we show that our scheme can address efficiently structured sparse matrix factorization problems in an online fashion, and offers new possibilities to [13, 19] such as the use of various loss or regularization functions.\nThis paper is organized as follows: Section 2 introduces first-order surrogate functions for batch optimization; Section 3 is devoted to our stochastic approach and its convergence analysis; Section 4 presents several applications and numerical experiments, and Section 5 concludes the paper."
    }, {
      "heading" : "2 Optimization with First-Order Surrogate Functions",
      "text" : "Throughout the paper, we are interested in the minimization of a continuous function f : Rp → R:\nmin θ∈Θ f(θ), (1)\nwhereΘ ⊆ Rp is a convex set. The majorization-minimization principle consists of computing a majorizing surrogate gn of f at iteration n and updating the current estimate by θn ∈ argminθ∈Θ gn(θ). The success of such a scheme depends on how well the surrogates approximate f . In this paper, we consider a particular class of surrogate functions introduced in [18] and defined as follows:\nDefinition 2.1 (Strongly Convex First-Order Surrogate Functions). Let κ be in Θ. We denote by SL,ρ(f, κ) the set of ρ-strongly convex functions g such that g ≥ f , g(κ) = f(κ), the approximation error g − f is differentiable, and the gradient ∇(g − f) is LLipschitz continuous. We call the functions g in SL,ρ(f, κ) “first-order surrogate functions”.\nAmong the first-order surrogate functions presented in [18], we should mention the following ones: • Lipschitz Gradient Surrogates. When f is differentiable and ∇f is L-Lipschitz, f admits the following surrogate g in S2L,L(f, κ):\ng : θ 7→ f(κ) +∇f(κ)⊤(θ − κ) + L 2 ‖θ − κ‖22.\nWhen f is convex, g is in SL,L(f, κ), and when f is µ-strongly convex, g is in SL−µ,L(f, κ). Minimizing g amounts to performing a classical classical gradient descent step θ ← κ− 1\nL ∇f(κ).\n• Proximal Gradient Surrogates. Assume that f splits into f = f1 + f2, where f1 is differentiable, ∇f1 is L-Lipschitz, and f2 is\n1To alleviate this issue, it is possible to cut the dataset into η mini-batches, reducing the memory load to O(pη), which remains cumbersome when p is very large.\nconvex. Then, the function g below is in S2L,L(f, κ):\ng : θ 7→ f1(κ) +∇f1(κ)⊤(θ − κ) + L\n2 ‖θ − κ‖22 + f2(θ).\nWhen f1 is convex, g is in SL,L(f, κ). If f1 is µ-strongly convex, g is in SL−µ,L(f, κ). Minimizing g amounts to a proximal gradient step [1, 23, 29]: θ ← argminθ 12‖κ− 1L∇f1(κ)− θ‖22 + 1Lf2(θ). • DC Programming Surrogates. Assume that f = f1 + f2, where f2 is concave and differentiable, ∇f2 is L2-Lipschitz, and g1 is in SL1,ρ1(f1, κ), Then, the following function g is a surrogate in SL1+L2,ρ1(f, κ): g : θ 7→ f1(θ) + f2(κ) +∇f2(κ)⊤(θ − κ). When f1 is convex, f1 + f2 is a difference of convex functions, leading to a DC program [8].\nWith the definition of first-order surrogates and a basic “batch” algorithm in hand, we now introduce our main contribution: a stochastic scheme for solving large-scale problems."
    }, {
      "heading" : "3 Stochastic Optimization",
      "text" : "As pointed out in [4], one is usually not interested in the minimization of an empirical cost on a finite training set, but instead in minimizing an expected cost. Thus, we assume from now on that f has the form of an expectation:\nmin θ∈Θ\n[ f(θ) , Ex[ℓ(x, θ)] ] , (2)\nwhere x from some set X represents a data point, which is drawn according to some unknown distribution, and ℓ is a continuous loss function. As often done in the literature [22], we assume that the expectations are well defined and finite valued; we also assume that f is bounded below.\nWe present our approach for tackling (2) in Algorithm 1. At each iteration, we draw a training point xn, assuming that these points are i.i.d. samples from the data distribution. Note that in practice, since it is often difficult to obtain true i.i.d. samples, the points xn are computed by cycling on a randomly permuted training set [4]. Then, we choose a surrogate gn for the function θ 7→ ℓ(xn, θ), and we use it to update a function ḡn that behaves as an approximate surrogate for the expected cost f . The function ḡn is in fact a weighted average of previously computed surrogates, and involves a sequence of weights (wn)n≥1 that will be discussed later. Then, we minimize ḡn, and obtain a new estimate θn. For convex problems, we also propose to use averaging schemes, denoted by “option 2” and “option 3” in Alg. 1. Averaging is a classical technique for improving convergence rates in convex optimization [10, 22] for reasons that are clear in the convergence proofs.\nAlgorithm 1 Stochastic Majorization-Minimization Scheme input θ0 ∈ Θ (initial estimate); N (number of iterations); (wn)n≥1, weights in (0, 1];\n1: initialize the approximate surrogate: ḡ0 : θ 7→ ρ2‖θ − θ0‖22; θ̄0 = θ0; θ̂0 = θ0; 2: for n = 1, . . . , N do 3: draw a training point xn; define fn : θ 7→ ℓ(xn, θ); 4: choose a surrogate function gn in SL,ρ(fn, θn−1); 5: update the approximate surrogate: ḡn = (1− wn)ḡn−1 + wngn; 6: update the current estimate:\nθn ∈ argmin θ∈Θ ḡn(θ);\n7: for option 2, update the averaged iterate: θ̂n , (1− wn+1)θ̂n−1 + wn+1θn; 8: for option 3, update the averaged iterate: θ̄n ,\n(1−wn+1)θ̄n−1+wn+1θn∑n+1 k=1 wk ;\n9: end for output (option 1): θN (current estimate, no averaging); output (option 2): θ̄N (first averaging scheme); output (option 3): θ̂N (second averaging scheme).\nWe remark that Algorithm 1 is only practical when the functions ḡn can be parameterized with a small number of variables, and when they can be easily minimized over Θ. Concrete examples are discussed in Section 4. Before that, we proceed with the convergence analysis."
    }, {
      "heading" : "3.1 Convergence Analysis - Convex Case",
      "text" : "First, We study the case of convex functions fn : θ 7→ ℓ(θ,xn), and make the following assumption:\n(A) for all θ in Θ, the functions fn are R-Lipschitz continuous. Note that for convex functions, this is equivalent to saying that subgradients of fn are uniformly bounded by R.\nAssumption (A) is classical in the stochastic optimization literature [22]. Our first result shows that with the averaging scheme corresponding to “option 2” in Alg. 1, we obtain an expected convergence rate that makes explicit the role of the weight sequence (wn)n≥1. Proposition 3.1 (Convergence Rate). When the functions fn are convex, under assumption (A), and when ρ = L, we have\nE[f(θ̄n−1)− f⋆] ≤ L‖θ⋆ − θ0‖22 + R\n2\nL ∑n k=1 w 2 k\n2 ∑n k=1 wk for all n ≥ 1, (3)\nwhere θ̄n−1 is defined in Algorithm 1, θ⋆ is a minimizer of f on Θ, and f⋆ , f(θ⋆).\nSuch a rate is similar to the one of stochastic gradient descent with averaging, see [22] for example. Note that the constraint ρ = L here is compatible with the proximal gradient surrogate. From Proposition 3.1, it is easy to obtain a O(1/ √ n) bound for a finite horizon—that is, when the total number of iterations n is known in advance. When n is fixed, such a bound can indeed be obtained by plugging constant weights wk = γ/ √ n for all k ≤ n in Eq. (3). Note that the upper-\nboundO(1/ √ n) cannot be improved in general without making further assumptions on the objective function [22]. The next corollary shows that in an infinite horizon setting and with decreasing weights, we lose a logarithmic factor compared to an optimal convergence rate [14,22] of O(1/ √ n).\nCorollary 3.1 (Convergence Rate - Infinite Horizon - Decreasing Weights). Let us make the same assumptions as in Proposition 3.1 and choose the weights wn = γ/ √ n. Then,\nE[f(θ̄n−1)− f⋆] ≤ L‖θ⋆ − θ0‖22\n2γ √ n\n+ R2γ(1 + log(n))\n2L √ n\n, ∀n ≥ 2.\nOur analysis suggests to use weights of the form O(1/ √ n). In practice, we have found that choosing wn = √ n0 + 1/ √ n0 + n performs well, where n0 is tuned on a subsample of the training set."
    }, {
      "heading" : "3.2 Convergence Analysis - Strongly Convex Case",
      "text" : "In this section, we introduce an additional assumption:\n(B) the functions fn are µ-strongly convex.\nWe show that our method achieves a rate O(1/n), which is optimal up to a multiplicative constant for strongly convex functions (see [14, 22]). Proposition 3.2 (Convergence Rate). Under assumptions (A) and (B), with ρ = L+ µ. Define β , µ\nρ and wn , 1+β 1+βn . Then,\nE[f(θ̂n−1)− f⋆] + ρ\n2 E[‖θ⋆ − θn‖22] ≤ max\n(\n2R2\nµ , ρ‖θ⋆ − θ0‖22\n)\n1\nβn+ 1 for all n ≥ 1,\nwhere θ̂n is defined in Algorithm 1, when choosing the averaging scheme called “option 3”.\nThe averaging scheme is slightly different than in the previous section and the weights decrease at a different speed. Again, this rate applies to the proximal gradient surrogates, which satisfy the constraint ρ = L+ µ. In the next section, we analyze our scheme in a non-convex setting."
    }, {
      "heading" : "3.3 Convergence Analysis - Non-Convex Case",
      "text" : "Convergence results for non-convex problems are by nature weak, and difficult to obtain for stochastic optimization [4, 9]. In such a context, proving convergence to a global (or local) minimum is out of reach, and classical analyses study instead asymptotic stationary point conditions, which involve directional derivatives (see [2, 18]). Concretely, we introduce the following assumptions:\n(C) Θ and the support X of the data are compact; (D) The functions fn are uniformly bounded by some constant M ; (E) The weights wn are non-increasing, w1 = 1, ∑ n≥1 wn=+∞, and ∑ n≥1 w 2 n √ n<+∞; (F) The directional derivatives ∇fn(θ, θ′ − θ), and ∇f(θ, θ′ − θ) exist for all θ and θ′ in Θ.\nAssumptions (C) and (D) combined with (A) are useful because they allow us to use some uniform convergence results from the theory of empirical processes [27]. In a nutshell, these assumptions ensure that the function class {x 7→ ℓ(x, θ) : θ ∈ Θ} is “simple enough”, such that a uniform law of large numbers applies. The assumption (E) is more technical: it resembles classical conditions used for proving the convergence of stochastic gradient descent algorithms, usually stating that the weights wn should be the summand of a diverging sum while the sum of w2n should be finite; the constraint ∑\nn≥1 w 2 n\n√ n < +∞ is slightly stronger. Finally, (F) is a mild assumption, which is\nuseful to characterize the stationary points of the problem. A classical necessary first-order condition [2] for θ to be a local minimum of f is indeed to have ∇f(θ, θ′−θ) non-negative for all θ′ in Θ. We call such points θ the stationary points of the function f . The next proposition is a generalization of a convergence result obtained in [19] in the context of sparse matrix factorization.\nProposition 3.3 (Non-Convex Analysis - Almost Sure Convergence). Under assumptions (A), (C), (D), (E), (f(θn))n≥0 converges with probability one. Under assumption (F), we also have that\nlim inf n→+∞ inf θ∈Θ ∇f̄n(θn, θ − θn) ‖θ − θn‖2 ≥ 0,\nwhere the function f̄n is a weighted empirical risk recursively defined as f̄n = (1−wn)f̄n−1+wnfn. It can be shown that f̄n uniformly converges to f .\nEven though f̄n converges uniformly to the expected cost f , Proposition 3.3 does not imply that the limit points of (θn)n≥1 are stationary points of f . We obtain such a guarantee when the surrogates that are parameterized, an assumption always satisfied when Algorithm 1 is used in practice.\nProposition 3.4 (Non-Convex Analysis - Parameterized Surrogates). Let us make the same assumptions as in Proposition 3.3, and let us assume that the functions ḡn are parameterized by some variables κn living in a compact set K of Rd. In other words, ḡn can be written as gκn , with κn in K. Suppose there exists a constant K > 0 such that |gκ(θ) − gκ′(θ)| ≤ K‖κ− κ′‖2 for all θ in Θ and κ, κ′ in K. Then, every limit point θ∞ of the sequence (θn)n≥1 is a stationary point of f—that is, for all θ in Θ,\n∇f(θ∞, θ − θ∞) ≥ 0.\nFinally, we show that our non-convex convergence analysis can be extended beyond first-order surrogate functions—that is, when gn does not satisfy exactly Definition 2.1. This is possible when the objective has a particular partially separable structure, as shown in the next proposition. This extension was motivated by the non-convex sparse estimation formulation of Section 4, where such a structure appears.\nProposition 3.5 (Non-Convex Analysis - Partially Separable Extension). Assume that the functions fn split into fn(θ) = f0,n(θ)+ ∑K\nk=1 fk,n(γk(θ)), where the functions γk :R\np→R are convex and R-Lipschitz, and the fk,n are non-decreasing for k ≥ 1. Consider gn,0 in SL0,ρ1(f0,n, θn−1), and some non-decreasing functions gk,n in SLk,0(fk,n, γk(θn−1)). Instead of choosing gn in SL,ρ(fn, θn−1) in Alg 1, replace it by gn,θ 7→g0,n(θ)+gk,n(γk(θ)). Then, Propositions 3.3 and 3.4 still hold."
    }, {
      "heading" : "4 Applications and Experimental Validation",
      "text" : "In this section, we introduce different applications, and provide numerical experiments. A C++/Matlab implementation is available in the software package SPAMS [19].2 All experiments were performed on a single core of a 2GHz Intel CPU with 64GB of RAM.\n2http://spams-devel.gforge.inria.fr/."
    }, {
      "heading" : "4.1 Stochastic Proximal Gradient Descent Algorithm",
      "text" : "Our first application is a stochastic proximal gradient descent method, which we call SMM (Stochastic Majorization-Minimization), for solving problems of the form:\nmin θ∈Θ Ex[ℓ(x, θ)] + ψ(θ), (4)\nwhere ψ is a convex deterministic regularization function, and the functions θ 7→ ℓ(x, θ) are differentiable and their gradients are L-Lipschitz continuous. We can thus use the proximal gradient surrogate presented in Section 2. Assume that a weight sequence (wn)n≥1 is chosen such that w1 = 1. By defining some other weights win recursively as w i n , (1−wn)wi−1n for i < n and wnn,wn, our scheme yields the update rule:\nθn ← argmin θ∈Θ\nn ∑\ni=1\nwin [ ∇fi(θi−1)⊤θ + L2 ‖θ − θi−1‖22 + ψ(θ) ] . (SMM)\nOur algorithm is related to FOBOS [6], to SMIDAS [25] or the truncated gradient method [16] (when ψ is the ℓ1-norm). These three algorithms use indeed the following update rule:\nθn ← argmin θ∈Θ ∇fn(θn−1)⊤θ + 12ηn ‖θ − θn−1‖ 2 2 + ψ(θ), (FOBOS)\nAnother related scheme is the regularized dual averaging (RDA) of [30], which can be written as\nθn ← argmin θ∈Θ\n1\nn\nn ∑\ni=1\n∇fi(θi−1)⊤θ + 12ηn ‖θ‖ 2 2 + ψ(θ). (RDA)\nCompared to these approaches, our scheme includes a weighted average of previously seen gradients, and a weighted average of the past iterates. Some links can also be drawn with approaches such as the “approximate follow the leader” algorithm of [10] and other works [12, 14].\nWe now evaluate the performance of our method for ℓ1-logistic regression. In summary, the datasets consist of pairs (yi,xi)Ni=1, where the yi’s are in {−1,+1}, and the xi’s are in Rp with unit ℓ2norm. The function ψ in (4) is the ℓ1-norm: ψ(θ) , λ‖θ‖1, and λ is a regularization parameter; the functions fi are logistic losses: fi(θ) , log(1 + e−yix ⊤\ni θ). One part of each dataset is devoted to training, and another part to testing. We used weights of the form wn , √\n(n0 + 1)/(n+ n0), where n0 is automatically adjusted at the beginning of each experiment by performing one pass on 5% of the training data. We implemented SMM in C++ and exploited the sparseness of the datasets, such that each update has a computational complexity of the order O(s), where s is the number of non zeros in ∇fn(θn−1); such an implementation is non trivial but proved to be very efficient. We consider three datasets described in the table below. rcv1 and webspam are obtained from the 2008 Pascal large-scale learning challenge.3 kdd2010 is available from the LIBSVM website.4\nname Ntr (train) Nte (test) p density (%) size (GB) rcv1 781 265 23 149 47 152 0.161 0.95 webspam 250 000 100 000 16 091 143 0.023 14.95 kdd2010 10 000 000 9 264 097 28 875 157 10−4 4.8\nWe compare our implementation with state-of-the-art publicly available solvers: the batch algorithm FISTA of [1] implemented in the C++ SPAMS toolbox and LIBLINEAR v1.93 [7]. LIBLINEAR is based on a working-set algorithm, and, to the best of our knowledge, is one of the most efficient available solver for ℓ1-logistic regression with sparse datasets. Because p is large, the incremental majorization-minimization method of [18] could not run for memory reasons. We run every method on 1, 2, 3, 4, 5, 10 and 25 epochs (passes over the training set), for three regularization regimes, respectively yielding a solution with approximately 100, 1 000 and 10 000 non-zero coefficients. We report results for the medium regularization in Figure 1 and provide the rest as supplemental material. FISTA is not represented in this figure since it required more than 25 epochs to achieve reasonable values. Our conclusion is that SMM often provides a reasonable solution after one epoch, and outperforms LIBLINEAR in the low-precision regime. For high-precision regimes, LIBLINEAR should be preferred. Such a conclusion is often obtained when comparing batch and stochastic algorithms [4], but matching the performance of LIBLINEAR is very challenging.\n3http://largescale.ml.tu-berlin.de. 4http://www.csie.ntu.edu.tw/˜cjlin/libsvm/."
    }, {
      "heading" : "4.2 Online DC Programming for Non-Convex Sparse Estimation",
      "text" : "We now consider the same experimental setting as in the previous section, but with a non-convex regularizer ψ : θ 7→ λ∑pj=1 log(|θ[j]| + ε), where θ[j] is the j-th entry in θ. A classical way for minimizing the regularized empirical cost 1\nN\n∑N\ni=1 fi(θ) +ψ(θ) is to resort to DC programming. It consists of solving a sequence of reweighted-ℓ1 problems [8]. A current estimate θn−1 is updated as a solution of minθ∈Θ 1N ∑N i=1 fi(θ) + λ ∑p j=1 ηj |θ[j]|, where ηj , 1/(|θn−1[j]|+ ε).\nIn contrast to this “batch” methodology, we can use our framework to address the problem online. At iteration n of Algorithm 1, we define the function gn according to Proposition 3.5:\ngn : θ 7→ fn(θn−1) +∇fn(θn−1)⊤(θ − θn−1) + L2 ‖θ − θn−1‖22 + λ ∑p j=1 |θ[j]| |θn−1[j]|+ε ,\nWe compare our online DC programming algorithm against the batch one, and report the results in Figure 2, with ε set to 0.01. We conclude that the batch reweighted-ℓ1 algorithm always converges after 2 or 3 weight updates, but suffers from local minima issues. The stochastic algorithm exhibits a slower convergence, but provides significantly better solutions. Whether or not there are good theoretical reasons for this fact remains to be investigated. Note that it would have been more rigorous to choose a bounded set Θ, which is required by Proposition 3.5. In practice, it turns not to be necessary for our method to work well; the iterates θn have indeed remained in a bounded set."
    }, {
      "heading" : "4.3 Online Structured Sparse Coding",
      "text" : "In this section, we show that we can bring new functionalities to existing matrix factorization techniques [13, 19]. We are given a large collection of signals (xi)Ni=1 in R m, and we want to find a\ndictionary D in Rm×K that can represent these signals in a sparse way. The quality of D is measured through the loss ℓ(x,D) , min\nα∈RK 1 2‖x−Dα‖22+λ1‖α‖1+ λ22 ‖α‖22, where the ℓ1-norm\ncan be replaced by any convex regularizer, and the squared loss by any convex smooth loss.\nThen, we are interested in minimizing the following expected cost:\nmin D∈Rm×K Ex [ℓ(x,D)] + ϕ(D),\nwhere ϕ is a regularizer for D. In the online learning approach of [19], the only way to regularize D is to use a constraint set, on which we need to be able to project efficiently; this is unfortunately not always possible. In the matrix factorization framework of [13], it is argued that some applications can benefit from a structured penalty ϕ, but the approach of [13] is not easily amenable to stochastic optimization. Our approach makes it possible by using the proximal gradient surrogate\ngn : D 7→ ℓ(xn,Dn−1) + Tr ( ∇Dℓ(xn,Dn−1)⊤(D−Dn−1) ) + L2 ‖D−Dn−1‖2F + ϕ(D). (5) It is indeed possible to show that D 7→ ℓ(xn,D) is differentiable, and its gradient is Lipschitz continuous with a constant L that can be explicitly computed [18, 19].\nWe now design a proof-of-concept experiment. We consider a set of N =400 000 whitened natural image patches xn of size m = 20 × 20 pixels. We visualize some elements from a dictionary D trained by SPAMS [19] on the left of Figure 3; the dictionary elements are almost sparse, but have some residual noise among the small coefficients. Following [13], we propose to use a regularization function ϕ encouraging neighbor pixels to be set to zero together, thus leading to a sparse structured dictionary. We consider the collection G of all groups of variables corresponding to squares of 4 neighbor pixels in {1, . . . ,m}. Then, we defineϕ(D) , γ1 ∑K j=1 ∑\ng∈G maxk∈g |dj [k]|+γ2‖D‖2F, where dj is the j-th column of D. The penalty ϕ is a structured sparsity-inducing penalty that encourages groups of variables g to be set to zero together [13]. Its proximal operator can be computed efficiently [20], and it is thus easy to use the surrogates (5). We set λ1 =0.15 and λ2 =0.01; after trying a few values for γ1 and γ2 at a reasonable computational cost, we obtain dictionaries with the desired regularization effect, as shown in Figure 3. Learning one dictionary of size K = 256 took a few minutes when performing one pass on the training data with mini-batches of size 100. This experiment demonstrates that our approach is more flexible and general than [13] and [19]. Note that it is possible to show that when γ2 is large enough, the iterates Dn necessarily remain in a bounded set, and thus our convergence analysis presented in Section 3.3 applies to this experiment."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this paper, we have introduced a stochastic majorization-minimization algorithm that gracefully scales to millions of training samples. We have shown that it has strong theoretical properties and some practical value in the context of machine learning. We have derived from our framework several new algorithms, which have shown to match or outperform the state of the art for solving large-scale convex problems, and to open up new possibilities for non-convex ones. In the future, we would like to study surrogate functions that can exploit the curvature of the objective function, which we believe is a crucial issue to deal with badly conditioned datasets."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work was supported by the Gargantua project (program Mastodons - CNRS)."
    }, {
      "heading" : "A Mathematical Background and Useful Results",
      "text" : "In this paper, we use subdifferential calculus for convex functions. The definition of subgradients and directional derivatives can be found in classical textbooks, see, e.g., [2], [37]. We denote by ∂f(θ) the subdifferential of a convex function f at a point θ. Other definitions can be found in the appendix of [18], which uses a similar notation as ours.\nIn this section, we present several classical optimization and probabilistic tools, which we use in our paper. The first lemma is a classical quadratic upper-bound for differentiable functions with a Lipschitz gradient. It can be found for instance in Lemma 1.2.3 of [35], or in the appendix of [18].\nLemma A.1 (Convex Surrogate for Functions with Lipschitz Gradient). Let f : Rp → R be differentiable and ∇f be L-Lipschitz continuous. Then, for all θ, θ′ in Rp,\n|f(θ′)− f(θ)−∇f(θ)⊤(θ′ − θ)| ≤ L 2 ‖θ − θ′‖22. (6)\nThe next lemma is a simple relation, which will allow us to identify the subdifferential of a convex function with the one of its surrogate at a particular point.\nLemma A.2 (Surrogate Functions and Subdifferential). Assume that f, g : Rp → R are convex, and that h , g − f is differentiable at θ in Rp with ∇h(θ) = 0. Then, ∂f(θ) = ∂g(θ).\nProof. It is easy to show that g and f have the same directional derivatives at θ since h is differentiable and ∇h(θ) = 0. This is sufficient to conclude that ∂g(θ) = ∂f(θ) by using Proposition 3.1.6 of [2], a simple lemma relating directional derivatives and subgradients.\nThe following lemma is a lower bound for strongly convex functions. It can be found for instance in [36].\nLemma A.3 (Lower Bound for Strongly Convex Functions). Let f : Rp → R be a µ-strongly convex function. Let z be in ∂f(κ) for some κ in Rp. Then, the following inequality holds for all θ in Rp:\nf(θ) ≥ f(κ) + z⊤(θ − κ) + µ 2 ‖θ − κ‖22.\nProof. The function l : θ 7→ f(θ)− µ2 ‖θ−κ‖22 is convex by definition of strong convexity, and l−f is differentiable with ∇(l− f)(κ) = 0. We apply Lemma A.2, which tells us that z is in ∂l(κ). This is sufficient to conclude, by noticing that a convex function is always above its tangents.\nThe next lemma is also classical (see the appendix of [18]).\nLemma A.4 (Second-Order Growth Property). Let f : Rp → R be a µ-strongly convex function and Θ ⊆ Rp be a convex set. Let θ⋆ be the minimizer of f on Θ. Then, the following condition holds for all θ in Θ:\nf(θ) ≥ f(θ⋆) + µ 2 ‖θ − θ⋆‖22.\nWe now introduce a sequence of probabilistic tools, which we use in our convergence analysis for non-convex functions. The first one is a classical theorem on quasi-martingales, which was used in [3] for proving the convergence of the stochastic gradient descent algorithm.\nTheorem A.1 (Convergence of Quasi-Martingales.). This presentation follows [3] and Proposition 9.5 and Theorem 9.4 of [34]. The original theorem is due to [33]. Let (Fn)n≥0 be an increasing family of σ-fields. Let (Xn)n≥0 be a real stochastic process such that every random variable Xn is bounded below by a constant independent of n, and Fn-measurable. Let\nδn ,\n{\n1 if E[Xn+1 −Xn|Fn] > 0, 0 otherwise.\nIf the series ∑∞\nn=0 E[δn(Xn+1 − Xn)] converges, then (Xn)n≥0 is a quasi-martingale and converges almost surely to an integrable random variable X∞. Moreover,\n∞ ∑\nn=0\nE [ |E[Xn+1 −Xn|Fn]| ] < ∞.\nThe next lemma is simple, but useful to prove the convergence of deterministic algorithms."
    }, {
      "heading" : "Lemma A.5. Deterministic Lemma on Non-negative Converging Series.",
      "text" : "Let (an)n≥1, (bn)n≥1 be two non-negative real sequences such that the series ∑∞ n=1 an diverges, the series ∑∞\nn=1 anbn converges, and there exists K > 0 such that |bn+1 − bn| ≤ Kan. Then, the sequence (bn)n≥1 converges to 0.\nProof. The proof is inspired by the one of Proposition 1.2.4 of [31]. Since the series ∑\nn≥1 an diverges, we necessarily have lim infn→+∞ bn = 0. Otherwise, it would be easy to contradict the assumption ∑\nn≥1 anbn < +∞. Let us now proceed by contradiction and assume that lim supn→+∞ bn = λ > 0. We can then build two sequences of indices (mj)j≥1 and (nj)j≥1 such that\n• mj < nj < mj+1,\n• λ3 < bk, for mj ≤ k < nj ,\n• bk ≤ λ3 , for nj ≤ k < mj+1.\nLet ε = λ 2\n9K and ̃ be large enough such that\n∞ ∑\nn=m̃\nanbn < ε.\nThen, we have for all j ≥ ̃ and all m with mj ≤ m ≤ nj − 1,\n|bnj − bm| ≤ nj−1 ∑\nk=m\n|bk+1 − bk| ≤ 3K\nλ\nnj−1 ∑\nk=m\nak λ 3 ≤ 3K λ\nnj−1 ∑\nk=m\nakbk ≤ 3K\nλ\n+∞ ∑\nk=m\nakbk\n≤ 3Kε λ ≤ λ 3 .\nTherefore, by using the triangle inequality,\nbm ≤ bnj + λ 3 ≤ 2λ 3 .\nand finally, for all m ≥ ̃, bm ≤ 2λ\n3 ,\nwhich contradicts lim supn→+∞ bn = λ > 0. Therefore, bn −→ n→+∞ 0.\nWe now provide a stochastic version of Lemma A.6."
    }, {
      "heading" : "Lemma A.6. Stochastic Lemma on Non-negative Converging Series.",
      "text" : "Let (Xn)n≥1 be a sequence of non-negative measurable random variables on a probability space. Let also an, bn be two non-negative sequences such that ∑ n≥1 an = +∞ and ∑\nn≥1 anbn < +∞. Assume that there exists a constant C such that for all n ≥ 1, E[Xn] ≤ bn and |Xn+1−Xn| ≤ Can almost surely. Then Xn almost surely converges to zero.\nProof. The following series is convergent\nE\n\n\n∑ n≥1 anXn\n\n = ∑\nn≥1 E [anXn] ≤\n∑ n≥1 anbn < +∞,\nwhere we use the fact that the random variables are non-negative to interchange the sum and the expectation. We thus have that ∑\nn≥1 anXn converges with probability one. Then, let us call a′n = an and b ′ n = Xn; the conditions of Lemma A.5 are satisfied for a ′ n and b ′ n with probability one, and Xn almost surely converges to zero."
    }, {
      "heading" : "B Auxiliary Lemmas",
      "text" : "In this section, we present auxiliary lemmas for our convex and non-convex analyses. We start by presenting a lemma which is useful for both of them, and which is in fact a core component for all results presented in [18]. The proof of this lemma is simple and available in [18].\nLemma B.1 (Basic Properties of First-Order Surrogate Functions). Let g be in SL,ρ(f, κ) for some κ in Θ. Define the approximation error function h , g−f and let θ′ be the minimizer of g over Θ. Then, for all θ in Θ,\n• ∇h(κ) = 0;\n• |h(θ)| ≤ L2 ‖θ − κ‖22;\n• f(θ′) ≤ g(θ′) ≤ f(θ) + L2 ‖θ − κ‖22 − ρ 2‖θ − θ′‖22."
    }, {
      "heading" : "B.1 Convex Analysis",
      "text" : "We introduce, for all n ≥ 0, the quantity ξn , 12E[‖θ⋆ − θn‖22], where θ⋆ is a minimizer of f on Θ. Our analysis also involves several quantities that are defined recursively for all n ≥ 1:\n\n    \n    \nAn , (1− wn)An−1 + wnξn−1 Bn , (1− wn)Bn−1 + wnE[f(θn−1)] Cn , (1− wn)Cn−1 + (Rwn) 2\n2ρ\nḡn , (1− wn)ḡn−1 + wngn f̄n , (1− wn)f̄n−1 + wnfn\n, (7)\nwhere A0 , 1L(ρξ0 − f⋆), B0 , 0, C0 , 0, ḡ0 = f̄0 , θ 7→ ρ 2‖θ− θ0‖22. Note that ḡ0 is ρ-strongly convex, and is minimized by θ0. The choice for A0, B0, C0 is driven by technical reasons, which appear in the proof of Lemma B.4, a stochastic version of Lemma B.1.\nNote that we also assume here that all the expectations above are well defined and finite-valued. In other words, we do not deal with measurability or integrability issues for simplicity, as often done in the literature [22].\nLemma B.2 (Auxiliary Lemma for Convex Analysis). When the functions fn are convex, and the surrogates gn are in SL,ρ(fn, θn−1), we have under assumption (A) that for all n ≥ 1,\nḡn(θn−1) ≤ ḡn(θn) + (Rwn)\n2\n2ρ .\nProof. First, we remark that the subdifferentials of gn and fn at θn−1 coincide by applying Lemma A.2. Then, we choose zn in ∂gn(θn−1) = ∂fn(θn−1), which is bounded by R accord-\ning to assumption (A), and we have\nḡn(θn) = (1− wn)ḡn−1(θn) + wngn(θn)\n≥ (1−wn) ( ḡn−1(θn−1)+ ρ\n2 ‖θn−θn−1‖22\n)\n+wn\n(\ngn(θn−1)+z ⊤ n (θn−θn−1)+\nρ 2 ‖θn−θn−1‖22 )\n= ḡn(θn−1) + wnz ⊤ n (θn − θn−1) +\nρ 2 ‖θn − θn−1‖22\n≥ ḡn(θn−1)−Rwn‖θn − θn−1‖2 + ρ\n2 ‖θn − θn−1‖22\n≥ ḡn(θn−1)− (Rwn)\n2\n2ρ .\nThe first inequality uses Lemma A.4 and Lemma A.3 since gn is ρ-strongly convex by definition (and by induction ḡn is ρ-strongly convex as well); the second inequality uses Cauchy-Schwarz’s inequality and the fact that the subgradients of the functions fn are bounded by R.\nLemma B.3 (Another Auxiliary Lemma for Convex Analysis). When the functions fn are convex, and the surrogates gn are in SL,ρ(fn, θn−1), we have under assumption (A) that for all n ≥ 0,\nBn ≤ E[ḡn(θn)] + Cn, (8)\nProof. We proceed by induction, and start by showing that Eq. (8) is true for n = 0.\nB0 = 0 = E[ḡ0(θ0)] = E[ḡ0(θ0)] + C0.\nLet us now assume that it is true for n− 1, and show that it is true for n. Bn = (1− wn)Bn−1 + wnE[f(θn−1)]\n≤ (1− wn)(E[ḡn−1(θn−1)] + Cn−1) + wnE[f(θn−1)] = (1− wn)(E[ḡn−1(θn−1)] + Cn−1) + wnE[fn(θn−1)] = (1− wn)(E[ḡn−1(θn−1)] + Cn−1) + wnE[gn(θn−1)] = E[ḡn(θn−1)] + (1− wn)Cn−1\n≤ E[ḡn(θn)] + (Rwn)\n2\n2ρ + (1− wn)Cn−1\n= E[ḡn(θn)] + Cn.\nThe first inequality uses the induction hypothesis; the last inequality uses Lemma B.2 and the definition of Cn. We also used the fact that E[fn(θn−1)] = E[E[fn(θn−1)]|Fn−1]] = E[E[f(θn−1)]|Fn−1]] = E[f(θn−1)], where Fn−1 corresponds to the filtration induced by the past information before time n, such that θn−1 is deterministic given Fn−1.\nThe next lemma is important; it is the stochastic version of Lemma B.1 for first-order surrogates.\nLemma B.4 (Basic Properties of Stochastic First-Order Surrogates). When the functions fn are convex and the functions gn are in SL,ρ(fn, θn−1), we have under assumption (A) that for all n ≥ 0,\nBn ≤ f⋆ + LAn − ρξn + Cn,\nProof. According to Lemma B.3, it is sufficient to show that E[ḡn(θn)] ≤ f⋆ + LAn − ρξn for all n ≥ 0. Since ḡn is ρ-strongly convex and θn is the minimizer of ḡn over Θ, we have E[ḡn(θn)] ≤ E[ḡn(θ\n⋆)] − ρξn, by using Lemma A.4. Thus, it is in fact sufficient to show that E[ḡn(θ⋆)] ≤ f⋆ + LAn. For n = 0, this inequality holds since E[ḡ0(θ⋆)] = ρξ0 = f⋆ + LA0. We can then proceed again by induction: assume that E[ḡn−1(θ⋆)] ≤ f⋆ + LAn−1. Then,\nE[ḡn(θ ⋆)] = (1− wn)E[ḡn−1(θ⋆)] + wnE[gn(θ⋆)]\n≤ (1− wn)(f⋆ + LAn−1) + wn(E[fn(θ⋆)] + Lξn−1) = (1− wn)(f⋆ + LAn−1) + wn(f⋆ + Lξn−1) = f⋆ + LAn,\nwhere we have used Lemma B.1 to upper-bound the difference E[gn(θ⋆)]−E[fn(θ⋆)] by ξn−1.\nFor strongly-convex functions, we also have the following simple but useful relation between An and Bn.\nLemma B.5 (Relation between An and Bn). Under assumption (B), if w1 = 1, we have for all n ≥ 1,\nf⋆ + µAn ≤ Bn.\nProof. This relation is true for n = 1 since we have f⋆ + µA1 = f⋆ + µξ0 ≤ f(θ0) = B1 by applying Lemma A.4, since f is µ-strongly convex according to assumption (B). The rest follows by induction."
    }, {
      "heading" : "B.2 Non-convex Analysis",
      "text" : "When the functions fn are not convex, the convergence analysis becomes more involved. One key tool we use is a uniform convergence result when the function class {x 7→ ℓ(θ,x) : θ ∈ Θ} is “simple enough” in terms of entropy. Under the assumptions made in our paper, it is indeed possible to use some results from empirical processes [27], which provides us the following lemma.\nLemma B.6 (Uniform Convergence). Under assumptions (A), (C), and (D), we have the following uniform law of large numbers:\nE\n[\nsup θ∈Θ\n∣ ∣ ∣ ∣ ∣ 1 n n ∑\ni=1\nfi(θ)− f(θ) ∣ ∣ ∣ ∣\n∣\n]\n≤ C√ n , (9)\nwhere C is a constant, and supθ∈Θ ∣ ∣ 1 n ∑n i=1 fi(θ) − f(θ) ∣ ∣ converges almost surely to zero.\nProof. We simply refer to Lemma 19.36 and Example 19.7 of [27], where assumptions (C) and (D) ensure uniform boundness and squared integrability conditions. Note that we assume that the quantities supθ∈Θ ∣ ∣ 1 n ∑n i=1 fi(θ) − f(θ) ∣\n∣ are measurable. This assumption does not incur a loss of generality, since measurability issues for empirical processes can be dealt with rigorously [27].\nThe next lemma shows that uniform convergence applies to the weighted empirical risk f̄n, defined in Eq. (7), but with a different rate.\nLemma B.7 (Uniform Convergence for f̄n). Under assumptions (A), (C), (D), and (E), we have for all n ≥ 1,\nE\n[\nsup θ∈Θ\n∣ ∣f̄n(θ) − f(θ) ∣ ∣\n] ≤ Cwn √ n,\nwhere C is the same as in Lemma B.6, and supθ∈Θ ∣ ∣f̄n(θ)− f(θ) ∣ ∣ converges almost surely to zero.\nProof. We prove the two parts of the lemma separately. As in Lemma B.6, we assume all the quantities of interest to be measurable."
    }, {
      "heading" : "First part of the lemma:",
      "text" : "Let us fix n > 0. It is easy to show that f̄n can be written as f̄n = ∑n i=1 w i nfi for some non-negative weights win with w n n = wn. Let us also define the empirical cost Fi , 1 n−i+1 ∑n j=i fj . According to (9), we have E [supθ∈Θ |Fi(θ) − f(θ)|] ≤ C√n−i+1 . We now remark that\nf̄n − f = n ∑\ni=1\n(win − wi−1n )(n− i+ 1)(Fi − f),\nwhere we have defined w0n , 0. This relation can be proved by simple calculation. We obtain the first part by using the triangle inequality, and the fact that win ≥ wi−1n for all i:\nE\n[\nsup θ∈Θ\n|f̄n(θ)− f(θ)| ] ≤ E [ n ∑\ni=1\n(win − wi−1n )(n− i+ 1) sup θ∈Θ\n|Fi(θ)− f(θ)| ]\n=\nn ∑\ni=1\n(win − wi−1n )(n− i+ 1)E [\nsup θ∈Θ\n|Fi(θ) − f(θ)| ]\n≤ n ∑\ni=1\n(win − wi−1n )C √ n− i+ 1\n≤ √ nC n ∑\ni=1\n(win − wi−1n )\n= C √ nwn.\nThis is unfortunately not sufficient to show that E [ supθ∈Θ |f̄n(θ) − f(θ)| ]\nconverges to zero almost surely. We will show this fact by using Lemma A.6."
    }, {
      "heading" : "Second part of the lemma:",
      "text" : "We call Xn = supθ∈Θ |f̄n(θ)− f(θ)|. We have\nXn −Xn−1 = sup θ∈Θ |(1− wn)(f̄n−1(θ) − f(θ)) + wn(fn(θ)− f(θ))| −Xn−1\n≤ sup θ∈Θ wn|fn(θ) − f(θ)| − wnXn−1 ≤ 2Mwn\nLet us denote by θ⋆n a point in Θ such that Xn = |f̄n(θ⋆n)− f(θ⋆n)|. We also have\nXn −Xn−1 = sup θ∈Θ |(1− wn)(f̄n−1(θ) − f(θ)) + wn(fn(θ)− f(θ))| −Xn−1\n≥ (1− wn)Xn−1 + wn(fn(θ⋆n−1)− f(θ⋆n−1))−Xn−1 ≥ −wnXn−1 + wn(fn(θ⋆n−1)− f(θ⋆n−1)) ≥ −wn4M,\nwhere we use again the fact that all functions fn, f̄n and f are bounded by M . Thus, we have shown that |Xn − Xn−1| ≤ 4Mwn. Call an = wn and bn = wn √ n, then the conditions of Lemma A.6 are satisfied, and Xn converges almost surely to zero.\nFinally, the next lemma illustrates why the strong convexity of the surrogates is important.\nLemma B.8 (Stability of the Estimates). When gn is in SL,ρ(f, θn−1),\n‖θn − θn−1‖2 ≤ 2Rwn\nρ .\nProof. Because the surrogates gn are ρ-strongly convex, we have from Lemma A.4\nρ 2 ‖θn − θn−1‖22 ≤ ḡn(θn−1)− ḡn(θn)\n= wn (gn(θn−1)− gn(θn)) + (1 − wn) (ḡn−1(θn−1)− ḡn−1(θn)) ≤ wn (gn(θn−1)− gn(θn)) ≤ wn (fn(θn−1)− fn(θn)) ≤ Rwn‖θn − θn−1‖2.\nThe second inequality comes from the fact that θn−1 is a minimizer of ḡn−1; the third inequality is because gn(θn−1) = fn(θn−1) and gn ≥ fn. This is sufficient to conclude."
    }, {
      "heading" : "C Proofs of the Main Lemmas and Propositions",
      "text" : ""
    }, {
      "heading" : "C.1 Proof of Proposition 3.1",
      "text" : "Proof. According to Lemma B.4, we have for all n ≥ 1,\nwnBn−1 ≤ wnf⋆ + LwnAn−1 − Lwnξn−1 + wnCn−1.\nBy using the relations (7), this is equivalent to\nBn−1 − Bn + wnE[f(θn−1)] ≤ wnf⋆ + L(An−1 −An) + Cn−1 − Cn + (Rwn)\n2\n2L .\nBy summing these inequalities between 1 and n, we obtain\nB0 −Bn + n ∑\nk=1\nwkE[f(θk−1)] ≤ ( n ∑\nk=1\nwk\n)\nf⋆ + LA0 − LAn − Cn + n ∑\nk=1\n(Rwk) 2\n2L .\nNote that we also have\nBn ≤ f⋆ + LAn + Cn = LAn + Cn +B0 − LA0 + Lξ0.\nTherefore, by combining the two previous inequalities,\nn ∑\nk=1\nwkE[f(θk−1)] ≤ ( n ∑\nk=1\nwk\n)\nf⋆ + Lξ0 +\nn ∑\nk=1\n(Rwk) 2\n2L ,\nand by using Jensen’s inequality,\nE[f(θ̄n−1)− f⋆] ≤ Lξ0 +\nR2 2L ∑n k=1 w 2 k\n∑n k=1 wk\n."
    }, {
      "heading" : "C.2 Proof of Corollary 3.1",
      "text" : ""
    }, {
      "heading" : "Proof.",
      "text" : "We choose weights of the form wn ,\nγ√ n . Then, we have\nn ∑\nk=1\nw2k ≤ γ2(1 + logn),\nby using the fact that ∑n k=1 1 k ≤ 1 + log(n). We also have for n ≥ 2,\nn ∑\nk=1\nwk ≥ 2γ( √ n+ 1− 1) ≥ γ √ n,\nwhere we use the fact that ∑n k=1 1√ k ≥ 2(\n√ n+ 1− 1), and the fact that 2( √ n+ 1− 1) ≥ √n for\nall n ≥ 2. Plugging this inequalities into (3) yields the desired result."
    }, {
      "heading" : "C.3 Proof of Proposition 3.2",
      "text" : "Proof. We proceed in several steps, proving the convergence rates of several quantities of interest.\nConvergence rate of Cn: Let us show by induction that we have Cn ≤ R 2 ρ wn for all n ≥ 1. This is obviously true for n = 1\nby definitions of w1 = 1 and C1 = R 2 2ρ . Let us now assume that it is true for n− 1. We have\nCn = (1− wn)Cn−1 + R2\n2ρ w2n\n≤ R 2\nρ wn\n(\n(1 − wn) wn−1 wn + wn 2\n)\n≤ R 2\nρ wn\n(\nβ(n− 1) βn+ 1 βn+ 1 β(n− 1) + 1 + 1 βn+ 1\n)\n≤ R 2\nρ wn\n(\nβ(n− 1) β(n− 1) + 1 +\n1\nβ(n− 1) + 1\n)\n= R2\nρ wn.\n(10)\nWe conclude by induction that this is true for all n ≥ 1. Convergence rate of An: From Lemma B.5 and B.4, we have for all n ≥ 2,\nµAn−1 ≤ LAn−1 − ρξn−1 + Cn−1.\nMultiplying this inequality by wn,\n2µwnAn−1 ≤ ρwn(An−1 − ξn−1) + wnCn−1,\nwhere the factor 2 comes from the fact that ρ = L+ µ. By using the definition of An in Eq. (7), we obtain the relation\nAn ≤ ( 1− 2µwn ρ ) An−1 + wn ρ Cn−1.\nLet us now show by induction that we have, for all n ≥ 1, the convergence rate An ≤ δwn, where δ , max ( R2\nρµ , ξ0\n)\n. For n = 1, we have that and w1 = 1, and thus A1 = ξ0 ≤ δ. Assume now that we have An−1 ≤ δwn−1 for some n ≥ 1. Then, by using the convergence rate (10) and the induction hypothesis,\nAn ≤ δwn (( 1− 2µwn ρ ) wn−1 wn + R2wn−1 ρ2δ )\n≤ δwn (( 1− 2µwn ρ ) wn−1 wn + µ wn−1 ρ )\n≤ δwn ( βn+ 1− 2µ(1+β) ρ\nβn+ 1\nβn+ 1 β(n− 1) + 1 + µ(1+β) ρ β(n− 1) + 1\n)\n= δwn\n(\nβn+ 1− µ(1+β) ρ\nβ(n− 1) + 1\n)\n≤ δwn.\nThe last inequality uses the fact that µ(1+β) ρ ≥ β because β ≤ µ L\n. we conclude by induction that"
    }, {
      "heading" : "An ≤ δwn for all n ≥ 1.",
      "text" : ""
    }, {
      "heading" : "Convergence rate of E[f(θ̂n)− f⋆] + ρξn:",
      "text" : "We use again Lemma B.4:\nBn − f⋆ + ρξn ≤ LAn + Cn,\nand we consider two possible cases\n• If R2 ρµ ≥ ξ0, then\nBn − f⋆ + ρξn ≤ R2\nρ\n(\n1 + L\nµ\n)\nwn\n= R2\nµ wn\n≤ 2R 2\nµ(βn+ 1) ,\nwhere we simply use the convergence rates of An and Cn computed before.\n• If instead R2 ρµ < ξ0, then\nBn − f⋆ + ρξn ≤ ( R2\nρ + Lξ0\n)\nwn\n≤ ρξ0wn\n≤ 2ρξ0 βn+ 1 .\nIt is then easy to prove that E[f(θ̂n)−f⋆] ≤ Bn by using Jensen’s inequality, which allows us to conclude."
    }, {
      "heading" : "C.4 Proof of Proposition 3.3",
      "text" : "Proof. We generalize the proof of convergence for online matrix factorization of [19]. The proof exploits Theorem A.1 about the convergence of quasi-martingales [33], similarly as [3] for proving the convergence of the stochastic gradient descent algorithm for non-convex functions."
    }, {
      "heading" : "Almost sure convergence of (ḡn(θn))n≥1:",
      "text" : "The first step consists of applying a convergence theorem for the sequence (ḡn(θn))n≥1 by bounding its positive expected variations. Define Yn , ḡn(θn). For n ≥ 2, we have\nYn−Yn−1 = ḡn(θn)−ḡn(θn−1)+ḡn(θn−1)−ḡn−1(θn−1) = (ḡn(θn)−ḡn(θn−1))+wn(gn(θn−1)−ḡn−1(θn−1)) = (ḡn(θn)−ḡn(θn−1))+wn(f̄n−1(θn−1)−ḡn−1(θn−1))+wn(gn(θn−1)−f̄n−1(θn−1)) = (ḡn(θn)−ḡn(θn−1))+wn(f̄n−1(θn−1)−ḡn−1(θn−1))+wn(fn(θn−1)−f̄n−1(θn−1)) ≤ wn(fn(θn−1)−f̄n−1(θn−1)).\n(11)\nThe final inequality comes from the inequality ḡn ≥ f̄n, which is easy to show by induction starting from n = 1 since w1 = 1. It follows,\nE[ḡn(θn)− ḡn−1(θn−1)|Fn−1] ≤ wnE[fn(θn−1)− f̄n−1(θn−1)|Fn−1] = wn(f(θn−1)− f̄n−1(θn−1)) ≤ wn sup\nθ∈Θ |f(θ)− f̄n−1(θ)|,\nwhere Fn−1 is the filtration representing the past information before time n. Call now\nδn ,\n{\n1 if E[ḡn(θn)− ḡn−1(θn−1)|Fn−1] > 0 0 otherwise.\nThen, the series below with non-negative summands converges: ∞ ∑\nn=1\nE[δn(ḡn(θn)− ḡn−1(θn−1))] = ∞ ∑\nn=1\nE[δnE[(ḡn(θn)− ḡn−1(θn−1))|Fn−1]]\n≤ ∞ ∑\nn=1\nE\n[\nwn sup θ∈Θ\n|f(θ)− f̄n−1(θ)| ]\n≤ ∞ ∑\nn=1\nCw2n √ n < +∞,\nThe second inequality comes from Lemma B.7. Since in addition ḡn is bounded below by some constant independent of n, we can apply Theorem A.1. This theorem tells us that (ḡn(θn))n≥1 converges almost surely to an integrable random variable g⋆ and that\n∑∞ n=1 E[|E[ḡn(θn) −\nḡn−1(θn−1)|Fn−1]|] converges almost surely."
    }, {
      "heading" : "Almost sure convergence of (f̄n(θn))n≥1:",
      "text" : "We will show by using Lemma A.5 that the non-positive term f̄n(θn) − ḡn(θn) almost surely converges to zero, and thus (f̄n(θn))n≥1 is also converging almost surely to g⋆.\nWe observe that ∞ ∑\nn=1\nE[|E[ḡn(θn)− ḡn−1(θn−1)|Fn−1]|] = E [ ∞ ∑\nn=1\n|E[ḡn(θn)− ḡn−1(θn−1)|Fn−1]| ] < +∞.\nThus, the series ∑∞\nn=1 |E[ḡn(θn) − ḡn−1(θn−1)|Fn−1]| is absolutely convergent with probability one, and the series\n∑∞ n=1 E[ḡn(θn)− ḡn−1(θn−1)|Fn−1] is also almost surely convergent.\nWe also remark that, using Lemma B.7,\nE\n[ +∞ ∑\nn=1\nwn|f(θn−1)− f̄n−1(θn−1)| ] ≤ C +∞ ∑\nn=1\nw2n √ n < +∞,\nand thus wn(f(θn−1)− f̄n−1(θn−1)) is the summand of an absolutely convergent series with probability one.\nTaking the expectation of Eq. (11) conditioned on Fn−1, it remains that the non-positive term wn(f̄n−1(θn−1)− ḡn−1(θn−1)) is also necessarily the summand of an almost surely convergent series, since all other terms in the equation are summands of almost surely converging sums. This is not sufficient to immediately conclude that f̄n(θn)−ḡn(θn) converges to zero almost surely, and thus we will use Lemma A.5. We have that\n∑+∞ n=1 wn diverges, that ∑+∞ n=1 wn(ḡn−1(θn−1)− f̄n−1(θn−1))\nconverges almost surely. Define Xn , (ḡn−1(θn−1) − f̄n−1(θn−1)). By definition of the surrogate functions, the differences hn , gn − fn are differentiable and their gradients are L-Lipschitz continuous. Since in addition Θ is compact and ∇hn(θn−1) = 0, ∇hn is bounded by some constant R′ independent of n, and the function hn is R′-Lipschitz. This is therefore also the case for h̄n = ḡn − f̄n.\n|Xn+1 −Xn| = |h̄n(θn)− h̄n−1(θn−1)| ≤ |h̄n(θn)− h̄n(θn−1)|+ |h̄n(θn−1)− h̄n−1(θn−1)| ≤ R′‖θn − θn−1‖2 + |h̄n(θn−1)− h̄n−1(θn−1)|\n≤ 2RR ′\nρ wn + wn|hn(θn−1)− h̄n−1(θn−1)|\n= 2RR′\nρ wn + wn|h̄n−1(θn−1)|\n≤ O(wn). The second inequality uses the fact that h̄n is R′-Lipschitz; The second inequality uses Lemma B.8; the last equality uses the fact that the functions hn are also bounded by some constant independent of n (using the fact that ∇hn is uniformly bounded). We can now apply Lemma A.5, and Xn converges to zero with probability one. Thus, (f̄n(θn))n≥1 converges almost surely to g⋆."
    }, {
      "heading" : "Almost sure convergence of (f(θn))n≥1:",
      "text" : "Since (f̄n(θn))n≥1 converges almost surely, we simply use Lemma A.6, which tells us that f̄n converges uniformly to f . Then, (f(θn))n≥1 converges almost surely to g⋆."
    }, {
      "heading" : "Asymptotic Stationary Point Condition:",
      "text" : "Let us call h̄n , ḡn − f̄n, which can be shown to be differentiable with a L-Lipschitz gradient by definition of the surrogate gn. For all θ in Θ,\n∇f̄n(θn, θ − θn) = ∇ḡn(θn, θ − θn)−∇h̄n(θn)⊤(θ − θn). Since θn is the minimizer of ḡn, we have ∇ḡn(θn, θ − θn) ≥ 0. Since h̄n is differentiable and its gradient is L-Lipschitz continuous, we can apply Lemma A.1 to θ = θn and θ′ = θn − 1L∇h̄n(θn), which gives h̄n(θ′) ≤ h̄n(θn)− 12L‖∇h̄n(θn)‖22. Since we have shown that h̄n(θn) = ḡn(θn)− f̄(θn) converges to zero and h̄n(θ′) ≥ 0, we have that ‖∇h̄n(θn)‖2 converges to zero. Thus,\ninf θ∈Θ ∇f̄n(θn, θ − θn) ‖θ − θn‖2 ≥ −‖∇h̄n(θn)‖2 −→ n→+∞ 0 a.s."
    }, {
      "heading" : "C.5 Proof of Proposition 3.4",
      "text" : "Proof. Since Θ is compact according to assumption (C), the sequence (θn)n≥1 admits limit points. Let us consider a converging subsequence (nk)k≥1 to a limit point θ∞ in Θ. In this converging subsequence, we can also find a subsequence (nk′ )k′≥1 such that κnk′ converges to a point κ∞ in K (which is compact). For the sake of simplicity, and without loss of generality, we remove the indices k and k′ from the notation and assume that θn converges to θ∞, while κn converges to κ∞. It is then easy to see that the functions ḡn converge uniformly to ḡ∞ , gκ∞ , given the assumptions made in the proposition.\nDefining h̄∞ , ḡ∞ − f , we have for all θ in Θ: ∇f(θ∞, θ − θ∞) = ∇ḡ∞(θ∞, θ − θ∞)−∇h̄∞(θ∞, θ − θ∞).\nTo prove the proposition, we will first show that ∇ḡ∞(θ∞, θ−θ∞) ≥ 0 and then that ∇h̄∞(θ∞, θ− θ∞) = 0.\nProof of ∇ḡ∞(θ∞, θ − θ∞) ≥ 0: It is sufficient to show that θ∞ is a minimizer of ḡ∞. This is straightforward, by taking the limit when n goes to infinity of ḡn(θ) ≥ ḡn(θn), where we use the uniform convergence of ḡn.\nProof of ∇h̄∞(θ∞, θ − θ∞) = 0: Since both f̄n and ḡn converges uniformly (according to Lemma B.7 for f̄n), we have that h̄n converges uniformly to h̄∞. Since h̄n is differentiable with a L-Lipschitz gradient, we have for all vector z in Rp, h̄n(θn + z) = h̄n(θn) +∇h̄n(θn)⊤z+O(‖z‖22), where the constant in O is independent of n. By taking the limit when n goes to infinity, it remains\nh̄∞(θ∞ + z) = h̄∞(θ∞) +O(‖z‖22), since we have shown in the proof of Proposition 3.3 that ‖∇h̄n(θn)‖2 converges to zero. Since h̄∞ admits a first order extension around θ∞ it is differentiable at this point and furthermore, ∇h̄∞(θ∞) = 0. This is sufficient to conclude."
    }, {
      "heading" : "C.6 Proof of Proposition 3.5",
      "text" : "Proof. First we notice that\n• gn ≥ fn;\n• gn(θn−1) = fn(θn−1);\n• gn is ρ1-strongly convex since θ 7→ gk,n(γk(θ)) can be shown to be convex, following elementary composition rules for convex functions (see [32], Section 3.2.4).\nThus, the only property missing is the smoothness of the approximation error hn , gn− fn. Rather than writing again a full proof, we now simply review the different places where this property is used, and which modifications should be made to the proofs of Propositions 3.3 and 3.4.\nIn the second step of this proof, we require the functions hn to be uniformly Lipschitz and uniformly bounded. It easy to check that it is still the case with the assumptions we made in Proposition 3.5.\nThe last step about the asymptotic point condition is however more problematic, where we cannot show anymore that the quantity ∇h̄n(θn) converges to zero (since h̄n is not differentiable anymore). Instead, we need to show that the directional derivative ∇h̄n(θn,θ−θn)‖θ−θn‖ uniformly converges to zero on Θ.\nWe will show the result for K = 1; it will be easy to extend it to any arbitrary K > 2. We remark that\n∇h̄n(θn, θ − θn) = ∇h̄0,n(θn)⊤(θ − θn) + lim t→0+ h̄1,n(γ1(θn + t(θ − θn)))− h̄1,n(γ1(θn)) t ,\nwhere h̄0,n and h̄1,n are defined similarly as h̄n for the functions h0,n , g0,n − f0,n and h1,n , g1,n − f1,n respectively. Since h̄n(θn) is shown to converge to zero, we have that the non-negative quantities h̄0,n(θn) and h̄1,n(γ1(θn)) converge to zero as well. Since h̄0,n and h̄1,n are differentiable and their gradients are Lipschitz, we use similar arguments as in the proof of Proposition 3.3, and we have that ∇h̄0,n(θn) and h̄′1,n(γ1(θn)) converge to zero (where h̄′1,n is the derivative of h̄1,n. Concerning the second term, we can make the following Taylor expansion for h̄1,n:\nh̄1,n(γ1(θn+z)) = h̄1,n(γ1(θn))+h̄ ′ 1,n(γ1(θn))(γ1(θn+z)−γ1(θn))+O ( (γ1(θn + z)− γ1(θn))2 ) ,\nwhere the constant in the O notation is independent of θn and z (since the derivative is L1-Lipschitz). Plugging z , t(θ − θn) in this last equation, and using the Lipschitz property of γ1, we have\nlim t→0+\n∣ ∣ ∣ ∣ h̄1,n(γ1(θn + t(θ − θn)))− h̄1,n(γ1(θn)) t ∣ ∣ ∣ ∣ ≤ |h̄′1,n(γ1(θn))|‖θ − θn‖.\nSince h̄′1,n(γ1(θn)) converges to zero, we can conclude the proof of the modified Proposition 3.3.\nThe proof of Proposition 3.4 can be modified with similar arguments."
    }, {
      "heading" : "D Additional Experimental Results",
      "text" : "We present in Figures 4 and 5 some additional experimental comparisons, which complement the ones of Section 4.1. Figures 6 and 7 present additional plots from the experiment of Section 4.2. Finally, we present three dictionaries corresponding to the experiment of Section 4.3 in Figures 8, 9 and 10."
    }, {
      "heading" : "Supplementary References",
      "text" : "[31] D.P. Bertsekas. Nonlinear programming. Athena Scientific Belmont, 1999. 2nd edition.\n[32] S.P. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.\n[33] D. L. Fisk. Quasi-martingales. T. Am. Math. Soc., 120(3):359–388, 1965.\n[34] M. Métivier. Semi-martingales. Walter de Gruyter, 1983.\n[35] Y. Nesterov. Introductory lectures on convex optimization. Kluwer Academic Publishers, 2004.\n[36] Y. Nesterov and J.-P. Vial. Confidence level solutions for stochastic programming. Automatica, 44(6):1559–1568, 2008.\n[37] J. Nocedal and S.J. Wright. Numerical optimization. Springer Verlag, 2006. 2nd edition."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2013,
    "abstractText" : "Majorization-minimization algorithms consist of iteratively minimizing a majorizing surrogate of an objective function. Because of its simplicity and its wide applicability, this principle has been very popular in statistics and in signal processing. In this paper, we intend to make this principle scalable. We introduce a stochastic majorization-minimization scheme which is able to deal with largescale or possibly infinite data sets. When applied to convex optimization problems under suitable assumptions, we show that it achieves an expected convergence rate of O(1/ √ n) after n iterations, and of O(1/n) for strongly convex functions. Equally important, our scheme almost surely converges to stationary points for a large class of non-convex problems. We develop several efficient algorithms based on our framework. First, we propose a new stochastic proximal gradient method, which experimentally matches state-of-the-art solvers for large-scale l1logistic regression. Second, we develop an online DC programming algorithm for non-convex sparse estimation. Finally, we demonstrate the effectiveness of our approach for solving large-scale structured matrix factorization problems.",
    "creator" : "LaTeX with hyperref package"
  }
}