{
  "name" : "1307.1674.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Stochastic Optimization of PCA with Capped MSG",
    "authors" : [ "Raman Arora", "Andrew Cotter" ],
    "emails" : [ "arora@ttic.edu,", "cotter@ttic.edu,", "nati@ttic.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Principal Component Analysis (PCA) is a ubiquitous tool used in many data analysis, machine learning and information retrieval applications. It is used for obtaining a lower dimensional representation of a high dimensional signal that still captures as much as possible of the original signal. Such a low dimensional representation can be useful for reducing storage and computational costs, as complexity control in learning systems, or to aid in visualization.\nPCA is typically phrased as a question about a fixed data set: given a data set of n vectors in Rd, what is the k-dimensional subspace that captures most of the variance in the data set (or equivalently, that is best in reconstructing the vectors, minimizing the sum squared distances, or residuals, to the subspace)? It is well known that this subspace is given by the leading k components of the singular value decomposition of the data matrix (or equivalently of the empirical second moment matrix). And so, the study of computational approaches for PCA has mostly focused on methods for finding the SVD (or leading components of the SVD) for a given n × d matrix [Oja and Karhunen, 1985, Sanger, 1989, Mitliagkas et al., 2013].\nIn this paper we approach PCA as a stochastic optimization problem, where the goal is to optimize a “population objective” based on i.i.d. draws from the population. That is, in the case of PCA, we consider a setting in which we have some unknown source (“population”) distribution D over Rd, and the goal is to find the k-dimensional subspace maximizing the (uncentered) variance of D inside the subspace (or equivalently, minimizing the average squared residual in the population), based on i.i.d. samples from D. The main point here is that\nar X\niv :1\n30 7.\n16 74\nv1 [\nst at\n.M L\n] 5\nJ ul\nthe true objective is not how well the subspace captures the sample (i.e. the “training error”), but rather how well the subspace captures the underlying source distribution (i.e. the “generalization error”). Furthermore, we are not concerned here with capturing some “true” subspace, and so do not measure the angle to it, but rather at finding a “good” subspace, that is almost as good as the optimal one.\nOf course, finding the subspace that best captures the sample is a very reasonable approach to PCA on the population. This is essentially an Empirical Risk Minimization (ERM) approach. However, when comparing it to alternative, perhaps computationally cheaper, approaches, we argue that one should not compare the error on the sample, but rather the population objective. Such a view can justify and favor computational approaches that are far from optimal on the sample, but are essentially as good as ERM on the population.\nSuch a population-based view of optimization has recently been advocated in machine learning, and has been used to argue for crude stochastic approximation approaches (online-type methods) over sophisticated deterministic optimization of the empirical (training) objective (i.e. “batch” methods) [Bottou and Bousquet, 2007, Shalev-Shwartz and Srebro, 2008]. A similar argument was also made in the context of stochastic optimization, where Nemirovski et al. [2009] argues for stochastic approximation (SA) approaches over ERM. Accordingly, SA approaches, mostly variants of Stochastic Gradient Descent, are often the methods of choice for many learning problems, especially when very large data sets are available [Shalev-Shwartz et al., 2007, Collins et al., 2008, ShalevShwartz and Tewari, 2009]. We would like to take the same view in order to advocate for, study, and develop stochastic approximation approaches for PCA.\nIn an empirical study of stochastic approximation methods for PCA, a heuristic “incremental” method showed very good empirical performance [Arora et al., 2012]. However, no theoretical guarantees or justification were given for incremental PCA. In fact, it was shown that for some distributions it can converge to a suboptimal solution with high probability (see Section 5.2 for more about this “incremental” algorithm). Also relevant is careful theoretical work on online PCA by Warmuth and Kuzmin [2008], in which an online regret guarantee was established. Using an online-to-batch conversion, this online algorithm can be converted to a stochastic approximation algorithm with good iteration complexity, however the runtime for each iteration is essentially the same as that of ERM (i.e. of PCA on the sample), and thus senseless as a stochastic approximation method (see Section 3.3 for more on this algorithm).\nIn this paper we borrow from these two approaches and present a novel algorithm for stochastic PCA—the Matrix Stochastic Gradient (MSG) algorithm. MSG enjoys similar iteration complexity to Warmuth’s and Kuzmin’s algorithm, and in fact we present a unified view of both algorithms as different instantiations of Mirror Descent for the same convex relaxation of PCA. We then present the capped MSG, which is a more practical variant of MSG, has very similar updates to those of the “incremental” method, and works well in practice, and does not get stuck like the “incremental” method. The Capped MSG is thus a clean, theoretically well founded method, with interesting connections to other\nstochastic/online PCA methods, and excellent practical performance—a “best of both worlds” algorithm."
    }, {
      "heading" : "2 Problem Setup",
      "text" : "We consider PCA as the problem of finding the maximal (uncentered) variance k-dimensional subspace with respect to an (unknown) distribution D over x ∈ Rd. We assume without loss of generality a scaling such that Ex∼D[‖x‖2] ≤ 1. We also require for our analysis a bounded fourth moment: Ex∼D[‖x‖4] ≤ 1. We represent a k-dimensional subspace by an orthonormal basis, collected in the columns of a matrix U . With this parametrization, PCA is defined as the following stochastic optimization problem,\nmaximize : Ex∼D[xTUUTx] (2.1) subject to : U ∈ Rd×k, UTU = I.\nIn a stochastic optimization setting we do not have direct knowledge of the distribution and have access to it only through i.i.d. samples—these can be thought of as “training examples”. As with other studies of stochastic approximation methods, we are less concerned with the number of required samples, but rather with the overall runtime required to obtain an -suboptimal solution.\nThe standard approach to (2.1) is empirical risk minimization (ERM): given samples {xt}Tt=1, from the distribution, we compute the empirical covariance matrix Ĉ = 1T ∑T t=1 xtx T t , and pick the columns of U to be the eigenvectors of Ĉ corresponding to the top-k eigenvalues. This approach requires O(d2) memory and O(d2) operations just in order to compute the covariance matrix, plus some additional time for the SVD. We are interested in methods with much lower sample time and space complexity, preferably linear rather than quadratic in d."
    }, {
      "heading" : "3 MSG and MEG",
      "text" : "A natural stochastic approximation (SA) approach to PCA is to perform projected stochastic gradient descent (SGD) on Problem 2.1, with respect to the variable U . This leads to the stochastic power method with each iteration given as\nU (t+1) = Porth ( U (t) + ηxtx T t ) ,\nwhere, xtx T t is the gradient of the PCA objective w.r.t. U , η is a step size, and Porth (·) projects its argument onto the set of orthogonal matrices. Unfortunately, although SGD is well understood for convex problems, Problem 2.1 is non-convex. Consequently, obtaining a theoretical understanding of the stochastic power method, or of how the step size should be set, has proved elusive. Under some conditions, convergence to the optimal solution can be ensured, but no rate is known [Oja and Karhunen, 1985, Sanger, 1989, Arora et al., 2012].\nInstead, we consider a re-parameterization of the PCA problem where the objective is convex. Instead of representing a linear subspace in terms of its basis matrix, U , we parametrize it using the corresponding projection matrix M = UUT . We can now reformulate the PCA problem as\nmaximize : Ex∼D[xTMx] (3.1) subject to : M ∈ Rd×d, σi (M) ∈ {0, 1} , rankM = k,\nwhere σi (M) is the i th eigenvalue of M .\nWe now have a convex (even linear) objective, but the constraints in (3.1) are not convex. This prompts us to consider its convex relaxation:\nmaximize : Ex∼D[xTMx] (3.2) subject to : M ∈ Rd×d, 0 M I, trM = k.\nSince the objective is linear, and the constraint set of (3.2) is just the convex hull of the constraints of (3.1), an optimum of (3.2) is always attained at a “vertex”, i.e. a point on the boundary of the original constraints (3.1). The optimum of (3.1) and (3.2) are thus the same (strictly speaking—every optimum of (3.1) is also an optimum of (3.2)), and solving (3.2) is equivalent to solving (3.1).\nFurthermore, even if some -suboptimal solution we find for (3.2) is not rankk, i.e. is not a feasible point of (3.1), we can easily sample from it a rank-k solution, feasible for (3.1), with the same value (in expectation). This follows from the following result of Warmuth and Kuzmin [2008].\nLemma 3.1 (Rounding [Warmuth and Kuzmin, 2008]). Any feasible solution of (3.2) can be expressed as a convex combination of at most d feasible solutions of (3.1).\nFurthermore, Algorithm 4.1 of Warmuth and Kuzmin [2008] shows how to efficiently find such a convex combination. Since the objective is linear, treating the coefficients of the convex combination as sampling weights and choosing randomly among the d components yields a rank-k matrix with the desired objective function value, in expectation."
    }, {
      "heading" : "3.1 Matrix Stochastic Gradient",
      "text" : "Performing SGD on the convex Problem 3.2 (w.r.t. the variable M) yields the following iterates: M (t+1) = P ( M (t) + ηxtx T t ) , (3.3)\nwhere the projection is now performed onto the (convex) constraints of (3.2). The Matrix Stochastic Gradient (MSG) algorithm entails:\n1. Choose step-size η, iteration count T , and starting point M (0). 2. Iterate the updates (3.3) T times, each time using an independent sample xt ∼ D.\n3. Average the iterates as M̄ = 1T ∑T t=1M (t).\nAlgorithm 1 Matrix stochastic gradient (MSG) update: compute an eigendecomposition of M ′ + ηxxT from a rank-n eigendecomposition M ′ = U ′diag(σ′)(U ′)T and project the resulting solution onto the constraint set. The computational cost of this algorithm is dominated by the matrix multiplication defining U (line 4 or 7) costing O(m2d) operations.\nmsg-step ( d, k,m : N, U ′ : Rd×m, σ′ : Rm, x : Rd, η : R ) 1 x̂← √η(U ′)Tx; x⊥ ← √ ηx− U ′x̂; r ← ‖x⊥‖; 2 if r > 0 3 V, σ ← eig([diag(σ′) + x̂x̂T , rx̂; rx̂T , r2]); 4 U ← [U ′, x⊥/r]V ; 5 else 6 V, σ ← eig(diag(σ′) + x̂x̂T ); 7 U ← U ′V ; 8 σ ← distinct eigenvalues in σ; κ← corresponding multiplicities; 9 σ ← project (d, k,m, σ, κ);\n10 return U, σ;\n4. Sample a rank-k solution M̃ from M̄ using the rounding procedure discussed in the previous section. Analyzing MSG is straightforward using the standard SGD analysis [Nemirovski and Yudin, 1983]: Theorem 1. After T iterations of MSG (on Problem 3.2), with step size η =√ k T , and starting at M (0) = 0,\nE[Ex∼D[xT M̃x]] ≥ Ex∼D[xTM∗x]− 1\n2\n√ k\nT ,\nwhere the expectation is w.r.t. the i.i.d. samples x1, . . . , xT ∼ D and the rounding, and M∗ is the optimum of (3.1).\nProof. Standard SGD analysis of Nemirovski and Yudin [1983] yields that\nE[xTM∗x− xT M̄x] ≤ η 2 Ex∼D[‖g‖2F ] + ‖M∗ −M (0)‖2F 2ηT , (3.4)\nwhere g = xxT is the gradient of the PCA objective. Now, Ex∼D[‖g‖2F ] = Ex∼D[‖x‖4] ≤ 1 and ∥∥M∗ −M (0)∥∥2 F\n= ‖M∗‖2F = k. In the last inequality, we used the fact that M∗ has k eigenvalues of value 1 each, and hence ‖M∗‖F =√ k."
    }, {
      "heading" : "3.2 Efficient Implementation and Projection",
      "text" : "A naive implementation of the MSG update requires O(d2) memory and O(d2) operations per iteration. In this section, we show how to perform this update\nefficiently by maintaining an up-to-date eigendecomposition of M (t). Pseudocode for the update is given as Algorithm 1. Consider the eigendecomposition M (t) = U ′diag(σ)(U ′)T , at the tth iteration, where rank(M (t)) = kt and U\n′ ∈ Rd×kt . Given a new observation xt, the eigendecomposition of M (t) + ηxtxTt can be updated efficiently using a (kt + 1)× (kt + 1) SVD [Brand, 2002, Arora et al., 2012] (steps 1-7 of Algorithm 1). This rank-one eigen-update is followed by projection onto the constraints of (3.2), invoked as project in step 8 of Algorithm 1, discussed in the following paragraphs and given as Algorithm 2. The projection procedure is based on the following lemma1:\nLemma 3.2. Let M ′ ∈ Rd×d be a symmetric matrix, with eigenvalues σ′1, . . . , σ′d and associated eigenvectors v′1, . . . , v ′ d. Its projection M = P (M ′) onto the feasible region of Problem 3.2 with respect to the Frobenius norm, is the unique feasible matrix which has the same eigenvectors as M ′, with the associated eigenvalues σ1, . . . , σd satisfying:\nσi = max (0,min (1, σ ′ i + S))\nwith S ∈ R being chosen in such a way that ∑d\ni=1 σi = k.\nProof. In Appendix A.\nThis result shows that projecting onto the feasible region amounts to finding the value of S such that, after shifting the eigenvalues by S and clipping the results to [0, 1], the result is feasible. Importantly, the projection operates only on the eigenvalues. Algorithm 2 contains pseudocode which finds S from a list of eigenvalues. It is optimized to efficiently handle repeated eigenvalues— rather than receiving the eigenvalues in a length-d list, it instead receives a length-n list containing only the distinct eigenvalues, with κ containing the corresponding multiplicities. In Sections 4 and 5, we will see why this is an important optimization.\nThe central idea motivating the algorithm is that, in a sorted array of eigenvalues, all elements with indices below some threshold i will be clipped to 0, and all of those with indices above another threshold j will be clipped to 1. The pseudocode simply searches over all possible pairs of such thresholds until it finds the one that works.\nThe rank-one eigen-update combined with the fast projection step yields an efficient MSG update that requires O(dkt) memory and O(dk 2 t ) operations per iteration, where recall that kt is the rank of the iterate M (t). This is a significant improvement over the O(d2) memory and O(d2) computation required by a standard implementation of MSG, if the iterates have relatively low rank.\n1Note that our projection problem onto the capped simplex, even when seen in the vector setting, is substantially different from Duchi et al. [2008]. We project onto the set {0 ≤ σ ≤ 1, ‖σ‖1 = k} in (3.2) and {0 ≤ σ ≤ 1, ‖σ‖1 = k, ‖σ‖0 ≤ K} in (5.1) whereas Duchi et al. [2008] project onto {0 ≤ σ, ‖σ‖1 = k}.\nAlgorithm 2 Routine which finds the S of Lemma 3.2. It takes as parameters the dimension d, “target” subspace dimension k, and the number of distinct eigenvalues n of the current iterate. The length-n arrays σ′ and κ′ contain the distinct eigenvalues and their multiplicities, respectively, of M ′ (with ∑n i=1 κ ′ i = d). Line 1 sorts σ\n′ and re-orders κ′ so as to match this sorting. The loop will be run at most 2n times (once for each possible increment to i or j on lines 12–15), so the computational cost is dominated by that of the sort: O(n logn).\nproject (d, k, n : N, σ′ : Rn, κ′ : Nn) 1 σ′, κ′ ← sort(σ′, κ′); 2 i← 1; j ← 1; si ← 0; sj ← 0; ci ← 0; cj ← 0; 3 while i ≤ n 4 if (i < j) 5 S ← (k − (sj − si)− (d− cj))/(cj − ci); 6 b← ( 7 (σ′i + S ≥ 0) and (σ′j−1 + S ≤ 1) 8 and ((i ≤ 1) or (σ′i−1 + S ≤ 0)) 9 and ((j ≥ n) or (σ′j+1 ≥ 1))\n10 ); 11 return S if b; 12 if (j ≤ n) and (σ′j − σ′i ≤ 1) 13 sj ← sj + κ′jσ′j ; cj ← cj + κ′j ; j ← j + 1; 14 else 15 si ← si + κ′iσ′i; ci ← ci + κ′i; i← i+ 1; 16 return error;"
    }, {
      "heading" : "3.3 Matrix Exponentiated Gradient",
      "text" : "Since M is constrained by its trace, and not by its Frobenius norm, it is tempting to consider mirror descent (MD) [Beck and Teboulle, 2003] instead of SGD updates for solving Problem 3.2. Recall that the Mirror Descent updates depend on a choice of “potential function” Ψ(·) which should be chosen according to the geometry of the feasible set and the subgradients [Srebro et al., 2011]. Using the squared Frobenius norm as a potential function, i.e. Ψ(M) = ‖M‖2F , yields SGD, i.e. the MSG updates (3.3). The trace-norm constraint suggests using the von Neumann entropy of the spectrum as the potential function, i.e. Ψh(M) = ∑ i λi(M) log λi(M) where λi are the eigenvalues of M . This leads to multiplicative updates which we refer to as Matrix Exponentiated Gradient (MEG) update similar to those presented by [Warmuth and Kuzmin, 2008]. In fact, Warmuth and Kuzmin’s algorithm exactly corresponds to online Mirror Descent on (3.2) with this potential function, but taking the optimization variable to be M⊥ = I −M (with the constraints trM⊥ = d− k and 0 M⊥ I). In either case, using the entropy potential, despite being well suited for the trace-geometry, does not actually lead to better dependence2 on d or k, and\n2This is because in our case, due to the other constraints, ‖M∗‖F = √\ntrM∗. Furthermore, the SGD analysis depends on the Frobenius norm of the stochastic gradients, but since all stochastic gradients are rank one, this is the same as their spectral norm, which comes up in\nMirror Descent analysis again yields an excess loss of √ k/T . Warmuth and Kuzmin do present an “optimistic” analysis, with a dependence on the “reconstruction error” L∗ = E[xT (I − M∗)x], which yields an excess error of\nO\n(√ L∗k log(d/k)\nT + k log(d/k) T\n) (their logarithmic term can be avoided by a more\ncareful analysis)."
    }, {
      "heading" : "4 MSG runtime and the rank of the iterates",
      "text" : "As we saw, MSG requires O(k/ 2) iterations to obtain an -suboptimal solution and each iteration of MSG costs O(k2t d) operations where kt is the rank of\niterate M (t). This yields a total runtime of O(k̄2dk/ 2), where k̄2 = ∑T\nt=1 k 2 t .\nClearly, the runtime for MSG depends critically on the rank of the iterates. If the rank of the iterates is as large as d, MSG achieves a runtime that is cubic in the dimensionality. On the other hand, if the rank of the iterates is O(k), the runtime is linear in the dimensionality. Fortunately, in practice the ranks are typically much lower than the dimensionality. The reason for this is that MSG performs a rank-1 update followed by a projection onto the constraints. Since M ′ = M (t) + ηxtx T t will have a larger trace than M\n(t) (i.e. trM ′ ≥ k), the projection, as is shown by Lemma 3.2, will subtract a quantity S from every eigenvalue of M ′, clipping each to 0 if it becomes negative. Therefore, each MSG update will increase the rank of the iterate by at most 1, and has the potential to decrease it, perhaps significantly. It’s very difficult to theoretically quantify how the rank of the iterates will evolve over time, but we have observed empirically that the iterates do tend to have relatively low rank.\nWe explore this issue in greater detail experimentally, on a distribution which we expect to be difficult for MSG. To this end, we generated data from known 32- dimensional distributions with diagonal covariance matrices Σ = diag(σ/ ‖σ‖), where σi = τ −i/ ∑32 j=1 τ −j , for i = 1, . . . , 32 and for some τ > 1. Observe that Σ(k) has a smoothly-decaying set of eigenvalues and the rate of decay is controlled by τ . As τ → 1, the spectrum becomes flatter resulting in distributions that present challenging test cases for MSG. We experimented with τ = 1.1 and k ∈ {1, 2, 4}, where k is the desired subspace dimension used by each algorithm. The data is generated by sampling the ith standard unit basis vector ei with probability √ Σii. We refer to this as the “orthogonal distribution”, since it is a discrete distribution over 32 orthogonal vectors. In Figure 1, we show the results with k = 4. We can see from the left-hand plot that MSG algorithm maintains a subspace of dimension around 15. The plot on the right shows how the set of nonzero eigenvalues of the MSG iterates evolves over time, from which we can see that many of the extra dimensions are “wasted” on very small eigenvalues, corresponding to directions which leave the state matrix only a handful of iterations after they enter. This suggests that constraining k′t can lead to significant speedups and motivates capped MSG\nthe entropy-case analysis, and again there is no benefit.\nupdates discussed in the next section."
    }, {
      "heading" : "5 Capped MSG",
      "text" : "While, as was observed in the previous section, MSG’s iterates will tend to have ranks k′t smaller than d, they will nevertheless also be larger than k. For this reason, in practice, we recommend adding a hard constraint K on the rank of the iterates:\nmaximize : Ex∼D[xTMx] (5.1) subject to : M ∈ Rd×d, 0 M I\ntrM = k, rankM ≤ K\nWe will refer MSG where the projection is replaced with a projection onto the constraints of (5.1) (i.e. where the iterates are SGD iterates on (5.1)) as “capped MSG”. For similar reasons as discussed before, as long as K ≥ k, Problem 5.1 and Problem 3.2 have the same optimum, and it is achieved at a rank-k matrix, and the extra rank constraint in 5.1 is inactive at the optimum. However, the rank constraint does affect the iterates, especially since Problem 5.1 is no longer convex. Nonetheless if K > k (i.e. the hard rank-constraint K is strictly larger than the target rank k), we can easily check if we are at a global optimum of 5.1, and hence of 3.2: if the capped MSG algorithm converges to a solution of rank K, then the upper bound K should be increased. Conversely, if it has converged to a rank-deficient solution, then it must be the global optimum. There is thus an advantage in using K > k, and we recommend setting K = k + 1, as we do in our experiments, and increasing K only if a rank deficient solution is not found.\nSetting K = k, the only way to satisfy the trace constraint is to have all non-zero eigenvalues be equal to one, and (5.1) becomes identical to (3.1). The detour through the convex problem (3.2), allows us to increase the search rank K, allowing for more flexibility in the search, while still encouraging the desired rank k through the rank constraint."
    }, {
      "heading" : "5.1 Implementing the projection",
      "text" : "Implementing capped MSG is similar to implementing MSG (Algorithm 1) except for the projection step. Reasoning as in the proof of Lemma 3.2 shows that if M (t+1) =P (M ′) with M ′ = M (t) + ηxtxTt , then M (t) and M ′ are simultaneously diagonalizable, and therefore we can consider only how the projection acts on the eigenvalues. Hence, if we let σ′ be the vector of the eigenvalues of M ′, and suppose that there are more than K such eigenvalues, then there is a sizeK subset of σ′ such that applying Algorithm 2 to this set gives the projected eigenvalues. Since we perform only a rank-1 update at every iteration, we must check at most K possibilities, at a total cost of O(K2 logK) operations, with no effect on asymptotic runtime because Algorithm 1 requires O(K2d) operations."
    }, {
      "heading" : "5.2 Relationship to the incremental PCA method",
      "text" : "The capped MSG updates with K = k are similar to the incremental algorithm of Arora et al. [2012]. The incremental algorithm maintains a rank-k approximation of the covariance matrix with updates given by\nM (t+1) = Prank-k ( M (t) + xtx T t ) ,\nwhere the projection is onto the set of rank-k matrices. Unlike MSG, incremental updates do not have a step-size. Updates can be performed efficiently much in the same way as described in Section 3.2, by maintaining the eigendecomposition of the iterates.\nThe incremental algorithm was found to perform extremely well in practice– it was the best, in fact, among the compared algorithms [Arora et al., 2012].\nHowever, there exist cases in which the incremental algorithm can get stuck at a suboptimal solution. For example, If the data are drawn from a discrete distribution D which samples [ √ 3, 0]T with probability 1/3 and [0, √ 2]T with probability 2/3, and one runs the incremental algorithm with k = 1, then it will converge to [1, 0]T with probability 5/9, despite the fact that the maximal eigenvector is [0, 1]T . The reason for this failure is essentially that the orthogonality of the data interacts poorly with the low-rank projection: any update which does not entirely displace the maximal eigenvector in one iteration will be removed entirely by the projection, causing the algorithm to fail to make progress. Capped MSG algorithm with K > k, will not get stuck in such situations, using the additional “dimensions” to “search” in the new direction. Only as it becomes more confident in its current candidate, the trace of M will become increasingly concentrated on the top k directions. To illustrate this empirically, we generalized the toy example above and generated the data using the 32-dimensional “orthogonal” distribution described in Sec. 4. This distribution presents challenging test-cases for MSG, capped MSG as well as incremental algorithm. Figure 2 shows plots of individual runs of MSG, capped MSG with K = k + 1, the incremental algorithm, and Warmuth and Kuzmin’s algorithm, all based on the same sequence of samples drawn from the orthogonal distribution. We compare algorithms in terms of the suboptimality on the population objective based on the largest k eigenvalues of the state matrix M (t). The plots show the incremental algorithm getting stuck for k ∈ {1, 4}, and the others intermittently plateauing at intermediate solutions before beginning to again converge rapidly towards the optimum. This behavior is to be expected on the capped MSG algorithm, due to the fact that the dimension of the subspace stored at each iterate is constrained. However, it is somewhat surprising that MSG and Warmuth and Kuzmin’s algorithm behaved similarly, and barely faster than capped MSG."
    }, {
      "heading" : "6 Experiments",
      "text" : "We also compared the algorithms on the real-world MNIST dataset, which consists of 70, 000 binary images of handwritten digits of size 28 × 28, resulting in a dimensionality of 784. We pre-normalized the data by mean centering the feature vectors and scaling each feature by the product of its standard deviation and the data dimension, so that each feature vector is zero mean and unit norm in expectation. In addition to MSG, capped MSG, the incremental algorithm and Warmuth and Kuzmin’s algorithm, we also compare to a Grassmannian SGD algorithm of Balzano et al. [2010]. All algorithms except the incremental algorithm have a step-size parameter. In these experiments, we ran each algorithm with decreasing step sizes ηt = c/ √ t for c ∈ {2−12, 2−19, . . . , 25} and picked the best c, in terms of the average suboptimality over the run, on a validation set. Since we cannot evaluate the true population objective, we estimate it by evaluating on a held-out test set. We use 40% of samples in the dataset for training, 20% for validation (tuning step-size), and 40% for testing. We are\ninterested in learning a maximum variance subspace of dimension k ∈ {1, 4, 8} in a single “pass” over the training sample. In order to compare MSG, capped MSG, incremental and Warmuth and Kuzmin’s algorithm in terms of runtime, we calculate the dominant term in the computational complexity: ∑t s=1(k ′ s)\n2. The results are averaged over 100 random splits into train-validation-test sets.\nWe can see from Figure 3 that the incremental algorithm makes the most progress per iteration and is also the fastest of all algorithms. MSG is comparable to the incremental algorithm in terms of the the progress made per iteration. However, its runtime is slightly worse than the incremental because it will often keep a slightly larger representation (of dimension k′t) than the incremental algorithm. The capped MSG variant (with K = k + 1) is significantly faster– almost as fast as the incremental algorithm, while, as we saw in the previous section, being less prone to getting stuck. Warmuth and Kuzmin’s algorithm fares well with k = 1, but its performance drops for higher k. Inspection of the underlying data shows that, in the k ∈ {4, 8} experiments, it also tends to have a larger k′t than MSG in these experiments, and therefore has a higher cost-periteration. Grassmannian SGD performs better than Warmuth and Kuzmin, but much worse when compared with MSG and capped MSG."
    }, {
      "heading" : "7 Conclusions",
      "text" : "In this paper, we presented a careful development and analysis of MSG, a stochastic approximation algorithm for PCA, which enjoys good theoretical guarantees and offers a computationally efficient variant, capped MSG. We show that capped MSG is well-motivated theoretically and that it does not get stuck at a suboptimal solution. Capped MSG is also shown to have excellent empirical performance and it therefore is a much better alternative to the recently proposed incremental PCA algorithm of Arora et al. [2012]. Furthermore, we provided a cleaner interpretation of PCA updates of Warmuth and Kuzmin [2008] in terms of Matrix Exponentiated Gradient (MEG) updates and showed that both MSG and MEG can be interpreted as mirror descent algorithms on the same relaxation of the PCA optimization problem but with different distance generating functions."
    }, {
      "heading" : "A Proof of Lemma 3.2",
      "text" : "Lemma 3.2. Let M ′ ∈ Rd×d be a symmetric matrix, with eigenvalues σ′1, . . . , σ′d and associated eigenvectors v′1, . . . , v ′ d. If M = P (M ′) projects M ′ onto the feasible region of Problem 3.2 with respect to the Frobenius norm, then M will be the unique feasible matrix which has the same set of eigenvectors as M ′, with the associated eigenvalues σ1, . . . , σd satisfying:\nσi = max (0,min (1, σ ′ i + S))\nwith S ∈ R being chosen in such a way that d∑\ni=1\nσi = k.\nProof. The problem of finding M can be written in the form of a convex optimization problem as:\nminimize : ‖M −M ′‖2F subject to : 0 M I, trM = k.\nBecause the objective is strongly convex, and the constraints are convex, this problem must have a unique solution. Letting σ1, . . . , σd and v1, . . . , vd be the eigenvalues and associated eigenvectors of M , we may write the KKT first-order optimality conditions [Boyd and Vandenberghe, 2004] as:\n0 = M −M ′ + µI − d∑\ni=1\nαiviv T i + d∑ i=1 βiviv T i , (A.1)\nwhere µ is the Lagrange multiplier for the constraint trM = k, and αi, βi ≥ 0 are the Lagrange multipliers for the constraints 0 M and M I, respectively. The complementary slackness conditions are that αiσi = βi (σi − 1) = 0. In addition, M must be feasible.\nBecause every term in Equation A.1 except for M ′ has the same set of eigenvectors as M , it follows that an optimal M must have the same set of eigenvectors as M ′, so we may take vi = v ′ i, and write Equation A.1 purely in terms of the eigenvalues:\nσi = σ ′ i − µ+ αi − βi.\nComplementary slackness and feasibility with respect to the constraints 0 M I gives that if 0 ≤ σ′i − µ ≤ 1, then σi = σ′i − µ. Otherwise, αi and βi will be chosen so as to clip σi to the active constraint:\nσi = max (0,min (1, σ ′ i − µ)) .\nPrimal feasibility with respect to the constraint trM = k gives that µ must be chosen in such a way that trM = k, completing the proof."
    } ],
    "references" : [ {
      "title" : "Stochastic optimization for pca and pls",
      "author" : [ "Raman Arora", "Andrew Cotter", "Karen Livescu", "Nathan Srebro" ],
      "venue" : "In 50th Annual Allerton Conference on Communication, Control, and Computing,",
      "citeRegEx" : "Arora et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Arora et al\\.",
      "year" : 2012
    }, {
      "title" : "Online identification and tracking of subspaces from highly incomplete information",
      "author" : [ "Laura Balzano", "Robert Nowak", "Benjamin Recht" ],
      "venue" : "CoRR, abs/1006.4046,",
      "citeRegEx" : "Balzano et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Balzano et al\\.",
      "year" : 2010
    }, {
      "title" : "Mirror descent and nonlinear projected subgradient methods for convex optimization",
      "author" : [ "A. Beck", "M. Teboulle" ],
      "venue" : "Operations Research Letters,",
      "citeRegEx" : "Beck and Teboulle.,? \\Q2003\\E",
      "shortCiteRegEx" : "Beck and Teboulle.",
      "year" : 2003
    }, {
      "title" : "The tradeoffs of large scale learning",
      "author" : [ "Leon Bottou", "Olivier Bousquet" ],
      "venue" : "In NIPS’07,",
      "citeRegEx" : "Bottou and Bousquet.,? \\Q2007\\E",
      "shortCiteRegEx" : "Bottou and Bousquet.",
      "year" : 2007
    }, {
      "title" : "Convex Optimization",
      "author" : [ "Stephen Boyd", "Lieven Vandenberghe" ],
      "venue" : null,
      "citeRegEx" : "Boyd and Vandenberghe.,? \\Q2004\\E",
      "shortCiteRegEx" : "Boyd and Vandenberghe.",
      "year" : 2004
    }, {
      "title" : "Incremental singular value decomposition of uncertain data with missing values",
      "author" : [ "Matthew Brand" ],
      "venue" : "In ECCV,",
      "citeRegEx" : "Brand.,? \\Q2002\\E",
      "shortCiteRegEx" : "Brand.",
      "year" : 2002
    }, {
      "title" : "Exponentiated gradient algorithms for conditional random fields and max-margin markov networks",
      "author" : [ "Michael Collins", "Amir Globerson", "Terry Koo", "Xavier Carreras", "Peter L. Bartlett" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "Collins et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Collins et al\\.",
      "year" : 2008
    }, {
      "title" : "Efficient projections onto the l1-ball for learning in high dimensions",
      "author" : [ "John Duchi", "Shai Shalev-Shwartz", "Yoram Singer", "Tushar Chandra" ],
      "venue" : "In Proceedings of the 25th international conference on Machine learning,",
      "citeRegEx" : "Duchi et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Duchi et al\\.",
      "year" : 2008
    }, {
      "title" : "Streaming, memory-limited pca",
      "author" : [ "Ioannis Mitliagkas", "Constantine Caramanis", "Prateek Jain" ],
      "venue" : "URL http://users.ece.utexas.edu/~cmcaram/ pubs/Streaming-PCA.pdf. UT-Austin",
      "citeRegEx" : "Mitliagkas et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mitliagkas et al\\.",
      "year" : 2013
    }, {
      "title" : "Problem complexity and method efficiency in optimization",
      "author" : [ "Arkadi Nemirovski", "David Yudin" ],
      "venue" : null,
      "citeRegEx" : "Nemirovski and Yudin.,? \\Q1983\\E",
      "shortCiteRegEx" : "Nemirovski and Yudin.",
      "year" : 1983
    }, {
      "title" : "Robust stochastic approximation approach to stochastic programming",
      "author" : [ "Arkadi Nemirovski", "Anatoli Juditsky", "Guanghui Lan", "Alexander Shapiro" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "Nemirovski et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Nemirovski et al\\.",
      "year" : 2009
    }, {
      "title" : "On stochastic approximation of the eigenvectors and eigenvalues of the expectation of a random matrix",
      "author" : [ "Erkki Oja", "Juha Karhunen" ],
      "venue" : "Journal of Mathematical Analysis and Applications,",
      "citeRegEx" : "Oja and Karhunen.,? \\Q1985\\E",
      "shortCiteRegEx" : "Oja and Karhunen.",
      "year" : 1985
    }, {
      "title" : "Optimal unsupervised learning in a single-layer linear feedforward neural network",
      "author" : [ "Terence D. Sanger" ],
      "venue" : "Neural Networks,",
      "citeRegEx" : "Sanger.,? \\Q1989\\E",
      "shortCiteRegEx" : "Sanger.",
      "year" : 1989
    }, {
      "title" : "SVM optimization: Inverse dependence on training set size",
      "author" : [ "Shai Shalev-Shwartz", "Nathan Srebro" ],
      "venue" : "In ICML’08,",
      "citeRegEx" : "Shalev.Shwartz and Srebro.,? \\Q2008\\E",
      "shortCiteRegEx" : "Shalev.Shwartz and Srebro.",
      "year" : 2008
    }, {
      "title" : "Stochastic methods for l1 regularized loss minimization",
      "author" : [ "Shai Shalev-Shwartz", "Ambuj Tewari" ],
      "venue" : "In Proceedings of the 26th Annual International Conference on Machine Learning,",
      "citeRegEx" : "Shalev.Shwartz and Tewari.,? \\Q2009\\E",
      "shortCiteRegEx" : "Shalev.Shwartz and Tewari.",
      "year" : 2009
    }, {
      "title" : "Pegasos: Primal Estimated sub-GrAdient SOlver for SVM",
      "author" : [ "Shai Shalev-Shwartz", "Yoram Singer", "Nathan Srebro" ],
      "venue" : "In ICML’07,",
      "citeRegEx" : "Shalev.Shwartz et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Shalev.Shwartz et al\\.",
      "year" : 2007
    }, {
      "title" : "On the universality of online mirror descent",
      "author" : [ "N. Srebro", "K. Sridharan", "A. Tewari" ],
      "venue" : "Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Srebro et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Srebro et al\\.",
      "year" : 2011
    }, {
      "title" : "Randomized online PCA algorithms with regret bounds that are logarithmic in the dimension",
      "author" : [ "Manfred K. Warmuth", "Dima Kuzmin" ],
      "venue" : "Journal of Machine Learning Research (JMLR),",
      "citeRegEx" : "Warmuth and Kuzmin.,? \\Q2008\\E",
      "shortCiteRegEx" : "Warmuth and Kuzmin.",
      "year" : 2008
    }, {
      "title" : "Because the objective is strongly convex, and the constraints are convex, this problem must have a unique solution",
      "author" : [ "M I", "trM = k" ],
      "venue" : "Letting σ1,",
      "citeRegEx" : "I and k.,? \\Q2004\\E",
      "shortCiteRegEx" : "I and k.",
      "year" : 2004
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "In an empirical study of stochastic approximation methods for PCA, a heuristic “incremental” method showed very good empirical performance [Arora et al., 2012].",
      "startOffset" : 139,
      "endOffset" : 159
    }, {
      "referenceID" : 2,
      "context" : "“batch” methods) [Bottou and Bousquet, 2007, Shalev-Shwartz and Srebro, 2008]. A similar argument was also made in the context of stochastic optimization, where Nemirovski et al. [2009] argues for stochastic approximation (SA) approaches over ERM.",
      "startOffset" : 18,
      "endOffset" : 186
    }, {
      "referenceID" : 0,
      "context" : "In an empirical study of stochastic approximation methods for PCA, a heuristic “incremental” method showed very good empirical performance [Arora et al., 2012]. However, no theoretical guarantees or justification were given for incremental PCA. In fact, it was shown that for some distributions it can converge to a suboptimal solution with high probability (see Section 5.2 for more about this “incremental” algorithm). Also relevant is careful theoretical work on online PCA by Warmuth and Kuzmin [2008], in which an online regret guarantee was established.",
      "startOffset" : 140,
      "endOffset" : 506
    }, {
      "referenceID" : 17,
      "context" : "This follows from the following result of Warmuth and Kuzmin [2008].",
      "startOffset" : 42,
      "endOffset" : 68
    }, {
      "referenceID" : 17,
      "context" : "1 (Rounding [Warmuth and Kuzmin, 2008]).",
      "startOffset" : 12,
      "endOffset" : 38
    }, {
      "referenceID" : 17,
      "context" : "1 of Warmuth and Kuzmin [2008] shows how to efficiently find such a convex combination.",
      "startOffset" : 5,
      "endOffset" : 31
    }, {
      "referenceID" : 9,
      "context" : "Analyzing MSG is straightforward using the standard SGD analysis [Nemirovski and Yudin, 1983]:",
      "startOffset" : 65,
      "endOffset" : 93
    }, {
      "referenceID" : 9,
      "context" : "Standard SGD analysis of Nemirovski and Yudin [1983] yields that",
      "startOffset" : 25,
      "endOffset" : 53
    }, {
      "referenceID" : 7,
      "context" : "1Note that our projection problem onto the capped simplex, even when seen in the vector setting, is substantially different from Duchi et al. [2008]. We project onto the set {0 ≤ σ ≤ 1, ‖σ‖1 = k} in (3.",
      "startOffset" : 129,
      "endOffset" : 149
    }, {
      "referenceID" : 7,
      "context" : "1Note that our projection problem onto the capped simplex, even when seen in the vector setting, is substantially different from Duchi et al. [2008]. We project onto the set {0 ≤ σ ≤ 1, ‖σ‖1 = k} in (3.2) and {0 ≤ σ ≤ 1, ‖σ‖1 = k, ‖σ‖0 ≤ K} in (5.1) whereas Duchi et al. [2008] project onto {0 ≤ σ, ‖σ‖1 = k}.",
      "startOffset" : 129,
      "endOffset" : 278
    }, {
      "referenceID" : 2,
      "context" : "Since M is constrained by its trace, and not by its Frobenius norm, it is tempting to consider mirror descent (MD) [Beck and Teboulle, 2003] instead of SGD updates for solving Problem 3.",
      "startOffset" : 115,
      "endOffset" : 140
    }, {
      "referenceID" : 16,
      "context" : "Recall that the Mirror Descent updates depend on a choice of “potential function” Ψ(·) which should be chosen according to the geometry of the feasible set and the subgradients [Srebro et al., 2011].",
      "startOffset" : 177,
      "endOffset" : 198
    }, {
      "referenceID" : 17,
      "context" : "This leads to multiplicative updates which we refer to as Matrix Exponentiated Gradient (MEG) update similar to those presented by [Warmuth and Kuzmin, 2008].",
      "startOffset" : 131,
      "endOffset" : 157
    }, {
      "referenceID" : 0,
      "context" : "The capped MSG updates with K = k are similar to the incremental algorithm of Arora et al. [2012]. The incremental algorithm maintains a rank-k approximation of the covariance matrix with updates given by",
      "startOffset" : 78,
      "endOffset" : 98
    }, {
      "referenceID" : 0,
      "context" : "The incremental algorithm was found to perform extremely well in practice– it was the best, in fact, among the compared algorithms [Arora et al., 2012].",
      "startOffset" : 131,
      "endOffset" : 151
    }, {
      "referenceID" : 1,
      "context" : "In addition to MSG, capped MSG, the incremental algorithm and Warmuth and Kuzmin’s algorithm, we also compare to a Grassmannian SGD algorithm of Balzano et al. [2010]. All algorithms except the incremental algorithm have a step-size parameter.",
      "startOffset" : 145,
      "endOffset" : 167
    }, {
      "referenceID" : 0,
      "context" : "Capped MSG is also shown to have excellent empirical performance and it therefore is a much better alternative to the recently proposed incremental PCA algorithm of Arora et al. [2012]. Furthermore, we provided a cleaner interpretation of PCA updates of Warmuth and Kuzmin [2008] in terms of Matrix Exponentiated Gradient (MEG) updates and showed that both MSG and MEG can be interpreted as mirror descent algorithms on the same relaxation of the PCA optimization problem but with different distance generating functions.",
      "startOffset" : 165,
      "endOffset" : 185
    }, {
      "referenceID" : 0,
      "context" : "Capped MSG is also shown to have excellent empirical performance and it therefore is a much better alternative to the recently proposed incremental PCA algorithm of Arora et al. [2012]. Furthermore, we provided a cleaner interpretation of PCA updates of Warmuth and Kuzmin [2008] in terms of Matrix Exponentiated Gradient (MEG) updates and showed that both MSG and MEG can be interpreted as mirror descent algorithms on the same relaxation of the PCA optimization problem but with different distance generating functions.",
      "startOffset" : 165,
      "endOffset" : 280
    } ],
    "year" : 2013,
    "abstractText" : "We study PCA as a stochastic optimization problem and propose a novel stochastic approximation algorithm which we refer to as “Matrix Stochastic Gradient” (MSG), as well as a practical variant, Capped MSG. We study the method both theoretically and empirically.",
    "creator" : "LaTeX with hyperref package"
  }
}