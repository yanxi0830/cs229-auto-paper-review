{
  "name" : "1709.02555.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Causality-Aided Falsification",
    "authors" : [ "Takumi Akazaki", "Yoshihiro Kumazawa", "Ichiro Hasuo" ],
    "emails" : [ "akazaki@ms.k.u-tokyo.ac.jp", "kumazawa@is.s.u-tokyo.ac.jp", "hasuo@nii.ac.jp" ],
    "sections" : [ {
      "heading" : null,
      "text" : "L. Bulwahn, M. Kamali, S. Linker (Eds.): First Workshop on Formal Verification of Autonomous Vehicles (FVAV 2017). EPTCS 257, 2017, pp. 3–18, doi:10.4204/EPTCS.257.2\nc© T. Akazaki, Y. Kumazawa, and I. Hasuo This work is licensed under the Creative Commons Attribution License.\nCausality-Aided Falsification\nTakumi Akazaki ∗\nThe University of Tokyo, Japan JSPS Research Fellow\nakazaki@ms.k.u-tokyo.ac.jp\nYoshihiro Kumazawa The University of Tokyo, Japan\nkumazawa@is.s.u-tokyo.ac.jp\nIchiro Hasuo †\nNational Institute of Informatics, Tokyo, Japan\nhasuo@nii.ac.jp\nFalsification is drawing attention in quality assurance of heterogeneous systems whose complexities are beyond most verification techniques’ scalability. In this paper we introduce the idea of causality aid in falsification: by providing a falsification solver—that relies on stochastic optimization of a certain cost function—with suitable causal information expressed by a Bayesian network, search for a falsifying input value can be efficient. Our experiment results show the idea’s viability."
    }, {
      "heading" : "1 Introduction",
      "text" : "Falsification In computer science, verification refers to the task of giving a mathematical proof to the claim that the behavior of a system M satisfies a desired property ϕ (called a specification), under any circumstances (such as choices of input to the system M ). A mathematical proof thus obtained gives a level of confidence that is fundamentally different from empirical guarantees given by testing.\nExtensive research efforts have yielded a number of effective verification techniques and they have seen successful real-world applications. At the same time, however, it is also recognized that large-scale heterogeneous systems are still beyond the scalability of most of these verification techniques. Notable among such are cyber-physical systems (CPSs) that exhibit not only discrete digital dynamics but also continuous physical dynamics. Imagine a car today: it contains not only dozens of chips (like ECUs) but also continuous dynamics (wheels, suspensions, internal combustion, etc.).\nIt is in this CPS context that the idea of falsification is found its use [17].\nThe falsification problem • Given: a model M (a function from an input signal to an output signal), and a specifi-\ncation ϕ (a temporal formula) • Answer: a critical path, that is, an input signal σin such that the corresponding output\nM (σin) does not satisfy ϕ\nTwo benefits of falsification are particularly appealing. For one, a system model M can be totally a black box: once M as a function σin 7→M (σin) is given as an oracle, we can check if an educated guess σin is a solution or not—without knowing M ’s internal working. This is an advantage given that many CPSs do have black-box components: they can come from external suppliers, or they can be physical dynamics too complex to mathematically model (typically one uses look-up tables to describe such).\n∗Supported by Grants-in-Aid for JSPS Fellows No. 15J09877. †Supported by JST ERATO HASUO Metamathematics for Systems Design Project (No. JPMJER1603), and JSPS Grant-\nin-Aid No. 15KT0012.\nAlgorithm 1 Falsification by optimization, with a cost function fϕ Input: σ0 . The initial guess\n1: v0 := fϕ (M (σ0)); . vi = fϕ (M (σi)) is the score of the input vi 2: for i = 1 . . .N do . N is the greatest number of iteration\n3: σi := argminσ ( fϕ (M (σ)) ∣∣∣∣ under the previous observations(σ0,v0),(σ1,v1), . . . ,(σi−1,vi−1) ) ; 4: vi := fϕ (M (σi)); 5: if vi < 0 then return σi; 6: end if . Falsification succeeded, because we assume fϕ (τ)< 0 implies τ 6|= ϕ 7: i := i+1; 8: end for\nAnother appealing feature of falsification is its affinity with machine learning (ML) and optimization techniques. In automatic verification techniques the greatest challenge is state-space explosion: the size of the input space for σin grows exponentially with respect to its dimension, often to the extent that exhaustive search in it is no longer possible. Recent surges in ML and optimization algorithms can offer potent countermeasures against this curse of dimensionality: specifically, after observing output M (σ1), . . . ,M (σn) for input σ1, . . . ,σn, those algorithms can “learn” from these previous attempts and suggest an input signal σn+1 with which M (σn+1) 6|= ϕ is likely.\nOne can say that falsification is after all adaptive testing: most falsification solvers rely on stochastic guess; hence their failure do not prove “M (σin) |= ϕ for every σin.” However in many real-world scenarios falsification is as good as one gets, because of systems’ complexity and their black-box components within. Existing stochastic optimization-based solvers (such as S-TaLiRo [8] and BREACH [13]) have shown striking performance, too, scaling up to various Simulink diagrams from automotive applications. Moreover, falsification has special appeal to real-world engineers: while it takes certain familiarity to come to appreciate correctness proofs, counterexamples discovered by falsification easily convince engineers that there are issues to be resolved. Search of Cost Functions A technical cornerstone that set off the study of falsification is robust semantics of temporal formulas [14, 15]. With CPS application in mind we assume that input and output of our system model M are given by (time-variant) signals. For them it is standard to specify properties using some temporal logic, such as metric interval temporal logic (MITL) [7] and signal temporal logic (STL) [20]. In robust semantics [14, 15] a signal σ and a formula ϕ are assigned a continuous truth value Jσ , ϕK ∈ R that designates how robustly the formula is satisfied. This departure from the conventional Boolean semantics (where Jσ , ϕK ∈ {tt, ff}) allows one to adopt a hill climbing-style optimization algorithm to look for a falsifying input signal.\nAlgorithm 1 is a high-level description of falsification by optimization. Here a cost function fϕ carries a signal (output of the system M ) to a real; we assume that its value is linked with satisfaction of ϕ , that is specifically, fϕ(τ)< 0 implies τ 6|= ϕ . We assume that the value of fϕ for a given input can be effectively computed; we assume the same for the function M . Still in Line 3 the true solution may not be available since the global structure of M is unknown—this reflects our black-box view on M . Therefore in Line 3 we make a guess based on the previous trials.\nThe robust semantics of temporal formulas in [14, 15] is a prototype of such a cost function (Algorithm 1). Subsequently in the study of falsification, search of better cost functions has been an important topic. For example, sometimes time robustness [14]—as opposed to space robustness in the original work [15]—yields smoother hills to climb down, aiding optimization. Combination of space and time robustness is pursued in [5], where they enrich logics with averaged modalities to systematically enhance expressivity. Additional bias is put on cost functions in [12] so that search for falsifying input\ncovers a greater number of discrete modes of a system M . After all, the question here is how to enrich cost functions, extracting additional information from a system M and/or a specification ϕ .\nContribution: Causality Aid in Falsification In this paper we build on the observations in [4] and propose to aid falsification using causal information. We lay out the idea using a simple example.\nExample 1 (incremental counter) Consider the pseudocode shown on the right. We think of: i0, i1, . . . iN ∈ [−1,1] as the values of a time-variant input signal i at time t = 0,1, . . . ,N, respectively; Lines 1–10 as a system M that takes such input and returns the value of cnt as output; and the assertion cnt≤N as a specification ϕ , that is 2[N,N](cnt≤N) in temporal logic. It is clear that, to falsify ϕ , all the input values i0, i1, . . . , iN must lie in (−0.2,0.2); otherwise the counter is reset and never reaches cnt= N +1.\nInput: i0, i1, . . . , iN ∈ [−1,1] Output: cnt\n1: t := 0;cnt := 0 2: while t ≤ N do 3: flag := (|it |< 0.2); 4: if flag then 5: cnt := cnt+1 6: else 7: cnt := 0; 8: end if 9: t := t +1;\n10: end while 11: assert(cnt≤ N)\nNow consider solving the falsification problem here. It turns out that existing falsification solvers have hard time in doing so: besides the apparent hardness of the problem (following the uniform distributions the success probability would be 0.2N), there is the following “causal” reason for the difficulty.\nAssume iN 6∈ (−0.2,0.2), meaning that cnt is reset to 0 at the last moment. In this case the earlier input values i0, i1, . . . , iN−1 have no effect in the final output cnt, nor in the robust semantics of the specification 2[N,N](cnt ≤ N). Therefore there is no incentive for stochastic optimization solvers to choose values i0, . . . , iN−1 from (−0.2,0.2). More generally, desired bias is imposed on earlier input values i0, . . . , ik only after later input values ik+1, . . . , iN have been suitably fixed. Given the system (the above program) as a black box and the specification 2[N,N](cnt ≤ N) alone, there is no way for optimization solvers to know such causal dependency.\nOur enhancement of falsification algorithms consists of leveraging causal information expressed as Bayesian networks. See Fig. 1, where we fix N = 5 for presentation. The Bayesian network expresses causal dependence of the original specification ϕ = ϕ5 on other specifications ϕ0, . . . ,ϕ4. The newly introduced specifications ϕi = [i,i](cnt ≤ i), for each i = 0, . . . ,4, express that the counter cnt has already been reset by time i. Therefore in order to falsify ϕ5, i.e. to keep incrementing cnt, these additional specifications must be falsified, too. The last observation is expressed in the Bayesian network in Fig. 1, specifically in the conditional probabilities Pr ( Jϕi+1K = ff | JϕiK = tt ) = 0.\nNow our falsification algorithm looks not only at ϕ5 but\nalso at the other predicates ϕ0, . . . ,ϕ4. This way we successfully impose bias on earlier input values i0, . . . , i4 to lie in (−0.2,0.2)—as demonstrated by our experimental results later.\nFollowing the idea illustrated in the last example, our main contribution in this paper is a causalityaided falsification algorithm that uses Bayesian networks of temporal formulas as input on the specification side. Such a Bayesian network can be derived from an original specification ϕ alone; they can also be derived through inspection of a system model M ; or we can use both ϕ and M . In order to efficiently leverage the causal information expressed by a Bayesian network, we follow [9,10] and use variations of Gaussian process optimization as our optimization algorithms (Line 3 of Algorithm 1). The feature that they allow to guess both average and variance (see §2.2) turns out to be particularly useful. We imple-\nmented the algorithm; our experimental results, although preliminary, seem to support the effectiveness of our approach.\nGeneral methodologies of deriving such Bayesian networks are outside the paper’s focus, although we do have some preliminary ideas and we exploited them for our current examples. One is the use of probabilistic predicate transformers that are a classic topic in semantics [18, 19, 22] and are shed fresh light on in the context of probabilistic programming languages (see e.g. [23]). This idea follows the earlier observations in [6]; it successfully generates the Bayesian network in Fig. 1 for Example 1. Another idea is parse tree-like decomposition of an original temporal formula ϕ; we decorate the resulting tree with conditional probabilities that we learn through sampling. These methods will be described in our forthcoming papers.\nRelated Work Besides search of better cost functions, an important direction in the study of falsification is improving optimization algorithms (that are used in Line 3 of Algorithm 1). In the falsification literature many different algorithms have been used and studied: they include simulated annealing, antcolony optimization, the cross-entropy method, the Nelder-Mead algorithm, and so on [8,13,25] . In [11] a discrete algorithm of Tabu search is employed for enhanced coverage.\nYet another important direction is multiple shooting falsification [27, 28] where, unlike single shooting approaches like in this paper, a bunch of trajectories are investigated in a single iteration relying on suitable abstraction of a system model and/or a specification. We believe our idea of causality aid in falsification is orthogonal to the choice between single and multiple shooting; we will study as future work the effect of causality in multiple shooting falsification."
    }, {
      "heading" : "2 Backgrounds",
      "text" : ""
    }, {
      "heading" : "2.1 STL and Robust Semantics",
      "text" : "Here we present signal temporal logic (STL) [20] as our formalism for expressing (original, without causal information) specifications. We also present its robust semantics [14] that give the prototype of the cost function fϕ in Algorithm 1. Our cost function will be derived from the robust semantics of the formulas in a Bayesian network. At the same time we emphasize that our methodology of causality-aided falsification does not depend on the specific underlying specification formalism of STL.\nDefinition 2.1 (syntax of STL) The set of STL formulas are recursively defined as follows.\nϕ ::= g(y)> 0 | ¬ϕ | ϕ1∨ϕ2 | ϕ1 UI ϕ\nHere g(y) is some real-value function over the set of variables y = {y1, . . . ,yn}, and I is a closed nonsingular interval in R≥0.\nWe also introduce the following standard temporal operators as abbreviations: the eventually operator 3Iϕ , (∞ > 0)UI ϕ and the always operator Iϕ , ¬3I¬ϕ .\nDefinition 2.2 (Boolean semantics of STL) Let σy : R≥0→Rn be a signal, that is, a function that maps time τ to the values σy(τ) of the variables y at time τ . We define the (Boolean) validity of an STL formula\nover a signal σy, as follows. Here σ τy stands for the time-shifted signal such that σ τy (τ ′), σy(τ + τ ′).\nσy g(y)> 0 def.⇐⇒ the inequality g(σy(0))> 0 holds σy ¬ϕ def.⇐⇒ σy 2 ϕ σy ϕ1∨ϕ2 def.⇐⇒ σy ϕ1 or σy ϕ2 σy ϕ1 UI ϕ2 def.⇐⇒ ∃τ ∈ I. ( σ τy ϕ2 and ∀τ ′ ∈ [0,τ].σ τ ′ y ϕ1\n) The following “quantitative refinement” of the semantics of STL initiated the research program of\nfalsification by optimization [14, 15].\nDefinition 2.3 (robust semantics of STL) For a signal σy and an STL formula ϕ , we define the robustness Jσy, ϕK ∈ R∪{∞,−∞} inductively as follows. Here u and t denote infimums and supremums of real numbers, respectively.\nJσy, g(y)> 0K , g(σy(0)) Jσy, ¬ϕK , −Jσy, ϕK Jσy, ϕ1∨ϕ2K , Jσy, ϕ1Kt Jσy, ϕ2K Jσy, ϕ1 UI ϕ2K , ⊔ τ∈I ( Jσ τy , ϕ2Ku d τ ′∈[0,t]Jσ τ ′ y , ϕ1K\n) Note that the sign of robustness coincides with the Boolean semantics. That is, Jσy, ϕK > 0 implies σy ϕ , and Jσy, ϕK < 0 implies σy 2 ϕ . Conversely, σy ϕ implies Jσy, ϕK ≥ 0, and σy 2 ϕ implies Jσy, ϕK≤ 0."
    }, {
      "heading" : "2.2 Gaussian Process Optimization",
      "text" : "In this paper we follow the workflow in Algorithm 1, deriving the cost function fϕ in it from a Bayesian network. For the optimization step (Line 3 of Algorithm 1) we use Gaussian process optimization— we follow [4, 9, 10] about this choice. It has a feature that it suggests the global shape of an unknown function; this feature turns out to be convenient for our purpose of integrating causal information in falsification. We present a brief review of the topic; see e.g. [24] for details."
    }, {
      "heading" : "2.2.1 Gaussian Process Regression",
      "text" : "Let f be an unknown function, from a certain input domain to the set of real numbers, about which we wish to infer certain properties. (For Algorithm 1 we would take f = fϕ(M ( ))). In Gaussian process regression the shape of f is estimated assuming that f is a probabilistic process called a Gaussian process.\nWe start with some formal definitions. For more detail, see e.g. [24].\nNotation 2.4 We let N (µ,k) stand for the probability density function of the multivariate Gaussian distribution whose mean vector is µ and covariance matrix is k.\nDefinition 2.5 (Gaussian process) A Gaussian process is a family of probabilistic variables (zx)x∈X such that each of its finite subset (zx1 , . . . ,zxt ) has a joint Gaussian distribution. A Gaussian process is determined by a pair (µ,k) of its mean function µ : X→ R and its covariance function k : X×X→ R; this Gaussian process is denoted by GP(µ,k). For this we have\n(zx1 , . . . ,zxt ) > ∼N (µ,k) where µ i = µ(xi) and ki j = k(xi,x j)\nfor each finite subset {x1, . . . ,xt} of X. We write GP(µ,k)(x1, . . . ,xt) for the above multivariate Gaussian distribution N (µ,k).\nIn Fig. 2 is how an unknown function f can be guessed by Gaussian processes. The blue pipe designates the estimated values of the unknown function f : the farther input x is from the observed points, the thicker the pipe is (that means bigger uncertainty).\nIn the regression of f using Gaussian processes, a choice of a covariance function k : X×X→ R determines smoothness of f . A common template for covariance functions is the squared-exponential kernel function kl(x,x′) , exp(−l · ‖x− x′‖2/2), where l is so-called the length scale parameter. In practice, we pick a good length scale parameter by cross validation. As we see in Fig. 2, the choice of a covariance function yields the following tendencies in Gaussian process regression:\n• The bigger the distance ‖x−x′‖ is, the smaller the covariance is, thus the harder it gets to estimate the value fϕ(x) from the observation of the value fϕ(x′).\n• Covariance is smaller too when the length scale parameter l is bigger.\nOne advantage of Gaussian process regression is that, given a set of observations, the posterior process is described analytically. Let random variables (zx)x∈X obey a prior Gaussian process GP(µ,k); and D = { (x1,z1), . . . ,(xt ,zt) } be a set of observations. Then the posterior distribution, denoted by GP(µ,k;D), is given by the Gaussian process GP(µ ′,k′), where\nµ ′(x) = µ(x)+kD(x)kDD−1 ( [z1 . . .zt ]>− [µ(x1) . . .µ(xt)]> ) ,\nk′(x,x′) = k′(x,x′)−kD(x)kDD−1kD(x′)>.\nHere kD(x) = [k(x1,x) . . .k(xt ,x)], and kDD is a t×t matrix whose i, j-component is k(xi,x j). In practice, given observed data D = { (x1, f (x1)), . . . ,(xt , f (xt)) } and a covariance kernel function k, we estimate the function f as GP(0,k;D) where 0 denotes the function constantly zero."
    }, {
      "heading" : "2.2.2 Gaussian Process Optimization and Acquisition Function",
      "text" : "Gaussian process regression allows us to predict, based on observations in D, the value f (x) for each input x as a normal distribution GP(µ,k)(x). To complete an optimization scenario, we wish to pick a candidate x ∈ X for which f (x) is small.\nIt is well-known that, for such choice, a balance is important between exploration (i.e. bias toward a bigger variance k(x,x)) and exploitation (bias toward a smaller expected value µ(x)). A criterion for this balance is called an acquisition function—we pick x at which the acquisition function is minimum. Assuming that an acquisition function ψ(x;GP(µ,k)) has been fixed, the whole procedure for Gaussian process optimization can be described as in Algorithm 2. Note that, in Line 3 of Algorithm 2, we usually employ another optimization solving method (such as simulated annealing).\nIn falsification, our goal would be to find x such that f (x) < 0. As a natural choice of acquisition functions, we focus on the following probability in this paper.\nDefinition 2.6 (Probability of Satisfaction)\nψ(x;GP(µ,k)) , PrGP(µ,k)( f (x)> 0) (1)\nHere PrGP(µ,k)( f (x)> c) is an abbreviation of Pr( f (x)> c | f (x)∼GP(µ,k)(x)).\nWe write GP-PSat for Algorithm 2 under ψ as an acquisition function; Fig. 3 illustrates how it works.\nThe acquisition functions we will use are extension of this ψ . We note, however, that this acquisition function is commonly known as “pure and impractical” in the field of Gaussian process optimization. More sophisticated acquisition functions that are known include probability improvement, expected improvement [21], upper confidence bound [26] and so on. At the time of writing it is not clear how these acquisition functions can be used as part of our framework in §4.\nAlgorithm 2 Gaussian process optimization GPOptimization(D,k,ψ) Input: a covariance function k : X×X→R; an acquisition function ψ; and an initial data set D= {(x′1, f (x′1)), . . . ,(x′s, f (x′s))} Output: input x ∈ X for which f (x) is small\n1: for t = 1,2, . . . do 2: GP(µ ′,k′) = GP(0,k;D); . Estimate the unknown function f 3: xt = argminx∈X ψ(x;GP(µ ′,k′)); . Choose new sample input 4: D = D∪{(xt , f (xt))}; . Observe the corresponding output 5: end for 6: return xt"
    }, {
      "heading" : "3 Causality in Falsification: Further Examples",
      "text" : "In addition to Example 1, we shall exhibit two more examples of falsification problems; for each, we introduce a Bayesian network that encodes suitable causal information, too. The latter will be exploited in our causality-aided algorithm in §4."
    }, {
      "heading" : "3.1 Example Model 2: Coincidental Sine Waves",
      "text" : "Let us consider the model in Fig. 4. In this simple model there are four sine waves x1(t), . . . ,x4(t) of different frequency, and we pick their initial phases i1, . . . , i4 as input of the system.\nAs a specification, we pick the following formula—it is falsified when the peaks of four sine waves correspond.\nϕ ≡ [0,10]( ∨\ni=1,...,4\nxi < 0.99) (2)\nWe see that falsifying ϕ with pure random sampling is difficult because ϕ is false only in rare cases. What is worth, the (conventional) robustness of ϕ does not always guide us to the counterexamples. Example 2 Let us consider the subformula ∨\ni=1,...,4 xi < 0.99. When we compare the values (x1,x2,x3,x4)= (1,1,1,0) and (0,0,0,1), we could say the former is “closer” to falsifying the subformula—the three out of four sine waves simultaneously at a peak. However, these robustness values are the same 0.99 in both cases.\nIn this case, we sometimes divide the difficulty into small pieces—first get x1 and x2 simultaneously at a peak; then get x3 and x4 at a peak; finally, try to make them synchronize. Let us introduce formulas ϕ12 and ϕ34 such that falsifying them means matching the peak of x1,x2, and x3,x4 respectively. Decomposing ϕ into ϕ12 and ϕ34 might help us in falsification for the following reasons.\n• The small formulas ϕ12 and ϕ34 are much easier to falsify compared to ϕ .\n• Moreover, the robustness mapping fϕ12(M ( )) and fϕ34(M ( )) have much simpler dynamics than the one of the original specification ϕ , so the Gaussian process regression for the small formulas tend to work better than the one for ϕ .\nThe Bayesian network B in Fig. 5 is devised to express this intuition. For example, the formula ϕ is true with probability 1 when either ϕ12 or ϕ34, otherwise ϕ becomes false with small probability 0.1. As shown in Fig. 6, the conditional joint distribution PrB(− | JϕK = ff) tells us the fact that ϕ is false only if both ϕ12 and ϕ34 are false.\nInput: i1, . . . i4 ∈ [0,1] Output: x1(t), . . . ,x4(t)\nfor each t ∈ R≥0 x1(t) = sin(1.1t + i1); x2(t) = sin(1.2t + i2); x3(t) = sin(1.3t + i3); x4(t) = sin(1.4t + i4);\nFigure 4: System model for §3.1\nϕ: (p12∨ p34)\nϕ12: p12\nϕ34: p34\ntt ff 0.9 0.1\ntt ff 0.9 0.1\nϕ12 ϕ34 tt ff tt tt 1 0 tt ff 1 0 ff tt 1 0 ff ff 0.9 0.1\nwhere( p12 ≡ x1 < 0.99∨ x2 < 0.99 p34 ≡ x3 < 0.99∨ x4 < 0.99 )\nFigure 5: Bayesian network for §3.1\nϕ ∼= ϕv∧ϕω\nϕv: v < 120\nϕω : ω < 4780\ntt ff 0.99 0.01\ntt ff 0.9 0.1\nϕv ϕω tt ff tt tt 1 0 tt ff 0 1 ff tt 0 1 ff ff 0 1\nFigure 7: Bayesian network for §3.2\nPrB(−) ϕv ϕw ϕ tt tt tt 0.891 tt tt ff 0 tt ff tt 0 tt ff ff 0.099 ff tt tt 0 ff tt ff 0.009 ff ff tt 0 ff ff ff 0.001\nPrB(− | JϕK = ff) ϕv ϕw ϕ tt tt tt 0 tt tt ff 0 tt ff tt 0 tt ff ff 0.908 ff tt tt 0 ff tt ff 0.083 ff ff tt 0 ff ff ff 0.009\nPrB(−) ϕv = tt 0.99 ϕv = ff 0.01 ϕw = tt 0.9 ϕw = ff 0.1 ϕ = tt 0.891 ϕ = ff 0.109\nPrB(− | JϕK = ff) ϕv = tt 0.908 ϕv = ff 0.092 ϕw = tt 0.083 ϕw = ff 0.917 ϕ = tt 0 ϕ = ff 1\nFigure 8: Unconditional/conditional joint distributions in the Bayesian network of Fig. 7"
    }, {
      "heading" : "3.2 Example Model 3: Automatic Transmission",
      "text" : "The last example is the automatic transmission model from the benchmark of temporal logic verification [16]. This model is still miniature, but an elaborate mimicry of the systems in the real world hence suitable for our purpose.\nAs a specification ϕ to falsify, we use the following formula. It is taken from [16] (it is ϕAT2 there).\n(v < 120∧ω < 4780)\nHere the variable v and ω stand for the speed of the vehicle and the angular velocity of the engine rotation respectively.\nWe know that we can falsify ϕ either by violating the speed limit (v < 120) or the engine rotation limit (ω < 4780). In this model, ω takes the values in the range around [0,4800] while v does around [0,120]. Note that their scales are very different: hence in the most of the cases, the robustness of the ω-component is likely to be shadowed by the one of the v-component. As a consequence, we expect that conventional falsification solver only try to falsify by the violation of the speed limit v < 120.\nThe Bayesian network annotation is also effective in such a situation. That is, we can add the information about “which is more likely to happen, the violation of the speed and the rotation limit.” (In actual deployment such insights will be provided by engineers’ domain knowledge.) Let assume that the probabilities of the violation of the speed and the rotation limit are 0.01 and 0.1 respectively. This information is expressed in Fig. 7, where the conditional probabilities for ϕ simply encode logical relationship (note that ϕ is semantically equivalent to ϕv∧ϕω ) however, the probabilities at leaves reflect the above insight.\nRemark 3.1 In §3.1 and §3.2, as an indicator of robustness, we employed the (space) robust semantics of STL in [14] and shown that it is not sensitive enough for some falsification scenarios. In contrast to [14], the metric-based robustness of MITL in [15] has a degree of freedom to capture the lacked notions. For example in §3.2, we could solve the falsification problem more efficiently if we could re-scale v and ω appropriately, and this re-scaling is nothing but the defining the metric space in [15]. However, defining such a metric space itself is challenging and needs expert’s domain knowledge—similarly as our framework needs suitable causal information. We expect that our causality-aided framework is a viable option compare to finding a suitable metric."
    }, {
      "heading" : "4 Falsification with Causality Annotation",
      "text" : "Given the backgrounds in §2 and the examples in §3, we are now ready to ask the question: given a falsification problem and a Bayesian network annotation about causality, what cost function should we optimize? In this section, we will give some answers to the question by lifting up the conventional notion of acquisition functions which we reviewed in §2.2 to the multi-formula setting.\nConsider one of the Bayesian networks that we have seen in the paper. Let B denote the Bayesian network; and let Φ = {ϕ1, . . . ,ϕN} be the set of formulas that appear there. Now assume that we are running the Gaussian regression not only for fϕ = JM ( ),ϕK but also fϕi = JM ( ),ϕiK for all the formulas ϕi in the Bayesian network.\nThe regression result for fϕi gives us the probabilistic “forecast” of the truth values assignment of the formulas Θ ∈ 2Φ as follows.\nNotation 4.1 Let GP(µi,ki)∼ fϕi be our estimate for fϕi ; we can use this data to estimate the probability of obtaining Θ as the truth assignment, under an input value x. Precisely: let Θ be the assignment (ϕ1 = θ1, . . . ,ϕN = θN) where θi ∈ {tt, ff}; then\nPrGP(x)(Θ) , PrGP(µ1,k1) ( fϕ1(x) R1 0 ) · · ·PrGP(µN ,kN) ( fϕN (x) RN 0 ) , (3)\nwhere Ri is > if θ1 = tt, and < otherwise."
    }, {
      "heading" : "4.1 KL Divergence based acquisition function",
      "text" : "Recall the scenario in §3.1—from the conditional joint distribution PrB(− | JϕK = ff), we see that the both small formulas ϕ12 and ϕ34 also should be false to synchronize all the peaks of the sine waves.\nInspired from the above example, we propose the following criteria to choose the next candidate x as falsifying input.\nDefinition 4.2 (An acquitision function ψB(x))\nx = argmin x\nψB(x) where ψB(x) = DKL ( PrB(Θ | JϕK = ff) ∣∣∣∣∣∣∣∣PrGP(x)(Θ))\nHere DKL is the Kullback Leibler divergence—a measure of the difference between two probabilistic distributions. Intuitively, with this criteria, we pick the next input x with which the probabilistic forecast PrGP(x)(Θ) by regression becomes “closer to the conditional joint distribution PrB(Θ | JϕK = ff)”.\nExample 3 Let us consider the sine waves model in §3.1. From simple calculation, we see that the acquisition function ψB is as follows.\nψB(x) =− logPrGP( fϕ(x)< 0)− logPrGP( fϕ12(x)< 0)− logPrGP( fϕ34(x)< 0)\nHence minimizing ψB(x) means trying to falsify all the formulas ϕ , ϕ12, and ϕ34.\nRemark 4.3 In this paper, we assume that PrB(JϕK = ff) is not 0 nor 1 on the given Bayesian network B. In the former case, ψB(x) is undefined because PrB(JϕK = ff) is 0, and the latter case, ψB(x) is constantly 0 because PrB(Θ | JϕK = ff) = PrB(Θ). We believe this is reasonable if we believe the given annotation B is correct—in case PrB(JϕK = ff) is 0 (or 1) falsification never succeeds (or always succeeds, respectively).\nThe resulting extension of the GP-PSat algorithm (§2.2) with Bayesian networks is presented in Algorithm 3."
    }, {
      "heading" : "4.2 Another acquisition function based on the difference of KL divergence",
      "text" : "Aside from the acquisition function ψB in Def. 4.2, we propose another criteria.\nDefinition 4.4 (Another acquitision function ψ ′B(x))\nx = argmin x\nψ ′B(x) where\nψ ′B(x) = DKL ( PrB(Θ | JϕK = ff) ∣∣∣∣∣∣∣∣PrGP(x)(Θ))−DKL(PrB(Θ) ∣∣∣∣∣∣∣∣PrGP(x)(Θ)).\nAlgorithm 3 Extension of the GP-PSat algorithm with Bayesian network annotation, for falsification Input: an input space X; a system M ; a specification ϕ to falsity; a Bayesian network B whose nodes are labeled with formu-\nlas Φ = {ϕ1, . . . ,ϕN}; a covariance function k : X×X→ R; and an initial data set Di = {(x′1, fϕi(x′1)), . . . ,(x′s, fϕi(x′s))} for each i = 1, . . . ,N 1: for t = 1 . . .T do 2: GP(µi,ki) = GP(0,k;Di) for each i = 1, . . . ,N; 3: . Estimate the cost functions fϕ1 , . . . , fϕN by Gaussian process regression 4: xt = argminx∈X ψB(x); . Choose a new sample input by the acquisition function 5: Di = Di∪{(xt ,JM (xt), ϕiK)} for each i = 1, . . . ,N; . Observe the robustness 6: if JM (xt), ϕK < 0 then return xt ; . The specification ϕ is falsified 7: end if 8: end for\nOne of the advantages of this acquisition function ψ ′B(x) is that we can extract it to a simpler form as follows.\nψ ′B(x) = ∑ ϕi∈Φ\n( ( PrB(JϕiK = tt)−PrB(JϕiK = tt | JϕK = ff) ) log PrGP( fϕi(x)> 0)\n+ ( PrB(JϕiK = ff)−PrB(JϕiK = ff | JϕK = ff) ) log PrGP( fϕi(x)< 0)\n) .\nExample 4 Consider the incremental counter in Example 1. From the Bayesian network in Fig. 1 we extract the following acquisition function ψ ′B.\nψ ′B(x) = ∑ t∈[0,5]\n(1−0.2t+1) ( logPrGP( fϕt (x)> 0)− logPrGP( fϕt (x)< 0) )\nFor each formula ϕt , when PrGP( fϕt (x) > 0) becomes bigger, so is the value ψ ′B(x). Therefore the algorithm tries to make all the formulas to be false. This matches our intuition in §1.\nExample 5 Let us consider the automatic transmission problem in §3.2. The Bayesian network in Fig. 7 tells that most of the failure of ϕ is caused by that of ϕω . The acquisition function ψ ′B is as follows.\nψ ′B(x) = logPrGP( fϕ(x)> 0)− logPrGP( fϕ(x)< 0) +0.082 ( logPrGP( fϕv(x)> 0)− logPrGP( fϕv(x)< 0) ) +0.817 ( logPrGP( fϕω (x)> 0)− logPrGP( fϕω (x)< 0)\n) Hence as we expected, the satisfaction of ϕω is a bigger factor than that of ϕv.\nWe note that extension of other (more sophisticated) acquisition functions (e.g. GP-UCB) is not straightforward. It is one direction of our future work."
    }, {
      "heading" : "5 Implementation and Experimental Results",
      "text" : ""
    }, {
      "heading" : "5.1 Implementation",
      "text" : "Our implementation of Algorithm 3 consists of the following three open source libraries and one new part. They are mostly written in MATLAB.\nComputing the robustness We employ BREACH [2] to compute the simulation output of the system M (x) and the robustness JM (x), ϕK as defined in Def. 2.3.\nGaussian process regression Line 2 in Algorithm 3 is done by GPML MATLAB Code version 4.0 [3], a widely used library for computation about Gaussian processes.\nInference on Bayesian networks We employ Bayes Net Toolbox for Matlab [1] for inference on Bayesian networks.\nThe algorithms GP-PSat and GP-PI aided by Bayesian networks This part is new. Optimization of an acquisition function ψ is done by the following two steps: 1) we randomly pick initial samples x1, . . . ,x100 and compute the corresponding values of ψ; and 2) from the minimum xi of the one hundred, we further do greedy hill-climbing search."
    }, {
      "heading" : "5.2 Experiments",
      "text" : "Using our implementation we conducted the following experiments. We do experiments for the three falsification problems; Problem 1 is from Examples 1, Problem 2 from §3.1 and Problem 3 from §3.2. For the automatic transmission example (in §3.2) we used two different parameters; Problem 3-1 is with the specification ϕ = (v >−1∧ω < 4780); and Problem 3-2 is with ϕ = (v < 120∧ω < 4780).\nThe experiments were done on a ThinkPad T530 with Intel Core i7-3520M 2.90GHz CPU with 3.7GB memory. The OS was Ubuntu14.04 LTS (64-bit). A single falsification trial consists of a number of iterations—iterations of for-loop in line 2 in Algorithm 1—before it succeeds or times out (after 100 seconds). For each problem we made ten falsification trials. We made multiple trials because of the stochastic nature of the optimization algorithm. We measured the performance by the following criteria:\n• Success rate: The number of successful trials (out of ten).\n• The number of iteration loops: The average number of iteration loops to find the counterexample.\n• The computational time: The average time to find the counterexample.\nBesides our two extended algorithms with the acquisition functions (in Def. 4.2 and 4.4), we measured the performance of the conventional Gaussian process optimization algorithms GP-PSat and compare them.\nThe experimental results are in Table 1. We see that our causality-aided approach (GP-PSat with ψB and ψ ′B) significantly outperformed others for Example 1. This suggests promising potential of the proposed approach in the context of probabilistic programs—all the more because Bayesian networks like in Fig. 1 could be systematically derived using probabilistic predicate transformers.\nOur algorithms performed at least as well as the conventional GP-PSat, for the other examples (Problem 2, 3-1 and 3-2). In Problem 3-1 and 3-2 we observe that our algorithms took fewer iterations before successful falsification. This is potentially an advantage when we wish to deal with bigger Simulink models as system models M (their numerical simulation, i.e. computation of M (σ), is computationally expensive). That said, we believe the idea of causality aid in falsification can be a breaking one, with a potential of accelerating falsification by magnitudes. Its current performance for Problem 3 (that is from cyber-physical systems, a main application domain of falsification) is therefore not satisfactory. We will therefore pursue further improvement of our algorithm (Algorithm 3)."
    }, {
      "heading" : "6 Future Work",
      "text" : "In this paper, we show that the causality information given in the form of a Bayesian network helps us to solve falsification problems efficiently. However, we still have many challenges in constructing\nsuch helpful Bayesian networks. As we discussed in §1, we expect that the theory of probabilistic programming languages will shed light on the problem, but at any rate we need more practical example scenarios to evaluate the viability of our approach.\nMoreover, we conceive that our proposed algorithm in §4 contains the potential for many improvements. As we note in §2.2.2, the acquisition function in GP-PSat is simple, but not the state-of-the-art in the field of Gaussian process optimization. Extending our approach to other type of the acquisition function is not straightforward, but we think it is within possibility."
    } ],
    "references" : [ {
      "title" : "Falsification of conditional safety properties for cyber-physical systems with gaussian process regression",
      "author" : [ "Takumi Akazaki" ],
      "venue" : "Runtime Verification - 16th International Conference, RV 2016,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2016
    }, {
      "title" : "Time robustness in MTL and expressivity in hybrid system falsification",
      "author" : [ "Takumi Akazaki", "Ichiro Hasuo" ],
      "venue" : "Computer Aided Verification - 27th International Conference,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2015
    }, {
      "title" : "Input synthesis for sampled data systems by program logic",
      "author" : [ "Takumi Akazaki", "Ichiro Hasuo", "Kohei Suenaga" ],
      "venue" : "Proceedings 4th Workshop on Hybrid Autonomous Systems, HAS 2014,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2014
    }, {
      "title" : "The benefits of relaxing punctuality",
      "author" : [ "Rajeev Alur", "Tomás Feder", "Thomas A. Henzinger" ],
      "venue" : "J. ACM,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1996
    }, {
      "title" : "S-TaLiRo: A tool for temporal logic falsification for hybrid systems",
      "author" : [ "Yashwanth Annpureddy", "Che Liu", "Georgios E. Fainekos", "Sriram Sankaranarayanan" ],
      "venue" : "editors, TACAS,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2011
    }, {
      "title" : "Active learning based requirement mining for cyberphysical systems. In 55th IEEE Conference on Decision and Control, CDC 2016",
      "author" : [ "Gang Chen", "Zachary Sabato", "Zhaodan Kong" ],
      "venue" : "Las Vegas, NV, USA, December 12-14,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2016
    }, {
      "title" : "Active requirement mining of bounded-time temporal properties of cyber-physical systems",
      "author" : [ "Gang Chen", "Zachary Sabato", "Zhaodan Kong" ],
      "venue" : null,
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2016
    }, {
      "title" : "Stochastic local search for falsification of hybrid systems",
      "author" : [ "Jyotirmoy V. Deshmukh", "Xiaoqing Jin", "James Kapinski", "Oded Maler" ],
      "venue" : "ATVA",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2015
    }, {
      "title" : "Requirements driven falsification with coverage metrics",
      "author" : [ "Adel Dokhanchi", "Aditya Zutshi", "Rahul T. Sriniva", "Sriram Sankaranarayanan", "Georgios E. Fainekos" ],
      "venue" : "editors, 2015 International Conference on Embedded Software,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2015
    }, {
      "title" : "A toolbox for verification and parameter synthesis of hybrid systems",
      "author" : [ "Alexandre Donzé. Breach" ],
      "venue" : "Computer Aided Verification, 22nd International Conference,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2010
    }, {
      "title" : "Robust satisfaction of temporal logic over real-valued signals",
      "author" : [ "Alexandre Donzé", "Oded Maler" ],
      "venue" : "Formal Modeling and Analysis of Timed Systems - 8th International Conference,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2010
    }, {
      "title" : "Robustness of temporal logic specifications for continuous-time signals",
      "author" : [ "Georgios E. Fainekos", "George J. Pappas" ],
      "venue" : "Theor. Comput. Sci.,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2009
    }, {
      "title" : "Benchmarks for temporal logic requirements for automotive systems",
      "author" : [ "Bardh Hoxha", "Houssam Abbas", "Georgios Fainekos" ],
      "venue" : "1st and 2nd International Workshop on Applied veRification for Continuous and Hybrid Systems,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2014
    }, {
      "title" : "Using S-TaLiRo on industrial size automotive models",
      "author" : [ "Bardh Hoxha", "Houssam Abbas", "Georgios E. Fainekos" ],
      "venue" : "1st and 2nd International Workshop on Applied veRification for Continuous and Hybrid Systems,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2014
    }, {
      "title" : "Probabilistic Non-Determinism",
      "author" : [ "Claire Jones" ],
      "venue" : "PhD thesis, Univ. Edinburgh,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 1990
    }, {
      "title" : "Semantics of probabilistic programs",
      "author" : [ "Dexter Kozen" ],
      "venue" : "J. Comput. Syst. Sci.,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 1981
    }, {
      "title" : "Monitoring temporal properties of continuous signals",
      "author" : [ "Oded Maler", "Dejan Nickovic" ],
      "venue" : "Formal Techniques, Modelling and Analysis of Timed and Fault-Tolerant Systems, Joint International Conferences on Formal Modelling and Analysis of Timed Systems, FORMATS 2004 and Formal Techniques in Real-Time and Fault-Tolerant Systems,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2004
    }, {
      "title" : "Bayesian approach to global optimization: theory and applications. Mathematics and its applications (Kluwer Academic Publishers).",
      "author" : [ "Jonas Mockus" ],
      "venue" : "Soviet series. Kluwer Academic,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 1989
    }, {
      "title" : "Probabilistic predicate transformers",
      "author" : [ "Carroll Morgan", "Annabelle McIver", "Karen Seidel" ],
      "venue" : "ACM Trans. Program. Lang. Syst.,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 1996
    }, {
      "title" : "Reasoning about recursive probabilistic programs",
      "author" : [ "Federico Olmedo", "Benjamin Lucien Kaminski", "Joost-Pieter Katoen", "Christoph Matheja" ],
      "venue" : "Proceedings of the 31st Annual ACM/IEEE Symposium on Logic in Computer Science, LICS ’16,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2016
    }, {
      "title" : "Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning)",
      "author" : [ "Carl Edward Rasmussen", "Christopher K.I. Williams" ],
      "venue" : null,
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2005
    }, {
      "title" : "Falsification of temporal properties of hybrid systems using the cross-entropy method",
      "author" : [ "Sriram Sankaranarayanan", "Georgios Fainekos" ],
      "venue" : "In Proceedings of the 15th ACM International Conference on Hybrid Systems: Computation and Control,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2012
    }, {
      "title" : "Gaussian process optimization in the bandit setting: No regret and experimental design",
      "author" : [ "Niranjan Srinivas", "Andreas Krause", "Sham Kakade", "Matthias W. Seeger" ],
      "venue" : "Proceedings of the 27th International Conference on Machine Learning",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2010
    }, {
      "title" : "Multiple shooting, cegar-based falsification for hybrid systems",
      "author" : [ "Aditya Zutshi", "Jyotirmoy V. Deshmukh", "Sriram Sankaranarayanan", "James Kapinski" ],
      "venue" : "In Proceedings of the 14th International Conference on Embedded Software,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2014
    }, {
      "title" : "A trajectory splicing approach to concretizing counterexamples for hybrid systems",
      "author" : [ "Aditya Zutshi", "Sriram Sankaranarayanan", "Jyotirmoy V. Deshmukh", "James Kapinski" ],
      "venue" : "In Proceedings of the 52nd IEEE Conference on Decision and Control,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 13,
      "context" : "It is in this CPS context that the idea of falsification is found its use [17].",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 4,
      "context" : "Existing stochastic optimization-based solvers (such as S-TaLiRo [8] and BREACH [13]) have shown striking performance, too, scaling up to various Simulink diagrams from automotive applications.",
      "startOffset" : 65,
      "endOffset" : 68
    }, {
      "referenceID" : 9,
      "context" : "Existing stochastic optimization-based solvers (such as S-TaLiRo [8] and BREACH [13]) have shown striking performance, too, scaling up to various Simulink diagrams from automotive applications.",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 10,
      "context" : "Search of Cost Functions A technical cornerstone that set off the study of falsification is robust semantics of temporal formulas [14, 15].",
      "startOffset" : 130,
      "endOffset" : 138
    }, {
      "referenceID" : 11,
      "context" : "Search of Cost Functions A technical cornerstone that set off the study of falsification is robust semantics of temporal formulas [14, 15].",
      "startOffset" : 130,
      "endOffset" : 138
    }, {
      "referenceID" : 3,
      "context" : "For them it is standard to specify properties using some temporal logic, such as metric interval temporal logic (MITL) [7] and signal temporal logic (STL) [20].",
      "startOffset" : 119,
      "endOffset" : 122
    }, {
      "referenceID" : 16,
      "context" : "For them it is standard to specify properties using some temporal logic, such as metric interval temporal logic (MITL) [7] and signal temporal logic (STL) [20].",
      "startOffset" : 155,
      "endOffset" : 159
    }, {
      "referenceID" : 10,
      "context" : "In robust semantics [14, 15] a signal σ and a formula φ are assigned a continuous truth value Jσ , φK ∈ R that designates how robustly the formula is satisfied.",
      "startOffset" : 20,
      "endOffset" : 28
    }, {
      "referenceID" : 11,
      "context" : "In robust semantics [14, 15] a signal σ and a formula φ are assigned a continuous truth value Jσ , φK ∈ R that designates how robustly the formula is satisfied.",
      "startOffset" : 20,
      "endOffset" : 28
    }, {
      "referenceID" : 10,
      "context" : "The robust semantics of temporal formulas in [14, 15] is a prototype of such a cost function (Algorithm 1).",
      "startOffset" : 45,
      "endOffset" : 53
    }, {
      "referenceID" : 11,
      "context" : "The robust semantics of temporal formulas in [14, 15] is a prototype of such a cost function (Algorithm 1).",
      "startOffset" : 45,
      "endOffset" : 53
    }, {
      "referenceID" : 10,
      "context" : "For example, sometimes time robustness [14]—as opposed to space robustness in the original work [15]—yields smoother hills to climb down, aiding optimization.",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 11,
      "context" : "For example, sometimes time robustness [14]—as opposed to space robustness in the original work [15]—yields smoother hills to climb down, aiding optimization.",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 1,
      "context" : "Combination of space and time robustness is pursued in [5], where they enrich logics with averaged modalities to systematically enhance expressivity.",
      "startOffset" : 55,
      "endOffset" : 58
    }, {
      "referenceID" : 8,
      "context" : "Additional bias is put on cost functions in [12] so that search for falsifying input",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 0,
      "context" : "Contribution: Causality Aid in Falsification In this paper we build on the observations in [4] and propose to aid falsification using causal information.",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 1,
      "context" : "φ5 = [5,5](cnt≤ 5) φ4 = [4,4](cnt≤ 4) φ0 = [0,0](cnt≤ 0) tt ff 0.",
      "startOffset" : 5,
      "endOffset" : 10
    }, {
      "referenceID" : 1,
      "context" : "φ5 = [5,5](cnt≤ 5) φ4 = [4,4](cnt≤ 4) φ0 = [0,0](cnt≤ 0) tt ff 0.",
      "startOffset" : 5,
      "endOffset" : 10
    }, {
      "referenceID" : 0,
      "context" : "φ5 = [5,5](cnt≤ 5) φ4 = [4,4](cnt≤ 4) φ0 = [0,0](cnt≤ 0) tt ff 0.",
      "startOffset" : 24,
      "endOffset" : 29
    }, {
      "referenceID" : 0,
      "context" : "φ5 = [5,5](cnt≤ 5) φ4 = [4,4](cnt≤ 4) φ0 = [0,0](cnt≤ 0) tt ff 0.",
      "startOffset" : 24,
      "endOffset" : 29
    }, {
      "referenceID" : 5,
      "context" : "In order to efficiently leverage the causal information expressed by a Bayesian network, we follow [9,10] and use variations of Gaussian process optimization as our optimization algorithms (Line 3 of Algorithm 1).",
      "startOffset" : 99,
      "endOffset" : 105
    }, {
      "referenceID" : 6,
      "context" : "In order to efficiently leverage the causal information expressed by a Bayesian network, we follow [9,10] and use variations of Gaussian process optimization as our optimization algorithms (Line 3 of Algorithm 1).",
      "startOffset" : 99,
      "endOffset" : 105
    }, {
      "referenceID" : 14,
      "context" : "One is the use of probabilistic predicate transformers that are a classic topic in semantics [18, 19, 22] and are shed fresh light on in the context of probabilistic programming languages (see e.",
      "startOffset" : 93,
      "endOffset" : 105
    }, {
      "referenceID" : 15,
      "context" : "One is the use of probabilistic predicate transformers that are a classic topic in semantics [18, 19, 22] and are shed fresh light on in the context of probabilistic programming languages (see e.",
      "startOffset" : 93,
      "endOffset" : 105
    }, {
      "referenceID" : 18,
      "context" : "One is the use of probabilistic predicate transformers that are a classic topic in semantics [18, 19, 22] and are shed fresh light on in the context of probabilistic programming languages (see e.",
      "startOffset" : 93,
      "endOffset" : 105
    }, {
      "referenceID" : 19,
      "context" : "[23]).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 2,
      "context" : "This idea follows the earlier observations in [6]; it successfully generates the Bayesian network in Fig.",
      "startOffset" : 46,
      "endOffset" : 49
    }, {
      "referenceID" : 4,
      "context" : "In the falsification literature many different algorithms have been used and studied: they include simulated annealing, antcolony optimization, the cross-entropy method, the Nelder-Mead algorithm, and so on [8,13,25] .",
      "startOffset" : 207,
      "endOffset" : 216
    }, {
      "referenceID" : 9,
      "context" : "In the falsification literature many different algorithms have been used and studied: they include simulated annealing, antcolony optimization, the cross-entropy method, the Nelder-Mead algorithm, and so on [8,13,25] .",
      "startOffset" : 207,
      "endOffset" : 216
    }, {
      "referenceID" : 21,
      "context" : "In the falsification literature many different algorithms have been used and studied: they include simulated annealing, antcolony optimization, the cross-entropy method, the Nelder-Mead algorithm, and so on [8,13,25] .",
      "startOffset" : 207,
      "endOffset" : 216
    }, {
      "referenceID" : 7,
      "context" : "In [11] a discrete algorithm of Tabu search is employed for enhanced coverage.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 23,
      "context" : "Yet another important direction is multiple shooting falsification [27, 28] where, unlike single shooting approaches like in this paper, a bunch of trajectories are investigated in a single iteration relying on suitable abstraction of a system model and/or a specification.",
      "startOffset" : 67,
      "endOffset" : 75
    }, {
      "referenceID" : 24,
      "context" : "Yet another important direction is multiple shooting falsification [27, 28] where, unlike single shooting approaches like in this paper, a bunch of trajectories are investigated in a single iteration relying on suitable abstraction of a system model and/or a specification.",
      "startOffset" : 67,
      "endOffset" : 75
    }, {
      "referenceID" : 16,
      "context" : "Here we present signal temporal logic (STL) [20] as our formalism for expressing (original, without causal information) specifications.",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 10,
      "context" : "We also present its robust semantics [14] that give the prototype of the cost function fφ in Algorithm 1.",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 10,
      "context" : "The following “quantitative refinement” of the semantics of STL initiated the research program of falsification by optimization [14, 15].",
      "startOffset" : 128,
      "endOffset" : 136
    }, {
      "referenceID" : 11,
      "context" : "The following “quantitative refinement” of the semantics of STL initiated the research program of falsification by optimization [14, 15].",
      "startOffset" : 128,
      "endOffset" : 136
    }, {
      "referenceID" : 0,
      "context" : "For the optimization step (Line 3 of Algorithm 1) we use Gaussian process optimization— we follow [4, 9, 10] about this choice.",
      "startOffset" : 98,
      "endOffset" : 108
    }, {
      "referenceID" : 5,
      "context" : "For the optimization step (Line 3 of Algorithm 1) we use Gaussian process optimization— we follow [4, 9, 10] about this choice.",
      "startOffset" : 98,
      "endOffset" : 108
    }, {
      "referenceID" : 6,
      "context" : "For the optimization step (Line 3 of Algorithm 1) we use Gaussian process optimization— we follow [4, 9, 10] about this choice.",
      "startOffset" : 98,
      "endOffset" : 108
    }, {
      "referenceID" : 20,
      "context" : "[24] for details.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 20,
      "context" : "[24].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "More sophisticated acquisition functions that are known include probability improvement, expected improvement [21], upper confidence bound [26] and so on.",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 22,
      "context" : "More sophisticated acquisition functions that are known include probability improvement, expected improvement [21], upper confidence bound [26] and so on.",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 6,
      "context" : "φ ≡ [0,10]( ∨",
      "startOffset" : 4,
      "endOffset" : 10
    }, {
      "referenceID" : 12,
      "context" : "The last example is the automatic transmission model from the benchmark of temporal logic verification [16].",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 12,
      "context" : "It is taken from [16] (it is φAT 2 there).",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 10,
      "context" : "2, as an indicator of robustness, we employed the (space) robust semantics of STL in [14] and shown that it is not sensitive enough for some falsification scenarios.",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 10,
      "context" : "In contrast to [14], the metric-based robustness of MITL in [15] has a degree of freedom to capture the lacked notions.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 11,
      "context" : "In contrast to [14], the metric-based robustness of MITL in [15] has a degree of freedom to capture the lacked notions.",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 11,
      "context" : "2, we could solve the falsification problem more efficiently if we could re-scale v and ω appropriately, and this re-scaling is nothing but the defining the metric space in [15].",
      "startOffset" : 173,
      "endOffset" : 177
    }, {
      "referenceID" : 1,
      "context" : "ψ ′ B(x) = ∑ t∈[0,5] (1−0.",
      "startOffset" : 15,
      "endOffset" : 20
    } ],
    "year" : 2017,
    "abstractText" : "Falsification is drawing attention in quality assurance of heterogeneous systems whose complexities are beyond most verification techniques’ scalability. In this paper we introduce the idea of causality aid in falsification: by providing a falsification solver—that relies on stochastic optimization of a certain cost function—with suitable causal information expressed by a Bayesian network, search for a falsifying input value can be efficient. Our experiment results show the idea’s viability.",
    "creator" : "LaTeX with hyperref package"
  }
}