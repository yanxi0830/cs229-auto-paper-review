{
  "name" : "1307.2118.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A PAC-Bayesian Tutorial with A Dropout Bound",
    "authors" : [ "David McAllester" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "PAC-Bayesian theory blends Bayesian and frequentist approaches to the theory of machine learning. PAC-Bayesians theory assumes a probability distribution on “situations” occurring in nature and a prior weighting on “rules” expressing a learners preference for some rules over others. There is no assumed relationship between the learner’s bias on rules and nature’s distribution on situations. This is different from Bayesian inference where the starting point is a (perhaps subjective) joint distribution on rules and situations inducing a conditional distribution on rules given situations. The acronym PAC stands for Probably Approximately Correct and is borrowed from Valiant’s notion of PAC learnability [13]. PAC-Bayesian generalization bounds [11, 12, 10, 7, 4, 6] govern the performance (loss) when stochastically selecting rules from a “posterior” distribution. The performance guarantee involves the learner’s bias and an (unrelated) sample of situations.\nThis tutorial provides a concise overview of existing PAC-Bayesian theory focusing on three bounds. The first is an Occam bound. An Occam bound assumes a discrete (countable) set of rules and bounds the loss of an individual rule. The Occam bound immediately yields guarantees for rules\nar X\niv :1\n30 7.\n21 18\nv1 [\ncs .L\nG ]\n8 J\nul 2\nwith sparse finite precision parameters. The second is a PAC-Bayesian bound governing the loss of a stochastic process which draws rules from a PAC-Bayesian rule posterior. The PAC-Bayesian bound easily handles L2 regularization of infinite-precision parameters producing bounds closely related to support vector machines. It also provides bounds for a form of dropout learning [5].\nThe third bound is a training-variance bound similar to a bias-variance analysis but with bias replaced by expected training loss. This bound assumes a given learning algorithm and provides an upper bound on the expected generalization loss in terms of the expected training loss and a measure of the variance of the output of the learning algorithm. While the training-variance bound is clearly tighter than the PAC-Bayesian bound, the training-variance bound is difficult to interpret. The training-variance bounds seems to suggest variance-reduction methods such as bagging [3].\nUnbounded loss functions, such as square loss or log loss, can lead to unstable learning algorithms. Learning algorithms that minimize training loss for an unbounded loss function tend to be overly sensitive to outliers — training points of very high loss. Learning algorithms based on bounded loss functions tend to be more robust (stable). An unbounded loss function can be converted to a bounded loss by selecting an “outlier threshold” Lmax and replacing the unbounded loss L by min(L,Lmax).\nPAC-Bayesian bounds apply only to bounded loss functions. We assume the loss is bounded to the interval [0, Lmax]. It is of course possible to rescale any bounded loss function into the interval [0, 1]. However, this obscures the significance of the choice of Lmax in the design of a robust versions of square loss or log loss. For this reason we leave Lmax explicit in the statement of the bounds.\nThis tutorial also discusses two improvements or clarifications of the three bounds mentioned above. The first applies the training-variance bound to the learning algorithm defined by the PAC-Bayesian posterior. Unfortunately the results suffer from looseness in the analysis and remain difficult to interpret. The second tightens the Occam bound by incorporating the loss variance into the bound. We show that the improvements achievable in this way a fundamentally limited."
    }, {
      "heading" : "2 An Occam Bound",
      "text" : "Let H be a set of “rules”, S be a set of “situations”, Lmax > 0 be a real number, and L be a loss function such that for a rule h ∈ H and a situation s ∈ S we have that L(h, s) ∈ [0, Lmax]. We let D be a probability distribution (measure) on S and let P be a distribution (measure) on H. We think of D as a distribution on situations occurring in nature and P as learner bias on rules. There is no assumed relationship between D and P .1 We are interested in drawing a sequence S of N situations IID from D (S ∼ DN ) and then selecting h based on S so as to minimize the “generalization loss” L(h) = Es∼D [L(h, s)]. When the sample S is clear from context we will write L̂(h) for 1\nN ∑ s∈S L(h, s).\n1We should assume that the loss function L is measurable with respect to D and P . Here we will avoid this level of rigor.\nFor Occam bounds we consider the case where H is discrete (countable). An Occam bound states that with probability at least 1−δ over the draw of the sample S ∼ DN we have L(h) ≤ B(P (h), S, δ) simultaneously for all h where B(P (h), S, δ) is different in different bounds. While various Occam bounds have appeared in the literature, here we will consider only the following.\nTheorem 1. With probability at least 1− δ over the draw of S ∼ DN we have that the following holds simultaneously for all h.\nL(h) ≤ inf λ> 1\n2\n1\n1− 1 2λ\n( L̂(h) +\nλLmax N\n( ln 1\nP (h) + ln\n1\nδ\n)) (1)\nProof. We consider the case of Lmax = 1, the case for general Lmax follows by rescaling the loss function. Define (h) by\n(h) = √√√√2L(h)(ln 1P (h) + ln 1δ) N .\nFor a given h ∈ H the relative Chernoff bound [1] states that\nPS∼DN ( L̂(h) ≤ L(h)− (h) ) ≤ e−N (h)2 2L(h) = δP (h).\nHence, for a fixed h the probability of L(h) > L̂(h) + (h) is at most P (h)δ. By the union bound the probability that there exists an h with L(h) > L̂(h) + (h) is at most the sum over h of P (h)δ which equals δ. Hence we get that with probability at least 1 − δ over the draw of the sample the following holds simultaneously for all h.\nL(h) ≤ L̂(h) + √√√√√L(h) 2 ( ln 1 P (h) + ln 1 δ ) N  Using\n√ ab = inf\nλ>0\na 2λ + λb 2\nwe get\nL(h) ≤ L̂(h) + L(h) 2λ\n+ λ ( ln 1 P (h) + ln 1 δ ) N\nSolving for L(h) yields the result.\nAn important observation for (1) is that there is no point in taking λ to be large. Restricting λ to be less than λmax increases the bound by a factor of at most 1/(1 − 1/2λmax). For example, restricting λ to be less than 10 increases the bound by a factor of at most 20/19. So for practical purposes we can assume that λ is no larger than 10."
    }, {
      "heading" : "2.1 Finite Precision Bounds",
      "text" : "As an example application of (1) we can consider rules of the form hΘ for some parameter vector Θ ∈ Rd where each component of the vector Θ is represented with a b-bit finite precision representation. In this case we can take the prior P to be uniform on the 2bd possible rules and (1) then gives that with probability at least 1− δ over the draw of the sample we have that the following holds simultaneously for all such Θ.\nL(hΘ) ≤ inf λ> 1\n2\n1\n1− 1 2λ\n( L̂(hΘ) +\nλLmax N\n( (ln 2)bd+ ln 1\nδ )) We can also consider sparse representations. For Θ ∈ Rd we say that Θ has sparsity level s if at most s components of Θ are non-zero. We can then represent a sparse vector by first specifying the sparsity s and then listing s pairs each of which specifies a non-zero component and its value. Intuitively we can write a rule by first using log2 d bits to specify s plus (log2 d)b bits for each pair of a component index and b-bit parameter value representation. The probability of a rule h can always be taken to be 2−|h| where h is the number of bits needed to name h. Formally we avoid coding and instead defining a probability distribution where we first select s uniformly from 1 to d and then select s pairs with indices drawn uniformly form 1 to d and a parameter representation drawn uniformly from all 2b bit strings. In this case we get that with probability at least 1−δ over the draw of the sample of situations we have the following holds simultaneously for all sparsity levels s and Θ with sparsity s and with b-bit representations for the non-zero components of Θ.\nL(hΘ) ≤ inf λ> 1\n2\n1\n1− 1 2λ\n( L̂(hΘ) +\nλLmax N\n( ln d+ s(ln d+ (ln 2)b) + ln 1\nδ )) More sophisticated codings of classifiers are possible. For example, variable precision codes for real numbers can be useful when the error rate is insensitive to the precision. However, the PAC-Bayesian theorem stated in section 3 handles infinite precision parameters and seems generally preferable. The Occam bound is included here primarily because of its conceptual simplicity and the intuitive value of its proof."
    }, {
      "heading" : "3 A PAC-Bayesian Bound",
      "text" : "Let H, S, Lmax, L, D and P be defined as in section 2. We now allow the rule set H to be continuous (uncountable). Let Q be a variable ranging over distributions (measures) on the rule space H. For s ∈ S we define the loss L(Q, s) to be Eh∼Q [L(h, s)]. We have that L(Q, s) is the loss of a stochastic process that selects the hypothesis h according to distribution Q. We define L(Q) to be Es∼D [L(Q, s)]. Given a sample S = {s1, . . . , sN} we define L̂(Q) to be 1\nN ∑ s∈S L(Q, s). Finally we will write D(Q,P ) for\nthe Kullback-Leibler divergence from Q to P . D(Q,P ) = Eh∼Q [ ln Q(h)\nP (h)\n]\nA PAC-Bayesian theorem uniformly bounds L(Q) in terms of L̂(Q) and D(Q,P ). The first PAC-Bayesian theorem was given in [11]. Tighter PACBayesian theorems have been given by various authors [12, 10, 7, 4, 6]. Here we will focus on the following PAC-Bayesian version of the Occam bound (1) which can be derived as a corollary of statements by Catoni [4]. A proof is included here in appendix A. Theorem 2. For λ > 1 2\nselected before the draw of the sample (for any fixed λ > 1/2) we have that, with probability at least 1 − δ over the draw of the sample, the following holds simultaneously for all distributions Q on H.\nL(Q) ≤ 1 1− 1\n2λ\n( L̂(Q) +\nλLmax N\n( D(Q,P ) + ln 1\nδ\n)) (2)\nAs with (1), there is no point in taking λ in (2) to be large — we can in practice assume that λ is no larger than 10. Second, although (2) is not uniform in λ we can select k different values λ1, . . ., λk (all of which are selected before the draw of the sample) and by a simple union bound over these values derive that with probability at least 1− δ over the draw of the sample the following holds simultaneously for all Q.\nL(Q) ≤ min 1≤i≤k\n1\n1− 1 2λi\n( L̂(Q) +\nλiLmax N\n( D(Q,P ) + ln k\nδ\n)) (3)\nFor minimizing the bound we can assume 1/2 ≤ λ ≤ 10 and a small number of values of λ should suffice. While it is possible to give a version of this theorem that is uniform over all λ > 1/2, achieving this uniformity increases the complexity of the proof."
    }, {
      "heading" : "3.1 An Infinite Precision L2 Bound",
      "text" : "As in section 2.1 we consider rules hω with ω ∈ Rd. Here we also assume that the rule is scale-invariant — that the rule hω depends only on the direction of the vector ω. For example linear predictors of the form\nhω(x) = argmax y\nωᵀΦ(x, y)\nare scale-invariant. For scale-invariant rules it is natural to consider the uniform distribution over the directions of ω. This uniform distribution can be formalized as an isotropic unit-variance prior P = N (0, 1)d where N (0, 1) is the zero mean unit-variance Gaussian distribution. For Θ ∈ Rd we define the distribution QΘ to be the isotropic unit-variance Gaussian centered on Θ. Since only the direction of ω matters, we should think of P as the uniform distribution over directions and think of QΘ as a non-uniform distribution over directions. We then have the following.\nL(QΘ) = E ∼N (0,1)d [L(fΘ+ )] L̂(QΘ) = E ∼N (0,1)d [ L̂(fΘ+ ) ] D(QΘ, P ) = 1\n2 ||Θ||2\nThe PAC-Bayesian bound (2) then gives that with probability at least 1 − δ over the draw of the sample the following holds simultaneously for all Θ.\nL(QΘ) ≤ 1\n1− 1 2λ\n( L̂(QΘ) +\nλLmax N\n( 1\n2 ||Θ||2 + ln 1 δ\n)) (4)"
    }, {
      "heading" : "3.2 Binary and Multi-Class classification",
      "text" : "As an example we can consider linear binary classification. In this case we have that each situation is a pair (x, y) with y ∈ {−1, 1} and we have\nhω(x) = sign(ω ᵀΦ(x))\nwhere Φ is a feature map such that Φ(x) ∈ Rd. We also use 0-1 loss\nL(h, (x, y)) = 1h(x)6=y.\nWe then have\nL(QΘ, (x, y)) = Pω∼QΘ [hω(x) 6= y] = P ∼N (0,1)( > yΘ ᵀΦ(x)/||Φ(x)||).\nIn this case we have that (4) is very similar to the objective defining a support vector machine but where the hinge loss is replaced by a (non-convex) sigmoidal loss function (the cumulative of a Gaussian). In practice the rule hΘ is used at test time noting that Θ is the mean of the distribution QΘ.\nAs another example we can consider expected loss for multi-class classification. In this case each situation is a pair (x, y) with x ∈ X and y ∈ Y and where Y is small enough to be feasibly enumerated. We assume a feature map Φ with Φ(x, y) ∈ Rd and a loss function L̃ with L̃(ŷ, y) ∈ [0, Lmax]. For β > 0 we then have the following definitions.\nhω(x, y) = ωᵀΦ(x, y)\n||ω|| ||Φ(x, y)||\nPβ,ω(ŷ|x) = 1\nZβ,ω,x eβhω(x,ŷ)\nZβ,ω,x = ∑ ŷ∈Y eβhω(x,ŷ)\nLβ(hω, (x, y)) = Eŷ∼Pβ,ω(·|x)\n[ L̃(ŷ, y) ] This particular formulation has the property that hω is scale invariant (depends only on the direction of ω) and β is a parameter of the loss function. This formulation also has the property that Lβ(hω, (x, y)) is differentiable in ω. We can then optimize the right hand side of (4) by stochastic gradient descent using\n∇Θ L̂(QΘ) = 1\nN N∑ i=1 E ∼N (0,1)d [∇Θ L(hΘ+ , si)] ."
    }, {
      "heading" : "3.3 Dropouts",
      "text" : "We now present a dropout bound inspired by the recent success of dropout training in deep neural networks [5]. This dropout bound is the only original contribution of this tutorial.\nFor a given dropout rate α ∈ [0, 1] and vector Θ ∈ Rd we can stochastically generate a vector w ∈ Rd by selecting, for each coordinate wi, the value 0 with probability α (dropping the coordinate ωi) or with probability 1 − α setting ωi = Θi + with ∼ N (0, 1). We let Qα,Θ denote the distribution on vectors defined by this generation process. To apply the PAC-Bayesian bound we will take Qα,0 as the prior distribution and Qα,Θ as the posterior distribution. The PAC-Bayesian theorem then implies that for a dropout rate α selected before the draw of the sample we have that with probability at least 1− δ over the draw of the sample the following holds simultaneously for all Θ.\nL(Qα,Θ) ≤ 1\n1− 1 2λ\n( L̂(Qα,Θ) +\nλLmax N\n( D(Qα,Θ, Qα,0) + ln 1\nδ )) To clarify formal notation we first consider the Boolean d-cube B which is the set of vector s ∈ Rd such that si ∈ {0, 1} for all 1 ≤ i ≤ d. We will call vectors s ∈ B “sparsity patterns”. We let Sα be the distribution on the d-cube B (the distribution on sparsity patterns) generated by selecting each si independently with the probability of si = 0 being α. For a sparsity pattern s and for ω ∈ Rd we will write s ◦ ω for the Hadamard product defined by (s ◦ ω)i = siωi. We then have that a draw from Qα,Θ can be made by first drawing a sparsity pattern s ∼ Sα and a noise vector ∼ N (0, 1)d and then constructing s ◦ (Θ + ). More formally we have the following.\nEω∼Qα,Θ [f(ω)] = Es∼Sα, ∼N (0,1)d [f(s ◦ (Θ + ))]\nWe then have\nD(Qα,Θ, Qα,0) = Es∼Sα, ∼N (0,1)d\n[ ln Sα(s)e − 1 2 ||s◦ ||2\nSα(s)e − 1 2 ||s◦(Θ+ )||2\n]\n= Es∼Sα\n[ 1\n2 ||s ◦Θ||2\n]\n= 1− α\n2 ||Θ||2\nThe PAC-Bayesian bound then gives that, for a dropout rate α selected before the draw of the sample, with probability at least 1−δ over the draw of the sample the following holds simultaneously for all Θ.\nL(Qα,Θ) ≤ 1\n1− 1 2λ\n( L̂(Qα,Θ) +\nλLmax N\n( 1− α\n2 ||Θ||2 + ln 1 δ\n)) (5)\nComparing (5) with (4) we see that a dropout rate of α reduces the complexity cost by a factor of 1 − α. However, for α very small we expect\nL̂(Qα,Θ) to be large. We can optimize the right hand side of this bound by stochastic gradient descent using the following.\n∇Θ L̂(Qα,Θ) = 1\nN N∑ i=1 Es∼Sα, ∼N (0,1)d [ ∇ΘL(hs◦(Θ+ ), si) ]"
    }, {
      "heading" : "3.4 The PAC-Bayesian Posterior",
      "text" : "It is important to note that (2) has a closed-form solution for the distribution Q minimizing the bound.\nQ∗ = argmin Q L̂(Q) + λLmax N D(Q,P )\nIn the case where the rule space H is finite we have the constraint that∑ h∈HQ(h) = 1 and a straightforward application of the KTT conditions yields the following.\nQ∗(h) = Qλ(h) = 1\nZλ P (h)e\n−NL̂(h) λLmax\nZλ = Eh∼P [ e − N λLmax L̂(h) ]\nHere we can think of Qλ as “the” PAC-Bayesian posterior distribution for regularization parameter λ. It is important to note that the choice of Lmax strongly influences the posterior distribution. In the case of (3) we can optimize over λ as follows.\nQ∗ = Qλi∗ i ∗ = argmin\n1≤i≤k L̂(Qλi) + λLmax N D(Qλi , P )\nL(Q∗) ≤ min 1≤i≤k\n1\n1− 1 2λi\n( L̂(Qλi) +\nλiLmax N\n( D(Qλi , P ) + ln k\nδ\n))"
    }, {
      "heading" : "4 A Training-Variance Bound",
      "text" : "We now consider a fixed learning algorithm A which takes as input a sample S ∼ DN and returns a rule distribution QA(S). For a given learning algorithm A we now consider the expected loss ES∼DN [L(QA(S))] and the expected posterior\nQ̄A(h) = ES∼DN [QA(S)(h)] .\nThe training-variance bound is the following where we will write ES [f(S)] for ES∼DN [f(S)]. Theorem 3. For any fixed learning algorithm A and for λ > 1 2 we have\nES [L(QA(S))] ≤ 11− 1 2λ\n( ES [ L̂(QA(S)) ] + λLmax N ES [ D(QA(S), Q̄A) ]) . (6)\nWe can think of ES [ D(QA(S), Q̄A) ] as a measure of the variation of QA(S) over the draw of S. For the PAC-Bayesian bound (2) we have a closed form solution for the optimal posterior. But for the trainingvariance bound (6) we do not have a solution for the optimal algorithm. The training-variance bound seems to motivate variance-reduction methods such as bagging [3].\nThe training-variance bound is an immediate corollary of the following more general theorem which is implicit in Catoni [4] and which is proved in appendix B.\nTheorem 4. For any rule distribution P , learning algorithm A, and for λ > 1\n2 , we have\nES [L(QA(S))] ≤ 11− 1 2λ\n( ES [ L̂(QA(S)) ] + λLmax\nN ES [D(QA(S), P )]\n) . (7)\nIt was observed by Langford [8] that the rule distribution P minimizing ES [D(QA(S), P )] is Q̄A. This can be shown as follows.\nES [D(QA(S), P )] = ES, h∼QA(S) [ ln QA(S)(h)\nP (h)\n]\n= ES, h∼QA(S)\n[ ln QA(S)(h)\nQ̄A(h)\n] + Eh∼Q̄A [ ln Q̄(h)\nP (h) ] = ES [ D(QA(S), Q̄A) ] +D(Q̄A, P )\nThis shows that (6) dominates (7) and is much better when D(Q̄A, P ) is large.\nFor a given learning algorithm A we can insert Q̄A for P in the PACBayesian bound (2) yielding the following high confidence version of (6).\nTheorem 5. For any given learning algorithm A and λ > 1 2\nwe have the following with probability at least 1− δ over the draw of the sample.\nL(QA(S)) ≤ 11− 1 2λ\n( L̂(QA(S)) +\nλLmax N ( D(QA(S), Q̄A) + ln 1δ )) (8)"
    }, {
      "heading" : "5 Applying the Training-Variance Bound",
      "text" : "to the PAC-Bayesian Posterior\nWe now consider the learning algorithm that maps a sample S to the PAC-Bayesian posterior Qλ(S). Here λ is a parameter of the learning algorithm. We should note that, although Qλ is the posterior optimizing the PAC-Bayesian bound (2), it seems unlikely that Qλ is the algorithm optimizing the training-variance bound (6). Also, as we will see below, the analysis given here is somewhat loose.\nFollowing Catoni [4] and Lever et al. [9] we approximate Q̄λ with the following.\nQ̈λ(h) = 1\nZ̈λ P (h)e\n−NL(h) λLmax\nZ̈λ = Eh∼P [ e −NL(h) λLmax ]\nInserting Q̈λ for P in (7) gives that for γ > 1 2 we have the following.\nES [L(Qλ(S))] ≤ 11− 1 2γ\n( ES [ L̂(Qλ(S)) ] + γLmax\nN ES\n[ D(Qλ(S), Q̈λ) ]) (9)\nNote that we allow γ to be different from λ. We now have the following bound from Catoni [4] and whose proof is given in appendix C.\nES [ D(Qλ(S), Q̈λ) ] ≤ N λLmax ( ES [L(Qλ(S))]− ES [ L̂(Qλ(S)) ]) (10)\nBy inserting (10) into (9), setting γ = 1 2 λ and solving for ES [L(Qλ(S))] one can derive the following for λ > 2.\nES [L(Qλ(S))] ≤ 11− 2 λ ES\n[ L̂(Qλ(S)) ] (11)\nNote that 1 2λ\nin the Occam and PAC-Bayesian bound has been replaced with 2\nλ . Also note that Lmax appears in the definition of Qλ(S).\nTo get a corresponding high-confidence bound we first note that by inserting Q̈λ for P in the PAC-Bayesian bound (2) we get that, for γ >\n1 2 ,\nwith probability at least 1− δ over the draw of the sample we have\nL(Qλ(S)) ≤ 11− 1 2γ\n( L̂(Qλ(S)) + γLmax N ( D(Qλ(S), Q̈λ) + ln 1δ )) . (12)\nThis can be combined with the following whose proof is given in appendix C.\nLemma 1. For λ > 0, with probability at least 1− δ over the draw of the sample we have\nD(Qλ(S), Q̈λ) ≤ N\nλLmax\n( L(Qλ(S))− L̂(Qλ(S)) ) + N\nλ\n√ ln 1\nδ\n2N . (13)\nTaking a union bound over (13) and (12) so that both are true simultaneously, then inserting (13) into (12), setting γ = 1\n2 λ, and solving for\nL(Qλ(S)), yields that with probability at least 1− δ over the draw of the sample we have\nL(Qλ(S)) ≤ 1\n1− 2 λ\nL̂(Qλ(S)) + Lmax √ ln 2 δ\n2N + λLmax ln\n2 δ\nN  (14) Improvements in these bounds should be possible. To see this consider\nthe PAC-Bayesian bound (2) for which Qλ(S) is the optimal posterior.\nL(Qλ(S)) ≤ 1\n1− 1 2λ\n( L̂(Qλ(S)) +\nλLmax N\n( D(Qλ(S), P ) + ln 1\nδ )) Replacing P by Q̄λ should significantly improve this bound. However, replacing P by Q̈λ and then inserting (13) makes the bound vacuous."
    }, {
      "heading" : "6 Incorporating Empirical Loss Variance",
      "text" : "For a given rule h and sample S = {s1, . . . , sn} one can measure an empirical loss variance.\nσ̂2(h) = 1\nN − 1 N∑ i=1 (L(h, si)− L̂(h))2\nIt is natural to ask whether tighter bounds are possible if we allow the bounds to involve σ̂2(h). Audibert, Munos and Szepesvari [2] give a bound motivated by this question. Consider a random variable x ∈ [0, Lmax] with expectation µ and an IID sample {x1, . . . , xn} with empirical mean µ̂ and empirical variance σ̂2. Audibert, Munos and Szepesvari prove that the following holds with probability at least 1 − δ over the draw of the sample.\nµ ≤ µ̂+\n√ 2σ̂2 ln 3\nδ\nN +\n3Lmax ln 3 δ\nN\nTaking a union bound over a prior P we get that with probability at least 1− δ the following holds for all h ∈ H.\nL(h) ≤ L̂(h) + √√√√2σ̂2(h)(ln 1P (h) + ln 3δ) N + 3Lmax ( ln 1 P (h) + ln 3 δ ) N\nTo show the limitations of these bounds we consider the best possible case where σ̂2(h) = 0. For this case we have the following theorem whose proof is given in appendix D.\nTheorem 6. With probability at least 1− δ over the draw of the sample we have that the following holds for all h such that σ̂2(h) = 0.\nL(h) ≤ L̂(h) + Lmax\n( ln 1\nP (h) + ln 1 δ ) N − 1 (15)\nThe inequality (15) is essentially the best that can be done using a union bound over P (h). The basic idea is that even if σ̂2(h) = 0 one cannot rule out the possibility of outliers which happened not to occur in the data. The probability of outliers cannot be bounded to be less than (ln 1\nP (h) + ln 1 δ )/N and an outlier can have loss Lmax so (15) is the best that can be done. But (15) is not significantly tighter than the general Occam bound (1). In particular, by taking λ = 1 in (1) we get a bound that is only a factor of 2 worse than (15). If (15) is dominated by L̂(h) then we can take λ in the Occam bound (1) to be large and the two bounds are essentially the same."
    }, {
      "heading" : "7 Conclusion",
      "text" : "This paper focuses on three generalization bounds — an Occam bound, a PAC-Bayesian bound, and a training-variance bound. The Occam\nbound and PAC-Bayesian bound seem to be important primarily because they provide the conceptual foundation required for the proof of the training-variance bound which dominates the other two. While the PAC-Bayesian posterior defines a learning algorithm optimizing the PACBayesian bound, there is no known analogous optimal algorithm for the training-variance bound. The bound seems to suggest variance reduction methods such as boosting. There is clearly room for improved theoretical understanding of the consequences of the training-variance bound."
    }, {
      "heading" : "A Proof of Theorem 2",
      "text" : "All proofs in these appendices are adapted from Catoni [4] except for the proof of theorem 6 which is straightforward.\nThe theorem states that for λ > 1 2\nselected before the draw of the sample (for any fixed λ > 1/2) we have that, with probability at least 1− δ over the draw of the sample, the following holds simultaneously for all distributions Q on H.\nL(Q) ≤ 1 1− 1\n2λ\n( L̂(Q) +\nλLmax N\n( D(Q,P ) + ln 1\nδ )) We will consider the case of Lmax = 1, the general case follows by rescaling the loss function. For real numbers p, q ∈ [0, 1] we define D(q, p) to be the divergence from a Bernoulli variable with bias q to a Bernoulli variable with bias p.\nD(q, p) = q ln q p + (1− q) ln 1− p 1− q\nFor a real number γ we define\nDγ(q, p) = γq − ln (1− p+ peγ) .\nBy a straightforward optimization over γ one can show\nD(q, p) = sup γ Dγ(q, p). (16)\nNow consider a random variable x with x ∈ [0, 1] and with mean µ. Let µ̂ be the mean of N independent draws of x. We first show that for any fixed γ we have\nE [ eNDγ(µ̂,µ) ] ≤ 1. (17)\nTo see this note that E [ eNγµ̂ ] = (E [eγx])N . For x ∈ [0, 1] we note that the convexity of the exponential function implies eγx ≤ 1− x+ xeγ . This gives E [ eNγµ̂ ] ≤ (1 − µ + µeγ)N . Dividing by the right hand side gives\nE [ eN(γµ̂−ln(1−µ+µe γ)) ] ≤ 1 which is the same as (17). It is interesting to\nnote that ES∼DN [ eND(µ̂,µ) ] = ES∼DN [ sup γ eNDγ(µ̂,µ) ]\n≥ sup γ\nES∼DN [ eNDγ(µ̂,µ) ] ≤ 1\nFor h fixed (17) implies the following. ES∼DN [ eNDγ(L̂(h),L(h)) ] ≤ 1\nEh∼P [ ES∼DN [ eNDγ(L̂(h),L(h)) ]] ≤ 1\nES∼DN [ Eh∼P [ eNDγ(L̂(h),L(h)) ]] ≤ 1 (18)\nApplying Markov’s inequality to (18) we get that with probability at least 1− δ over the draw of S we have\nEh∼P [ eNDγ(L̂(h),L(h,P )) ] ≤ 1 δ . (19)\nNext we observe the shift of measure lemma Eh∼Q [f(h)] ≤ D(Q,P ) + ln Eh∼P [ ef(h) ] (20)\nwhich can be derived as follows. Eh∼Q [f(h)] = Eh∼Q [ ln ef(h) ] = Eh∼Q [ ln P (h)\nQ(h) ef(h) + ln\nQ(h)\nP (h) ] ≤ ln Eh∼Q [ P (h)\nQ(h) ef(h)\n] +D(Q,P )\n= D(Q,P ) + ln Eh∼P [ ef(h) ] Setting f(h) = NDγ(L̂(h), L(h)) in (20) and using (19) we get\nEh∼Q [ NDγ(L̂(h), L(h)) ] ≤ D(Q,P ) + ln 1\nδ .\nNoting that Dγ(q, p) is jointly convex in q and p we get\nDγ(L̂(Q), L(Q)) ≤ 1\nN\n( D(Q,P ) + ln 1\nδ\n) . (21)\nTheorem 2 is now implied by the following lemma.\nLemma 2. For λ > 1 2\n, if D− 1 λ (p, q) ≤ c then p ≤ 1 1− 1\n2λ\n(q + λc).\nProof. Let γ abbreviate − 1 λ . We are given qγ−ln (1− p+ peγ) ≤ c. Since λ > 1\n2 we have γ ∈ (−2, 0). We then get\np ≤ 1− e γq−c\n1− eγ .\nApplying eγ ≥ 1 + γ in the numerator and eγ ≤ 1 + γ + 1 2 γ2 ≤ 1 for γ ∈ (−2, 0) in the denominator we get\np ≤ −γq + c −γ − 1\n2 γ2\n= q − c γ\n1 + 1 2 γ\nReplacing γ by −1/λ proves the lemma."
    }, {
      "heading" : "B Proof of Theorem 4",
      "text" : "The theorem states that for distribution P on rules, any algorithm A, and for λ > 1\n2 , we have\nES [L(QA(S))] ≤ 11− 1 2λ\n( ES [ L̂(QA(S)) ] + λLmax\nN ES [D(QA(S), P )]\n) .\nProof. The proof is a slight modification of the proof of theorem 2 given in section A. By the shift of measure lemma (20) we have the following for any fixed sample S.\nEh∼QA(S)\n[ NDγ(L̂(h), L(h)) ] ≤ D(QA(S), P )+ln Eh∼P [ eNDγ(L̂(h),L(h)) ] .\nBy the joint convexity of Dγ we then have\nDγ(L̂(QA(S)), L(QA(S))) ≤ 1\nN\n( D(QA(S), P ) + ln Eh∼P [ eNDγ(L̂(h),L(h)) ]) .\nTaking the expectation of both sides with respect to S and using the convexity of Dγ and the concavity of ln we get\nDγ ( ES [ L̂(QA(S)) ] ,ES [L(QA(S))] ) ≤ 1\nN\n( ES [D(QA(S), P )] + ln Eh∼P,S∼DN [ eNDγ(L̂(h),L(h)) ]) .\nTheorem 4 now follows from (17) and lemma 2."
    }, {
      "heading" : "C Proof of (10) and (13)",
      "text" : "(10) is the following.\nES [ D(Qλ(S), Q̈λ) ] ≤ N λLmax ( ES [L(Qλ(S))]− ES [ L̂(Qλ(S)) ]) Proof.\nES [ D(Qλ(S), Q̈λ) ] = ES,h∼Qλ(S) [ ln Qλ(S)(h)\nQ̈λ(h)\n]\n= ES,h∼Qλ(S)\n[ N\nλLmax L(h)− N λLmax L̂(h)) ] −ES [lnZλ(S)] + ln Z̈λ\n= N\nλLmax\n( ES [L(Qλ(S))]− ES [ L̂(Qλ(S)) ]) −ES [lnZλ(S)] + ln Z̈λ\nBut the log partition function is convex in energy which gives ES [lnZλ(S)] = ES [ ln Eh∼P [ e − N λLmax L̂(h) ]]\n≥ ln Eh∼P [ e − N λLmax ES[L̂(h)] ] = ln Eh∼P [ e − N λLmax L(h) ]\n= Z̈λ\n(13) states that with probability at least 1− δ we have\nD(Qλ(S), Q̈λ) ≤ N\nλLmax\n( ES [ L(Qλ(S))− L̂(Qλ(S)) ]) + N\nλ\n√ ln 1\nδ\n2N .\nProof. D(Qλ(S), Q̈λ) = Eh∼Qλ(S) [ ln Qλ(S)(h)\nQ̈λ(h)\n]\n= Eh∼Qλ(S)\n[ N\nλLmax L(h)− N λLmax L̂(h)) ] − lnZλ(S) + ln Z̈λ\n= N\nλLmax\n( L(Qλ(S))− L̂(Qλ(S))) ) − lnZλ(S) + ln Z̈λ\nlnZλ(S) = ln Eh∼P [ e − N λLmax L̂(h) ]\n= ln Eh∼Q̈λ\n[ P (h)\nQ̈λ(h) e − N λLmax L̂(h)\n]\n≥ Eh∼Q̈λ\n[ ln P (h)\nQ̈λ(h) − N λLmax L̂(h)\n]\n= ln Z̈λ + N( L(Q̈λ)− L̂(Q̈λ) )\nλLmax\nSince λ is selected before the draw of the sample, a Hoeffding bound can be used to bound L(Q̈λ) − L̂(Q̈λ) yielding that with probability at least 1− δ over the draw of the sample we have\nlnZλ(S) ≥ ln Z̈λ − N\nλ\n√ ln 1\nδ\n2N . (22)"
    }, {
      "heading" : "D Proof of Theorem 6",
      "text" : "The theorem states that with probability at least 1−δ over the draw of the sample we we have that the following holds for all h such that σ̂2(h) = 0.\nL(h) ≤ L̂(h) + Lmax\n( ln 1\nP (h) + ln 1 δ ) N − 1\nProof. Consider a sample S = {s1, . . . , sn}. We let the sample s1 define a target loss value for each rule. We consider a sample si for i > 1 to be an “outlier” for rule h if L(h, si) 6= L(h, s1). We can then use the standard “realizable” analysis over the sample {s2, . . . , sN} to bound the outlier rate. More specifically, let µ(h) be the probability that a new draw of a situation from D is an outlier for h.\nµ(h) = Ps∼D(L(h, s) 6= L(h, s1))\nWe will first show that with probability at least 1 − δ over the draw of {s2, . . . , sN} we have that the following holds simultaneously for all h such that σ̂2(h) = 0.\nµ(h) ≤ ln 1 P (h) + ln 1 δ\nN − 1 (23)\nThe probability over the draw of {s2, . . . , sN} ∼ DN−1 that σ̂2(h) = 0 equals (1 − µ(h))N−1 ≤ e−(N−1)µ(h). So if h violates (23) then the probability that σ2(h) = 0 is at most P (h)δ. By the union bound the probability that there exists an h with σ̂2(h) = 0 and violating (23) is at most ∑ h P (h)δ = δ and thus with high probability (23) holds for all h. The theorem then follows from the observation that if σ̂2(h) = 0 then L̂(h) = L(h, s1) and L(h) ≤ L(h, s1) + Lmaxµ(h)."
    } ],
    "references" : [ {
      "title" : "Fast probabilistic algorithms for hamiltonian circuits and matchings",
      "author" : [ "Dana Angluin", "Leslie G Valiant" ],
      "venue" : "In Proceedings of the ninth annual ACM symposium on Theory of computing,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1977
    }, {
      "title" : "Use of variance estimation in the multi-armed bandit problem",
      "author" : [ "Jean-Yves Audibert", "Rémi Munos", "Csaba Szepesvari" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2006
    }, {
      "title" : "Pac-bayesian supervised classification: the thermodynamics of statistical learning",
      "author" : [ "Olivier Catoni" ],
      "venue" : "arXiv preprint arXiv:0712.0248,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2007
    }, {
      "title" : "New types of deep neural network learning for speech recognition and related applications: An overview",
      "author" : [ "Li Deng", "Geoffrey Hinton", "Brian Kingsbury" ],
      "venue" : "Proc. ICASSP,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2013
    }, {
      "title" : "Pac-bayesian learning of linear classifiers",
      "author" : [ "Pascal Germain", "Alexandre Lacasse", "François Laviolette", "Mario Marchand" ],
      "venue" : "In Proceedings of the 26th Annual International Conference on Machine Learning,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2009
    }, {
      "title" : "Tutorial on practical prediction theory for classification",
      "author" : [ "John Langford" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2006
    }, {
      "title" : "Microchoice bounds and self bounding learning algorithms",
      "author" : [ "John Langford", "Avrim Blum" ],
      "venue" : "In Proceedings of the twelfth annual conference on Computational learning theory,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1999
    }, {
      "title" : "Distribution-dependent pac-bayes priors",
      "author" : [ "Guy Lever", "François Laviolette", "John Shawe-Taylor" ],
      "venue" : "In Algorithmic Learning Theory,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2010
    }, {
      "title" : "A note on the pac-bayesian theorem",
      "author" : [ "Andreas Maurer" ],
      "venue" : "arXiv preprint cs/0411099,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2004
    }, {
      "title" : "Pac-bayesian model averaging",
      "author" : [ "David A. McAllester" ],
      "venue" : "In COLT, pages 164–170,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1999
    }, {
      "title" : "Pac-bayesian generalisation error bounds for gaussian process classification",
      "author" : [ "Matthias Seeger" ],
      "venue" : "J. Mach. Learn. Res,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2003
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : "PAC-Bayesian generalization bounds [11, 12, 10, 7, 4, 6] govern the performance (loss) when stochastically selecting rules from a “posterior” distribution.",
      "startOffset" : 35,
      "endOffset" : 56
    }, {
      "referenceID" : 10,
      "context" : "PAC-Bayesian generalization bounds [11, 12, 10, 7, 4, 6] govern the performance (loss) when stochastically selecting rules from a “posterior” distribution.",
      "startOffset" : 35,
      "endOffset" : 56
    }, {
      "referenceID" : 8,
      "context" : "PAC-Bayesian generalization bounds [11, 12, 10, 7, 4, 6] govern the performance (loss) when stochastically selecting rules from a “posterior” distribution.",
      "startOffset" : 35,
      "endOffset" : 56
    }, {
      "referenceID" : 5,
      "context" : "PAC-Bayesian generalization bounds [11, 12, 10, 7, 4, 6] govern the performance (loss) when stochastically selecting rules from a “posterior” distribution.",
      "startOffset" : 35,
      "endOffset" : 56
    }, {
      "referenceID" : 2,
      "context" : "PAC-Bayesian generalization bounds [11, 12, 10, 7, 4, 6] govern the performance (loss) when stochastically selecting rules from a “posterior” distribution.",
      "startOffset" : 35,
      "endOffset" : 56
    }, {
      "referenceID" : 4,
      "context" : "PAC-Bayesian generalization bounds [11, 12, 10, 7, 4, 6] govern the performance (loss) when stochastically selecting rules from a “posterior” distribution.",
      "startOffset" : 35,
      "endOffset" : 56
    }, {
      "referenceID" : 3,
      "context" : "It also provides bounds for a form of dropout learning [5].",
      "startOffset" : 55,
      "endOffset" : 58
    }, {
      "referenceID" : 0,
      "context" : "It is of course possible to rescale any bounded loss function into the interval [0, 1].",
      "startOffset" : 80,
      "endOffset" : 86
    }, {
      "referenceID" : 0,
      "context" : "For a given h ∈ H the relative Chernoff bound [1] states that",
      "startOffset" : 46,
      "endOffset" : 49
    }, {
      "referenceID" : 9,
      "context" : "The first PAC-Bayesian theorem was given in [11].",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 10,
      "context" : "Tighter PACBayesian theorems have been given by various authors [12, 10, 7, 4, 6].",
      "startOffset" : 64,
      "endOffset" : 81
    }, {
      "referenceID" : 8,
      "context" : "Tighter PACBayesian theorems have been given by various authors [12, 10, 7, 4, 6].",
      "startOffset" : 64,
      "endOffset" : 81
    }, {
      "referenceID" : 5,
      "context" : "Tighter PACBayesian theorems have been given by various authors [12, 10, 7, 4, 6].",
      "startOffset" : 64,
      "endOffset" : 81
    }, {
      "referenceID" : 2,
      "context" : "Tighter PACBayesian theorems have been given by various authors [12, 10, 7, 4, 6].",
      "startOffset" : 64,
      "endOffset" : 81
    }, {
      "referenceID" : 4,
      "context" : "Tighter PACBayesian theorems have been given by various authors [12, 10, 7, 4, 6].",
      "startOffset" : 64,
      "endOffset" : 81
    }, {
      "referenceID" : 2,
      "context" : "Here we will focus on the following PAC-Bayesian version of the Occam bound (1) which can be derived as a corollary of statements by Catoni [4].",
      "startOffset" : 140,
      "endOffset" : 143
    }, {
      "referenceID" : 3,
      "context" : "We now present a dropout bound inspired by the recent success of dropout training in deep neural networks [5].",
      "startOffset" : 106,
      "endOffset" : 109
    }, {
      "referenceID" : 0,
      "context" : "For a given dropout rate α ∈ [0, 1] and vector Θ ∈ R we can stochastically generate a vector w ∈ R by selecting, for each coordinate wi, the value 0 with probability α (dropping the coordinate ωi) or with probability 1 − α setting ωi = Θi + with ∼ N (0, 1).",
      "startOffset" : 29,
      "endOffset" : 35
    }, {
      "referenceID" : 2,
      "context" : "The training-variance bound is an immediate corollary of the following more general theorem which is implicit in Catoni [4] and which is proved in appendix B.",
      "startOffset" : 120,
      "endOffset" : 123
    }, {
      "referenceID" : 6,
      "context" : "It was observed by Langford [8] that the rule distribution P minimizing ES [D(QA(S), P )] is Q̄A.",
      "startOffset" : 28,
      "endOffset" : 31
    }, {
      "referenceID" : 2,
      "context" : "Following Catoni [4] and Lever et al.",
      "startOffset" : 17,
      "endOffset" : 20
    }, {
      "referenceID" : 7,
      "context" : "[9] we approximate Q̄λ with the following.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 2,
      "context" : "We now have the following bound from Catoni [4] and whose proof is given in appendix C.",
      "startOffset" : 44,
      "endOffset" : 47
    }, {
      "referenceID" : 1,
      "context" : "Audibert, Munos and Szepesvari [2] give a bound motivated by this question.",
      "startOffset" : 31,
      "endOffset" : 34
    } ],
    "year" : 2013,
    "abstractText" : "This tutorial gives a concise overview of existing PAC-Bayesian theory focusing on three generalization bounds. The first is an Occam bound which handles rules with finite precision parameters and which states that generalization loss is near training loss when the number of bits needed to write the rule is small compared to the sample size. The second is a PAC-Bayesian bound providing a generalization guarantee for posterior distributions rather than for individual rules. The PAC-Bayesian bound naturally handles infinite precision rule parameters, L2 regularization, provides a bound for dropout training, and defines a natural notion of a single distinguished PAC-Bayesian posterior distribution. The third bound is a training-variance bound — a kind of bias-variance analysis but with bias replaced by expected training loss. The training-variance bound dominates the other bounds but is more difficult to interpret. It seems to suggest variance reduction methods such as bagging and may ultimately provide a more meaningful analysis of dropouts.",
    "creator" : "LaTeX with hyperref package"
  }
}