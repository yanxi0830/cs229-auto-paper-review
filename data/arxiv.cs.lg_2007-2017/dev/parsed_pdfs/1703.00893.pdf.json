{
  "name" : "1703.00893.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Being Robust (in High Dimensions) Can Be Practical",
    "authors" : [ "Ilias Diakonikolas", "Gautam Kamath", "Daniel M. Kane", "Jerry Li", "Ankur Moitra", "Alistair Stewart" ],
    "emails" : [ "diakonik@usc.edu", "g@csail.mit.edu", "dakane@cs.ucsd.edu", "jerryzli@mit.edu", "moitra@mit.edu", "alistais@usc.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Robust statistics was founded in the seminal works of [Tuk60] and [Hub64]. The overarching motto is that any model (especially a parametric one) is only approximately valid, and that any estimator designed for a particular distribution that is to be used in practice must also be stable in the presence of model misspecification. The standard setup is to assume that the samples we are given come from a nice distribution, but that an adversary has the power to arbitrarily corrupt a constant fraction of the observed data. After several decades of work, the robust statistics community has discovered a myriad of estimators that are provably robust. An important feature of this line of work is that it can tolerate a constant fraction of corruptions independent of the dimension and that there are estimators for both the location (e.g., the mean) and scale (e.g., the covariance). See [HR09] and [HRRS86] for further background.\nIt turns out that there are vast gaps in our understanding of robustness, when computational considerations are taken into account. In one dimension, robustness and computational efficiency are in perfect harmony. The empirical mean and empirical variance are not robust, because a single corruption can arbitrarily bias these estimates, but alternatives such as the median and the interquartile range are straightforward to compute and are provably robust.\n∗Supported by NSF CAREER Award CCF-1652862, a Sloan Research Fellowship, and a Google Faculty Research Award. †Supported by NSF CCF-1551875, CCF-1617730, CCF-1650733, and ONR N00014-12-1-0999. ‡Supported by NSF CAREER Award CCF-1553288 and a Sloan Research Fellowship. §Supported by NSF CAREER Award CCF-1453261, a Google Faculty Research Award, and an NSF Fellowship. ¶Supported by NSF CAREER Award CCF-1453261, a grant from the MIT NEC Corporation, and a Google Faculty Research\nAward. ‖Research supported by a USC startup grant.\nar X\niv :1\n70 3.\n00 89\n3v 1\n[ cs\n.L G\n] 2\nBut in high dimensions, there is a striking tension between robustness and computational efficiency. Let us consider estimators for location. The Tukey median [Tuk60] is a natural generalization of the onedimensional median to high-dimensions. It is known that it behaves well (i.e., it needs few samples) when estimating the mean for various symmetric distributions [DG92, CGR15]. However, it is hard to compute in general [JP78, AK95] and the many heuristics for computing it degrade badly in the quality of their approximation as the dimension scales [CEM+93, Cha04, MS10]. The same issues plague estimators for scale. The minimum volume ellipsoid [Rou85] is a natural generalization of the one-dimensional interquartile range and is provably robust in high-dimensions, but is also hard to compute. And once again, heuristics for computing it [VAR09, RS98] work poorly in high dimensions.\nThe fact that robustness in high dimensions seems to come at such a steep price has long been a point of consternation within robust statistics. In a 1997 retrospective on the development of robust statistics, Huber laments:\n“It is one thing to design a theoretical algorithm whose purpose is to prove [large fractions of corruptions can be tolerated] and quite another thing to design a practical version that can be used not merely on small, but also on medium sized regression problems, with a 2000 by 50 matrix or so. This last requirement would seem to exclude all of the recently proposed [techniques].”\nThe goal of this paper is to answer Huber’s call to action and design estimators for both the mean and covariance that are highly practical, provably robust, and work in high-dimensions. Such estimators make the promise of robust statistics – estimators that work in high-dimensions and guarantee that their output has not been heavily biased by some small set of noisy samples – much closer to a reality.\nFirst, we make some remarks to dispel some common misconceptions. There has been a considerable amount of recent work on robust principal component analysis, much of it making use of semidefinite programming. Some of these works can tolerate a constant fraction of corruptions [CLMW11], however require that the locations of the corruptions are evenly spread throughout the dataset so that no individual sample is entirely corrupted. In contrast, the usual models in robust statistics are quite rigid in what they require and they do this for good reason. A common scenario that is used to motivate robust statistical methods is if two studies are mixed together, and one subpopulation does not fit the model. Then one wants estimators that work without assuming anything at all about these outliers.\nThere have also been semidefinite programming methods proposed for robust principal component analysis with outliers [XCS10]. These methods assume that the uncorrupted matrix is rank r and that the fraction of outliers is at most 1/r, which again degrades badly as the rank of the matrix increases. Moreover, any method that uses semidefinite programming will have difficulty scaling to the sizes of the problems we consider here. For sake of comparison – even with state-of-the-art interior point methods – it is not currently feasible to solve the types of semidefinite programs that have been proposed when the matrices have dimension larger than a hundred."
    }, {
      "heading" : "1.1 Robustness in a Generative Model",
      "text" : "Recent works in theoretical computer science have sought to circumvent the usual difficulties of designing efficient and robust algorithms by instead working in a generative model. The starting point for our paper is the work of [DKK+16] who gave an efficient algorithm for the problem of agnostically learning a Gaussian:\nGiven a polynomial number of samples from a high-dimensional Gaussian N (µ,Σ), where an adversary has arbitrarily corrupted an ε-fraction, find a set of parameters N ′(µ̂, Σ̂) that satisfy dTV (N ,N ′) ≤ Õ(ε).\nTotal variation distance is the natural metric to use to measure closeness of the parameters, since a 1 − ε fraction of the observed samples came from a Gaussian. [DKK+16] gave an algorithm for the above problem (note that the guarantees are dimension independent), whose running time and sample complexity are polynomial in the dimension d and 1/ε. [LRV16] independently gave an algorithm for the unknown mean case that achieves dTV (N ,N ′) ≤ Õ(ε √\nlog d), and in the unknown covariance case achieves guarantees in a weaker metric that is not affine invariant. A crucial feature is that both algorithms work even when the\nmoments of the underlying distribution satisfy certain conditions, and thus are not necessarily brittle to the modeling assumption that the inliers come from a Gaussian distribution.\nA more conceptual way to view such work is as a proof-of-concept that the Tukey median and minimum volume ellipsoid can be computed efficiently in a natural family of distributional models. This follows because not only would these be good estimates for the mean and covariance in the above model, but in fact any estimates that are good must also be close to them. Thus, these works fit into the emerging research direction of circumventing worst-case lower bounds by going beyond worst-case analysis.\nSince the dissemination of the aforementioned works [DKK+16, LRV16], there has been a flurry of research activity on computationally efficient robust estimation in a variety of high-dimensional settings [DKS16a, DKS16b, CSV16, DKK+17, Li17, DBS17], including studying graphical distributional models [DKS16a], understanding the computation-robustness tradeoff for statistical query algorithms [DKS16b], tolerating much more noise by allowing the algorithm to output a list of candidate hypotheses [CSV16], and developing robust algorithms under sparsity assumptions [Li17, DBS17], where the number of samples is sublinear in the dimension."
    }, {
      "heading" : "1.2 Our Results",
      "text" : "Our goal in this work is to show that high-dimensional robust estimation can be highly practical. However, there are two major obstacles to achieving this. First, the sample complexity and running time of the algorithms in [DKK+16] is prohibitively large for high-dimensional applications. We just would not be able to store as many samples as we would need, in order to compute accurate estimates, in high-dimensional applications.\nOur first main contribution is to show essentially tight bounds on the sample complexity of the filtering based algorithm of [DKK+16]. Roughly speaking, we accomplish this with a new definition of the good set which plugs into the existing analysis in a straightforward manner and shows that it is possible to estimate the mean with Õ(d/ε2) samples (when the covariance is known) and the covariance with Õ(d2/ε2) samples. Both of these bounds are information-theoretically optimal, up to logarithmic factors.\nOur second main contribution is to vastly improve the fraction of adversarial corruptions that can be tolerated in applications. The fraction of errors that the algorithms of [DKK+16] can tolerate is indeed a constant that is independent of the dimension, but it is very small both in theory and in practice. This is due to the fact that many of the steps in the algorithm are overly conservative. In fact, we found that a naive implementation of the algorithm did not remove any outliers in many realistic scenarios. We combat this by giving new ways to empirically tune the threshold for where to remove points from the sample set. These optimizations dramatically improve the empirical performance.\nFinally, we show that the same bounds on the error guarantee continue to work even when the underlying distribution is sub-Gaussian. This theoretically confirms that the robustness guarantees of such algorithms are in fact not overly brittle to the distributional assumptions. In fact, the filtering algorithm of [DKK+16] is easily shown to be robust under much weaker distributional assumptions, while retaining near-optimal sample and error guarantees. As an example, we show that it yields a near sample-optimal efficient estimator for robustly estimating the mean of a distribution, under the assumption that its covariance is bounded. Even in this regime, the filtering algorithm guarantees optimal error, up to a constant factor. Furthermore we empirically corroborate this finding by showing that the algorithm works well on real world data, as we describe below.\nNow we come to the task of testing out our algorithms. To the best of our knowledge, there have been no experimental evaluations of the performance of the myriad of approaches to robust estimation. It remains mostly a mystery which ones perform well in high-dimensions, and which do not. To test out our algorithms, we design a synthetic experiment where a (1 − ε)-fraction of the samples come from a Gaussian and the rest are noise and sampled from another distribution (in many cases, Bernoulli). This gives us a baseline to compare how well various algorithms recover µ and Σ, and how their performance degrades based on the dimension. Our plots show a predictable and yet striking phenomenon: All earlier approaches have error rates that scale polynomially with the dimension and ours is a constant that is almost indistinguishable from the error that comes from sample noise alone. Moreover, our algorithms are able to scale to hundreds of dimensions.\nBut are algorithms for agnostically learning a Gaussian unduly sensitive to the distributional assumptions\nthey make? We are able to give an intriguing visual demonstration of our techniques on real data. The famous study of [NJB+08] showed that performing principal component analysis on a matrix of genetic data recovers a map of Europe. More precisely, the top two singular vectors define a projection into the plane and when the groups of individuals are color-coded with where they are from, we recover familiar country boundaries that corresponds to the map of Europe. The conclusion from their study was that genes mirror geography. Given that one of the most important applications of robust estimation ought to be in exploratory data analysis, we ask: To what extent can we recover the map of Europe in the presence of noise? We show that when a small number of corrupted samples are added to the dataset, the picture becomes entirely distorted (and this continues to hold even for many other methods that have been proposed). In contrast, when we run our algorithm, we are able to once again recover the map of Europe. Thus, even when some fraction of the data has been corrupted (e.g., medical studies were pooled together even though the subpopulations studied were different), it is still possible to perform principal component analysis and recover qualitatively similar conclusions as if there were no noise at all!"
    }, {
      "heading" : "2 Formal Framework",
      "text" : "Notation. For a vector v, we will let ‖v‖2 denote its Euclidean norm. If M is a matrix, we will let ‖M‖2 denote its spectral norm and ‖M‖F denote its Frobenius norm. We will write X ∈u S to denote that X is drawn from the empirical distribution defined by S.\nRobust Estimation. We consider the following powerful model of robust estimation that generalizes many other existing models, including Huber’s contamination model:\nDefinition 2.1. Given ε > 0 and a distribution family D, the adversary operates as follows: The algorithm specifies some number of samples m. The adversary generates m samples X1, X2, . . . , Xm from some (unknown) D ∈ D. It then draws m′ from an appropriate distribution. This distribution is allowed to depend on X1, X2, . . . , Xm, but when marginalized over the m samples satisfies m\n′ ∼ Bin(ε,m). The adversary is allowed to inspect the samples, removes m′ of them, and replaces them with arbitrary points. The set of m points is then given to the algorithm.\nIn summary, the adversary is allowed to inspect the samples before corrupting them, both by adding corrupted points and deleting uncorrupted points. In contrast, in Huber’s model the adversary is oblivious to the samples and is only allowed to add corrupted points.\nWe remark that there are no computational restrictions on the adversary. The goal is to return the parameters of a distribution D̂ in D that are close to the true parameters in an appropriate metric. For the case of the mean, our metric will be the Euclidean distance. For the covariance, we will use the Mahalanobis distance, i.e., ‖Σ−1/2Σ̂Σ−1/2 − I‖F . This is a strong affine invariant distance that implies corresponding bounds in total variation distance.\nWe will use the following terminology:\nDefinition 2.2. We say that a set of samples is ε-corrupted if it is generated by the process described in Definition 2.1."
    }, {
      "heading" : "3 Nearly Sample-optimal Efficient Robust Learning",
      "text" : "In this section, we present near-sample optimal efficient robust estimators for the mean and the covariance of high-dimensional distributions under various structural assumptions of varying strength. Our estimators rely on the filtering technique introduced in [DKK+16].\nWe note that [DKK+16] gave two algorithmic techniques: the first one was a spectral technique to iteratively remove outliers from the dataset (filtering), and the second one was a soft-outlier removal method relying on convex programming. The filtering technique seemed amenable to practical implementation (as it only uses simple eigenvalue computations), but the corresponding sample complexity bounds given in [DKK+16] are polynomially worse than the information-theoretic minimum. On the other hand, the convex programming technique of [DKK+16] achieved better sample complexity bounds (e.g., near-sample optimal for robust mean estimation), but relied on the ellipsoid method.\nIn this work, we achieve the best of both worlds: we provide a more careful analysis of the filter technique that yields sample-optimal bounds (up to logarithmic factors) for both the mean and the covariance. Moreover, we show that the filtering technique easily extends to much weaker distributional assumptions (e.g., under bounded second moments). Roughly speaking, the filtering technique follows a general iterative recipe: (1) via spectral methods, find some univariate test which is violated by the corrupted points, (2) find some concrete tail bound violated by the corrupted set of points, and (3) throw away all points which violate this tail bound.\nWe start with sub-gaussian distributions. Recall that if P is sub-gaussian on Rd with mean vector µ and parameter ν > 0, then for any unit vector v ∈ Rd we have that PrX∼P [|v · (X − µ)| ≥ T ] ≤ exp(−t2/2ν).\nTheorem 3.1. Let G be a sub-gaussian distribution on Rd with parameter ν = Θ(1), mean µG, covariance matrix I, and ε > 0. Let S be an ε-corrupted set of samples from G of size Ω((d/ε2) poly log(d/ε)). There exists an efficient algorithm that, on input S and ε > 0, returns a mean vector µ̂ so that with probability at least 9/10 we have ‖µ̂− µG‖2 = O(ε √ log(1/ε)).\n[DKK+16] gave algorithms for robustly estimating the mean of a Gaussian distribution with known covariance and for robustly estimating the mean of a binary product distribution. The main motivation for considering these specific distribution families is that robustly estimating the mean within Euclidean distance immediately implies total variation distance bounds for these families. The above theorem establishes that these guarantees hold in a more general setting with near-sample optimal bounds. Under a bounded second moment assumption, we show:\nTheorem 3.2. Let P be a distribution on Rd with unknown mean vector µP and unknown covariance matrix ΣP σ2I. Let S be an ε-corrupted set of samples from P of size Θ((d/ε) log d). There exists an efficient algorithm that, on input S and ε > 0, with probability 9/10 outputs µ̂ with ‖µ̂− µP ‖2 ≤ O( √ εσ).\nThe sample size above is optimal, up to a logarithmic factor, and the error guarantee is easily seen to best possible up to a constant factor. The main difference between the filtering algorithm establishing the above theorem and the filtering algorithm for the sub-gaussian case is how we choose the threshold for the filter. Instead of looking for a violation of a concentration inequality, here we will choose a threshold at random. In this case, randomly choosing a threshold weighted towards higher thresholds suffices to throw out more corrupted samples than uncorrupted samples in expectation. Although it is possible to reject many good samples this way, we show that the algorithm still only rejects a total of O(ε) samples with high probability.\nFinally, for robustly estimating the covariance of a Gaussian distribution, we have:\nTheorem 3.3. Let G ∼ N (0,Σ) be a Gaussian in d dimensions, and let ε > 0. Let S be an ε-corrupted set of samples from G of size Ω((d2/ε2) poly log(d/ε)). There exists an efficient algorithm that, given S and ε, returns the parameters of a Gaussian distribution G′ ∼ N (0, Σ̂) so that with probability at least 9/10, it holds ‖I − Σ−1/2Σ̂Σ−1/2‖F = O(ε log(1/ε)).\nWe now provide a high-level description of the main ingredient which yields these improved sample complexity bounds. The initial analysis of [DKK+16] had a sample complexity sub-optimal by polynomial factors because it insisted that the set of good samples (i.e., before the corruption) satisfied very tight tail bounds. To some degree such bounds are necessary, as when we perform our filtering procedure, we need to ensure that not too many good samples are thrown away. However, the old analysis required that fairly strong tail bounds hold uniformly. The idea for the improvement is as follows: If the errors are sufficient to cause the variance of some polynomial p (linear in the unknown mean case or quadratic in the unknown covariance case) to increase by more than ε, it must be the case that for some T , roughly an ε/T 2 fraction of samples are error points with |p(x)| > T . As long as we can ensure that less than an ε/T 2 fraction of our good sample points have |p(x)| > T , this will suffice for our filtering procedure to work. For small values of T , these are much weaker tail bounds than we were needed previously, and thus can be achieved with a smaller number of samples. For large values of T , these tail bounds are comparable to those used in previous work [DKK+16] , but in such cases we can take advantage of the fact that |p(G)| > T only with very small probability, again allowing us to reduce the sample complexity. The details are deferred to Appendix A."
    }, {
      "heading" : "4 Filtering",
      "text" : "We now describe the filtering technique more rigorously. We also describe some additional heuristics we found useful in practice."
    }, {
      "heading" : "4.1 Robust Mean Estimation",
      "text" : "We first consider mean estimation. The algorithms which achieve Theorems 3.1 and 3.2 both follow the general recipe in Algorithm 1. We must specify three parameter functions:\n• Thres(ε) is a threshold function—we terminate if the covariance has spectral norm bounded by Thres(ε).\n• Tail(T, d, ε, δ, τ) is an univariate tail bound, which would only be violated by a τ fraction of points if they were uncorrupted, but is violated by many more of the current set of points.\n• δ(ε, s) is a slack function, which we require for technical reasons. Given these objects, our filter is fairly easy to state: first, we compute the empirical covariance. Then, we check if the spectral norm of the empirical covariance exceeds Thres(ε). If it does not, we output the empirical mean with the current set of data points. Otherwise, we project onto the top eigenvector of the empirical covariance, and throw away all points which violate Tail(T, d, ε, δ, τ), for some choice of slack function δ.\nAlgorithm 1 Filter-based algorithm template for robust mean estimation\n1: Input: An ε-corrupted set of samples S, Thres(ε),Tail(T, d, ε, δ, τ), δ(ε, s) 2: Compute the sample mean µS ′ = EX∈uS′ [X] 3: Compute the sample covariance matrix Σ 4: Compute approximations for the largest absolute eigenvalue of Σ, λ∗ := ‖Σ‖2, and the associated unit\neigenvector v∗. 5: if ‖Σ‖2 ≤ Thres(ε) then 6: return µS ′ . 7: Let δ = δ(ε, ‖Σ‖2). 8: Find T > 0 such that\nPr X∈uS′\n[ |v∗ · (X − µS ′ )| > T + δ ] > Tail(T, d, ε, δ, τ).\n9: return {x ∈ S′ : |v∗ · (x− µS′)| ≤ T + δ}.\nSub-gaussian case To concretely instantiate this algorithm for the subgaussian case, we take Thres(ε) = O(ε log 1/ε), δ(ε, s) = 3 √ ε(s− 1), and\nTail(T, d, ε, δ, τ) = 8 exp(−T 2/2ν) + 8 ε T 2 log(d log(d/ετ)) ,\nwhere ν is the subgaussian parameter. See Section A.1 for details.\nSecond moment case To concretely instantiate this algorithm for the second moment case, we take Thres(ε) = 9, δ = 0, and we take Tail to be a random rescaling of the largest deviation in the data set, in the direction v∗. See Section A.2 for details."
    }, {
      "heading" : "4.2 Robust Covariance Estimation",
      "text" : "Our algorithm for robust covariance follows the exact recipe outlined above, with one key difference—we check for deviations in the empirical fourth moment tensor. Intuitively, just as in the robust mean setting, we used degree-2 information to detect outliers for the mean (the degree-1 moment), here must use degree-4 information to detect outliers for the covariance (the degree-2 moment).\nMore concretely, this corresponds to finding a normalized degree-2 polynomial whose empirical variance is too large. By then filtering along this polynomial, with an appropriate choice of Thres(ε), δ(ε, s), and Tail, we achieve the desired bounds. See Section A.3 for the formal pseudocode and more details."
    }, {
      "heading" : "4.3 Better Univariate Tests",
      "text" : "In the algorithms described above for robust mean estimation, after projecting onto one dimension, we center the points at the empirical mean along this direction. This is theoretically sufficient, however, introduces additional constant factors since the empirical mean along this direction may be corrupted. Instead, one can use a robust estimate for the mean in one direction. Namely, it is well-known that the median is an optimal robust statistics for the mean in one dimension [DKW56, DK14]. By centering the points at the median instead of the mean, we are able to achieve better error in practice."
    }, {
      "heading" : "4.4 Adaptive Tail Bounding",
      "text" : "In our empirical evaluation, we found that it was important to find a good choice of Tail, to achieve good error rates, especially for robust covariance estimation. Concretely, in this setting, our tail bound is given by\nTail(T, d, ε, δ, τ) = C1 exp(−C2T ) + Tail2(T, d, ε, δ, τ) ,\nfor some function Tail2, and constants C1, C2. We found that for reasonable settings, the term that dominated was always the first term on the RHS, and that Tail2 is less significant. Thus, we focused on optimizing the first term.\nWe found that depending on the setting, it was useful to change the constant C2. In particular, in low dimensions, we could be more stringent, and enforce a stronger tail bound (which corresponds to a higher C2), but in higher dimensions, we must be more lax with the tail bound. To do this in a principled manner, we introduced a heuristic we call adaptive tail bounding. Our goal is to find a choice of C2 which throws away roughly an ε-fraction of points. The heuristic is fairly simple: we start with some initial guess for C2. We then run our filter with this C2. If we throw away too many data points, we increase our C2, and retry. If we throw away too few, then we decrease our C2 and retry. Since increasing C2 strictly decreases the number of points thrown away, and vice versa, we binary search over our choice of C2 until we reach something close to our target accuracy. In our current implementation, we stop when the fraction of points we throw away is between ε/2 and 3ε/2, or if we’ve binary searched for too long. We found that this heuristic drastically improves our accuracy, and allows our algorithm to scale fairly smoothly from low to high dimension."
    }, {
      "heading" : "5 Experiments",
      "text" : "We performed an empirical evaluation of the above algorithms on synthetic and real data sets with and without synthetic noise. All experiments were done on a laptop computer with a 2.7 GHz Intel Core i5 CPU and 8 GB of RAM. The focus of this evaluation was on statistical accuracy, not time efficiency. In this measure, our algorithm performs the best of all algorithms we tried. In all synthetic trials, our algorithm consistently had the smallest error. In fact, in some of the synthetic benchmarks, our error was orders of magnitude better than any other algorithms. In the semi-synthetic benchmark, our algorithm also (arguably) performs the best, though there is no way to tell for sure, since there is no ground truth. We also note that despite not optimizing our code for runtime, the runtime of our algorithm is always comparable, and in many cases, better than the alternatives which provided comparable error."
    }, {
      "heading" : "5.1 Synthetic Data",
      "text" : "Experiments with synthetic data allow us to verify the error guarantees and the sample complexity rates proven in Section 3 for unknown mean and unknown covariance. In both cases, the experiments validate the accuracy and usefulness of our algorithm, almost exactly matching the best rate without noise.\nUnknown mean The results of our synthetic mean experiment are shown in Figure 1. In the synthetic mean experiment, we set ε = 0.1, and for dimension d = [100, 150, . . . , 400], we generate n = 10dε2 samples,\nwhere a (1 − ε)-fraction come from N (µ, I), and an ε fraction come from a noise distribution. Our goal is to produce an estimator which minimizes the `2 error the estimator has to the truth. As a baseline, we compute the error that is achieved by only the uncorrupted sample points. This error will be used as the gold standard for comparison, since in the presence of error, this is roughly the best one could do even if all the noise points were identified exactly.1\nOn this data, we compared the performance of our Filter algorithm to that of (1) the empirical mean of all the points, (2) a trivial pruning procedure, (3) the geometric median of the data, (4) a RANSAC-based mean estimation algorithm, and (5) a recently proposed robust estimator for the mean due to [LRV16], which we will call LRVMean. For (5), we use the implementation available in their Github.2 In Figure 1, the x-axis indicates the dimension of the experiment, and the y-axis measures the `2 error of our estimated mean minus the `2 error of the empirical mean of the true samples from the Gaussian, i.e., the excess error induced over the sampling error.\nWe tried various noise distributions, and found that the same qualitative pattern arose for all of them. In the reported experiment, our noise distribution was a mixture of two binary product distributions, where one had a couple of large coordinates (see Section B.1 for a detailed description). For all (nontrivial) error distributions we tried, we observed that indeed the empirical mean, pruning, geometric median, and RANSAC all have error which diverges as d grows, as the theory predicts. On the other hand, both our algorithm and LRVMean have markedly smaller error as a function of dimension. Indeed, our algorithm’s error is almost identical to that of the empirical mean of the uncorrupted sample points.\nUnknown covariance The results of our synthetic covariance experiment are shown in Figure 2. Our setup is similar to that for the synthetic mean. Since both our algorithm and LRVCov require access to fourth moment objects, we ran into issues with limited memory on machines. Thus, we could not perform experiments at as high a dimension as for the unknown mean setting, and we could not use as many samples. We set ε = 0.05, and for dimension d = [10, 20, . . . , 100], we generate n = 0.5dε2 samples, where a (1 − ε)-fraction come from N (0,Σ), and an ε fraction come from a noise distribution. We measure distance in the natural affine invariant way, namely, the Mahalanobis distance induced by Σ to the identity: err(Σ̂) = ‖Σ−1/2Σ̂Σ−1/2− I‖F . As explained above, this is the right affine-invariant metric for this problem. As before, we use the empirical error of only the uncorrupted data points as a benchmark.\nOn this corrupted data, we compared the performance of our Filter algorithm to that of (1) the empirical covariance of all the points, (2) a trivial pruning procedure, (3) a RANSAC-based minimal volume ellipsoid (MVE) algorithm, and (5) a recently proposed robust estimator for the covariance due to [LRV16], which we will call LRVCov. For (5), we again obtained the implementation from their Github repository.\nWe tried various choices of Σ and noise distribution. Figure 2 shows two choices of Σ and noise. Again, the x-axis indicates the dimension of the experiment and the y-axis indicates the estimator’s excess Mahalanobis error over the sampling error. In the left figure, we set Σ = I, and our noise points are simply all located at the all-zeros vector. In the right figure, we set Σ = I + 10e1e T 1 , where e1 is the first basis vector, and our noise distribution is a somewhat more complicated distribution, which is similarly spiked, but in a different, random, direction. We formally define this distribution in Section B.1. For all choices of Σ and noise we tried, the qualitative behavior of our algorithm and LRVCov was unchanged. Namely, we seem to match the empirical error without noise up to a very small slack, for all dimensions. On the other hand, the performance of empirical mean, pruning, and RANSAC varies widely with the noise distribution. The performance of all these algorithms degrades substantially with dimension, and their error gets worse as we increase the skew of the underlying data. The performance of LRVCov is the most similar to ours, but again is worse by a large constant factor. In particular, our excess risk was on the order of 10−4 for large d, for both experiments, whereas the excess risk achieved by LRVCov was in all cases a constant between 0.1 and 2.\nDiscussion These experiments demonstrate that our statistical guarantees are in fact quite strong. In particular, since our excess error is almost zero (and orders of magnitude smaller than other approaches), this suggests that our sample complexity is indeed close to optimal, since we match the rate without noise, and that the constants and logarithmic factors in the theoretical recovery guarantee are often small or non-existent.\n1We note that it is possible that an estimator may achieve slightly better error than this baseline. 2https://github.com/kal2000/AgnosticMean\\AndCovarianceCode"
    }, {
      "heading" : "5.2 Semi-synthetic Data",
      "text" : "To demonstrate the efficacy of our method on real data, we revisit the famous study of [NJB+08]. In this study, the authors investigated data collected as part of the Population Reference Sample (POPRES) project. This dataset consists of the genotyping of thousands of individuals using the Affymetrix 500K single nucleotide polymorphism (SNP) chip. The authors pruned the dataset to obtain the genetic data of over 1387 European individuals, annotated by their country of origin. Using principal components analysis, they produce a two-dimensional summary of the genetic variation, which bears a striking resemblance to the map of Europe.\nOur experimental setup is as follows. While the original dataset is very high dimensional, we use a 20 dimensional version of the dataset as found in the authors’ GitHub3. We first randomly rotate the data, as then 20 dimensional data was diagonalized, and the high dimensional data does not follow such structure. We then add an additional ε1−ε fraction of points (so that they make up an ε-fraction of the final points). These added points were discrete points, following a simple product distribution (see Section B.1 for full details). We used a number of methods to obtain a covariance matrix for this dataset, and we projected the data onto the top two singular vectors of this matrix. In Figure 3, we show the results when we compare our techniques to pruning. In particular, our output was able to more or less reproduce the map of Europe,\n3https://github.com/NovembreLab/Novembre_etal_2008_misc\nwhereas pruning fails to. In Section B.2, we also compare our result with a number of other techniques, including those we tested against in the unknown covariance experiments, and other robust PCA techniques. The only alternative algorithm which was able to produce meaningful output was LRVCov, which produced output that was similar to ours, but which produced a map which was somewhat more skewed. We believe that our algorithm produces the best picture.\nIn Figure 3, we also display the actual points which were output by our algorithm’s Filter. While it manages to remove most of the noise points, it also seems to remove some of the true data points, particularly those from Eastern Europe and Turkey. We attribute this to a lack of samples from these regions, and thus one could consider them as outliers to a dataset consisting of Western European individuals. For instance, Turkey had 4 data points, so it seems quite reasonable that any robust algorithm would naturally consider these points outliers.\nDiscussion We view our experiments as a proof of concept demonstration that our techniques can be useful in real world exploratory data analysis tasks, particularly those in high-dimensions. Our experiments reveal that a minimal amount of noise can completely disrupt a data analyst’s ability to notice an interesting phenomenon, thus limiting us to only very well-curated data sets. But with robust methods, this noise does not interfere with scientific discovery, and we can still recover interesting patterns which otherwise would have been obscured by noise."
    }, {
      "heading" : "A Omitted Details from Section 3",
      "text" : "A.1 Robust Mean Estimation for Sub-Gaussian Distributions\nIn this section, we use our filter technique to give a near-sample optimal computationally efficient algorithm to robustly estimate the mean of a sub-gaussian density with a known covariance matrix, thus proving Theorem 3.1.\nWe emphasize that the algorithm and its analysis is essentially identical to the filtering algorithm given in Section 8.1 of [DKK+16] for the case of a Gaussian N (µ, I). The only difference is a weaker definition of the “good set of samples” (Definition A.4) and a simple concentration argument (Lemma A.5) showing that a random set of uncorrupted samples of the appropriate size is good with high probability. Given these, the analysis of this subsection follows straightforwardly from the analysis in Section 8.1 of [DKK+16] by plugging in the modified parameters. For the sake of completeness, we provide the details below.\nWe start by formally defining sub-gaussian distributions:\nDefinition A.1. A distribution P on R with mean µ, is sub-gaussian with parameter ν > 0 if\nEX∼P [exp(λ(X − µ))] ≤ exp(νλ2/2)\nfor all λ ∈ R. A distribution P on Rd with mean vector µ is sub-gaussian with parameter ν > 0, if for all unit vectors v, the one-dimensional random variable v ·X, X ∼ P , is sub-gaussian with parameter ν.\nWe will use the following simple fact about the concentration of sub-gaussian random variables:\nFact A.2. If P is sub-gaussian on Rd with mean vector µ and parameter ν > 0, then for any unit vector v ∈ Rd we have that PrX∼P [|v · (X − µ)| ≥ T ] ≤ exp(−t2/2ν).\nThe following theorem is a high probability version of Theorem 3.1:\nTheorem A.3. Let G be a sub-gaussian distribution on Rd with parameter ν = Θ(1), mean µG, covariance matrix I, and ε, τ > 0. Let S′ be an ε-corrupted set of samples from G of size Ω((d/ε2) poly log(d/ετ)). There exists an efficient algorithm that, on input S′ and ε > 0, returns a mean vector µ̂ so that with probability at least 1− τ we have ‖µ̂− µG‖2 = O(ε √ log(1/ε)). Notation. We will denote µS = 1|S| ∑ X∈S X and MS = 1 |S| ∑ X∈S(X−µG)(X−µG)T for the sample mean and modified sample covariance matrix of the set S.\nWe start by defining our modified notion of good sample, i.e, a set of conditions on the uncorrupted set of samples under which our algorithm will succeed.\nDefinition A.4. Let G be an identity covariance sub-gaussian in d dimensions with mean µG and covariance matrix I and ε, τ > 0. We say that a multiset S of elements in Rd is (ε, τ)-good with respect to G if the following conditions are satisfied:\n(i) For all x ∈ S we have ‖x− µG‖2 ≤ O( √ d log(|S|/τ)).\n(ii) For every affine function L : Rd → R such that L(x) = v · (x − µG) − T , ‖v‖2 = 1, we have that |PrX∈uS [L(X) ≥ 0]− PrX∼G[L(X) ≥ 0]| ≤ εT 2 log(d log( dετ )) .\n(iii) We have that ‖µS − µG‖2 ≤ ε.\n(iv) We have that ‖MS − I‖2 ≤ ε.\nWe show in the following subsection that a sufficiently large set of independent samples from G is (ε, τ)good (with respect to G) with high probability. Specifically, we prove:\nLemma A.5. Let G be sub-gaussian distribution with parameter ν = Θ(1) and with identity covariance, and ε, τ > 0. If the multiset S is obtained by taking Ω((d/ε2) poly log(d/ετ)) independent samples from G, it is (ε, τ)-good with respect to G with probability at least 1− τ.\nWe require the following definition that quantifies the extent to which a multiset has been corrupted:\nDefinition A.6. Given finite multisets S and S′ we let ∆(S, S′) be the size of the symmetric difference of S and S′ divided by the cardinality of S.\nThe starting point of our algorithm will be a simple NaivePrune routine (Section 4.3.1 of [DKK+16]) that removes obvious outliers, i.e., points which are far from the mean. Then, we iterate the algorithm whose performance guarantee is given by the following:\nProposition A.7. Let G be a sub-gaussian distribution on Rd with parameter ν = Θ(1), mean µG, covariance matrix I, ε > 0 be sufficiently small and τ > 0. Let S be an (ε, τ)-good set with respect to G. Let S′ be any multiset with ∆(S, S′) ≤ 2ε and for any x, y ∈ S′, ‖x − y‖2 ≤ O( √ d log(d/ετ)). There exists a polynomial time algorithm Filter-Sub-Gaussian-Unknown-Mean that, given S′ and ε > 0, returns one of the following:\n(i) A mean vector µ̂ such that ‖µ̂− µG‖2 = O(ε √ log(1/ε)).\n(ii) A multiset S′′ ⊆ S′ such that ∆(S, S′′) ≤ ∆(S, S′)− ε/α, where α def= d log(d/ετ) log(d log( dετ )).\nWe start by showing how Theorem A.3 follows easily from Proposition A.7.\nProof of Theorem A.3. By the definition of ∆(S, S′), since S′ has been obtained from S by corrupting an ε-fraction of the points in S, we have that ∆(S, S′) ≤ 2ε. By Lemma A.5, the set S of uncorrupted samples is (ε, τ)-good with respect to G with probability at least 1− τ. We henceforth condition on this event.\nSince S is (ε, τ)-good, all x ∈ S have ‖x− µG‖2 ≤ O( √ d log |S|/τ). Thus, the NaivePrune procedure does not remove from S′ any member of S. Hence, its output, S′′, has ∆(S, S′′) ≤ ∆(S, S′) and for any x ∈ S′′, there is a y ∈ S with ‖x − y‖2 ≤ O( √ d log |S|/τ). By the triangle inequality, for any x, z ∈ S′′,\n‖x− z‖2 ≤ O( √ d log |S|/τ) = O( √ d log(d/ετ)).\nThen, we iteratively apply the Filter-Sub-Gaussian-Unknown-Mean procedure of Proposition A.7 until it terminates returning a mean vector µ with ‖µ̂− µG‖2 = O(ε √ log(1/ε)). We claim that we need at most O(α) iterations for this to happen. Indeed, the sequence of iterations results in a sequence of sets S′i, so that ∆(S, S′i) ≤ ∆(S, S′)− i · ε/α. Thus, if we do not output the empirical mean in the first 2α iterations, in the next iteration there are no outliers left and the algorithm terminates outputting the sample mean of the remaining set.\nA.1.1 Algorithm Filter-Sub-Gaussian-Unknown-Mean: Proof of Proposition A.7\nIn this subsection, we describe the efficient algorithm establishing Proposition A.7 and prove its correctness. Our algorithm calculates the empirical mean vector µS ′ and empirical covariance matrix Σ. If the matrix Σ has no large eigenvalues, it returns µS ′ . Otherwise, it uses the eigenvector v∗ corresponding to the maximum magnitude eigenvalue of Σ and the mean vector µS ′\nto define a filter. Our efficient filtering procedure is presented in detailed pseudocode below.\nA.1.2 Proof of Correctness of Filter-Sub-Gaussian-Unknown-Mean\nBy definition, there exist disjoint multisets L,E, of points in Rd, where L ⊂ S, such that S′ = (S \\ L) ∪ E. With this notation, we can write ∆(S, S′) = |L|+|E||S| . Our assumption ∆(S, S\n′) ≤ 2ε is equivalent to |L|+|E| ≤ 2ε · |S|, and the definition of S′ directly implies that (1− 2ε)|S| ≤ |S′| ≤ (1 + 2ε)|S|. Throughout the proof, we assume that ε is a sufficiently small constant.\nWe define µG, µS , µS ′ , µL, and µE to be the means of G,S, S′, L, and E, respectively. Our analysis will make essential use of the following matrices:\n• MS′ denotes EX∈uS′ [(X − µG)(X − µG)T ],\n• MS denotes EX∈uS [(X − µG)(X − µG)T ],\n• ML denotes EX∈uL[(X − µG)(X − µG)T ], and\nAlgorithm 2 Filter algorithm for a sub-gaussian with unknown mean and identity covariance\n1: procedure Filter-Sub-Gaussian-Unknown-Mean(S′, ε, τ) input: A multiset S′ such that there exists an (ε, τ)-good S with ∆(S, S′) ≤ 2ε output: Multiset S′′ or mean vector µ̂ satisfying Proposition A.7 2: Compute the sample mean µS ′ = EX∈uS′ [X] and the sample covariance matrix Σ , i.e., Σ =\n(Σi,j)1≤i,j≤d with Σi,j = EX∈uS′ [(Xi − µS ′ i )(Xj − µS ′\nj )]. 3: Compute approximations for the largest absolute eigenvalue of Σ − I, λ∗ := ‖Σ − I‖2, and the\nassociated unit eigenvector v∗. 4: if ‖Σ− I‖2 ≤ O(ε log(1/ε)), then return µS ′ .\n5: Let δ := 3 √ ε‖Σ− I‖2. Find T > 0 such that\nPr X∈uS′\n[ |v∗ · (X − µS ′ )| > T + δ ] > 8 exp(−T 2/2ν) + 8 ε\nT 2 log ( d log( dετ ) ) . 6: return the multiset S′′ = {x ∈ S′ : |v∗ · (x− µS′)| ≤ T + δ}.\n• ME denotes EX∈uE [(X − µG)(X − µG)T ].\nOur analysis will hinge on proving the important claim that Σ− I is approximately (|E|/|S′|)ME . This means two things for us. First, it means that if the positive errors align in some direction (causing ME to have a large eigenvalue), there will be a large eigenvalue in Σ− I. Second, it says that any large eigenvalue of Σ − I will correspond to an eigenvalue of ME , which will give an explicit direction in which many error points are far from the empirical mean.\nUseful Structural Lemmas. We begin by noting that we have concentration bounds on G and therefore, on S due to its goodness.\nFact A.8. Let w ∈ Rd be any unit vector, then for any T > 0, PrX∼G [ |w · (X − µG)| > T ] ≤ 2 exp(−T 2/2ν)\nand PrX∈uS [ |w · (X − µG)| > T ] ≤ 2 exp(−T 2/2ν) + ε\nT 2 log(d log( dετ )) .\nProof. The first line is Fact A.2, and the former follows from it using the goodness of S.\nBy using the above fact, we obtain the following simple claim:\nClaim A.9. Let w ∈ Rd be any unit vector, then for any T > 0, we have that:\nPr X∼G\n[|w · (X − µS ′ )| > T + ‖µS ′ − µG‖2] ≤ 2 exp(−T 2/2ν).\nand Pr\nX∈uS [|w · (X − µS\n′ )| > T + ‖µS ′ − µG‖2] ≤ 2 exp(−T 2/2ν) +\nε T 2 log ( d log( dετ ) ) . Proof. This follows from Fact A.8 upon noting that |w · (X−µS′)| > T +‖µS′−µG‖2 only if |w · (X−µG)| > T .\nWe can use the above facts to prove concentration bounds for L. In particular, we have the following lemma:\nLemma A.10. We have that ‖ML‖2 = O (log(|S|/|L|) + ε|S|/|L|).\nProof. Since L ⊆ S, for any x ∈ Rd, we have that\n|S| · Pr X∈uS (X = x) ≥ |L| · Pr X∈uL (X = x) . (1)\nSince ML is a symmetric matrix, we have ‖ML‖2 = max‖v‖2=1 |vTMLv|. So, to bound ‖ML‖2 it suffices to bound |vTMLv| for unit vectors v. By definition of ML, for any v ∈ Rd we have that\n|vTMLv| = EX∈uL[|v · (X − µG)|2].\nFor unit vectors v, the RHS is bounded from above as follows:\nEX∈uL [ |v · (X − µG)|2 ] = 2 ∫ ∞ 0 Pr X∈uL [ |v · (X − µG)| > T ] TdT\n= 2 ∫ O(√d log(d/ετ)) 0 Pr X∈uL [|v · (X − µG)| > T ]TdT\n≤ 2 ∫ O(√d log(d/ετ))\n0\nmin { 1, |S| |L| · Pr X∈uS [ |v · (X − µG)| > T ]} TdT\n∫ 4√ν log(|S|/|L|)\n0\nTdT\n+ (|S|/|L|) ∫ O(√d log(d/ετ))\n4 √ ν log(|S|/|L|)\n( exp(−T 2/2ν) + ε\nT 2 log ( d log( dετ ) ))TdT log(|S|/|L|) + ε · |S|/|L| ,\nwhere the second line follows from the fact that ‖v‖2 = 1, L ⊂ S, and S satisfies condition (i) of Definition A.4; the third line follows from (1); and the fourth line follows from Fact A.8.\nAs a corollary, we can relate the matrices MS′ and ME , in spectral norm:\nCorollary A.11. We have that MS′ − I = (|E|/|S′|)ME + O(ε log(1/ε)), where the O(ε log(1/ε)) term denotes a matrix of spectral norm O(ε log(1/ε)).\nProof. By definition, we have that |S′|MS′ = |S|MS − |L|ML + |E|ME . Thus, we can write\nMS′ = (|S|/|S′|)MS − (|L|/|S′|)ML + (|E|/|S′|)ME = I +O(ε) +O(ε log(1/ε)) + (|E|/|S′|)ME ,\nwhere the second line uses the fact that 1 − 2ε ≤ |S|/|S′| ≤ 1 + 2ε, the goodness of S (condition (iv) in Definition A.4), and Lemma A.10. Specifically, Lemma A.10 implies that (|L|/|S′|)‖ML‖2 = O(ε log(1/ε)). Therefore, we have that\nMS′ = I + (|E|/|S′|)ME +O(ε log(1/ε)) ,\nas desired.\nWe now establish a similarly useful bound on the difference between the mean vectors:\nLemma A.12. We have that µS ′ − µG = (|E|/|S′|)(µE − µG) +O(ε √ log(1/ε)), where the O(ε √ log(1/ε))\nterm denotes a vector with `2-norm at most O(ε √ log(1/ε)).\nProof. By definition, we have that\n|S′|(µS ′ − µG) = |S|(µS − µG)− |L|(µL − µG) + |E|(µE − µG).\nSince S is a good set, by condition (iii) of Definition A.4, we have ‖µS−µG‖2 = O(ε). Since 1−2ε ≤ |S|/|S′| ≤ 1 + 2ε, it follows that (|S|/|S′|)‖µS − µG‖2 = O(ε). Using the valid inequality ‖ML‖2 ≥ ‖µL − µG‖22 and Lemma A.10, we obtain that ‖µL − µG‖2 ≤ O (√ log(|S|/|L|) + √ ε|S|/|L| ) . Therefore,\n(|L|/|S′|)‖µL − µG‖2 ≤ O ( (|L|/|S|) √ log(|S|/|L|) + √ ε|L|/|S| ) = O(ε √ log(1/ε)) .\nIn summary, µS ′ − µG = (|E|/|S′|)(µE − µG) +O(ε √ log(1/ε)) ,\nas desired. This completes the proof of the lemma.\nBy combining the above, we can conclude that Σ−I is approximately proportional to ME . More formally, we obtain the following corollary:\nCorollary A.13. We have Σ− I = (|E|/|S′|)ME +O(ε log(1/ε)) +O(|E|/|S′|)2‖ME‖2, where the additive terms denote matrices of appropriately bounded spectral norm.\nProof. By definition, we can write Σ − I = MS′ − I − (µS ′ − µG)(µS′ − µG)T . Using Corollary A.11 and Lemma A.12, we obtain:\nΣ− I = (|E|/|S′|)ME +O(ε log(1/ε)) +O((|E|/|S′|)2‖µE − µG‖22) +O(ε2 log(1/ε)) = (|E|/|S′|)ME +O(ε log(1/ε)) +O(|E|/|S′|)2‖ME‖2 ,\nwhere the second line follows from the valid inequality ‖ME‖2 ≥ ‖µE −µG‖22. This completes the proof.\nCase of Small Spectral Norm. We are now ready to analyze the case that the mean vector µS ′ is returned by the algorithm in Step 4. In this case, we have that λ∗ def = ‖Σ − I‖2 = O(ε log(1/ε)). Hence, Corollary A.13 yields that (|E|/|S′|)‖ME‖2 ≤ λ∗ +O(ε log(1/ε)) +O(|E|/|S′|)2‖ME‖2 ,\nwhich in turns implies that (|E|/|S′|)‖ME‖2 = O(ε log(1/ε)) .\nOn the other hand, since ‖ME‖2 ≥ ‖µE − µG‖22, Lemma A.12 gives that\n‖µS ′ − µG‖2 ≤ (|E|/|S′|) √ ‖ME‖2 +O(ε √ log(1/ε)) = O(ε √ log(1/ε)).\nThis proves part (i) of Proposition A.7.\nCase of Large Spectral Norm. We next show the correctness of the algorithm when it returns a filter in Step 5.\nWe start by proving that if λ∗ def = ‖Σ − I‖2 > Cε log(1/ε), for a sufficiently large universal constant C, then a value T satisfying the condition in Step 5 exists. We first note that that ‖ME‖2 is appropriately large. Indeed, by Corollary A.13 and the assumption that λ∗ > Cε log(1/ε) we deduce that\n(|E|/|S′|)‖ME‖2 = Ω(λ∗) . (2)\nMoreover, using the inequality ‖ME‖2 ≥ ‖µE − µG‖22 and Lemma A.12 as above, we get that\n‖µS ′ − µG‖2 ≤ (|E|/|S′|) √ ‖ME‖2 +O(ε √ log(1/ε)) ≤ δ/2 , (3)\nwhere we used the fact that δ def = √ ελ∗ > C ′ε √ log(1/ε).\nSuppose for the sake of contradiction that for all T > 0 we have that\nPr X∈uS′\n[ |v∗ · (X − µS ′ )| > T + δ ] ≤ 8 exp(−T 2/2ν) + 8 ε\nT 2 log ( d log( dετ ) ) . Using (3), we obtain that for all T > 0 we have that\nPr X∈uS′\n[ |v∗ · (X − µG)| > T + δ/2 ] ≤ 8 exp(−T 2/2ν) + 8 ε\nT 2 log ( d log( dετ ) ) . (4) Since E ⊆ S′, for all x ∈ Rd we have that |S′|PrX∈uS′ [X = x] ≥ |E|PrY ∈uE [Y = x]. This fact combined with (4) implies that for all T > 0\nPr X∈uE\n[ |v∗ · (X − µG)| > T + δ/2 ] (|S′|/|E|) ( exp(−T 2/2ν) + ε\nT 2 log ( d log( dετ )\n)) . (5)\nWe now have the following sequence of inequalities: ‖ME‖2 = EX∈uE [ |v∗ · (X − µG)|2 ] = 2 ∫ ∞ 0 Pr X∈uE [ |v∗ · (X − µG)| > T ] TdT\n= 2 ∫ O(√d log(d/ετ)) 0 Pr X∈uE [ |v∗ · (X − µG)| > T ] TdT\n≤ 2 ∫ O(√d log(d/ετ))\n0\nmin { 1, |S′| |E| · Pr X∈uS′ [ |v∗ · (X − µG)| > T ]} TdT\n∫ 4√ν log(|S′|/|E|)+δ\n0\nTdT + (|S′|/|E|) ∫ O(√d log(d/ετ))\n4 √ ν log(|S′|/|E|)+δ\n( exp(−T 2/2ν) + ε\nT 2 log ( d log( dετ ) ))TdT log(|S′|/|E|) + δ2 +O(1) + ε · |S′|/|E| log(|S′|/|E|) + ελ∗ + ε · |S′|/|E| .\nRearranging the above, we get that\n(|E|/|S′|)‖ME‖2 (|E|/|S′|) log(|S′|/|E|) + (|E|/|S′|)ελ∗ + ε = O(ε log(1/ε) + ε2λ∗).\nCombined with (2), we obtain λ∗ = O(ε log(1/ε)), which is a contradiction if C is sufficiently large. Therefore, it must be the case that for some value of T the condition in Step 5 is satisfied.\nThe following claim completes the proof:\nClaim A.14. Fix α def = d log(d/ετ) log(d log( dετ )). We have that ∆(S, S ′′) ≤ ∆(S, S′)− 2ε/α . Proof. Recall that S′ = (S \\L)∪E, with E and L disjoint multisets such that L ⊂ S. We can similarly write S′′ = (S \\ L′) ∪ E′, with L′ ⊇ L and E′ ⊂ E. Since\n∆(S, S′)−∆(S, S′′) = |E \\ E ′| − |L′ \\ L| |S| ,\nit suffices to show that |E \\E′| ≥ |L′ \\L|+ ε|S|/α. Note that |L′ \\L| is the number of points rejected by the filter that lie in S ∩ S′. Note that the fraction of elements of S that are removed to produce S′′ (i.e., satisfy |v∗ · (x − µS′)| > T + δ) is at most 2 exp(−T 2/2ν) + ε/α. This follows from Claim A.9 and the fact that T = O( √ d log(d/ετ)).\nHence, it holds that |L′ \\ L| ≤ (2 exp(−T 2/2ν) + ε/α)|S|. On the other hand, Step 5 of the algorithm ensures that the fraction of elements of S′ that are rejected by the filter is at least 8 exp(−T 2/2ν) + 8ε/α). Note that |E \\ E′| is the number of points rejected by the filter that lie in S′ \\ S. Therefore, we can write:\n|E \\ E′| ≥ (8 exp(−T 2/2ν) + 8ε/α)|S′| − (2 exp(−T 2/2ν) + ε/α)|S| ≥ (8 exp(−T 2/2ν) + 8ε/α)|S|/2− (2 exp(−T 2/2ν) + ε/α)|S| ≥ (2 exp(−T 2/2ν) + 3ε/α)|S| ≥ |L′ \\ L|+ 2ε|S|/α ,\nwhere the second line uses the fact that |S′| ≥ |S|/2 and the last line uses the fact that |L′ \\ L|/|S| ≤ 2 exp(−T 2/2ν) + ε/α. Noting that log(d/ετ) ≥ 1, this completes the proof of the claim.\nA.1.3 Proof of Lemma A.5\nProof. Let N = Ω((d/ε2) poly log(d/ετ)) be the number of samples drawn from G. For (i), the probability that a coordinate of a sample is at least √ 2ν log(Nd/3τ) is at most τ/3dN by Fact A.2. By a union bound,\nthe probability that all coordinates of all samples are smaller than √\n2ν log(Nd/3τ) is at least 1 − τ/3. In this case, ‖x‖2 ≤ √ 2νd log(Nd/3τ) = O( √ dν log(Nν/τ)).\nAfter translating by µG, we note that (iii) follows immediately from Lemmas 4.3 of [DKK+16] and (iv) follows from Theorem 5.50 of [Ver10], as long as N = Ω(ν4d log(1/τ)/ε2), with probability at least 1− τ/3. It remains to show that, conditioned on (i), (ii) holds with probability at least 1− τ/3.\nTo simplify some expressions, let δ := ε/(log(d log d/ετ)) and R = C √ d log(|S|/τ). We need to show that for all unit vectors v and all 0 ≤ T ≤ R that∣∣∣∣ PrX∈uS[|v · (X − µG)| > T ]− PrX∼G[|v · (X − µG) > T ≥ 0] ∣∣∣∣ ≤ δT 2 . (6)\nFirstly, we show that for all unit vectors v and T > 0∣∣∣∣ PrX∈uS[|v · (X − µG)| > T ]− PrX∼G[|v · (X − µG)| > T ≥ 0] ∣∣∣∣ ≤ δ/4ν ln(1/δ)\nwith probability at least 1 − τ/6. Since the VC-dimension of the set of all halfspaces is d + 1, this follows from the VC inequality [DL01], since we have more than Ω(d/(δ/(4ν log(1/δ))2) samples. We thus only need to consider the case when T ≥ √ 4ν ln(1/δ).\nLemma A.15. For any fixed unit vector v and T > √\n4ν ln(1/δ), except with probability exp(−Nδ/6Cν), we have that\nPr X∈uS [|v · (X − µG)| > T ] ≤ δ CT 2 ,\nwhere C = 8.\nProof. Let E be the event that |v · (X − µG)| > T . Since G is sub-gaussian, Fact A.2 yields that PrG[E] = PrY∼G[|v · (X − µG)| ≤ exp(−T 2/2ν). Note that, thanks to our assumption on T , we have that T ≤ exp(T 2/4ν)/2C, and therefore T 2 PrG[E] ≤ exp(−T 2/4ν)/2C ≤ δ/2C.\nConsider ES [exp(t2/3ν ·N PrS [E])]. Each individual sample sample Xi for 1 ≤ i ≤ N , is an independent copy of Y ∼ G, and hence:\nES [ exp(T 2/3ν ·N Pr\nS [E])\n] = ES [ exp(T 2/3ν ·\nn∑ i=1 1Xi∈E)\n]\n= N∏ i=1 EXi\n[ exp(T 2/3ν ·\nn∑ i=1 1Xi∈E)\n]\n= N∏ i=1 exp(T 2/3ν · Pr G [E])\n≤ exp ( NT 2/3ν · exp(−T 2/2ν) ) ≤ exp(N/3ν · δ/2C) = exp(Nδ/6Cν) .\nNote that if PrS [E] > δ CνT 2 , then exp(T 2/3ν · N PrS [E]) = exp(Nδ/3Cν). By Markov’s inequality, this happens with probability at most exp(−Nδ/6Cν).\nNow let C be a 1/2-cover in Euclidean distance for the set of unit vectors of size 2O(d). By a union bound, for all v′ ∈ C and T ′ a power of 2 between √ 4ν ln(1/δ) and R, we have that\nPr X∈uS [|v′ · (X − µG)| > T ′] ≤ δ 8T 2\nexcept with probability\n2O(d) log(R) exp(−Nδ/6Cν) = exp (O(d) + log logR−Nδ/6Cν) ≤ τ/6 .\nHowever, for any unit vector v and √\n4ν ln(1/δ) ≤ T ≤ R, there is a v′ ∈ C and such a T ′ such that for all x ∈ Rd, we have |v · (X − µG)| ≥ |v′ · (X − µG)|/2, and so |v′ · (X − µG)| > 2T ′ implies |v′ · (X − µG)| > T.\nThen, by a union bound, (6) holds simultaneously for all unit vectors v and all 0 ≤ T ≤ R, with probability a least 1− τ/3. This completes the proof.\nA.2 Robust Mean Estimation Under Second Moment Assumptions\nIn this section, we use our filtering technique to give a near-sample optimal computationally efficient algorithm to robustly estimate the mean of a density with a second moment assumption. We show:\nTheorem A.16. Let P be a distribution on Rd with unknown mean vector µP and unknown covariance matrix ΣP I. Let S be an ε-corrupted set of samples from P of size Θ((d/ε) log d). Then there exists an algorithm that given S, with probability 2/3, outputs µ̂ with ‖µ̂− µP ‖2 ≤ O( √ ε) in time poly(d/ε).\nNote that Theorem 3.2 follows straightforwardly from the above (divide every sample by σ, run the algorithm of Theorem A.16, and multiply its output by σ).\nAs usual in our filtering framework, the algorithm will iteratively look at the top eigenvalue and eigenvector of the sample covariance matrix and return the sample mean if this eigenvalue is small (Algorithm 3). The main difference between this and the filter algorithm for the sub-gaussian case is how we choose the threshold for the filter. Instead of looking for a violation of a concentration inequality, here we will choose a threshold at random (with a bias towards higher thresholds). The reason is that, in this setting, the variance in the direction we look for a filter in needs to be a constant multiple larger – instead of the typical Ω̃(ε) relative for the sub-gaussian case. Therefore, randomly choosing a threshold weighted towards higher thresholds suffices to throw out more corrupted samples than uncorrupted samples in expectation. Although it is possible to reject many good samples this way, the algorithm still only rejects a total of O(ε) samples with high probability.\nWe would like our good set of samples to have mean close to that of P and bounded variance in all directions. This motivates the following definition:\nDefinition A.17. We call a set S ε-good for a distribution P with mean µP and covariance ΣP I if the mean µS and covariance ΣS of S satisfy ‖µS − µP ‖2 ≤ √ ε and ‖ΣS‖2 ≤ 2.\nHowever, since we have no assumptions about higher moments, it may be be possible for outliers to affect our sample covariance too much. Fortunately, such outliers have small probability and do not contribute too much to the mean, so we will later reclassify them as errors.\nLemma A.18. Let S be N = Θ((d/ε) log d) samples drawn from P . Then, with probability at least 9/10, a random X ∈u S satisfies\n(i) ‖E[X]− µP ‖2 ≤ √ ε/3,\n(ii) Pr [ ‖X − µP ‖2 ≥ 80 √ d/ε ] ≤ ε/160,\n(iii) E [ ‖X − µP ‖2 · 1‖X−µP ‖2≥80 √ d/ε ] ≤ √ ε/2, and\n(iv) ∥∥∥E [(X − µP )(X − µP )T · 1‖X−µP ‖2≤80√d/ε]∥∥∥2 ≤ 3/2.\nProof. For (i), note that ES [‖E[X]− µP ‖22] = ∑ i ES [(E[X]i − µPi )2] ≤ d/N ≤ ε/360 ,\nand so by Markov’s inequality, with probability at least 39/40, we have ‖E[X]− µP ‖22 ≤ ε/9. For (ii), similarly to (i), note that\nE[‖Y − µP ‖22] = ∑ i E[(Yi − µPi )2] ≤ d ,\nfor Y ∼ P . By Markov’s inequality, Pr[‖Y − µP ‖2 ≥ 80 √ d/ε] ≤ ε/160 with probability at least 39/40.\nFor (iii), note that by an application of the Cauchy-Schwarz inequality\nE[|Y − µP |1‖Y−µP ‖2≥80 √ d/ε\n] ≤ √ E[(Y − µP )2] Pr[‖Y − µP ‖2 ≥ 80 √ d/ε] ≤ √ ε/80 .\nThus, ES [E[|X − µP |1‖X−µP ‖2≥80 √ d/ε ]] ≤ √ ε/80 ,\nand by Markov’s inequality, with probability at least 39/40 E [ |X − µP |1‖X−µP ‖2≥80 √ d/ε ] ≤ √ ε/2 .\nFor (iv), we require the following Matrix Chernoff bound:\nLemma A.19 (Part of Theorem 5.1.1 of [T+15]). Consider a sequence of d×d positive semi-definite random matrices Xk with ‖Xk‖2 ≤ L for all k. Let µmax = ‖ ∑ k E[Xk]‖2. Then, for θ > 0,\nE [∥∥∥∥∥∑ k Xk ∥∥∥∥∥ 2 ] ≤ (eθ − 1)µmax/θ + L log(d)/θ ,\nand for any δ > 0,\nPr [∥∥∥∥∥∑ k Xk ∥∥∥∥∥ 2 ≥ (1 + δ)µmax ] ≤ d(eδ/(1 + δ)1+δ)µ max/L .\nWe apply this lemma with Xk = (xk − µP )(xk − µP )T 1‖xk−µP ‖2≤80 √ d/ε for {x1, . . . , xN} = S. Note that ‖Xk‖2 ≤ (80)2d/ε = L and that µmax ≤ N‖ΣP ‖2 ≤ N .\nSuppose that µmax ≤ N/80. Then, taking θ = 1, we have\nE[ ∥∥∥∥∥∑ k Xk ∥∥∥∥∥ 2 ] ≤ (e− 1)N/80 +O(d log(d)/ε) .\nBy Markov’s inequality, except with probability 39/40, we have ‖ ∑ kXk‖2 ≤ N + O(d log(d)/ε) ≤ 3N/2, for N a sufficiently high multiple of d log(d)/ε. Suppose that µmax ≥ N/80, then we take δ = 1/2 and obtain\nPr [∥∥∥∥∥∑ k Xk ∥∥∥∥∥ 2 ≥ 3µmax2 ] ≤ d(e3/2/(5/2)3/2)Nε/20d .\nFor N a sufficiently high multiple of d log(d)/ε, we get that Pr[‖ ∑ kXk‖2 ≥ 3µ\nmax/2] ≤ 1/40. Since µmax ≤ N , we have with probability at least 39/40, ‖ ∑ kXk‖2 ≤ 3N/2.\nNoting that ‖ ∑ kXk‖2 /N = ‖E[1‖X−µP ‖2≤80 √ d/ε\n(X − µP )(X − µP )T ]‖2, we obtain (iv). By a union bound, (i)-(iv) all hold simultaneously with probability at least 9/10.\nNow we can get a 2ε-corrupted good set from an ε-corrupted set of samples satisfying Lemma A.18, by reclassifying outliers as errors:\nLemma A.20. Let S = R ∪E \\L, where R is a set of N = Θ(d log d/ε) samples drawn from P and E and L are disjoint sets with |E|, |L| ≤ ε. Then, with probability 9/10, we can also write S = G ∪ E′ \\ L′, where G ⊆ R is ε-good, L′ ⊆ L and E′ ⊆ E′ has |E′| ≤ 2ε|S|.\nProof. Let G = {x ∈ R : ‖x‖2 ≤ 80 √ d/ε}. Since R satisfies (ii) of Lemma A.18, |G|−|R| ≤ ε|R|/160 ≤ ε|S|. Thus, E′ = E ∪ (R \\ G) has |E′| ≤ 3ε/2. Note that (iv) of Lemma A.18 for R in terms of G is exactly |G|‖ΣG‖2/|R| ≤ 3/2, and so ‖ΣG‖2 ≤ 3|R|/2|G| ≤ 2.\nIt remains to check that ‖µG − µP ‖2 ≤ √ ε. But note that (iii) of Lemma A.18 is exactly EX∈uR[‖X −\nµP ‖21X∈R\\G] ≤ √ ε/2, and we have∣∣|G|EX∈uG[‖X − µP ‖2]− |R|EX∈uR[‖X − µP ‖2]∣∣ ≤ |R|√ε/2 , and since by (i), EX∈uR[‖X − µP ‖2] ≤ √ ε/3, it follows that EX∈uG′ [‖X − µP ‖2] ≤ √ ε.\nAlgorithm 3 Filter under second moment assumptions\n1: function FilterUnder2ndMoment(S) 2: Compute µS , ΣS , the mean and covariance matrix of S. 3: Find the eigenvector v∗ with highest eigenvalue λ∗ of ΣS . 4: if λ∗ ≤ 9 then 5: return µS 6: else 7: Draw Z from the distribution on [0, 1] with probability density function 2x. 8: Let T = Z max{|v∗ · x− µS | : x ∈ S}. 9: Return the set S′ = {x ∈ S : |v∗ · (X − µS)| < T}.\nAn iteration of FilterUnder2ndMoment may throw out more samples from G than corrupted samples. However, in expectation, we throw out many more corrupted samples than from the good set:\nProposition A.21. If we run FilterUnder2ndMoment on a set S = G ∪ E \\ L for some ε-good set G and disjoint E,L with |E| ≤ 2ε|S|, |L| ≤ 9ε|S|, then either it returns µS with ‖µS − µP ‖2 ≤ O( √ ε), or else it returns a set S′ ⊂ S with S′ = G ∪ E′ \\ L′ for disjoint E′ and L′. In the latter case we have EZ [|E′|+ 2|L′|] ≤ |E|+ 2|L|.\nFor D ∈ {G,E,L, S}, let µD be the mean of D and MD be the matrix EX∈uD[(X − µS)(X − µS)T ]. Lemma A.22. If G is an ε-good set with x ≤ 40 √ d/ε for x ∈ S ∪G, then ‖MG‖2 ≤ 2‖µG − µS‖22 + 2 .\nProof. For any unit vector v, we have\nvTMGv = EX∈uG[(v · (X − µS))2] = EX∈uG[(v · (X − µG) + v · (µP − µG))2] = vTΣGv + (v · (µG − µS))2\n≤ 2 + 2‖µG − µS‖22 .\nLemma A.23. We have that |L|‖ML‖2 ≤ 2|G|(1 + ‖µG − µS‖22) .\nProof. Since L ⊆ G, for any unit vector v, we have\n|L|vTMLv = |L|EX∈uL[(v · (X − µS))2] ≤ |G|EX∈uG[(v · (X − µS))2] ≤ 2|G|(1 + ‖µG − µS‖22) .\nLemma A.24. ‖µG − µS‖2 ≤ √ 2ε‖MS‖2 + 12 √ ε.\nProof. We have that |E|ME ≤ |S|MS + |L|ML and so\n|E|‖ME‖2 ≤ |S|‖MS‖2 + 2|G|(1 + ‖µG − µS‖22) .\nBy Cauchy Schwarz, we have that ‖ME‖2 ≥ ‖µE − µS‖22, and so√ |E|‖µE − µS‖2 ≤ √ |S|‖MS‖2 + 2|G|(1 + ‖µG − µS‖22) .\nBy Cauchy-Schwarz and Lemma A.23, we have that√ |L|‖µL − µS‖2 ≤ √ |L|‖ML‖2 ≤ √ 2|G|(1 + ‖µG − µS‖22) .\nSince |S|µS = |G|µG + |E|µE − |L|µL and |S| = |G|+ |E| − |L|, we get\n|G|(µG − µS) = |E|(µE − µS)− |L|(µE − µS) .\nSubstituting into this, we obtain |G|‖µG − µS‖2 ≤ √ |E||S|‖MS‖2 + 2|E||G|(1 + ‖µG − µS‖22) + √ 2|L||G|(1 + ‖µG − µS‖22) .\nSince for x, y > 0, √ x+ y ≤ √ x+ √ y, we have\n|G|‖µG − µS‖2 ≤ √ |E||S|‖MS‖2 + ( √ 2|E||G|+ √ 2|L||G|)(1 + ‖µG − µS‖2) .\nSince ||G| − |S|| ≤ ε|S| and |E| ≤ 2ε|S|, |L| ≤ 9ε|S|, we have\n‖µG − µS‖2 ≤ √ 2ε‖MS‖2 + (6 √ ε)(1 + ‖µG − µS‖2) .\nMoving the ‖µG − µS‖2 terms to the LHS, using 6 √ ε ≤ 1/2, gives\n‖µG − µS‖2 ≤ √ 2ε‖MS‖2 + 12 √ ε .\nSince λ∗ = ‖MS‖2, the correctness if we return the empirical mean is immediate.\nCorollary A.25. If λ∗ ≤ 9, we have that ‖µG − µS‖2 = O( √ ε).\nFrom now on, we assume λ∗ > 9. In this case we have ‖µG − µS‖22 ≤ O(ελ∗). Using Lemma A.22, we have\n‖MG‖2 ≤ 2 +O(ελ∗) ≤ 2 + λ∗/5\nfor sufficiently small ε. Thus, we have that\nv∗TMSv ∗ ≥ 4v∗TMGv∗ . (7)\nNow we can show that in expectation, we throw out many more corrupted points from E than from G\\L:\nLemma A.26. Let S′ = G∪E′ \\L′ for disjoint E′, L′ be the set of samples returned by the iteration. Then we have EZ [|E′|+ 2|L′|] ≤ |E|+ 2|L|.\nProof. Let a = maxx∈S |v∗ · x− µS |. Firstly, we look at the expected number of samples we reject:\nEZ [|S′|]− |S| = EZ [ |S| Pr\nX∈uS [|X − µS | ≥ aZ] ] = |S|\n∫ 1 0 Pr X∈uS [ |v∗ · (X − µS)| ≥ ax ] 2xdx\n= |S| ∫ a\n0\nPr X∈uS\n[ |v∗ · (X − µS)| ≥ T ] (2T/a)dT\n= |S|EX∈uS [ (v∗ · (X − µS))2 ] /a = (|S|/a) · v∗TMSv∗ .\nNext, we look at the expected number of false positive samples we reject, i.e., those in L′ \\ L. EZ [|L′|]− |L| = EZ [ (|G| − |L|) Pr\nX∈uG\\L\n[ |X − µS | ≥ T ]] ≤ EZ [ |G| Pr\nX∈uG [|v∗ · (X − µS)| ≥ aZ] ] = |G|\n∫ 1 0 Pr X∈uG [|v∗ · (X − µS)| ≥ ax]2x dx\n= |G| ∫ a\n0\nPr X∈uG\n[|v∗ · (X − µS)| ≥ T ](2T/a) dT\n≤ |G| ∫ ∞\n0\nPr X∈uG\n[|v∗ · (X − µS)| ≥ T ](2T/a) dT\n= |G|EX∈uG [ (v∗ · (X − µS))2 ] /a = (|G|/a) · v∗TMGv∗ .\nUsing (7), we have |S|v∗TMSv∗ ≥ 4|G|v∗TMGv∗ and so EZ [S′] − S ≥ 3(EZ [L′] − L). Now consider that |S′| = |G|+ |E′| − |L′| = |S| − |E|+ |E′|+ |L| − |L′|, and thus |S′| − |S| = |E| − |E′|+ |L′| − |L|. This yields that |E| − EZ [|E′|] ≥ 2(EZ [L′]− L), which can be rearranged to EZ [|E′|+ 2|L′|] ≤ |E|+ 2|L|.\nProof of Proposition A.21. If λ∗ ≤ 9, then we return the mean in Step 5, and by Corollary A.25, ‖µS−µP ‖2 ≤ O( √ ε).\nIf λ∗ > 9, then we return S′. Since at least one element of S has |v∗ ·X| = maxx∈S |v∗ ·X|, whatever value of Z is drawn, we still remove at least one element, and so have S′ ⊂ S. By Lemma A.26, we have EZ [|E′|+ 2|L′|] ≤ |E|+ 2|L|.\nProof of Theorem A.16. Our input is a set S of N = Θ((d/ε) log d) ε-corrupted samples so that with probability 9/10, S is a 2ε-corrupted set of ε-good samples for P by Lemmas A.18 and A.20. We have a set S = G ∪ E′ \\ L, where G′ is an ε-good set, |E| ≤ 2ε, and |L| ≤ ε. Then, we iteratively apply FilterUnder2ndMoment until it outputs an approximation to the mean. Since each iteration removes a sample, this must happen within N iterations. The algorithm takes at most poly(N, d) = poly(d, 1/ε) time.\nAs long as we can show that the conditions of Proposition A.21 hold in each iteration, it ensures that ‖µS − µP ‖2 ≤ O( √ ε). However, the condition that |L| ≤ 9ε|S| need not hold in general. Although in expectation we reject many more samples in E than G, it is possible that we are unlucky and reject many samples in G, which could make L large in the next iteration. Thus, we need a bound on the probability that we ever have |L| > 9ε.\nWe analyze the following procedure: We iteratively run FilterUnder2ndMoment starting with a set Si ∪ Ei \\ Li of samples with S0 = S and producing a set Si+1 = G ∪ Ei+1 \\ Li+1. We stop if we output an approximation to the mean or if |Li+1| ≥ 13ε|S|. Since we do now always satisfy the conditions of Proposition A.21, this gives that EZ [|Ei+1|+ |Li+1|] = |Ei|+ 2|Li|. This expectation is conditioned on the state of the algorithm after previous iterations, which is determined by Si. Thus, if we consider the random variables Xi = |Ei| + 2|Li|, then we have E[Xi+1|Si] ≤ Xi, i.e., the sequence Xi is a sub-martingale with respect to Xi. Using the convention that Si+1 = Si, if we stop in less than i iterations, and recalling that we always stop in N iterations, the algorithm fails if and only if |LN | > 9ε|S|. By a simple induction or standard results on sub-martingales, we have E[XN ] ≤ X0. Now X0 = |E0|+ 2|L0| ≤ 3ε|S|. Thus, E[XN ] ≤ 3ε|S|. By Markov’s inequality, except with probability 1/6, we have XN ≤ 18ε|S|. In this case, |LN | ≤ XN/2 ≤ 9ε|S|. Therefore, the probability that we ever have |Li| > 9ε is at most 1/6.\nBy a union bound, the probability that the uncorrupted samples satisfy Lemma A.18 and Proposition A.21 applies to every iteration is at least 9/10−1/6 ≥ 2/3. Thus, with at least 2/3 probability, the algorithm outputs a vector µ̂ with ‖µ̂− µP ‖2 ≤ O( √ ε).\nA.3 Robust Covariance Estimation\nIn this subsection, we give a near-sample optimal efficient robust estimator for the covariance of a zero-mean Gaussian density, thus proving Theorem 3.3. Our algorithm is essentially identical to the filtering algorithm\ngiven in Section 8.2 of [DKK+16]. As in Section A.1 the only difference is a weaker definition of the “good set of samples” (Definition A.27) and a concentration argument (Lemma A.28) showing that a random set of uncorrupted samples of the appropriate size is good with high probability. Given these, the analysis of this subsection follows straightforwardly from the analysis in Section 8.2 of [DKK+16] by plugging in the modified parameters.\nThe algorithm Filter-Gaussian-Unknown-Covariance to robustly estimate the covariance of a mean 0 Gaussian in [DKK+16] is as follows:\nAlgorithm 4 Filter algorithm for a Gaussian with unknown covariance matrix.\n1: procedure Filter-Gaussian-Unknown-Covariance(S′, ε, τ) input: A multiset S′ such that there exists an (ε, τ)-good set S with ∆(S, S′) ≤ 2ε output: Either a set S′′ with ∆(S, S′′) < ∆(S, S′) or the parameters of a Gaussian G′ with dTV (G,G\n′) = O(ε log(1/ε)).\nLet C > 0 be a sufficiently large universal constant. 2: Let Σ′ be the matrix EX∈uS′ [XXT ] and let G′ be the mean 0 Gaussian with covariance matrix Σ′. 3: if there is any x ∈ S′ so that xT (Σ′)−1x ≥ Cd log(|S′|/τ) then 4: return S′′ = S′ − {x : xT (Σ′)−1x ≥ Cd log(|S′|/τ)}. 5: Compute an approximate eigendecomposition of Σ′ and use it to compute Σ′−1/2 6: Let x(1), . . . , x(|S′|) be the elements of S ′. 7: For i = 1, . . . , |S′|, let y(i) = Σ′−1/2x(i) and z(i) = y⊗2(i) . 8: Let TS′ = −I[I[T + (1/|S′|) ∑|S′| i=1 z(i)z T (i).\n9: Approximate the top eigenvalue λ∗ and corresponding unit eigenvector v∗ of TS′ .. 10: Let p∗(x) = 1√\n2 ((Σ′−1/2x)T v∗](Σ′−1/2x)− tr(v∗]))\n11: if λ∗ ≤ (1 + Cε log2(1/ε))QG′(p∗) then 12: return G′ 13: Let µ be the median value of p∗(X) over X ∈ S′. 14: Find a T ≥ C ′ so that\nPr X∈uS′\n(|p∗(X)− µ| ≥ T + 4/3) ≥ Tail(T, d, ε, τ)\n15: return S′′ = {X ∈ S′ : |p∗(X)− µ| < T}.\nIn [DKK+16], we take Tail(T, d, ε, τ) = 12 exp(−T ) + 3ε/(d log(N/τ))2, where N = Θ((d log(d/ετ))6/ε2) is the number of samples we took there.\nTo get a near-sample optimal algorithms, we will need a weaker definition of a good set. To use this, we will need to weaken the tail bound in the algorithm to Tail(T, d, ε, τ) = ε/(T 2 log2(T )), when T ≥ 10 log(1/ε). For T ≤ 10 log(1/ε), we take Tail(T, d, ε, τ) = 1 so that we always choose T ≥ 10 log(1/ε). It is easy to show that the integrals of this tail bound used in the proofs of Lemma 8.19 and Claim 8.22 of [DKK+16] have similar bounds. Thus, our analysis here will sketch that these tail bounds hold for a set of Ω(d2 log5(d/ετ)/ε2) samples from the Guassian.\nFirstly, we state the new, weaker, definition of a good set:\nDefinition A.27. Let G be a Gaussian in Rd with mean 0 and covariance Σ. Let ε > 0 be sufficiently small. We say that a multiset S of points in Rd is ε-good with respect to G if the following hold:\n1. For all x ∈ S, xTΣ−1x < d+O( √ d log(d/ε)).\n2. We have that ‖Σ−1/2Cov(S)Σ−1/2 − I‖F = O(ε).\n3. For all even degree-2 polynomials p, we have that Var(p(S)) = Var(p(G))(1 +O(ε)).\n4. For p an even degree-2 polynomial with E[p(G)] = 0 and Var(p(G)) = 1, and for any T > 10 log(1/ε) we have that\nPr x∈uS\n(|p(x)| > T ) ≤ ε/(T 2 log2(T )).\nIt is easy to see that the algorithm and analysis of [DKK+16] can be pushed through using the above weaker definition. That is, if S is a good set, then G can be recovered to Õ(ε) error from an ε-corrupted version of S. Our main task will be to show that random sets of the appropriate size are good with high probability.\nProposition A.28. Let N be a sufficiently large constant multiple of d2 log5(d/ε)/ε2. Then a set S of N independent samples from G is ε-good with respect to G with high probability.\nProof. First, note that it suffices to prove this when G = N(0, I). Condition 1 follows by standard concentration bounds on ‖x‖22. Condition 2 follows by estimating the entry-wise error between Cov(S) and I. Condition 3 is slightly more involved. Let {pi} be an orthonormal basis for the set of even, degree-2, mean-0 polynomials with respect to G. Define the matrix Mi,j = Ex∈uS [pi(x)pj(x)]− δi,j . This condition is equivalent to ‖M‖2 = O(ε). Thus, it suffices to show that for every v with ‖v‖2 = 1 that vTMv = O(ε). It actually suffices to consider a cover of such v’s. Note that this cover will be of size 2O(d\n2). For each v, let pv = ∑ i vipi. We need to show that Var(pv(S)) = 1 + O(ε). We can show this happens with probability 1− 2−Ω(d2), and thus it holds for all v in our cover by a union bound. Condition 4 is substantially the most difficult of these conditions to prove. Naively, we would want to find a cover of all possible p and all possible T , and bound the probability that the desired condition fails. Unfortunately, the best a priori bound on Pr(|p(G)| > T ) are on the order of exp(−T ). As our cover would need to be of size 2d 2\nor so, to make this work with T = d, we would require on the order of d3 samples in order to make this argument work.\nHowever, we will note that this argument is sufficient to cover the case of T < 10 log(1/ε) log2(d/ε). Fortunately, most such polynomials p satisfy much better tail bounds. Note that any even, mean zero\npolynomial p can be written in the form p(x) = xTAx−tr(A) for some matrix A. We call A the associated matrix to p. We note by the Hanson-Wright inequality that Pr(|p(G)| > T ) = exp(−Ω(min((T/‖A‖F )2, T/‖A‖2))). Therefore, the tail bounds above are only as bad as described when A has a single large eigenvalue. To take advantage of this, we will need to break p into parts based on the size of its eigenvalues. We begin with a definition:\nDefinition A.29. Let Pk be the set of even, mean-0, degree-2 polynomials, so that the associated matrix A satisfies:\n1. rank(A) ≤ k 2. ‖A‖2 ≤ 1/ √ k.\nNote that for p ∈ Pk that |p(x)| ≤ |x|2/ √ k + √ k. Importantly, any polynomial can be written in terms of these sets.\nLemma A.30. Let p be an even, degree-2 polynomial with E[p(G)] = 0,Var(p(G)) = 1. Then if t = blog2(d)c, it is possible to write p = 2(p1 + p2 + . . .+ p2t + pd) where pk ∈ Pk.\nProof. Let A be the associated matrix to p. Note that ‖A‖F = Var p = 1. Let Ak be the matrix corresponding to the top k eigenvalues of A. We now let p1 be the polynomial associated to A1/2, p2 be associated to (A2 − A1)/2, p4 be associated to (A4 − A2)/2, and so on. It is clear that p = 2(p1 + p2 + . . . + p2t + pd). It is also clear that the matrix associated to pk has rank at most k. If the matrix associated to pk had an eigenvalue more than 1/ √ k, it would need to be the case that the k/2nd largest eigenvalue of A had size at least 2/ √ k. This is impossible since the sum of the squares of the eigenvalues of A is at most 1.\nThis completes our proof.\nWe will also need covers of each of these sets Pk.\nLemma A.31. For each k, there exists a set Ck ⊂ Pk so that\n1. For each p ∈ Pk there exists a q ∈ Ck so that ‖p(G)− q(G)‖2 ≤ (ε/d)2.\n2. |Ck| = 2O(dk log(d/ε)).\nProof. We note that any such p is associated to a matrix A of the form A = ∑k i=1 λiviv T i , for λi ∈ [0, 1/ √ k] and vi orthonormal. It suffices to let q correspond to the matrix A ′ = ∑k i=1 µiwiw T i for with |λi−µi| < (ε/d)3 and |vi − wi| < (ε/d)3 for all i. It is easy to let µi and wi range over covers of the interval and the sphere with appropriate errors. This gives a set of possible q’s of size 2O(dk log(d/ε)) as desired. Unfortunately, some of these q will not be in Pk as they will have eigenvalues that are too large. However, this is easily fixed by replacing each such q by the closest element of Pk. This completes our proof.\nWe next will show that these covers are sufficient to express any polynomial.\nLemma A.32. Let p be an even degree-2 polynomial with E[p(G)] = 0 and Var(p(G)) = 1. It is possible to write p as a sum of O(log(d)) elements of some Ck plus another polynomial of L2 norm at most ε/d.\nProof. Combining the above two lemmas we have that any such p can be written as\np = (q1 + p1) + (q2 + p2) + . . . (q2t + p2t) + (qd + pd) = q1 + q2 + . . .+ q 2t + qd + p′ ,\nwhere qk above is in Ck and ‖pk(G)‖2 < (ε/d)2. Thus, p′ = p1 + p2 + . . . + p2t + pd has ‖p′(G)‖2 ≤ (ε/d). This completes the proof.\nThe key observation now is that if |p(x)| ≥ T for ‖x‖2 ≤ √ d/ε, then writing p = q1 +q2 +q4 + . . .+qd+p ′ as above, it must be the case that |qk(x)| > (T−1)/(2 log(d)) for some k. Therefore, to prove our main result, it suffices to show that, with high probability over the choice of S, for any T ≥ 10 log(1/ε) log2(d/ε) and any q ∈ Ck for some k, that Prx∈uS(|q(x)| > T/(2 log(d))) < ε/(2T 2 log\n2(T ) log(d)). Equivalently, it suffices to show that for T ≥ 10 log(1/ε) log(d/ε) it holds Prx∈uS(|q(x)| > T/(2 log(d))) < ε/(2T 2 log\n2(T ) log2(d)). Note that this holds automatically for T > (d/ε), as p(x) cannot possibly be that large for ‖x‖2 ≤ √ d/ε. Furthermore, note that losing a constant factor in the probability, it suffices to show this only for T a power of 2.\nTherefore, it suffices to show for every k ≤ d, every q ∈ Ck and every d/ √ kε T log(1/ε) log(d/ε)\nthat with probability at least 1 − 2−Ω(dk log(d/ε)) over the choice of S we have that Prx∈uS(|q(x)| > T ) ε/(T 2 log4(d/ε)). However, by the Hanson-Wright inequality, we have that\nPr(|q(G)| > T ) = exp(−Ω(min(T 2, T √ k))) < (ε/(T 2 log4(d/ε)))2 .\nTherefore, by Chernoff bounds, the probability that more than a ε/(T 2 log4(d/ε))-fraction of the elements of S satisfy this property is at most\nexp(−Ω(min(T 2, T √ k))|S|ε/(T 2 log4(d/ε))) = exp(−Ω(|S|ε/(log4(d/ε)) min(1, √ k/T )))\n≤ exp(−Ω(|S|ε2/(log4(d/ε))k/d)) ≤ exp(−Ω(dk log(d/ε))) ,\nas desired. This completes our proof."
    }, {
      "heading" : "B Omitted Details from Section 5",
      "text" : "B.1 Full description of the distributions for experiments\nHere we formally describe the distributions we used in our experiments. In all settings, our goal was to find noise distributions so that noise points were not “obvious” outliers, in the sense that there is no obvious pointwise pruning process which could throw away the noise points, which still gave the algorithms we tested the most difficulty. We again remark that while other algorithms had varying performances depending on the noise distribution, it seemed that the performance of ours was more or less unaffected by it.\nDistribution for the synthetic mean experiment Our uncorrupted points were generated by N (µ, I), where µ is the all-ones vector. Our noise distribution is given as\nN = 1\n2 Π1 +\n1 2 Π2 ,\nwhere Π1 is the product distribution over the hypercube where every coordinate is 0 or 1 with probability 1/2, and Π2 is a product distribution where the first coordinate is ether 0 or 12 with equal probability, the second coordinate is −2 or 0 with equal probability, and all remaining coordinates are zero.\nDistribution for the synthetic covariance experiment For the isotropic synthetic covariance experiment, our uncorrupted points were generated by N (0, I), and the noise points were all zeros. For the skewed synthetic covariance experiment, our uncorrupted points were generated by N (0, I + 10e1eT1 ), where e1 is the first unit vector, and our noise points were generated as follows: we took a fixed random rotation of points of the form Yi ∼ Π, where Π is a product distribution whose first d/2 coordinates are ±0.5 with probability 1/2, and whose next d/2 − 1 coordinates are each 0.8 × Ai, where for each coordinate i, Ai is an independent random integer between −2 and 2, and whose last coordinate is a uniformly random integer between [−10, 10].\nSetup for the semi-synthetic geographic experiment We took the 20 dimensional data from [NJB+08], which was diagonalized, and randomly rotated it. This was to simulate the higher dimensional case, since the singular vectors that [NJB+08] obtained did not seem to be sparse or analytically sparse. Our noise was distributed as Π, where Π is a product distribution whose first d/2 coordinates are each uniformly random integers between 0 and 2, multiplied by 1/12, and whose last d/2 coordinates are each random integers between 2 and 3.\nB.2 Comparison with other robust PCA methods on semi-synthetic data\nIn addition to comparing our results with simple pruning techniques, as we did in Figure 3 in the main text, we also compared our algorithm with implementations of other robust PCA techniques from the literature with accessible implementations. In particular, we compared our technique with RANSAC-based techniques, LRVCov, two SDPs ([CLMW11, XCS10]) for variants of robust PCA, and an algorithm proposed by [CLMW11] to speed up their SDP based on alternating descent. For the SDPs, since black box methods were too slow to run on the full data set (as [CLMW11] mentions, black-box solvers for the SDPs are impractical above perhaps 100 data points), we subsample the data, and run the SDP on the subsampled data. For each of these methods, we ran the algorithm on the true data points plus noise, where the noise was generated as described above. We then take the estimate of the covariance it outputs, and project the data points onto the top two singular values of this matrix, and plot the results in Figure 4.\nSimilar results occurred for most noise patterns we tried. We found that only our algorithm and LRVCov were able to reasonably reconstruct Europe, in the presence of this noise. It is hard to judge qualitatively which of the two maps generated is preferable, but it seems that ours stretches the picture somewhat less than LRVCov."
    } ],
    "references" : [ {
      "title" : "The complexity and approximability of finding maximum feasible subsystems of linear relations",
      "author" : [ "E. Amaldi", "V. Kann" ],
      "venue" : "Theoretical Computer Science,",
      "citeRegEx" : "Amaldi and Kann.,? \\Q1995\\E",
      "shortCiteRegEx" : "Amaldi and Kann.",
      "year" : 1995
    }, {
      "title" : "Approximating center points with iterated radon points",
      "author" : [ "K.L. Clarkson", "D. Eppstein", "G.L. Miller", "C. Sturtivant", "S.-H. Teng" ],
      "venue" : "In Proceedings of the Ninth Annual Symposium on Computational Geometry,",
      "citeRegEx" : "Clarkson et al\\.,? \\Q1993\\E",
      "shortCiteRegEx" : "Clarkson et al\\.",
      "year" : 1993
    }, {
      "title" : "A general decision theory for huber’s ε-contamination model",
      "author" : [ "M. Chen", "C. Gao", "Z. Ren" ],
      "venue" : "CoRR, abs/1511.04144,",
      "citeRegEx" : "Chen et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2015
    }, {
      "title" : "An optimal randomized algorithm for maximum tukey depth",
      "author" : [ "T.M. Chan" ],
      "venue" : "In Proceedings of the Fifteenth Annual ACM-SIAM Symposium on Discrete Algorithms (SODA),",
      "citeRegEx" : "Chan.,? \\Q2004\\E",
      "shortCiteRegEx" : "Chan.",
      "year" : 2004
    }, {
      "title" : "Robust principal component analysis",
      "author" : [ "E.J. Candès", "X. Li", "Y. Ma", "J. Wright" ],
      "venue" : "J. ACM,",
      "citeRegEx" : "Candès et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Candès et al\\.",
      "year" : 2011
    }, {
      "title" : "Computationally efficient robust estimation of sparse functionals",
      "author" : [ "S.S. Du", "S. Balakrishnan", "A. Singh" ],
      "venue" : null,
      "citeRegEx" : "Du et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Du et al\\.",
      "year" : 2017
    }, {
      "title" : "Breakdown properties of location estimates based on halfspace depth and projected outlyingness",
      "author" : [ "D.L. Donoho", "M. Gasko" ],
      "venue" : "Ann. Statist., 20(4):1803–1827,",
      "citeRegEx" : "Donoho and Gasko.,? \\Q1992\\E",
      "shortCiteRegEx" : "Donoho and Gasko.",
      "year" : 1992
    }, {
      "title" : "Faster and sample near-optimal algorithms for proper learning mixtures of gaussians",
      "author" : [ "C. Daskalakis", "G. Kamath" ],
      "venue" : "In Proceedings of The 27th Conference on Learning Theory, COLT",
      "citeRegEx" : "Daskalakis and Kamath.,? \\Q2014\\E",
      "shortCiteRegEx" : "Daskalakis and Kamath.",
      "year" : 2014
    }, {
      "title" : "Robust estimators in high dimensions without the computational intractability",
      "author" : [ "I. Diakonikolas", "G. Kamath", "D.M. Kane", "J. Li", "A. Moitra", "A. Stewart" ],
      "venue" : "In Proceedings of FOCS’16,",
      "citeRegEx" : "Diakonikolas et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Diakonikolas et al\\.",
      "year" : 2016
    }, {
      "title" : "Efficient and optimally robust learning of high-dimensional gaussians",
      "author" : [ "I. Diakonikolas", "G. Kamath", "D.M. Kane", "J. Li", "A. Moitra", "A. Stewart" ],
      "venue" : "In Manuscript,",
      "citeRegEx" : "Diakonikolas et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Diakonikolas et al\\.",
      "year" : 2017
    }, {
      "title" : "Robust learning of fixed-structure bayesian networks",
      "author" : [ "I. Diakonikolas", "D.M. Kane", "A. Stewart" ],
      "venue" : null,
      "citeRegEx" : "Diakonikolas et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Diakonikolas et al\\.",
      "year" : 2016
    }, {
      "title" : "Statistical query lower bounds for robust estimation of high-dimensional gaussians and gaussian mixtures",
      "author" : [ "I. Diakonikolas", "D.M. Kane", "A. Stewart" ],
      "venue" : null,
      "citeRegEx" : "Diakonikolas et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Diakonikolas et al\\.",
      "year" : 2016
    }, {
      "title" : "Asymptotic minimax character of the sample distribution function and of the classical multinomial estimator",
      "author" : [ "A. Dvoretzky", "J. Kiefer", "J. Wolfowitz" ],
      "venue" : "Ann. Mathematical Statistics,",
      "citeRegEx" : "Dvoretzky et al\\.,? \\Q1956\\E",
      "shortCiteRegEx" : "Dvoretzky et al\\.",
      "year" : 1956
    }, {
      "title" : "Combinatorial methods in density estimation",
      "author" : [ "L. Devroye", "G. Lugosi" ],
      "venue" : null,
      "citeRegEx" : "Devroye and Lugosi.,? \\Q2001\\E",
      "shortCiteRegEx" : "Devroye and Lugosi.",
      "year" : 2001
    }, {
      "title" : "Robust statistics. The approach based on influence functions",
      "author" : [ "F.R. Hampel", "E.M. Ronchetti", "P.J. Rousseeuw", "W.A. Stahel" ],
      "venue" : null,
      "citeRegEx" : "Hampel et al\\.,? \\Q1986\\E",
      "shortCiteRegEx" : "Hampel et al\\.",
      "year" : 1986
    }, {
      "title" : "Robust estimation of a location parameter",
      "author" : [ "P.J. Huber" ],
      "venue" : "The Annals of Mathematical Statistics,",
      "citeRegEx" : "Huber.,? \\Q1964\\E",
      "shortCiteRegEx" : "Huber.",
      "year" : 1964
    }, {
      "title" : "The densest hemisphere problem",
      "author" : [ "D.S. Johnson", "F.P. Preparata" ],
      "venue" : "Theoretical Computer Science,",
      "citeRegEx" : "Johnson and Preparata.,? \\Q1978\\E",
      "shortCiteRegEx" : "Johnson and Preparata.",
      "year" : 1978
    }, {
      "title" : "Robust sparse estimation tasks in high",
      "author" : [ "J. Li" ],
      "venue" : "dimensions. CoRR,",
      "citeRegEx" : "Li.,? \\Q2017\\E",
      "shortCiteRegEx" : "Li.",
      "year" : 2017
    }, {
      "title" : "Agnostic estimation of mean and covariance",
      "author" : [ "K.A. Lai", "A.B. Rao", "S. Vempala" ],
      "venue" : "In Proceedings of FOCS’16,",
      "citeRegEx" : "Lai et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Lai et al\\.",
      "year" : 2016
    }, {
      "title" : "Approximate centerpoints with proofs",
      "author" : [ "G.L. Miller", "D. Sheehy" ],
      "venue" : "Comput. Geom.,",
      "citeRegEx" : "Miller and Sheehy.,? \\Q2010\\E",
      "shortCiteRegEx" : "Miller and Sheehy.",
      "year" : 2010
    }, {
      "title" : "Genes mirror geography within europe",
      "author" : [ "J. Novembre", "T. Johnson", "K. Bryc", "Z. Kutalik", "A.R. Boyko", "A. Auton", "A. Indap", "K.S. King", "S. Bergmann", "M.R. Nelson" ],
      "venue" : null,
      "citeRegEx" : "Novembre et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Novembre et al\\.",
      "year" : 2008
    }, {
      "title" : "Multivariate estimation with high breakdown point",
      "author" : [ "P. Rousseeuw" ],
      "venue" : "Mathematical Statistics and Applications,",
      "citeRegEx" : "Rousseeuw.,? \\Q1985\\E",
      "shortCiteRegEx" : "Rousseeuw.",
      "year" : 1985
    }, {
      "title" : "Computing location depth and regression depth in higher dimensions",
      "author" : [ "P.J. Rousseeuw", "A. Struyf" ],
      "venue" : "Statistics and Computing,",
      "citeRegEx" : "Rousseeuw and Struyf.,? \\Q1998\\E",
      "shortCiteRegEx" : "Rousseeuw and Struyf.",
      "year" : 1998
    }, {
      "title" : "An introduction to matrix concentration inequalities",
      "author" : [ "J.A. Tropp" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "Tropp,? \\Q2015\\E",
      "shortCiteRegEx" : "Tropp",
      "year" : 2015
    }, {
      "title" : "A survey of sampling from contaminated distributions",
      "author" : [ "J.W. Tukey" ],
      "venue" : "Contributions to probability and statistics,",
      "citeRegEx" : "Tukey.,? \\Q1960\\E",
      "shortCiteRegEx" : "Tukey.",
      "year" : 1960
    }, {
      "title" : "Minimum volume ellipsoid",
      "author" : [ "S. Van Aelst", "P. Rousseeuw" ],
      "venue" : "Wiley Interdisciplinary Reviews: Computational Statistics,",
      "citeRegEx" : "Aelst and Rousseeuw.,? \\Q2009\\E",
      "shortCiteRegEx" : "Aelst and Rousseeuw.",
      "year" : 2009
    }, {
      "title" : "Introduction to the non-asymptotic analysis of random matrices",
      "author" : [ "R. Vershynin" ],
      "venue" : null,
      "citeRegEx" : "Vershynin.,? \\Q2010\\E",
      "shortCiteRegEx" : "Vershynin.",
      "year" : 2010
    }, {
      "title" : "Robust pca via outlier pursuit",
      "author" : [ "H. Xu", "C. Caramanis", "S. Sanghavi" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Xu et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2010
    } ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "Robust estimation is much more challenging in high dimensions than it is in one dimension: Most techniques either lead to intractable optimization problems or estimators that can tolerate only a tiny fraction of errors. Recent work in theoretical computer science has shown that, in appropriate distributional models, it is possible to robustly estimate the mean and covariance with polynomial time algorithms that can tolerate a constant fraction of corruptions, independent of the dimension. However, the sample and time complexity of these algorithms is prohibitively large for high-dimensional applications. In this work, we address both of these issues by establishing sample complexity bounds that are optimal, up to logarithmic factors, as well as giving various refinements that allow the algorithms to tolerate a much larger fraction of corruptions. Finally, we show on both synthetic and real data that our algorithms have state-of-the-art performance and suddenly make high-dimensional robust estimation a realistic possibility.",
    "creator" : "LaTeX with hyperref package"
  }
}