{
  "name" : "1603.05642.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Optimal Black-Box Reductions Between Optimization Objectives",
    "authors" : [ "Zeyuan Allen-Zhu", "Elad Hazan" ],
    "emails" : [ "zeyuan@csail.mit.edu", "ehazan@cs.princeton.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "We show how these new reductions give rise to faster running times on training linear classifiers for certain families of loss functions, and that our reductions are optimal and cannot be improved in general. We conclude with experiments showing our reductions successfully transform methods between domains and achieve the desired performance predicted by theory."
    }, {
      "heading" : "1 Introduction",
      "text" : "The basic machine learning optimization problem of minimizing a loss function and regularizer over a set of examples for a given hypothesis class comes in numerous different variations and names. Examples include ridge regression, lasso, support vector machines, logistic regression and many more. In recent years multitude of optimization methods were introduced, many times specialized to the particular convexity and smoothness properties of the corresponding optimization formulation.\nWith ever increasing specialized methods and settings, the tasks of comparing different methods and choosing the appropriate one, or even keeping track of the different methods, becomes complicated even for the expert.\nIn this paper we explore reductions that take an algorithm devised for one setting, and apply it to the entire spectrum of strong-convexity and smoothness parameters. The advantages of our approach are:\n• shaving off a non-optimal log(1/ε) factor incurred from classical reductions in most settings; • simpler and faster comparison of different methods for any domain; • allow the algorithm designer to focus only on one case, and immediately obtain results more\nbroadly;\n• by applying our reductions to existing methods such as SVRG, SDCA, or even accelerated ones such as APCG, we obtain faster, direct method for training ERM problems in settings that the cited method were not originally designed for.\nFormally, our focus is minimizing a composite objective function\nmin x∈Rd\n{ F (x) def = f(x) + ψ(x) } (1.1)\nar X\niv :1\n60 3.\n05 64\n2v 1\n[ m\nat h.\nO C\n] 1\n7 M\nar 2\nwhere, f(x) is a differentiable convex function and ψ(x) is a relatively simple (but possibly nondifferentiable) convex function, sometimes referred to as the proximal function. We consider the problem of finding an approximate minimizer x ∈ Rd satisfying F (x) ≤ F (x∗) + ε, where x∗ is a minimizer of F (x).\nRecall that there are a few interesting categories of the function F (x) and each of them correspond to some well-known training problems in machine learning. Suppose we are given n training examples {(a1, b1), . . . (an, bn)}, where each ai ∈ Rd is the feature vector of example i and each bi ∈ R is the label of example i.\nCase 1: ψ(x) is σ strongly convex and f(x) is L-smooth. Examples:\n• ridge regression: f(x) = 12n ∑n\ni=1(〈ai, x〉 − bi)2 and ψ(x) = σ2 ‖x‖22. • elastic net : f(x) = 12n ∑n i=1(〈ai, x〉 − bi)2 and ψ(x) = σ2 ‖x‖22 + λ‖x‖1.\nCase 2: ψ(x) is not strongly convex and f(x) is L-smooth. Examples:\n• Lasso: f(x) = 12n ∑n\ni=1(〈ai, x〉 − bi)2 and ψ(x) = λ‖x‖1. • `1 logistic regression: f(x) = 1n ∑n i=1 log(1 + exp(−bi〈ai, x〉)) and ψ(x) = λ‖x‖1.\nCase 3: ψ(x) is σ strongly convex and f(x) is non-smooth (but Lipschitz continuous). Examples:\n• SVM : f(x) = 1n ∑n i=1 max{0, 1− bi〈ai, x〉} and ψ(x) = σ‖x‖22.\nCase 4: ψ(x) is not strongly convex and f(x) is non-smooth (but Lipschitz continuous). Examples:\n• `1-SVM : f(x) = 1n ∑n\ni=1 max{0, 1− bi〈ai, x〉} and ψ(x) = λ‖x‖1. In many machine learning applications, the function f(x) is can be written as an average of n\nfunctions fi(〈ai, x〉) where fi : R → R, that is, f(x) = 1n ∑n\ni=1 fi(〈x, ai〉). We refer to this as the finite-sum case of (1.1).\nCase 1 is perhaps the most studied category in convex optimization. For instance, it is famously known that by applying (proximal) gradient descent starting from vector x0, one can obtain an εapproximate minimizer of F (x) in T = O ( L σ log F (x0)−F (x∗) ε ) iterations [17]. This can be improved to\nT = O( √ L√ σ log F (x0)−F (x ∗) ε ) if the so-called (proximal) accelerated gradient descent is used, see [16] or Constant Step Scheme in [17]. Most first-order algorithms for Case 1 enjoy a linear convergence rate, that is, a logarithmic dependence on log 1ε in the running time.\nHowever, many algorithms for Case 1 do not directly apply to the other three cases, and this forces researchers to take one of the following two approaches. The first approach is to design a non-strongly convex variant or a non-smooth variant which usually requires non-trivial changes of the algorithm. For instance, the accelerated gradient descent method has its non-strongly convex variant (see General Scheme in [17]) and its non-smooth variant [18]. The second approach is to apply a reduction from non-strongly convex or non-smooth objectives into a strongly convex and smooth one, via the classical regularization or smoothing techniques: (see [7] Section 2.1 for a more detailed treatment)\n• Classical Regularization Reduction. Given a non-strongly convex objective F (x) and suppose the desired training error is ε, one\ncan define a new objective F ′(x) def= F (x) + σ2 ‖x0 − x‖2 in with σ on the order of ε. Then, one can apply a method that minimizes this strongly convex objective F ′(x) instead.\nFor instance, if F (x) is L-smooth and one applies accelerated gradient method to minimize F ′(x) that converges in O( √ L/σ log(1/ε) iterations, this yields an algorithm that converges\nin O( √ L/ε log 1ε ) iterations for minimizing F (x).\n• Classical Smoothing Reduction (finite sum). Given a non-smooth objective F (x) where f(x) = 1n ∑n i=1 fi(〈x, ai〉) is of a finite-sum form,1\none can define a smoothed variant of f̂i(α) for each fi(α) and set F ′(x) = 1n ∑n i=1 f̂i(〈ai, x〉)+ ψ(x). Informally, we need this smoothed variant to satisfy |f̂i(α) − fi(α)| ≤ ε for all α so minimizing F ′(x) is approximately the same as minimizing F (x). This can be done at least in two classical ways if f̂i(α) is Lipschitz continuous. 2 In both cases, f̂i(α) will be (1/ε) smooth.\nFor instance, if F (x) is σ-strongly convex and one applies an accelerated gradient method on F ′(x) that converges in O (√ L/σ log(1/ε) ) iterations, this yields an algorithm that converges in O( √\n1/σε log 1ε ) iterations for minimizing F (x).\nUnfortunately, neither the above regularization or smoothing is tight. For instance, the optimal dependence on ε should be 1/ √ ε for first-order methods on smooth but non-strongly convex objectives (see General Scheme of [17]), as compared to log(1/ε)/ √ ε obtained via regularization. The optimal dependence on ε should be 1/ √ ε for first-order methods on finite-sum, non-smooth but strongly convex objectives [3, 18], as compared to log(1/ε)/ √ ε obtained via smoothing. In this paper, we prove theorems that tighten both these reductions. In addition, for experimentalists, applying the above reduction gives only biased algorithms. One has to tune the regularization or smoothing parameter, and the algorithm only converges to the minimum of the regularized or smoothed problem, which can be away from the original minimizer of F (x) by a distance proportional to the parameter. This makes the algorithm hard to apply in practice. For this reason, many algorithm designers invent direct algorithms to solve Case 2, 3, or 4. This includes SAG [20], SAGA [4], and MISO [14] for the finite sum form of Case 2; the algorithms [8, 10, 19] for solving the general form of Case 3; as well as the saddle-point based algorithms [3, 15, 18] for solving the finite sum form of Case 4.\nOur Black-Box Oracle. In this paper, we assume that we are given a black-box algorithm Alg(F, x0) for solving solving (1.1) of Case 1 that satisfies the homogenous objective decrease (HOOD) property. That is, it produces an output x′ = Alg(F, x0) satisfying F (x′) − F (x∗) ≤ F (x0)−F (x∗) 4 in time Time(L, σ). In other words, Alg can decrease the objective value distance to the minimum by a constant factor in time Time(L, σ), regardless of how large or small F (x0)− F (x∗) is.\nWe give a few example algorithms that satisfy HOOD:\n• The full gradient descent method solves Case 1 and satisfies HOOD with Time(L, σ) = O(L/σ) · C where C is the time needed to compute a gradient ∇f(x) and performing a proximal gradient update [17].\n• The accelerated gradient descent method of Nesterov [16, 17], when puts in its proximal form, solves Case 1 and satisfies HOOD with Time(L, σ) = O( √ L/ √ σ) ·C. Many subsequent works\nin this line of research also satisfy HOOD, including [2, 11, 12].\n• The SVRG [9] method and its proximal version [25] solve the finite-sum form of Case 1 and satisfy HOOD with Time(L, σ) = O ( n + Lσ ) · C1 where C1 is the time need to compute a\n1Smoothing reduction is typically applied to the finite sum form only because, for a general high dimensional function f(x), its smoothed variant f̂(x) may not be efficiently computable.\n2One is to define f̂i(α) = Ev∈[−1,1][fi(α + εv)] as an integral of f over the scaled unit interval, see for instance Chapter 2.3 of [7], and the other is to define f̂i(α) = maxβ { β · α − f∗i (β) − ε2α2} using the Fenchel dual f∗i (β) of fi(α), see for instance [18].\nstochastic gradient ∇fi(x) and performing a proximal gradient update. It can be verified that AdaGrad [5] also satisfies HOOD.\nAdaptReg: Our New Regularization Reduction. For objectives F (x) that are not strongly convex but only L-smooth, our AdaptReg calls the an oracle satisfying HOOD a logarithmic number of times. Each time we call the oracle, we apply it on F (x) + σ2 ‖x − x0‖2 for an exponentially decreasing value of σ. In the end, AdaptReg produces an output x̂ satisfying F (x̂)− F (x∗) ≤ O(ε) with a total running time ∑∞ t=0 Time(L, ε · 2t).\nSince most algorithms have an inverse polynomial dependence on σ in its running time for solving Case 1, when summing up Time(L, ε · 2t) for positive values t, we do not incur the additional factor log 1ε as opposed to the old reduction. In addition, AdaptReg is an unbiased and anytime algorithm. F (x̂) converges to F (x∗) as the time goes without the necessity of changing the parameters. Therefore, AdaptReg can be interrupted at any time and the longer it runs the closer it gets to the minimum.\nApplications. Some accelerated gradient methods only apply to Case 1 and do not directly apply to Case 2 [2, 11, 12]. They incur a non-optimal overhead of log(1/ε) factor for minimizing (weakly) convex and smooth functions, see for instance Section 5.4 of [12]. Applying AdaptReg on these methods, we obtain new algorithms that converge in time O( √ L/ √ ε) ·C for Case 2, and this matches the optimum dependence on ε for first-order methods. The SVRG [9] method does not provide any theoretical guarantee for the finite-sum form of Case 2. By applying AdaptReg on SVRG, we obtain a new algorithm that runs in time O ( n log 1ε + L ε ) ·C1 for the finite-sum form of Case 2. This improves on the best known theoretical running time obtained by non-accelerated methods, including O ( n log 1ε + L ε log 1 ε ) ·C1 through the old reduction,\nas well as O ( n+L ε ) · C1 through direct methods such as SAGA [4] and SAG [20]. AdaptSmooth: Our New Smoothing Reduction. For objectives F (x) that is finite-sum, σstrongly convex, but not smooth, our AdaptSmooth calls an oracle satisfying HOOD a logarithmic number of times. Each time we call the oracle, we apply it on a smoothed version of F (x) (using Fenchel duality) with a smoothing parameter λ that is exponentially decreasing. In the end, AdaptSmooth produces an output x̂ satisfying F (x̂) − F (x∗) ≤ O(ε) with a total running time∑∞\nt=0 Time( 1 ε·2t , σ).\nSince most algorithms has a polynomial dependence on L in its running time for solving Case 1, when summing up Time( 1ε·2t , σ) for positive values t, we do not incur an additional factor of log 1ε as opposed to the old reduction. In addition, AdaptSmooth is also an unbiased and anytime algorithm for the same reason as AdaptReg.\nWe also demonstrate that AdaptReg and AdaptSmooth can effectively work together, providing a reduction from the finite-sum form of Case 4 to Case 1. We call this it JointAdaptRegSmooth.\nApplications. We can apply AdaptSmooth or JointAdaptRegSmooth to the aforementioned algorithms [2, 11, 12] in order to solve the finite-sum forms of Case 3 or 4. One can carefully verify that, the resulting algorithms are optimal first-order methods (in terms of the dependence on ε) for solving these two classes of problems respectively. In contrast, to obtain such optimal methods, one usually needs very different primal-dual approaches such as [15] or [3].\nTheory vs. Practice. In theory, not all algorithms solving Case 1 satisfy HOOD. Some machine learning algorithms such as APCG [13], SPDC [26], AccSDCA [24] and SDCA [23] are not known to satisfy HOOD. For example, APCG solves the finite-sum form of Case 1 and produces an output x satisfying F (x)− F (x∗) ≤ ε in time O (( n+ Lσ ) · log ( L σε )) ·C1. This running time does not have\na logarithmic dependence on ε that has the form log (F (x0)−F (x∗)\nε\n) . In other words, APCG might\nin principle take a much longer running time in order to decrease the objective distance to the minimum from 1 to 1/4, as compared to the time needed to decrease from 10−10 to 10−10/4.\nFortunately, although without theoretical guarantee, these methods also benefit from our new reductions, and we include experiments in this paper to confirm such findings.\nAn Open Question. Focusing on first-order stochastic gradient methods, the best known dependence on ε remains to be log(1/ε)/ √ ε for the finite-sum settings of Case 2 and 3, and to be log(1/ε)/ε for the finite-sum setting of Case 4. Such dependence can be improved to 1/ √ ε or 1/ε respectively if full gradient methods are used. To the best of our knowledge, it remains open how to tighten such an ε dependence for stochastic gradient methods. Our present paper sheds light on one potential path towards answering all the three open questions at once: if one can design an accelerated method for Case 1 that has running time O (( n + Lσ ) · log (F (x0)−F (x∗) ε ) · C1, our new reductions immediately tighten the ε dependence for Case 2, 3 and 4.\nRoadmap. We include the description and analysis of AdaptReg in Section 3. We only include the description of AdaptSmooth in Section 4 and leave its analysis to Appendix A. We leave both the description and analysis of JointAdaptRegSmooth to the appendix. We include experimental results in Section 6."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "In this paper we denote by ∇f(x) the full gradient vector of function f if it is differentiable, or the subgradient vector if f is only Lipschitz continuous. Recall some classical definitions on strong convexity and smoothness.\nDefinition 2.1 (Smoothness and strong convexity). For a convex function f : Rn → R, • We say f is σ-strongly convex if ∀x, y ∈ Rn, it satisfies f(y) ≥ f(x)+〈∇f(x), y−x〉+ σ2 ‖x−y‖2. • We say f is L-smooth if ∀x, y ∈ Rn, it satisfies ‖∇f(x)−∇f(y)‖ ≤ L‖x− y‖.\nConsider the minimization problem (1.1) in Case 1: let L be the smoothness parameter of f(·) and σ the strong convexity parameter of F (·). Definition 2.2. We say that an algorithm Alg(F, x0) solving Case 1 satisfies the homogenous objective decrease (HOOD) property with time Time(L, σ), if for every starting point x0, it produces an output x′ ← Alg(F, x0) such that F (x′)−minx F (x) ≤ F (x0)−minx F (x)4 in time Time(L, σ).3\nIn this paper we denote by C the time needed for computing a full gradient ∇f(x) and performing a proximal gradient update of the form x′ ← arg minx { 1 2‖x−x0‖2 +η(〈∇f(x), x−x0〉+ψ(x)) } . For the finite-sum case of problem (1.1), we denote by C1 the time needed for computing a stochastic (sub-)gradient ∇fi(〈ai, x〉) and performing a proximal gradient update of the form x′ ← arg minx { 1 2‖x − x0‖2 + η(〈∇fi(〈ai, x〉)ai, x − x0〉 + ψ(x)) } . For finite-sum forms of (1.1), C is usually on the magnitude of n× C1.\n3 AdaptReg: Reduction from Case 2 to Case 1\nIn this section, we focus on solving Case 2 of problem (1.1): that is, f(·) is L-smooth, but ψ(·) is not necessarily strongly convex. We achieve so by reducing the problem to an algorithm Alg solving Case 1 that satisfies HOOD.\n3Although our definition is only for deterministic algorithms, if the guarantee is probabilistic, i.e., E [ F (x′) ] −\nminx F (x) ≤ F (x0)−minx F (x)4 , all the results of this paper remain true.\nAlgorithm 1 The AdaptReg Reduction Input: an objective F (·) in Case 2 (smooth and not necessarily strongly convex); x0 a starting vector, σ0 an initial regularization parameter, T the number of epochs; an algorithm Alg that solves Case 1 of problem (1.1). Output: x̂T . 1: x̂0 ← x0. 2: for t← 0 to T − 1 do 3: Define F (σt)(x)\ndef = σt2 ‖x− x0‖+ F (x).\n4: x̂t+1 ← Alg(F (σt), x̂t). 5: σt+1 ← σt/2. 6: end for 7: return x̂T .\nAdaptReg works as follows (see Algorithm 1). At the beginning of AdaptReg, we set x̂0 to equal x0, an arbitrary given starting vector. AdaptReg consists of T epochs. At each epoch t = 0, 1, . . . , T − 1, we define a σt-strongly convex objective F (σt)(x) def= σt2 ‖x− x0‖2 + F (x). Here, the parameter σt+1 = σt/2 for each t ≥ 0 and σ0 is an input parameter to AdaptReg that will be specified later. We run Alg on F (σt)(x) with starting vector x̂t in each epoch, and let the output be x̂t+1. After all T epochs are finished, AdaptReg simply outputs x̂T . (Alternatively, if one sets T to be infinity, AdaptReg can be interrupted at an arbitrary moment and output x̂t of the current epoch.)\nWe state our main theorem for AdaptReg below and prove it in Section 3.1.\nTheorem 3.1. Suppose that in problem (1.1) f(·) is L-smooth. Let x0 be a starting vector such that F (x0) − F (x∗) ≤ ∆ and ‖x0 − x∗‖2 ≤ Θ. Then, AdaptReg with σ0 = ∆/Θ and T = log2(∆/ε) produces an output x̂T satisfying F (x̂T ) − minx F (x) ≤ O(ε) in a total running time of ∑T−1 t=0 Time(L, σ0 · 2−t).4\nExample 3.2. When AdaptReg is applied on gradient descent which satisfies HOOD with Time(L, σ) = O(L/σ) ·C, we solve Case 2 with a total running time ∑T−1t=0 Time(L, σ0 · 2−t) = O(Time(L, σT )) = O(L/σT ) · C = O(LΘ/ε) · C. This matches the running time of proximal gradient descent when directly applied to Case 2.\nWhen AdaptReg is applied to an accelerated gradient descent method designed for Case 1 such as [1, 2, 11, 12], we solve Case 2 with a total running time ∑T−1 t=0 Time(L, σ0 ·2−t) = O(Time(L, σT )) =\nO( √ L/σT ) ·C = O( √ LΘ/ε) ·C. This matches the best known running time to solve Case 2 using full-gradient first-order methods, without the necessity to change the algorithm. In contrast, one usually needs to perform very non-trivial changes on an accelerated gradient method in order to tune it to work for Case 2.\nWhen AdaptReg is applied to SVRG, we solve the finite-sum case of Case 2 with a total running time ∑T−1 t=0 Time(L, σ0 · 2−t) = ∑T−1 t=0 O(n+ L2t σ0 ) · C1 = O(n log(∆/ε) + LΘ/ε) · C1. This is faster\nthan applying the old reduction which gives a running time O (( n+ Lε ) log LΘε ) ·C1, and also faster\nthan using direct methods such as SAGA [4] and SAG [20] which give a running time O ( n+LΘ ε ) ·C1.\n4If the HOOD property is only satisfied probabilistically as per Footnote 3, our error guarantee also becomes probabilistic, i.e., E [ F (x̂T ) ] −minx F (x) ≤ O(ε). This is also true for the other reduction theorems of this paper.\n3.1 Convergence Analysis for AdaptReg\nFor analysis purpose only, we define xt+1 to be the exact minimizer of F (σt)(x). The HOOD property of the given oracle Alg ensures that\nF (σt)(x̂t+1)− F (σt)(xt+1) ≤ F (σt)(x̂t)− F (σt)(xt+1)\n4 . (3.1)\nWe denote by x∗ an arbitrary minimizer of F (x), and the following claim states a simple property about the minimizers of F (σt)(x):\nClaim 3.3. We have ‖xt+1 − x∗‖ ≤ ‖x0 − x∗‖ for each t ≥ 0. Proof. By the strong convexity of F (σt)(x) and the fact that xt+1 is its exact minimizer, we have\nF (σt)(xt+1)− F (σt)(x∗) ≤ − σt 2 ‖xt+1 − x∗‖2 .\nUsing the fact that F (σt)(xt+1) ≥ F (xt+1), as well as the definition F (σt)(x∗) = σt2 ‖x∗−x0‖2+F (x∗), we immediately have\nσt 2 ‖x0 − x∗‖2 − σt 2 ‖xt+1 − x∗‖2 ≥ F (xt+1)− F (x∗) ≥ 0 .\nDefine Dt def = F (σt)(x̂t) − F (σt)(xt+1) to be the initial objective distance to the minimum on function F (σt)(·) before we call Alg in epoch t. At epoch 0, we simply have the upper bound D0 = F (σ0)(x̂0)−minx F (σ0)(x) ≤ F (x0)− F (x∗). For each epoch t ≥ 1, we compute that\nDt def = F (σt)(x̂t)− F (σt)(xt+1) ¬ = F (σt−1)(x̂t)−\nσt−1 − σt 2 ‖x0 − x̂t‖2 − F (σt−1)(xt+1)\n+ σt−1 − σt\n2 ‖x0 − xt+1‖2\n ≤ F (σt−1)(x̂t)− σt−1 − σt 2 ‖x0 − x̂t‖2 − F (σt−1)(xt)\n− σt−1 2 ‖xt − xt+1‖2 + σt−1 − σt 2 ‖x0 − xt+1‖2\n≤ F (σt−1)(x̂t)− F (σt−1)(xt) + σt−1 − σt\n2 ‖x0 − xt+1‖2\n® ≤ F (σt−1)(x̂t)− F (σt−1)(xt)\n+ σt−1 − σt\n2\n( 2‖x0 − x∗‖2 + 2‖xt+1 − x∗‖2 )\n¯ ≤ F (σt−1)(x̂t)− F (σt−1)(xt) + 2(σt−1 − σt)‖x0 − x∗‖2 ° ≤ Dt−1\n4 + 2(σt−1 − σt)‖x0 − x∗‖2\n± = Dt−1 4 + 2σt‖x0 − x∗‖2 .\nAbove, ¬ follows from the definition of F (σt)(·) and F (σt−1)(·);  follows from the strong convexity of F (σt−1)(·) as well as the fact that xt is its minimizer; ® follows because for any two vectors a, b it satisfies ‖a− b‖2 ≤ 2‖a‖2 + 2‖b‖2; ¯ follows from Claim 3.3; ° follows from the definition of Dt−1 and (3.1); and ± uses the choice that σt = σt−1/2 for t ≥ 1.\nBy telescoping the above inequality, we have\nDT ≤ D0 4T\n+ ‖x0 − x∗‖2 · ( 2σT +\n2σT−1 4\n+ · · · ) ≤ 1\n4T (F (x0)− F (x∗)) + 4σT ‖x0 − x∗‖2 , (3.2)\nwhere the second inequality uses our choice σt = σt−1/2 again. In sum, we obtain a vector x̂T satisfying\nF (x̂T )− F (x∗) ¬ ≤ F (σT )(x̂T )− F (σT )(x∗) +\nσT 2 ‖x0 − x∗‖2\n ≤ F (σT )(x̂T )− F (σT )(xT+1) + σT 2 ‖x0 − x∗‖2 ® = DT +\nσT 2 ‖x0 − x∗‖2\n¯ ≤ 1 4T (F (x0)− F (x∗)) + 4.5σT ‖x0 − x∗‖2 . (3.3)\nAbove, ¬ uses the fact that F (σT )(x) ≥ F (x) for every x;  uses the definition that xT+1 is the minimizer of F (σT )(·); ® uses the definition of DT ; and ¯ uses (3.2).\nFinally, after appropriately choosing σ0 and T , (3.3) directly implies Theorem 3.1.\nRemark 3.4. When applying AdaptReg, if the original problem F (x) is already σ-strongly convex (and thus in Case 1), the total running time of the algorithm becomes ∑T−1 t=0 Time(L, σ + σt) ≤ Time(L, σ) · O(log ∆ε ). This matches the running time of the oracle itself on solving Case 1 up to constant factors. In other words, AdaptReg not only turns the oracle into an unbiased solver for Case 2, it also solves Case 1 with the same asymptotic running time comparing to the oracle itself.\n4 AdaptSmooth: Reduction from Case 3 to 1\nIn this section, we focus on solving the finite-sum form of Case 3 for problem (1.1). That is,\nmin x F (x) =\n1\nn\nn∑\ni=1\nfi(〈ai, x〉) + ψ(x) ,\nwhere ψ(x) is σ-strongly convex and each fi(·) may not be smooth (but is Lipschitz continuous). Without loss of generality, we assume ‖ai‖ = 1 for each i ∈ [n]. We solve this problem by reducing it to an algorithm Alg solving the finite-sum form of Case 1 that satisfies HOOD.\nRecall the following definition using Fenchel conjugate:\nDefinition 4.1. For each function fi : R → R, let f∗i (β) def = maxα{α · β − fi(α)} be its Fenchel conjugate. Then, we define the following smoothed variant of fi parameterized by λ > 0:\nf (λ) i (α) def = max\nβ\n{ β · α− f∗i (β)− λ 2 β2 } .\nAccordingly, we define\nF (λ)(x) def =\n1\nn\nn∑\ni=1\nf (λ) i (〈ai, x〉) + ψ(x) .\nAlgorithm 2 The AdaptSmooth Reduction Input: an objective F (·) in finite-sum form of Case 3 (strongly convex and not necessarily smooth); x0 a starting vector, λ0 an initial smoothing parameter, T the number of epochs; an algorithm Alg that solves the finite-sum form of Case 1 for problem (1.1). Output: x̂T . 1: x̂0 ← x0. 2: for t← 0 to T − 1 do 3: Define F (λt)(x)\ndef = 1n ∑n i=1 f (λt) i (〈ai, x〉) + ψ(x) using Definition 4.1.\n4: x̂t+1 ← Alg(F (λt), x̂t). 5: λt+1 ← λt/2. 6: end for 7: return x̂T .\nFrom the property of Fenchel conjugate (see for instance the textbook [21]), we know that f (λ) i (·) is a (1/λ)-smooth function and therefore the objective F (λ)(x) falls into the finite-sum form of Case 1 for problem (1.1) with L = 1/λ. AdaptSmooth works as follows (see Algorithm 2). At the beginning of AdaptSmooth, we set x̂0 to equal x0, an arbitrary given starting vector. AdaptSmooth consists of T epochs. At each epoch t = 0, 1, . . . , T − 1, we define a (1/λt)-smooth objective F (λt)(x) using Definition 4.1 above. Here, the parameter λt+1 = λt/2 for each t ≥ 0 and λ0 is an input parameter to AdaptSmooth that will be specified later. We run Alg on F (λt)(x) with starting vector x̂t in each epoch, and let the output be x̂t+1. After all T epochs are finished, AdaptSmooth simply outputs x̂T . (Alternatively, if one sets T to be infinity, AdaptSmooth can be interrupted at an arbitrary moment and output x̂t of the current epoch.)\nWe state our main theorem for AdaptSmooth below and prove it in Appendix A.\nTheorem 4.2. Suppose that in problem (1.1), ψ(·) is σ strongly convex and each fi(·) is GLipschitz continuous. Let x0 be a starting vector such that F (x0)−F (x∗) ≤ ∆. Then, AdaptSmooth with λ0 = ∆/G\n2 and T = log2(∆/ε) produces an output x̂T satisfying F (x̂T )−minx F (x) ≤ O(ε) in a total running time of ∑T−1 t=0 Time(2 t/λ0, σ).\n5 JointAdaptRegSmooth: From Case 4 to 1\nWe show in Appendix B that AdaptReg and AdaptSmooth can work together to reduce the finitesum form of Case 4 to Case 1. We call this reduction JointAdaptRegSmooth and it relies on a jointly exponentially decreasing sequence of (σt, λt), where σt is the weight of the convexity parameter that we add on top of F (x), and λt is the smoothing parameter that determines how we change each fi(·). The analysis is analogous to a careful combination of the proofs for AdaptReg and AdaptSmooth."
    }, {
      "heading" : "6 Experiments",
      "text" : "We perform experiments to confirm our theoretical speed-ups obtained for the regularization and smoothing reductions. In particular, we work on empirical risk minimizations for the following three datasets that can be publicly downloaded from the LibSVM website [6]: • the covtype (binary.scale) dataset (581, 012 samples and 54 features). • the mnist (class 1) dataset (60, 000 samples and 780 features).\n• the rcv1 (train.binary) dataset (20, 242 samples and 47, 236 features). To make easier comparison across datasets, for each dataset, we scale every vector by the average Euclidean norm of all the vectors. In other words, we ensure that the data vectors have an average Euclidean norm 1. This step is for comparison only and not necessary in practice.\n6.1 Experiments on AdaptReg\nTo test the performance of AdaptReg, in this subsection we consider the Lasso training problem which is not strongly convex but smooth (i.e., finite-sum form of Case 2). We apply AdaptReg to reduce it to Case 1 and apply either APCG [13], an accelerated method, or (Prox-)SDCA [22, 23], a non-accelerated method. Let us make a few remarks: • APCG and SDCA are both indirect solvers for non-strongly convex objectives and therefore\nregularization is intrinsically required in order to run them for LASSO or more generally Case 2.5 • APCG and SDCA do not satisfy HOOD in theory. However, they still benefit from AdaptReg as we shall see, demonstrating the practical strength of AdaptReg. We use the default step-length choice for APCG which requires solving a quadratic univariate function per iteration; for SDCA, to avoid the issue for tuning step lengths, we use the steepest descent (i.e., automatic) choice which is Option I for SDCA (see [22]).\nA Practical Implementation. In principle, one can implement AdaptReg by setting the termination criteria of the oracle in the inner loop as precisely suggested by the theory, such as setting the number of iterations for SDCA to be exactly T = O(n + Lσt ) for the t-th epoch. However, in practice, it is more desirable to automatically terminate the oracle whenever the objective distance to the minimum has been sufficiently decreased, say, by a factor of 4.\nUnfortunately, the oracle (SDCA or APCG) does not know the exact minimizer and cannot compute the exact objective distance to the minimum (i.e., Dt). Since both SDCA and APCG are primal-dual methods, in our experiments, we compute instead the duality gap which gives a reasonable approximation on Dt. More specifically, for both experiments, we compute the duality gap every n/3 iterations inside the implementation of APCG/SDCA, and terminate it whenever\n5Note that some other methods, such as SVRG, although only providing theoretical results for strongly convex and smooth objectives (Case 1), in practice works for Case 2 directly. Therefore, it is not needed to apply AdaptReg on such methods at least in practice.\nthe duality gap is below 1/4 times the last recorded duality gap of the previous epoch. Although one can further tune this parameter 1/4 for a better performance, to perform a fair comparison, we simply set it to be identically 1/4 across all the datasets and analysis tasks.\nExperimental Results. For each dataset (covtype/mnist/rcv1), we consider three different magnitudes of regularization weights for the `1 regularizer in the Lasso objective. This totals 9 analysis tasks for each algorithm.\nFor each such a task, we first implement the old reduction by adding an additional σ2 ‖x‖2 term to the Lasso objective and then apply APCG or SDCA. We consider different values of σ in the set {10k, 3 · 10k : k ∈ Z} and show the most representative six of them in the plots (blue solid curves in Figure 3 and Figure 4). Note that for a larger value of σ, the old reduction converges faster but converges to a point that is farther from the exact minimizer. We then implement AdaptReg where we choose the initial regularization parameter σ0 also from the set {10k, 3 ·10k : k ∈ Z} and present the best one in each of 18 plots (red dashed curves in Figure 3 and Figure 4).\nDue to space limitations, we provide only 3 of the 18 plots for medium-sized λ and for APCG in the main body of this paper (see Figure 1), and include Figure 3 and Figure 4 only in the appendix.\nIt is clear from our experiments that AdaptReg is more efficient than the old regularization reduction. Perhaps more importantly, AdaptReg is unbiased and greatly simplifies the parameter tuning procedure. It is easy to find the best σ0 for AdaptReg which works for all desired training errors. This is in contrast to the old reduction where if the desired error is somehow changed for the application, one has to select a different σ and restart the algorithm.\n6.2 Experiments on AdaptSmooth\nTo test the performance of AdaptSmooth, in this subsection we consider the L2SVM training problem which is smooth but strongly convex (i.e., finite-sum form of Case 3). We apply AdaptSmooth to reduce it to Case 1 and apply SVRG [9]. We emphasize that SVRG is an indirect solver for non-smooth objectives and therefore regularization is intrinsically required in order to run it for L2SVM or more generally Case 3.6\n6Note that some other methods, such as APCG or SDCA, although only providing theoretical guarantees for strongly convex and smooth objectives (Case 1), in practice work for Case 2 directly without smoothing (see for instance the discussion in [22]). Therefore, it is unnecessary to apply AdaptSmooth to such methods at least in practice.\nA Practical Implementation. In principle, one can implement AdaptSmooth by setting the termination criteria of the oracle in the inner loop as precisely suggested by the theory, such as setting the number of iterations for SVRG to be exactly T = O(n + 1σλt ) for the t-th epoch. However, in practice, it is more desirable to automatically terminate the oracle whenever the objective distance to the minimum has been sufficiently decreased, say, by a factor of 4.\nUnfortunately, SVRG does not know the exact minimizer and cannot compute the exact objective distance to the minimum (i.e., Dt in our analysis). Neither can we compute the duality gap like in Section 6.2 because SVRG is a primal-only method. In our experiment, we compute instead the Euclidean norm of the full gradient of the objective (i.e., ‖∇f(x)‖) which gives a reasonable approximation on Dt. More specifically, we use the default setting of SVRG Option I that is to compute a “gradient snapshot” every 2n iterations. When a gradient snapshot is computed, we can also compute its Euclidean norm almost for free. If this norm is below 1/3 times the last normof-gradient of the previous epoch, we terminate SVRG for the current epoch. Note that one can further tune this parameter 1/3 for a better performance; however, to perform a fair comparison in this paper, we simply set it to be identically 1/3 across all the datasets and analysis tasks.\nExperimental Results. For each dataset (covtype/mnist/rcv1), we consider three different magnitudes of regularization weights for the `2 regularizer in the L2SVM objective. This totals 9 analysis tasks.\nFor each such a task, we first implement the old reduction by smoothing the hinge loss functions (using Definition 4.1) with parameter λ > 0 and then apply SVRG. We consider different values of λ in the set {10k, 3 · 10k : k ∈ Z} and show the most representative six of them in the plots (blue solid curves in Figure 5). For a larger λ, the old reduction converges faster but converges to a point that is farther from the exact minimizer. We then implement AdaptSmooth where we choose the initial smoothing parameter λ0 also from the set {10k, 3 · 10k : k ∈ Z} and present the best one in each of the 9 plots (red dashed curves in Figure 5).\nDue to space limitations, we provide only 3 of the 9 plots for small-sized σ in the main body of this paper (see Figure 2, and include Figure 5 only in the appendix.\nIt is clear from our experiments that AdaptSmooth is more efficient than the old one, especially when the desired training error is small. Perhaps more importantly, AdaptSmooth is unbiased and greatly simplifies the parameter tuning procedure. It is easy to find the best λ0 for AdaptSmooth which works for all desired training errors. This is in contrast to the old reduction where if the desired error is somehow changed for the application, one has to select a different λ and restart the algorithm."
    } ],
    "references" : [ {
      "title" : "Linear coupling: An ultimate unification of gradient and mirror descent",
      "author" : [ "Allen-Zhu", "Zeyuan", "Orecchia", "Lorenzo" ],
      "venue" : "ArXiv e-prints,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2014
    }, {
      "title" : "A geometric alternative to Nesterov’s accelerated gradient descent",
      "author" : [ "Bubeck", "Sébastien", "Lee", "Yin Tat", "Singh", "Mohit" ],
      "venue" : "ArXiv e-prints,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2015
    }, {
      "title" : "A first-order primal-dual algorithm for convex problems with applications to imaging",
      "author" : [ "Chambolle", "Antonin", "Pock", "Thomas" ],
      "venue" : "Journal of Mathematical Imaging and Vision,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2011
    }, {
      "title" : "SAGA: A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives",
      "author" : [ "Defazio", "Aaron", "Bach", "Francis", "Lacoste-Julien", "Simon" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2014
    }, {
      "title" : "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization",
      "author" : [ "Duchi", "John", "Hazan", "Elad", "Singer", "Yoram" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2011
    }, {
      "title" : "LIBSVM Data: Classification, Regression and Multi-label",
      "author" : [ "Fan", "Rong-En", "Lin", "Chih-Jen" ],
      "venue" : null,
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2015
    }, {
      "title" : "DRAFT: Introduction to online convex optimimization",
      "author" : [ "Hazan", "Elad" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2015
    }, {
      "title" : "Beyond the regret minimization barrier: Optimal algorithms for stochastic strongly-convex optimization",
      "author" : [ "Hazan", "Elad", "Kale", "Satyen" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2014
    }, {
      "title" : "Accelerating stochastic gradient descent using predictive variance reduction",
      "author" : [ "Johnson", "Rie", "Zhang", "Tong" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2013
    }, {
      "title" : "A simpler approach to obtaining an o(1/t) convergence rate for the projected stochastic subgradient method",
      "author" : [ "Lacoste-Julien", "Simon", "Schmidt", "Mark W", "Bach", "Francis R" ],
      "venue" : "ArXiv e-prints,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2012
    }, {
      "title" : "Efficient accelerated coordinate descent methods and faster algorithms for solving linear systems",
      "author" : [ "Lee", "Yin Tat", "Sidford", "Aaron" ],
      "venue" : "In Foundations of Computer Science (FOCS),",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2013
    }, {
      "title" : "Analysis and design of optimization algorithms via integral quadratic constraints",
      "author" : [ "Lessard", "Laurent", "Recht", "Benjamin", "Packard", "Andrew" ],
      "venue" : "CoRR, abs/1408.3595,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2014
    }, {
      "title" : "An Accelerated Proximal Coordinate Gradient Method and its Application to Regularized Empirical Risk Minimization",
      "author" : [ "Lin", "Qihang", "Lu", "Zhaosong", "Xiao" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2014
    }, {
      "title" : "Incremental Majorization-Minimization Optimization with Application to Large-Scale Machine Learning",
      "author" : [ "Mairal", "Julien" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2015
    }, {
      "title" : "Prox-Method with Rate of Convergence O(1/t) for Variational Inequalities with Lipschitz Continuous Monotone Operators and Smooth Convex-Concave Saddle Point Problems",
      "author" : [ "Nemirovski", "Arkadi" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2004
    }, {
      "title" : "A method of solving a convex programming problem with convergence rate O(1/k2)",
      "author" : [ "Nesterov", "Yurii" ],
      "venue" : "In Doklady AN SSSR (translated as Soviet Mathematics Doklady),",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1983
    }, {
      "title" : "Introductory Lectures on Convex Programming Volume: A Basic course, volume I",
      "author" : [ "Nesterov", "Yurii" ],
      "venue" : null,
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2004
    }, {
      "title" : "Smooth minimization of non-smooth functions",
      "author" : [ "Nesterov", "Yurii" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2005
    }, {
      "title" : "Making gradient descent optimal for strongly convex stochastic optimization",
      "author" : [ "Rakhlin", "Alexander", "Shamir", "Ohad", "Sridharan", "Karthik" ],
      "venue" : "In Proceedings of the 29th International Conference on Machine Learning,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2012
    }, {
      "title" : "Minimizing finite sums with the stochastic average gradient",
      "author" : [ "Schmidt", "Mark", "Le Roux", "Nicolas", "Bach", "Francis" ],
      "venue" : "arXiv preprint arXiv:1309.2388,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2013
    }, {
      "title" : "Online Learning and Online Convex Optimization",
      "author" : [ "Shalev-Shwartz", "Shai" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2012
    }, {
      "title" : "Proximal Stochastic Dual Coordinate Ascent",
      "author" : [ "Shalev-Shwartz", "Shai", "Zhang", "Tong" ],
      "venue" : "arXiv preprint arXiv:1211.2717,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2012
    }, {
      "title" : "Stochastic dual coordinate ascent methods for regularized loss minimization",
      "author" : [ "Shalev-Shwartz", "Shai", "Zhang", "Tong" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2013
    }, {
      "title" : "Accelerated Proximal Stochastic Dual Coordinate Ascent for Regularized Loss Minimization",
      "author" : [ "Shalev-Shwartz", "Shai", "Zhang", "Tong" ],
      "venue" : "In Proceedings of the 31st International Conference on Machine Learning,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2014
    }, {
      "title" : "A Proximal Stochastic Gradient Method with Progressive Variance Reduction",
      "author" : [ "Xiao", "Lin", "Zhang", "Tong" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2014
    }, {
      "title" : "Stochastic Primal-Dual Coordinate Method for Regularized Empirical Risk Minimization",
      "author" : [ "Zhang", "Yuchen", "Xiao", "Lin" ],
      "venue" : "In Proceedings of the 32nd International Conference on Machine Learning,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 16,
      "context" : "For instance, it is famously known that by applying (proximal) gradient descent starting from vector x0, one can obtain an εapproximate minimizer of F (x) in T = O ( L σ log F (x0)−F (x∗) ε ) iterations [17].",
      "startOffset" : 203,
      "endOffset" : 207
    }, {
      "referenceID" : 15,
      "context" : "This can be improved to T = O( √ L √ σ log F (x0)−F (x ∗) ε ) if the so-called (proximal) accelerated gradient descent is used, see [16] or Constant Step Scheme in [17].",
      "startOffset" : 132,
      "endOffset" : 136
    }, {
      "referenceID" : 16,
      "context" : "This can be improved to T = O( √ L √ σ log F (x0)−F (x ∗) ε ) if the so-called (proximal) accelerated gradient descent is used, see [16] or Constant Step Scheme in [17].",
      "startOffset" : 164,
      "endOffset" : 168
    }, {
      "referenceID" : 16,
      "context" : "For instance, the accelerated gradient descent method has its non-strongly convex variant (see General Scheme in [17]) and its non-smooth variant [18].",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 17,
      "context" : "For instance, the accelerated gradient descent method has its non-strongly convex variant (see General Scheme in [17]) and its non-smooth variant [18].",
      "startOffset" : 146,
      "endOffset" : 150
    }, {
      "referenceID" : 6,
      "context" : "The second approach is to apply a reduction from non-strongly convex or non-smooth objectives into a strongly convex and smooth one, via the classical regularization or smoothing techniques: (see [7] Section 2.",
      "startOffset" : 196,
      "endOffset" : 199
    }, {
      "referenceID" : 16,
      "context" : "For instance, the optimal dependence on ε should be 1/ √ ε for first-order methods on smooth but non-strongly convex objectives (see General Scheme of [17]), as compared to log(1/ε)/ √ ε obtained via regularization.",
      "startOffset" : 151,
      "endOffset" : 155
    }, {
      "referenceID" : 2,
      "context" : "The optimal dependence on ε should be 1/ √ ε for first-order methods on finite-sum, non-smooth but strongly convex objectives [3, 18], as compared to log(1/ε)/ √ ε obtained via smoothing.",
      "startOffset" : 126,
      "endOffset" : 133
    }, {
      "referenceID" : 17,
      "context" : "The optimal dependence on ε should be 1/ √ ε for first-order methods on finite-sum, non-smooth but strongly convex objectives [3, 18], as compared to log(1/ε)/ √ ε obtained via smoothing.",
      "startOffset" : 126,
      "endOffset" : 133
    }, {
      "referenceID" : 19,
      "context" : "This includes SAG [20], SAGA [4], and MISO [14] for the finite sum form of Case 2; the algorithms [8, 10, 19] for solving the general form of Case 3; as well as the saddle-point based algorithms [3, 15, 18] for solving the finite sum form of Case 4.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 3,
      "context" : "This includes SAG [20], SAGA [4], and MISO [14] for the finite sum form of Case 2; the algorithms [8, 10, 19] for solving the general form of Case 3; as well as the saddle-point based algorithms [3, 15, 18] for solving the finite sum form of Case 4.",
      "startOffset" : 29,
      "endOffset" : 32
    }, {
      "referenceID" : 13,
      "context" : "This includes SAG [20], SAGA [4], and MISO [14] for the finite sum form of Case 2; the algorithms [8, 10, 19] for solving the general form of Case 3; as well as the saddle-point based algorithms [3, 15, 18] for solving the finite sum form of Case 4.",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 7,
      "context" : "This includes SAG [20], SAGA [4], and MISO [14] for the finite sum form of Case 2; the algorithms [8, 10, 19] for solving the general form of Case 3; as well as the saddle-point based algorithms [3, 15, 18] for solving the finite sum form of Case 4.",
      "startOffset" : 98,
      "endOffset" : 109
    }, {
      "referenceID" : 9,
      "context" : "This includes SAG [20], SAGA [4], and MISO [14] for the finite sum form of Case 2; the algorithms [8, 10, 19] for solving the general form of Case 3; as well as the saddle-point based algorithms [3, 15, 18] for solving the finite sum form of Case 4.",
      "startOffset" : 98,
      "endOffset" : 109
    }, {
      "referenceID" : 18,
      "context" : "This includes SAG [20], SAGA [4], and MISO [14] for the finite sum form of Case 2; the algorithms [8, 10, 19] for solving the general form of Case 3; as well as the saddle-point based algorithms [3, 15, 18] for solving the finite sum form of Case 4.",
      "startOffset" : 98,
      "endOffset" : 109
    }, {
      "referenceID" : 2,
      "context" : "This includes SAG [20], SAGA [4], and MISO [14] for the finite sum form of Case 2; the algorithms [8, 10, 19] for solving the general form of Case 3; as well as the saddle-point based algorithms [3, 15, 18] for solving the finite sum form of Case 4.",
      "startOffset" : 195,
      "endOffset" : 206
    }, {
      "referenceID" : 14,
      "context" : "This includes SAG [20], SAGA [4], and MISO [14] for the finite sum form of Case 2; the algorithms [8, 10, 19] for solving the general form of Case 3; as well as the saddle-point based algorithms [3, 15, 18] for solving the finite sum form of Case 4.",
      "startOffset" : 195,
      "endOffset" : 206
    }, {
      "referenceID" : 17,
      "context" : "This includes SAG [20], SAGA [4], and MISO [14] for the finite sum form of Case 2; the algorithms [8, 10, 19] for solving the general form of Case 3; as well as the saddle-point based algorithms [3, 15, 18] for solving the finite sum form of Case 4.",
      "startOffset" : 195,
      "endOffset" : 206
    }, {
      "referenceID" : 16,
      "context" : "We give a few example algorithms that satisfy HOOD: • The full gradient descent method solves Case 1 and satisfies HOOD with Time(L, σ) = O(L/σ) · C where C is the time needed to compute a gradient ∇f(x) and performing a proximal gradient update [17].",
      "startOffset" : 246,
      "endOffset" : 250
    }, {
      "referenceID" : 15,
      "context" : "• The accelerated gradient descent method of Nesterov [16, 17], when puts in its proximal form, solves Case 1 and satisfies HOOD with Time(L, σ) = O( √ L/ √ σ) ·C.",
      "startOffset" : 54,
      "endOffset" : 62
    }, {
      "referenceID" : 16,
      "context" : "• The accelerated gradient descent method of Nesterov [16, 17], when puts in its proximal form, solves Case 1 and satisfies HOOD with Time(L, σ) = O( √ L/ √ σ) ·C.",
      "startOffset" : 54,
      "endOffset" : 62
    }, {
      "referenceID" : 1,
      "context" : "Many subsequent works in this line of research also satisfy HOOD, including [2, 11, 12].",
      "startOffset" : 76,
      "endOffset" : 87
    }, {
      "referenceID" : 10,
      "context" : "Many subsequent works in this line of research also satisfy HOOD, including [2, 11, 12].",
      "startOffset" : 76,
      "endOffset" : 87
    }, {
      "referenceID" : 11,
      "context" : "Many subsequent works in this line of research also satisfy HOOD, including [2, 11, 12].",
      "startOffset" : 76,
      "endOffset" : 87
    }, {
      "referenceID" : 8,
      "context" : "• The SVRG [9] method and its proximal version [25] solve the finite-sum form of Case 1 and satisfy HOOD with Time(L, σ) = O ( n + Lσ ) · C1 where C1 is the time need to compute a Smoothing reduction is typically applied to the finite sum form only because, for a general high dimensional function f(x), its smoothed variant f̂(x) may not be efficiently computable.",
      "startOffset" : 11,
      "endOffset" : 14
    }, {
      "referenceID" : 24,
      "context" : "• The SVRG [9] method and its proximal version [25] solve the finite-sum form of Case 1 and satisfy HOOD with Time(L, σ) = O ( n + Lσ ) · C1 where C1 is the time need to compute a Smoothing reduction is typically applied to the finite sum form only because, for a general high dimensional function f(x), its smoothed variant f̂(x) may not be efficiently computable.",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 6,
      "context" : "3 of [7], and the other is to define f̂i(α) = maxβ { β · α − f∗ i (β) − ε 2α2} using the Fenchel dual f∗ i (β) of fi(α), see for instance [18].",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 17,
      "context" : "3 of [7], and the other is to define f̂i(α) = maxβ { β · α − f∗ i (β) − ε 2α2} using the Fenchel dual f∗ i (β) of fi(α), see for instance [18].",
      "startOffset" : 138,
      "endOffset" : 142
    }, {
      "referenceID" : 4,
      "context" : "It can be verified that AdaGrad [5] also satisfies HOOD.",
      "startOffset" : 32,
      "endOffset" : 35
    }, {
      "referenceID" : 1,
      "context" : "Some accelerated gradient methods only apply to Case 1 and do not directly apply to Case 2 [2, 11, 12].",
      "startOffset" : 91,
      "endOffset" : 102
    }, {
      "referenceID" : 10,
      "context" : "Some accelerated gradient methods only apply to Case 1 and do not directly apply to Case 2 [2, 11, 12].",
      "startOffset" : 91,
      "endOffset" : 102
    }, {
      "referenceID" : 11,
      "context" : "Some accelerated gradient methods only apply to Case 1 and do not directly apply to Case 2 [2, 11, 12].",
      "startOffset" : 91,
      "endOffset" : 102
    }, {
      "referenceID" : 11,
      "context" : "4 of [12].",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 8,
      "context" : "The SVRG [9] method does not provide any theoretical guarantee for the finite-sum form of Case 2.",
      "startOffset" : 9,
      "endOffset" : 12
    }, {
      "referenceID" : 3,
      "context" : "This improves on the best known theoretical running time obtained by non-accelerated methods, including O ( n log 1ε + L ε log 1 ε ) ·C1 through the old reduction, as well as O ( n+L ε ) · C1 through direct methods such as SAGA [4] and SAG [20].",
      "startOffset" : 228,
      "endOffset" : 231
    }, {
      "referenceID" : 19,
      "context" : "This improves on the best known theoretical running time obtained by non-accelerated methods, including O ( n log 1ε + L ε log 1 ε ) ·C1 through the old reduction, as well as O ( n+L ε ) · C1 through direct methods such as SAGA [4] and SAG [20].",
      "startOffset" : 240,
      "endOffset" : 244
    }, {
      "referenceID" : 1,
      "context" : "We can apply AdaptSmooth or JointAdaptRegSmooth to the aforementioned algorithms [2, 11, 12] in order to solve the finite-sum forms of Case 3 or 4.",
      "startOffset" : 81,
      "endOffset" : 92
    }, {
      "referenceID" : 10,
      "context" : "We can apply AdaptSmooth or JointAdaptRegSmooth to the aforementioned algorithms [2, 11, 12] in order to solve the finite-sum forms of Case 3 or 4.",
      "startOffset" : 81,
      "endOffset" : 92
    }, {
      "referenceID" : 11,
      "context" : "We can apply AdaptSmooth or JointAdaptRegSmooth to the aforementioned algorithms [2, 11, 12] in order to solve the finite-sum forms of Case 3 or 4.",
      "startOffset" : 81,
      "endOffset" : 92
    }, {
      "referenceID" : 14,
      "context" : "In contrast, to obtain such optimal methods, one usually needs very different primal-dual approaches such as [15] or [3].",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 2,
      "context" : "In contrast, to obtain such optimal methods, one usually needs very different primal-dual approaches such as [15] or [3].",
      "startOffset" : 117,
      "endOffset" : 120
    }, {
      "referenceID" : 12,
      "context" : "Some machine learning algorithms such as APCG [13], SPDC [26], AccSDCA [24] and SDCA [23] are not known to satisfy HOOD.",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 25,
      "context" : "Some machine learning algorithms such as APCG [13], SPDC [26], AccSDCA [24] and SDCA [23] are not known to satisfy HOOD.",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 23,
      "context" : "Some machine learning algorithms such as APCG [13], SPDC [26], AccSDCA [24] and SDCA [23] are not known to satisfy HOOD.",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 22,
      "context" : "Some machine learning algorithms such as APCG [13], SPDC [26], AccSDCA [24] and SDCA [23] are not known to satisfy HOOD.",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 0,
      "context" : "When AdaptReg is applied to an accelerated gradient descent method designed for Case 1 such as [1, 2, 11, 12], we solve Case 2 with a total running time ∑T−1 t=0 Time(L, σ0 ·2−t) = O(Time(L, σT )) = O( √ L/σT ) ·C = O( √ LΘ/ε) ·C.",
      "startOffset" : 95,
      "endOffset" : 109
    }, {
      "referenceID" : 1,
      "context" : "When AdaptReg is applied to an accelerated gradient descent method designed for Case 1 such as [1, 2, 11, 12], we solve Case 2 with a total running time ∑T−1 t=0 Time(L, σ0 ·2−t) = O(Time(L, σT )) = O( √ L/σT ) ·C = O( √ LΘ/ε) ·C.",
      "startOffset" : 95,
      "endOffset" : 109
    }, {
      "referenceID" : 10,
      "context" : "When AdaptReg is applied to an accelerated gradient descent method designed for Case 1 such as [1, 2, 11, 12], we solve Case 2 with a total running time ∑T−1 t=0 Time(L, σ0 ·2−t) = O(Time(L, σT )) = O( √ L/σT ) ·C = O( √ LΘ/ε) ·C.",
      "startOffset" : 95,
      "endOffset" : 109
    }, {
      "referenceID" : 11,
      "context" : "When AdaptReg is applied to an accelerated gradient descent method designed for Case 1 such as [1, 2, 11, 12], we solve Case 2 with a total running time ∑T−1 t=0 Time(L, σ0 ·2−t) = O(Time(L, σT )) = O( √ L/σT ) ·C = O( √ LΘ/ε) ·C.",
      "startOffset" : 95,
      "endOffset" : 109
    }, {
      "referenceID" : 3,
      "context" : "This is faster than applying the old reduction which gives a running time O (( n+ Lε ) log LΘ ε ) ·C1, and also faster than using direct methods such as SAGA [4] and SAG [20] which give a running time O ( n+LΘ ε ) ·C1.",
      "startOffset" : 158,
      "endOffset" : 161
    }, {
      "referenceID" : 19,
      "context" : "This is faster than applying the old reduction which gives a running time O (( n+ Lε ) log LΘ ε ) ·C1, and also faster than using direct methods such as SAGA [4] and SAG [20] which give a running time O ( n+LΘ ε ) ·C1.",
      "startOffset" : 170,
      "endOffset" : 174
    }, {
      "referenceID" : 20,
      "context" : "From the property of Fenchel conjugate (see for instance the textbook [21]), we know that f (λ) i (·) is a (1/λ)-smooth function and therefore the objective F (λ)(x) falls into the finite-sum form of Case 1 for problem (1.",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 5,
      "context" : "In particular, we work on empirical risk minimizations for the following three datasets that can be publicly downloaded from the LibSVM website [6]: • the covtype (binary.",
      "startOffset" : 144,
      "endOffset" : 147
    }, {
      "referenceID" : 12,
      "context" : "We apply AdaptReg to reduce it to Case 1 and apply either APCG [13], an accelerated method, or (Prox-)SDCA [22, 23], a non-accelerated method.",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 21,
      "context" : "We apply AdaptReg to reduce it to Case 1 and apply either APCG [13], an accelerated method, or (Prox-)SDCA [22, 23], a non-accelerated method.",
      "startOffset" : 107,
      "endOffset" : 115
    }, {
      "referenceID" : 22,
      "context" : "We apply AdaptReg to reduce it to Case 1 and apply either APCG [13], an accelerated method, or (Prox-)SDCA [22, 23], a non-accelerated method.",
      "startOffset" : 107,
      "endOffset" : 115
    }, {
      "referenceID" : 21,
      "context" : ", automatic) choice which is Option I for SDCA (see [22]).",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 8,
      "context" : "We apply AdaptSmooth to reduce it to Case 1 and apply SVRG [9].",
      "startOffset" : 59,
      "endOffset" : 62
    }, {
      "referenceID" : 21,
      "context" : "6 Note that some other methods, such as APCG or SDCA, although only providing theoretical guarantees for strongly convex and smooth objectives (Case 1), in practice work for Case 2 directly without smoothing (see for instance the discussion in [22]).",
      "startOffset" : 244,
      "endOffset" : 248
    } ],
    "year" : 2017,
    "abstractText" : "The diverse world of machine learning applications has given rise to a plethora of algorithms and optimization methods, finely tuned to the smoothness, convexity, and other parameterizations of the objective. In this paper we attempt to simplify and reduce the complexity of algorithm design for machine learning by reductions: we develop reductions that take a method developed for one setting and apply it to the entire spectrum of smoothness and strong-convexity found in practice. We show how these new reductions give rise to faster running times on training linear classifiers for certain families of loss functions, and that our reductions are optimal and cannot be improved in general. We conclude with experiments showing our reductions successfully transform methods between domains and achieve the desired performance predicted by theory.",
    "creator" : "LaTeX with hyperref package"
  }
}