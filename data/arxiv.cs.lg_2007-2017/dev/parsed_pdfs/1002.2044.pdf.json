{
  "name" : "1002.2044.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "On the Stability of Empirical Risk Minimization in the Presence of Multiple Risk Minimizers",
    "authors" : [ "Benjamin I. P. Rubinstein", "Aleksandr Simma" ],
    "emails" : [ "benr@eecs.berkeley.edu).", "asimma@eecs.berkeley.edu)." ],
    "sections" : [ {
      "heading" : null,
      "text" : "Index Terms—empirical risk minimization, algorithmic stability, threshold phenomena\nI. INTRODUCTION\nDEVROYE and Wagner [3] first studied the effect ofalgorithmic stability on the statistical generalization of learning. Since then many authors have proposed numerous alternate notions of stability and have used them to study the performance of algorithms not easily analyzed with other techniques [1], [2], [4]–[6]. While the learnability of a concept class can now be characterized by the admission of a stable learner, finding natural definitions of stability with such properties is still open [5], [6]. As part of their general investigation into algorithmic stability in [5], Kutin and Niyogi observed that Empirical Risk Minimization (ERM) on hypothesis spaces of cardinality two experiences a phase transition in achievable rates of stability as the number of risk minimizers increases from one to two. Under a unique minimizer stability scales exponentially with sample size, while in the presence of multiple minimizers no rate faster than quadratic is possible. Kutin and Niyogi extended the unique risk minimizer result to finite spaces, and conjectured that ERM on a finite hypothesis space containing multiple risk minimizers is (0, δ)-trainingstable for no δ = o ( m−1/2 ) [5, Conjecture 10.11]. We\npositively resolve this conjecture for weaker CV-stability and show furthermore that ERM on such spaces is (0, δ)-CV-stable for δ = O ( m−1/2 ) .\nStudying the effects of multiple risk minimizers is well motivated by the practical consideration of feature selection. Learning from irrelevant features can lead to multiple risk\nManuscript received February 10, 2010. This work was supported by the Siebel Scholars Foundation and the National Science Foundation under grant DMS-0707060.\nThe authors are with the Computer Science Division, University of California, Berkeley (e-mail: {benr,asimma}@eecs.berkeley.edu).\nminimizers, for example when learning in a symmetric hypothesis class with positive minimum risk. Results showing poor generalization in such cases, as implied by slow rates of stability, help to justify the use of feature selection. It is also noteworthy that restricting focus to finite hypothesis classes is far from unreasonable—practitioners often attempt to learn on extracted features, which frequently involves discretization, and when learning on a finite domain any hypothesis space becomes effectively finite."
    }, {
      "heading" : "II. PRELIMINARIES",
      "text" : "We follow the setting of [5] closely. X denotes the input space, Y = {−1, 1} the output space and Z = X × Y their product example space. D is a distribution on Z . Unless stated otherwise we assume that m examples are drawn i.i.d. according to D. A classifier or hypothesis is a function h : X → {−1, 1};H denotes a set of classifiers or a hypothesis space. Whenever we refer to a finite H we assume, without loss of generality, that no h1, h2 ∈ H exist such that h1 6= h2 and h1(X) = h2(X) a.s.\nA learning algorithm on hypothesis space H is a function A : ⋃ m>0Zm → H. When A is understood from the context, we write fs = A(s) for s ∈ Zm. The loss incurred by classifier h on example z ∈ Z is denoted by `(h, z). We will assume the 0-1 loss throughout, where `(h, (x, y)) = 1 [h(x) 6= y]. Define the risk of h with respect to D as RD(h) = EZ∼D[`(h, Z)]; and with a slight abuse of notation the empirical risk as Rs(h) = m−1 ∑m i=1 `(h, zi) for s = (z1, . . . , zm) ∈ Zm. The risk minimizers of space H with respect to distribution D is the set H? = arg minh∈HRD(h). A learning algorithm A is said to implement Empirical Risk Minimization if A(s) ∈ arg minh∈HRs(h) for s ∈ Zm.\nBased on sequence s = (z1, . . . , zm) ∈ Zm, index i ∈ [m] and example u ∈ Z , we define sequences si = (z1, . . . , zi−1, zi+1, . . . , zm) ∈ Zm−1 and si,u = (z1, . . . , zi−1, u, zi+1, . . . , zm) ∈ Zm. The following definitions capture the relevant notions of stability.\nDefinition 2.1: A learning algorithm A is weakly (β, δ)hypothesis-stable or has weak hypothesis stability (β, δ) if for any i ∈ [m],\nPr(S,U)∼Dm+1 ( max z∈Z |`(fS , z)− `(fSi,U , z)| ≤ β ) ≥ 1− δ .\nThe notion of weak hypothesis stability was first used by Devroye and Wagner in [3] under the name of stability. Kearns and Ron [4] then used the term hypothesis stability for the same concept.\nar X\niv :1\n00 2.\n20 44\nv1 [\ncs .L\nG ]\n1 0\nFe b\n20 10\n2 Definition 2.2: A learning algorithm A is (β, δ)-crossvalidation-stable, or (β, δ)-CV-stable, if, for any i ∈ [m],\nPr(S,U)∼Dm+1(|`(fS , U)− `(fSi,U , U)| ≤ β) ≥ 1− δ .\nDefinition 2.3: A learning algorithm A is (β, δ)-overlapstable, or has overlap stability (β, δ), if, for any i ∈ [m],\nPr(S,U)∼Dm+1(|RSi(fS)− RSi(fSi,U )| ≤ β) ≥ 1− δ .\nCombining CV and overlap stability, Kutin and Niyogi arrive at the next definition.\nDefinition 2.4: A learning algorithm A is (β, δ)-trainingstable or, has training stability (β, δ), if A has\ni. CV stability (β, δ); and ii. Overlap stability (β, δ).\nTrivially weak hypothesis stability (β, δ) implies training stability (β, δ). Kutin and Niyogi show that training stability is sufficient for good bounds on generalization error (for ERM, CV stability is sufficient). They also consider the stability of ERM on a two-classifier hypothesis class [5, Theorems 9.2 and 9.5], showing that the achievable rate on δ undergoes a phase transition as the number of risk minimizers increases from one to two.\nTheorem 2.5: Consider the class H consisting of the two constant classifiers mapping X to −1 and 1 respectively, and let A implement ERM on H (i.e., A outputs a majority label of training set S). Let p = Pr(X,Y )∼D(Y = 1). 1. If p ≥ 12 then A is (0, δ)-training-stable for δ(m) ≈\n(2πm)−1/2. 2. If p > 12 then A is weakly (0, δ)-hypothesis-stable for δ = exp ( − ( 2− p−1 )2 m/8 + O(1) ) .\n3. If p = 12 then A is not (β, δ)-CV-stable, or (β, δ)-overlapstable for any β < 1 and any δ = o ( m−1/2 ) .\nThe case of finite hypothesis spaces is then considered. In particular the authors show that fast rates are achieved in the presence of a unique risk minimizer.\nTheorem 2.6: Let H be a finite collection of classifiers, and let A be a learning algorithm which performs ERM over H. Suppose there exists a unique risk minimizer h? ∈ H, then A is weakly (0, δ)-hypothesis-stable for δ = exp(−Ω(m)).\nThe authors then conjecture that the analogue of Theorem 2.5.(3) holds for general |H| < ∞: [5, Conjecture 10.11] predicts that under any distribution D inducing multiple risk minimizers, ERM is not (0, δ)-training-stable for any δ = o ( m−1/2 ) ."
    }, {
      "heading" : "III. FINITE CLASSES WITH MULTIPLE RISK MINIMIZERS",
      "text" : "Lemma 3.1: Let H be a finite hypothesis space with |H| ≥ 2 and with risk minimizers satisfying |H?| = 2. Then ERM on H with respect to the 0-1 loss, on a sample of m examples, is not (0, δ)-CV-stable for any δ = o ( m−1/2 ) . Furthermore,\nERM on H is (0, δ)-CV-stable for δ = O ( m−1/2 ) .\nProof: Let H? = {h1, h2}. We proceed by first showing that fS lies in H? with exponentially increasing probability and then that switching within H? occurs often. Let = minh∈H\\H? RD(h) − RD(h1), which exists and is positive by the finite cardinality of H. Then by the union bound,\nRD(h) ≥ RD(h1) + for all h ∈ H\\H?, and Chernoff’s bound\nPr(fS ∈ H?) ≥ Pr(∀h ∈ H\\H?,RS(h1) < RS(h)) ≥ Pr(∀h ∈ H\\H?,RS(h1) < RD(h1) + /2 ≤ RS(h)) ≥ 1− Pr(RS(h1) ≥ RD(h1) + /2) −\n∑ h∈H\\H? Pr(RD(h1) + /2 > RS(h))\n≥ 1− Pr(RS(h1) ≥ RD(h1) + /2) − ∑ h∈H\\H? Pr(RD(h)− /2 > RS(h))\n≥ 1− exp ( − 2m/2 ) − ∑ h∈H\\H? exp ( − 2m/2 ) = 1− (|H| − 1) exp ( − 2m/2 ) . (1)\nConsider ERM on H?. Without loss of generality assume that ERM on H?, when h1 and h2 have equal empirical risk, selects h1. Let Z1 = {z ∈ Z | `(h1, z) < `(h2, z)} and Z2 be defined analogously. Let p = Pr(Z ∈ Z1 ∪ Z2), which is positive by assumption that h′(X) = h′′(X) a.s. implies h′ = h′′. Then for all i ∈ [m]\nPrD2((Zi, U) ∈ (Z1 ×Z2) ∪ (Z2 ×Z1)) = p2/2 . (2)\nFor the moment, assume that for all i ∈ [m] Pr ( |RSi(h1)− RSi(h2)| ≤ (m− 1)−1 ) = O ( m−1/2 ) . (3)\nConditioned on the events of (2) and (3), the probability of ERM on S outputting a different hypothesis than on Si,U is at least 1/2. This fact can be proved by cases on |RSi(h1) − RSi(h2) |, while assuming that (Zi, U) ∈ (Z1 ×Z2) ∪ (Z2 ×Z1). If RSi(h1) = RSi(h2), then fS 6= fSi,U trivially. If |RSi(h1) − RSi(h2) | = (m − 1)−1 then {|RSi,Z (h1)− RSi,Z (h2) | : Z ∈ {Zi, U}} = { 0, 2m−1\n} and it follows that |RS(h1) − RS(h2) | 6= |RSi,U (h1) − RSi,U (h2) |; and since by symmetry the probability that RSi,Z (h2) < RSi,Z (h1) conditioned on the corresponding difference being 2m−1 is 1/2, it follows that fS 6= fSi,U with probability 1/2. Thus we have shown that on H?, for all i ∈ [m]\nPr(S,U)∼Dm+1(fS 6= fSi,U | A ∩B) ≥ 1/2 , (4)\nwhere\nA = {(S,U) : (Zi, U) ∈ (Z1 ×Z2) ∪ (Z2 ×Z2)} B = { (S,U) : |RSi(h1)− RSi(h2)| ≤ (m− 1)−1 } .\nNotice that since fS , fSi,U ⊆ H? and U ∈ Z1 ∪ Z2, fS 6= fSi,U implies that `(fS , U) 6= `(fSi,U , U). Then together (1)– (4) lead to the following statement about ERM over H:\nPr(S,U)∼Dm+1(|`(fS , U)− `(fSi,U , U)| > 0) ≥ Pr(|`(fS , U)− `(fSi,U , U)| > 0, fS , fSi,U ∈ H?) ≥ O ( p2m−1/2 ) ( 1− 2(|H| − 1) exp ( − 2m/2\n)) = O ( m−1/2 ) .\n3 And so ERM is not (0, δ)-CV-stable for any δ = o ( m−1/2\n) as claimed. Furthermore ERM is (0, δ)-CV-stable for δ = O ( m−1/2 ) since\nPr(S,U)∼Dm+1(|`(fS , U)− `(fSi,U , U)| > 0) ≤ Pr(|`(fS , U)− `(fSi,U , U)| > 0, fS , fSi,U ∈ H?)\n+ Pr(fS /∈ H? ∨ fSi,U /∈ H?) ≤ O ( m−1/2 ) + 2(|H| − 1) exp ( − 2m/2 ) = O ( m−1/2 ) .\nAll that remains is to verify (3). Consider Xn,q ∼ Bin (n, q) for n ∈ N, q ∈ [0, 1] and note that for k ∈ N ∪ {0}\nPr (∣∣∣∣Xk, 12 − k2 ∣∣∣∣ ≤ 12 ) = Pr ( Xk, 12 = k 2 ) , k even\n2Pr ( Xk, 12 = k−1 2 ) , k odd\n≥ Pr ( X2b k2 c+1, 12 = ⌊ k\n2\n⌋) .\nSplitting on ∑m\nj=1:j 6=i 1 [Zj ∈ Z1 ∪ Z2]—the Bin (m− 1, p) number of examples in Si on which h1, h2 disagree—and noting that when conditioned on this sum being equal to k, the random variable m−1 2 (RSi(h1)− RSi(h2)) + k 2 ∼ Bin (k, 1/2), leads to the lower-bound\nPr ( |RSi(h1)− RSi(h2)| ≤ (m− 1)−1 ) =\nm−1∑ k=0 Pr(Xm−1,p = k) Pr (∣∣∣∣Xk, 12 − k2 ∣∣∣∣ ≤ 12 )\n≥ min 0≤k≤m−1 Pr (∣∣∣∣Xk, 12 − k2 ∣∣∣∣ ≤ 12 )m−1∑ j=0 Pr(Xm−1,p = j)\n= min 0≤k≤m−1 Pr (∣∣∣∣Xk, 12 − k2 ∣∣∣∣ ≤ 12 ) ≥ Pr ( X2bm2 c+1, 12 = ⌊m 2\n⌋) = O ( m−1/2 ) ,\nwhere the last relation is a consequence of Stirling’s approximation. Let c = dp(m− 1)/2e. The upper-bound follows similarly:\nPr ( |RSi(h1)− RSi(h2)| ≤ (m− 1)−1 ) =\nc∑ k=0 Pr(Xm−1,p = k) Pr (∣∣∣∣Xk, 12 − k2 ∣∣∣∣ ≤ 12 )\n+ m−1∑ k=c+1 Pr(Xm−1,p = k) Pr (∣∣∣∣Xk, 12 − k2 ∣∣∣∣ ≤ 12 ) ≤ Pr(Xm−1,p ≤ c)\n+ sup k∈{c+1,...,m−1} Pr (∣∣∣∣Xk, 12 − k2 ∣∣∣∣ ≤ 12 ) ≤ O ( e−pm ) + O ( m−1/2\n) = O ( m−1/2 ) ,\nwhere the penultimate relation follows from an application of Chernoff’s inequality to the first term and Stirling’s approximation to the second.\nTheorem 3.2: Let H be a finite hypothesis space with |H| ≥ 2 and with risk minimizers |H?| > 1. Then any learning algorithm A implementing ERM on H with respect to the 0-1 loss, on a sample of m examples, is not (0, δ)-CV-stable for any δ = o ( m−1/2 ) . Furthermore, A is (0, δ)-CV-stable for\nδ = O ( m−1/2 ) .\nProof: Define as before. Arbitrarily order the risk minimizersH? = {h1, . . . , hn} where n = |H?|, and let pij = Pr(hi(X) 6= hj(X)) > 0 by assumption that h′(X) = h′′(X) a.s. implies h′ = h′′ for each h′, h′′ ∈ H. Let f ijT be the result of running A on sample T in the hypothesis space {hi, hj}, where ties between empirical risk minimizers are broken as in ERM over H?. Let f?T be the result of running A on T in H?. It follows that for all {j, k} ⊆ [n], conditional upon event Cjk = {f?S = hj , f?Si,U = hk}, f jk S = f ? S = hj and f jk Si,U = f?Si,U = hk, and so by (2)–(4)\nPr(S,U)∼Dm+1(|`(f?S , U)− `(f?Si,U , U)| > 0 | Cjk) = Pr (∣∣∣`(f jkS , U)− `(f jkSi,U , U)∣∣∣ > 0 ∣∣∣ Cjk)\n≥ O (\nmin {j,k}⊆[n]\np2jkm −1/2\n) . (5)\nIt follows that\nPr(S,U)∼Dm+1(|`(f?S , U)− `(f?Si,U , U)| > 0) ≥ min {j,k}⊆[n] Pr(|`(f?S , U)− `(f?Si,U , U)| > 0 | Cjk)\n≥ O (\nmin {j,k}⊆[n]\np2jkm −1/2\n) . (6)\nObserving that Pr(fS ∈ H?) ≥ 1−|H| exp(− 2m/2) by the same argument that lead to (1), we have as before\nPr(|`(fS , U)− `(fSi,U , U)| > 0) ≥ Pr(|`(fS , U)− `(fSi,U , U)| > 0, fS , fSi,U ∈ H?) ≥ Pr(|`(f?S , U)− `(f?Si,U , U)| > 0) × (1− Pr(fS /∈ H? ∨ fSi,U /∈ H?))\n= O ( min\n{j,k}⊆[n] p2jkm\n−1/2 )( 1− 2|H| exp ( − 2m/2 )) = O ( m−1/2 ) . (7)\nThis implies that ERM on H is not (0, δ)-CV-stable for any δ = o ( m−1/2 ) . Similarly we derive an upper-bound analogous\nto (7) by the same technique used in the proof of Lemma 3.1:\nPr(|`(fS , U)− `(fSi,U , U)| > 0) ≤ Pr(|`(fSi,U , U)− `(fS , U)| > 0, fS , fSi,U ∈ H?)\n+ Pr(fS /∈ H? ∨ fSi,U /∈ H?)\n= O ( max {j,k}⊆[n] p2jkm −1/2 ) + 2|H| exp ( − 2m/2 ) = O ( m−1/2 ) .\nThis implies that ERM on H is (0, δ)-CV-stable for δ = O ( m−1/2 ) .\n4"
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "The authors thank Peter Bartlett for his helpful feedback on this research."
    } ],
    "references" : [ {
      "title" : "Theory of Classification: a Survey of Recent Advances",
      "author" : [ "S. Boucheron", "O. Bousquet", "G. Lugosi" ],
      "venue" : "ESAIM: Probability and Statistics,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2005
    }, {
      "title" : "Stability and Generalization",
      "author" : [ "O. Bousquet", "A. Elisseeff" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2002
    }, {
      "title" : "Distribution-free performance bounds for potential function rules",
      "author" : [ "L. Devroye", "T. Wagner" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1979
    }, {
      "title" : "Algorithmic stability and sanity-check bounds for leave-one-out cross-validation",
      "author" : [ "M. Kearns", "D. Ron" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1999
    }, {
      "title" : "Almost-Everywhere Algorithmic Stability and Generalization Error",
      "author" : [ "S. Kutin", "P. Niyogi" ],
      "venue" : "Technical report TR-2002-03,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2002
    }, {
      "title" : "Statistical Learning: stability is sufficient and generalization is necessary and sufficient for consistency of Empirical Risk Minimization",
      "author" : [ "S. Mukherjee", "P. Niyogi", "T. Poggio", "R. Rifkin" ],
      "venue" : "AI Memo",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2002
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "DEVROYE and Wagner [3] first studied the effect of algorithmic stability on the statistical generalization of learning.",
      "startOffset" : 19,
      "endOffset" : 22
    }, {
      "referenceID" : 0,
      "context" : "Since then many authors have proposed numerous alternate notions of stability and have used them to study the performance of algorithms not easily analyzed with other techniques [1], [2], [4]–[6].",
      "startOffset" : 178,
      "endOffset" : 181
    }, {
      "referenceID" : 1,
      "context" : "Since then many authors have proposed numerous alternate notions of stability and have used them to study the performance of algorithms not easily analyzed with other techniques [1], [2], [4]–[6].",
      "startOffset" : 183,
      "endOffset" : 186
    }, {
      "referenceID" : 3,
      "context" : "Since then many authors have proposed numerous alternate notions of stability and have used them to study the performance of algorithms not easily analyzed with other techniques [1], [2], [4]–[6].",
      "startOffset" : 188,
      "endOffset" : 191
    }, {
      "referenceID" : 5,
      "context" : "Since then many authors have proposed numerous alternate notions of stability and have used them to study the performance of algorithms not easily analyzed with other techniques [1], [2], [4]–[6].",
      "startOffset" : 192,
      "endOffset" : 195
    }, {
      "referenceID" : 4,
      "context" : "While the learnability of a concept class can now be characterized by the admission of a stable learner, finding natural definitions of stability with such properties is still open [5], [6].",
      "startOffset" : 181,
      "endOffset" : 184
    }, {
      "referenceID" : 5,
      "context" : "While the learnability of a concept class can now be characterized by the admission of a stable learner, finding natural definitions of stability with such properties is still open [5], [6].",
      "startOffset" : 186,
      "endOffset" : 189
    }, {
      "referenceID" : 4,
      "context" : "As part of their general investigation into algorithmic stability in [5], Kutin and Niyogi observed that Empirical Risk Minimization (ERM) on hypothesis spaces of cardinality two experiences a phase transition in achievable rates of stability as the number of risk minimizers increases from one to two.",
      "startOffset" : 69,
      "endOffset" : 72
    }, {
      "referenceID" : 4,
      "context" : "PRELIMINARIES We follow the setting of [5] closely.",
      "startOffset" : 39,
      "endOffset" : 42
    }, {
      "referenceID" : 2,
      "context" : "The notion of weak hypothesis stability was first used by Devroye and Wagner in [3] under the name of stability.",
      "startOffset" : 80,
      "endOffset" : 83
    }, {
      "referenceID" : 3,
      "context" : "Kearns and Ron [4] then used the term hypothesis stability for the same concept.",
      "startOffset" : 15,
      "endOffset" : 18
    }, {
      "referenceID" : 0,
      "context" : "Consider Xn,q ∼ Bin (n, q) for n ∈ N, q ∈ [0, 1] and note that for k ∈ N ∪ {0}",
      "startOffset" : 42,
      "endOffset" : 48
    } ],
    "year" : 2010,
    "abstractText" : "Recently Kutin and Niyogi investigated several notions of algorithmic stability—a property of a learning map conceptually similar to continuity—showing that training-stability is sufficient for consistency of Empirical Risk Minimization while distribution-free CV-stability is necessary and sufficient for having finite VC-dimension. This paper concerns a phase transition in the training stability of ERM, conjectured by the same authors. Kutin and Niyogi proved that ERM on finite hypothesis spaces containing a unique risk minimizer has training stability that scales exponentially with sample size, and conjectured that the existence of multiple risk minimizers prevents even super-quadratic convergence. We prove this result for the strictly weaker notion of CV-stability, positively resolving the conjecture.",
    "creator" : "LaTeX with hyperref package"
  }
}