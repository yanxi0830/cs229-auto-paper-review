{
  "name" : "1702.05571.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Thresholding based Efficient Outlier Robust PCA",
    "authors" : [ "Yeshwanth Cherapanamjeri", "Prateek Jain Praneeth Netrapalli" ],
    "emails" : [ "t-yecher@microsoft.com", "prajain@microsoft.com", "praneeth@microsoft.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 2.\n05 57\n1v 1\n[ cs\n.L G\n] 1\n8 Fe\nb 20\nIn this work, we provide a novel thresholding based iterative algorithm with per-iteration complexity at most linear in the data size. Moreover, the fraction of outliers, α, that our method can handle is tight up to constants while providing nearly optimal computational complexity for a general noise setting. For the special case where the inliers are obtained from a low-dimensional subspace with additive Gaussian noise, we show that a modification of our thresholding based method leads to significant improvement in recovery error (of the subspace) even in the presence of a large fraction of outliers."
    }, {
      "heading" : "1 Introduction",
      "text" : "Principal Component Analysis (PCA) is a critical first step for any typical data exploration/analysis effort and is widely used in a variety of applications. A key reason for the success of PCA is that it can be performed efficiently using Singular Value Decomposition (SVD).\nHowever, due to various practical reasons like measurement error, presence of anomalies etc., a large fraction of data points can be corrupted in a somewhat correlated and even adversarial manner. Unfortunately, SVD is fragile with respect to outliers and can lead to arbitrarily inaccurate principal directions in the presence of even a small number of outliers. So, designing an outlier robust PCA (OR-PCA) algorithm is critical for several application domains.\nFormally, the setting of OR-PCA is as follows: given a data matrix M∗ = D∗ + C∗ ∈ Rd×n where D∗ = [x1, . . . , xn] corresponds to n clean “inlier” data points and C∗ has at most α-fraction of non-zero columns that can corrupt the corresponding clean data points arbitrarily, the goal of OR-PCA is to estimate the principal components of D∗ accurately, i.e., recover U∗ ∈ Rd×r, the top-r left singular vectors of D∗.\nVanilla SVD does not do the job since the top singular vectors of M∗ can be arbitrarily far from U∗ if the operator norm of C∗ (‖C∗‖2) is large, as can be the case when α = Ω(1). Any algorithm trying to solve OR-PCA needs to exploit the column sparsity of C∗ to obtain a better estimate of U∗. In particular, they need to find S∗ = Supp (C∗)—Supp (A) is the index of non-zero columns of A—so that U∗ can be estimated using top singular directions of M∗\\S∗ , i.e., columns of M ∗ restricted to complement set of S∗.\nExisting results for OR-PCA fall into two categories based on: a) Nuclear norm [Xu et al., 2012a, Zhang et al., 2016], b) iterative PCA [Xu et al., 2013]. Nuclear norm based approaches work with exactly same setting as ours, but require O(nd2) computational time which is prohibitive for typical applications. Iterative PCA based techniques require O(n2d) computation which in general is significantly higher than our algorithms. Moreover, these results do not recover the exact principal directions even if the inliers are restricted to a low-dimensional subspace and just a constant number of outliers are present.\nOur approach is based on solving the following natural optimization problem:\nOR-PCA : min D,C∈Rd×n\n‖M∗ −D − C‖2F s.t. rank(D) ≤ r, |Supp (C) | ≤ αn.\nTechnical Challenges: The main challenges with OR-PCA are its non-convexity and combinatorial structure which rules out standard tools from convex optimization. Furthermore, due to the column-sparsity constraint standard SVD based techniques also do not apply. Instead, we propose a simple iterative method that constructs an estimate of outliers C and using that, an estimate of the inliers M∗−C. SVD of the estimated inliers is then used to obtain an estimate of the principal directions. Now, a significant challenge is to use these principal directions to re-estimate outliers. There are two different scenarios here:\n• Length of all the outliers is smaller than the smallest singular value of the inliers and hence do not stand out : In this case however, the principal directions are not much affected by outliers. So, outliers can be recognized by taking a projection on to the orthogonal space to the estimated principal directions. As we get better estimate of principal directions, we can be more aggressive in determining the outliers.\n• Length of at least one of the outliers is larger than the small singular values of inliers : In this case again, length based thresholding fails since the lengths of the inliers are dominated by the larger singular values. Similarly, the above mentioned thresholding scheme also fails as some of the estimated principal directions will be heavily biased towards those outliers. A key and somewhat surprising algorithmic insight of our work is that: as some of the estimated singular vectors are heavily affected by outliers, projection of such outliers on these spurious singular vectors will be inordinately high. So, in contrast to the above thresholding operator, we can use the length of projection of points along estimated principal directions as well to detect and threshold outliers.\nIn both the scenarios, we can identify more outliers and repeat the procedure till convergence. A key assumption we make, in order to ensure that inliers are not thresholded, is that most of the inliers have “limited” influence on the principal directions, i.e., the data matrix is incoherent (see Assumption 1). Such an assumption holds for various typical settings, for example when inliers are noisy and uniform samples from a low-dimensional subspace. Contributions: The main contribution of our work is to show that, under some regularity conditions, it is indeed possible to solve OR-PCA near optimally, in essentially the same time as that taken by vanilla PCA. In particular, we propose a thresholding based approach that iteratively estimates inliers using two different thresholding operators and then use SVD to estimate the principal directions. We also show that our method recovers U∗ nearly optimally and efficiently as long as the fraction of corrupted data points (column-support of C∗) is less than O(1/r) where r is the dimensionality of principal subspace U∗. More concretely, we study the problem in three settings:\n1. Noiseless setting: M∗ = D∗+C∗ where the clean data matrix D∗ is a rank-r matrix with µ-incoherent right singular vectors (see Assumption 1) and C∗ has at most α ·n non-zero columns. For this setting, we design a novel Thresholding based Ouliter Robust PCA algorithm (TORP) that recovers U∗ up\nto an error of ǫ in time O ( ndr log\nn‖M∗‖2 ǫ ) , if α ≤ 1128µ2r . Note that this is essentially the time taken\nfor vanilla PCA as well. In contrast, existing results for the same setting require O(nd 2\nǫ2 ) 1 computation\nto recover U∗. Note that the number of outliers our results can handle (i.e., α ≤ 1/128µ2r) is optimal up to constant factors in the sense that if α > 1/µ2r, then there exists a matrix M∗ which has more than one decomposition satisfying the above conditions.\n2. Arbitrary noise: In the second setting, M∗ = D∗ +C∗ where the clean data matrix D∗ can be written as L∗ + N∗ and L∗, the rank-r projection of D∗ has µ incoherent right singular vectors. C∗ on the other hand, again has at most α · n non-zero columns. If α ≤ 1128µ2r , our proposed algorithm\n1Dependence on ǫ is due to the standard rates of gradient descent when applied to the non-smooth non-strongly convex optimization problem given in (1); however, using more refined RSC-style analysis, ǫ dependency might be improved but we are not aware of such an existing result.\nTORP-N guarantees recovery of U∗ (left singular vectors of L∗) up to O ( √ r ‖N∗‖F + ǫ) error, in\nO ( ndr log\nn‖M∗‖2√ r‖N∗‖F+ǫ\n) time. Again this is essentially the same as the time taken for vanilla PCA up\nto log factors. In contrast, existing results for this problem get stuck at a significantly larger error of ( √ n ‖N∗‖F ), with a runtime of O(nd 2 ǫ2 ), which is slower than ours by a factor of d.\n3. Gaussian noise: In this setting, we again have M∗ = D∗ + C∗ where the clean data matrix D∗ is a sum of low-rank matrix L∗ and a Gaussian noise matrix N∗, i.e., each element of N∗ is sampled independently and identically (iid) from N (0, σ2). TORP-G, which is our proposed algorithm for this special case, recovers U∗ up to an error of O (√ r log d ‖N∗‖2 ) . This not only improves upon the result\nwe obtained for the arbitrary noise case above, which is O ( √ r ‖N∗‖F ), it also improves significantly upon the existing results. However, in order to achieve this improvement in error, we require n > d2 and the algorithm has a runtime of O ( n2d ) .\nTo summarize, our results obtain stronger guarantees in terms of both final error and runtime, while being able to handle a large number of outliers, for three different settings of OR-PCA. Moreover, in the first two settings, our run time matches that of standard PCA up to log factors. Please refer Tables 1 and 2 for comparison of our results with existing results.\nPaper Outline: The paper is organized as follows. We will review related work in Section 1.1. We then present a formal definition of the problem in Section 2 and our main results in Section 3. We then present our algorithm for each of the three settings: a) noise-less setting, b) arbitrary data, c) Gaussian noise, in Section 4, 5, 6, respectively. We provide a brief overview of our proofs in Section 7. Finally, we conclude with a few open problems and promising future directions in Section 8."
    }, {
      "heading" : "1.1 Related Works",
      "text" : "In this section, we will discuss related work and compare existing results with ours. Existing theoretical results for OR-PCA fall into two categories:\na) The first category of approaches, more in-line with our own work, are based on Outlier-Pursuit (Xu et al. [2012a]) which optimizes a convex relaxation of the OR-PCA problem where the rank and column sparsity constraints are replaced by the trace-norm (sum of singular values) and ‖.‖2,1 (sum of the column lengths) penalties. That is, they solve the following optimization problem:\n(Outlier Pursuit): min ‖L‖∗ + λ ‖C‖2,1 s.t ‖M − L+ C‖F ≤ ‖N∗‖F . (1)\nWhile outlier pursuit obtains optimal recovery guarantees in absence of any noise, a main drawback is that its computational complexity is quadratic in d, i.e., O(nd2). Moreover, in presence of noise the bounds given in Xu et al. [2012a] are O( √ n) worse than our result. Extensions of Outlier-Pursuit to the partially observed setting (Chen et al. [2016]) and online setting Feng et al. [2013] have also been proposed but share the drawback of high computational complexity. Recently, Zhang et al. [2016] showed that Outlier Pursuit achieves recovery even with the fraction of outliers larger than the information theoretic lower bound. However, this requires the outliers to be “well-spread” which in practice is restrictive; our results allow the corruption matrix to be constructed in adversarial manner although the corruptions cannot depend on the Gaussian noise in the setting (3) described in previous section.\nb) The second line of approaches based on HR-PCA (Xu et al. [2013]) iteratively prune or reweigh data points which have a large influence on the singular vectors and select an estimate with the Robust Variance Estimator metric. When applied to our finite sample setting, these results cannot achieve exact recovery even in the noiseless case with a single outlier. Moreover, their running time in this setting is O ( n2dr ) while ours is nearly linear in the input size. In the special case of Gaussian noise, these results incur at least a constant error O (σ1(L\n∗)) whereas our recovery guarantee scales linearly with the standard deviation of the noise O ( √ rσ log d), achieving exact recovery when σ = 0. Feng et al. [2012] propose a deterministic variant of HR-PCA and Yang and Xu [2015] extend HR-PCA to PCA-like algorithms like Sparse PCA and Non-Negative PCA.\nThere has been much recent work on the related problem of Robust PCA (Candès et al. [2011], Netrapalli et al. [2014], Yi et al. [2016], Cherapanamjeri et al. [2016]). In contrast to the setting considered here, the corruptions are assumed to be both row and column sparse i.e., unlike our setting no data can be corrupted in all its dimensions. This restriction allows stronger recovery guarantees but makes the results inapplicable to the setting of outlier robust PCA."
    }, {
      "heading" : "1.2 Notations",
      "text" : "We use the following notations in this paper. For a vector v, ‖v‖ and ‖v‖2 denote the ℓ2 norm of v. For a matrix M , ‖M‖ and ‖M‖2 denote the operator norm of M while ‖M‖F denotes the Frobenius norm of M . σk(M) denotes the k\nth largest singular value of M . SVD refers to singular value decomposition of a matrix. SVDr(M) refers to the rank-r SVD of M . Given a matrix M , Mi denotes the ith column of M while Mi,: denotes the ith row of M . Given a matrix M ∈ Rd×n and a set S ⊆ [n], MS is defined as\n(MS)i = { Mi for i ∈ S, 0 otherwise.\nM\\S denotes M[n]\\S. Supp (M) denotes column support of M , i.e., the set of indices of non-zero columns of M .\nWe use two hard -thresholding operators in this paper. Given a matrix R, the first hard-thresholding operator, HT ρ (R) denotes the set of indices j of the top ρ fraction of largest columns (in ℓ2 norm) in R. Given a matrix R, the second hard-thresholding operator H̃T ζ(R) is defined as,\nH̃T ζ(N) = {i : s.t. ‖Ni‖2 ≥ ζ}. (2)\nPU⊥ (M) denotes (I − UU⊤)M . For any set S ∈ Rd, we will use PS to denote the projection onto the set S. We will also use PU to denote the projection onto the column space of U for U ∈ Rd×r."
    }, {
      "heading" : "2 Problem Formulation",
      "text" : "In this section, we will formally present the setting of the paper. We are given M∗ = D∗ + C∗ ∈ Rd×n, where columns of D∗ are inliers and C∗ are outliers. Only an α fraction of the points are outliers, i.e., only alpha fraction of the columns of C∗ are non-zero. Broadly, we consider three scenarios:\n• OR-PCA (Noiseless setting): The points inD∗ lie entirely in a low-dimensional subspace i.e., D∗ = L∗ is a rank-r matrix.\n• OR-PCAN (Noisy setting): The points in D∗ lie approximately in a low-dimensional subspace i.e., D∗ = L∗ +N∗ where L∗ is the rank-r projection of D∗ and N∗ is the noise matrix.\n• OR-PCAG (Gaussian noise setting): The points in D∗ come from a low-dimensional subspace with additive Gaussian noise i.e., D∗ = L∗ + N∗ where L∗ is a rank-r matrix and N∗ is a Gaussian noise matrix i.e., each element of N∗ is sampled iid from N (0, σ2).\nIn all the above settings, the goal is to find the low dimensional subspace spanned by the columns of L∗. This problem is in general ill-posed. Consider for instance the case, when most of the true data points L∗ are zero and only an α fraction of them are non-zero. These points can either be considered inliers or outliers. In order to overcome this issue, standard assumption used in literature [Xu et al., 2012a, Feng et al., 2013] is that of incoherence. Also, incoherence is satisfied in several standard settings; for example, when the inliers are noise and uniform samples from a low-dimensional subspace.\nAssumption 1. Rank and incoherence of L∗: L∗ ∈ Rd×n is a rank-r incoherent matrix, i.e.,∥∥e⊤i V ∗ ∥∥ 2 ≤ µ √ r n ∀i ∈ [n], where L∗ = U∗Σ∗(V ∗)⊤ is the SVD of L∗."
    }, {
      "heading" : "3 Our Results",
      "text" : "In this section, we will present our results for the three settings mentioned above."
    }, {
      "heading" : "3.1 OR-PCA – Noiseless Setting",
      "text" : "Recall that in the noiseless setting, we observe M∗ = D∗ + C∗ where D∗ is a rank-r, µ-incoherent matrix corresponding to clean data points and the column-support of C∗ is at most αn. The following theorem is our main result for this setting.\nTheorem 1 (Noise-less Setting). Let M∗, D∗ and C∗ be as described above. If α ≤ 1128µ2r , then Algorithm 1 run with parameters ρ = 1128µ2r and T = log 10n‖M∗‖2 ǫ , returns a subspace U such that,\n∥∥(I − UU⊤)D∗ ∥∥ F ≤ ǫ.\nRemarks:\n• Note that the guarantee of Theorem 1 can be right away converted to a bound on the subspace distance between U and that spanned by the columns of D∗. In particular, we obtain ∥∥(I − UU⊤)U∗ ∥∥ F\n≤ ǫ/σr(D\n∗), where σr(D∗) denotes the smallest singular value of D∗ and U∗ contains the singular vectors of D∗.\n• Since the most time consuming step in each iteration is computing the top-r SVD of an n× d matrix, the total runtime of the algorithm is O ( ndr log 10n‖M\n∗‖2 ǫ\n) .\n• The above assumption on the column sparsity of C∗ is tight up to constant factors i.e., we may construct an incoherent matrix L∗ and column sparse matrix C∗ such that it is not possible to recover the true column space of L∗ when the column sparsity of C∗ is larger than 1µ2r ."
    }, {
      "heading" : "3.2 OR-PCAN – Arbitrary Noise",
      "text" : "We now consider the noisy setting. Here we observe M∗ = D∗ + C∗, where D∗ is a near low rank matrix i.e., D∗ = L∗+N∗ where L∗ is the best rank r approximation to D∗ and is a µ incoherent matrix, while N∗ is a noise matrix. C∗ is again column sparse with at most an α fraction of the columns being non-zero.\nTheorem 2 (Arbitrary Noise). Consider the setting above. If α ≤ 1128µ2r , then Algorithm 2 when run with parameters ρ = 1128µ2r , η = 2µ √ r n and T = log 20‖M∗‖2·n ǫ iterations, returns a subspace U such that:\n∥∥(I − UU⊤)L∗ ∥∥ F ≤ 60√r ‖N∗‖F + ǫ.\nRemarks: • The theorem shows that up to √r ‖N∗‖F error, recovered directions U contains top r principle directions of inliers. We do not optimize the constants in our proof. In fact, we obtain a stronger result in Theorem 4 which for certain regime of noise N∗ can lead to significantly better error bound. Note that when there is no noise i.e., N∗ = 0, we recover Theorem 1.\n• The guarantee here can again be converted to a bound on subspace distance. For instance, for any k ≤ r, we have ∥∥∥(I − UU⊤)U∗[k] ∥∥∥ F\n≤ (60√r ‖N∗‖F + ǫ) /σk(L∗), where U∗k denotes the top-k left singular subspace of L∗ and σk(L∗) denotes the kth largest singular value of L∗.\n• The total runtime of the algorithm is O ( ndr2 log 10‖M\n∗‖2·n ǫ\n) . However, the outer loop over k in\nAlgorithm 2 can be replaced by a binary search for values of k between 1 and r. This reduces the runtime to O ( ndr log r log 10‖M\n∗‖2·n ǫ\n) . See Algorithm 4 for more details."
    }, {
      "heading" : "3.3 OR-PCAG– Gaussian Noise",
      "text" : "We now consider the Gaussian noise setting. Here we observe M∗ = D∗ + C∗, where D∗ is a near low rank matrix i.e., D∗ = L∗ + N∗ where L∗ is a rank-r, µ incoherent matrix, while N∗ is a Gaussian matrix with each entry sampled iid from N (0, σ2). C∗ is again column sparse with at most an α fraction of the columns being non-zero.\nTheorem 3 (Gaussian Data). Consider the setting mentioned above. Suppose α ≤ 11024µ2r . Then, Algorithm 3 stops after at most T = αn iterations and returns a subspace U such that:\n‖(I − UU⊤)L∗‖2 ≤ 4 √ log d‖N∗‖2.\nwith probability at least 1 − δ as long as n ≥ 16µ2r2dc1 [ log ( 1 3δ ) + d log(80d) ] for some absolute constants c1 and c2.\nRemarks:\n• Data points coming from a low-dimensional subspace with additive Gaussian noise is a standard statistical model that is used to justify PCA. Though this can be seen as a special case of arbitrary noise model, we get a much tighter bound than that obtained from Theorem 2. • While Theorem 2 gives an asymptotic error bound of ∥∥(I − UU⊤)L∗ ∥∥ F\n≤ 60√r ‖N∗‖F , Theorem 3 gives an asymptotic error bound of ∥∥(I − UU⊤)L∗ ∥∥ F ≤ 4 ‖N∗‖2. Note that the right hand sides above refer to Frobenius and operator norms respectively.\n• The improvement mentioned above is obtained by carefully leveraging the fact that Gaussian random vectors are spread uniformly in all directions and that there is a small fraction of vectors which is correlated. However, in order to make this argument, we need n = O ( d2 ) . It is an open problem to\nget rid of this assumption.\n• Note also that our result is tight in the sense that as σ → 0, we recover the result of Theorem 1. However, the running time of the algorithm is O(n2d) which is significantly worse than that of TORP. We leave design/analysis of a more efficient algorithm that achieves similar error bounds as Theorem 3 as an open problem.\n• We can obtain the above result even when each column inN∗ is drawn from a sub-Gaussian distribution rather than each entry being iid N (0, σ2)."
    }, {
      "heading" : "4 Outlier Robust PCA: Noiseless Setting",
      "text" : "In this section, we present our algorithm TORP(Algorithm 1) that applies to the special case of noise-less data, i.e., when M∗ = L∗+C∗, L∗ is rank-r, µ-incoherent matrix. While restrictive, this setting allows us to illustrate the main ideas behind our algorithm approach and the analysis techniques in a relatively simpler fashion.\nRecall that the goal is to estimate U∗, the left singular vectors of L∗. However, SVD of M∗ can lead to singular vectors arbitrary far from U∗, because a few column of C∗ can be so large that they can bias entire singular vectors in their direction.\nOur algorithm instead tries to exploit two key structural properties of the problem: sparsity of C∗ and incoherence of L∗. Our algorithm maintains a column-sparse estimate C(t) of C∗. Each iteration of the algorithm computes a low-rank approximation of an estimate of the inliers M∗ − C(t) = L∗ + C∗ − C(t). Note that if (I − U∗(U∗)⊤)C∗ = (I − U∗(U∗)⊤)C(t), then left singular vectors of M∗ − C(t) will be U∗.\nOur next step finds residual length of each column M∗i when projected on to the orthogonal subspace to U (t). If length of each outlier is smaller compared to the smallest singular value of L∗, then using sparsity of C(t) and C∗, we can show that U (t) is ”close” to U∗ in all directions. So, the residual of some of the outliers will stand out and those columns can be removed. This is achieved by the hard-thresholding step 5, 8 of Algorithm 1.\nA big challenge in this scheme is that if a column of the perturbation matrix C∗ − C(t) is ”very” long compared to smaller singular values of L∗, then they can perturb some directions of U∗ significantly. This will lead to a failure of the above thresholding approach. However, in such a case, some of the columns of C∗ − C(t) will be close to a few spurious singular vectors in U (t) (our current estimate of U∗). Hence, projection of such outliers along U (t) will be inordinately long. On the other hand, due to incoherence of L∗, inliers’ projection along U (t) can be bounded in magnitude. So, we can safely threshold out certain outliers. Steps 6, 8 of Algorithm 1 perform this thresholding operation.\nIn summary, our algorithm computes low-rank approximation ofM∗−C(t) and uses the obtained singular vectors U (t) to threshold out a few columns of C(t) to obtain next estimate C(t+1) of C∗. See Algorithm 1 for a pseudo-code of our approach.\nTime Complexity: Note that the computationally most expensive operation in each iteration is that of SVD which requires O(ndr) time. So, the overall time complexity of the algorithm is O(ndr · T ). As we show in Section 7, as long as C∗ is column-sparse, T ≈ log 1ǫ suffices to obtain an ǫ approximation to U∗. So, the overall complexity of the algorithm is O(ndr · log 20‖M\n∗‖2 ǫ ). Note that typically SVD computation\nis approximate, while all three of our algorithms and analyses assumes exact SVD. However, extension of our analysis to allow for small additive error is straightforward and we ignore it in favor of simplicity and readability.\nParameters: The algorithm requires an estimate of rank r and threshold parameter ρ which in turn depends on estimate of incoherence µ of L∗. We propose to set these parameters via cross-validation. Note that setting rank to be any value larger than rank of L∗ will lead to recovery of U∗, as long as C∗ is sparse enough. Similarly, if estimation of µ is larger than incoherence of L∗, then it only effects number of corrupted columns in C∗ that can be allowed. So, a simple cross-validation approach with appropriately chosen grid-size leads to recovery of U∗ as long as C∗ is sparse enough (as specified in Theorem 1).\nAlgorithm 1 Thresholding based Outlier Robust PCA (TORP)\n1: Input: Data M∗ ∈ Rd×n, Target rank r, Threshold fraction ρ, Number of iterations T 2: C(0) ← 0 3: for Iteration t = 0 to t = T do 4: [U (t),Σ(t), V (t)] ← SVDr ( M∗ − C(t) ) ; L(t) ← U (t)Σ(t)(V (t))⊤ } Projection onto space of low rank matrices5: R ← (I − U (t)(U (t))⊤)M∗ /* Compute residual */ 6: E ← (Σ(t))−1(U (t))⊤M∗/* Compute incoherence */ 7: CS(t+1) ← HT ρ (R) ∪HT ρ (E)\n   Projection onto space of\ncolumn sparse matrices8: C (t+1) ← M∗S /* Threshold points with high coherence or high residual*/\n9: end for 10: [U,Σ, V ] ← SVDr ( M∗ − C(T+1) ) 11: Return: U"
    }, {
      "heading" : "5 Outlier Robust PCA: General Noise",
      "text" : "In this section, we introduce our algorithm for the general case of Outlier Robust PCA with arbitrary inlier data D∗ = L∗ + N∗, i.e., the noise matrix N∗ is arbitrary. Recall that the goal is to recover left singular vectors of L∗.\nOur algorithm for the general OR-PCA problem builds upon the TORP algorithm but with added complexity due to the presence of noise matrix N∗. That is the algorithm alternately updates estimate of the outliers C(t) and the principal direction U (t) using two thresholding operators along with SVD. However due to noise N∗, our estimate of U (t) gets perturbed furthermore leading to arbitrary perturbation of the singular vectors of U∗ corresponding to smaller eigenvalues of L∗ and hence cannot be recovered. To alleviate this concern, our TORP-N algorithm proceeds via a pair of nested loops:\nOuter Iteration on k: The outer loop iterates over the rank-variable k which represents the rank of the principal subspace we wish to estimate.\nInner Iteration on t: The inner loop iteratively revises estimates of the principal subspace and a set of outliers until a stopping criteria is triggered.\nIntuitively, as in Algorithm 1, each inner iteration of Algorithm 2 obtains a better estimate of C∗ and top-k singular components of L∗. That is, the k-th outer iteration after several of such inner iterations estimates U∗ up to ≈ σk(L∗). But when the noise ‖N∗‖F becomes comparable to the kth singular value of L∗, then the algorithm terminates (Line 14, Algorithm 2) as at that point it may not be possible to estimate the remaining singular vectors of L∗. As we don’t know ‖N∗‖F explicitly, we detect this event based on the number of data points which have a large influence on the estimated singular vectors (see lines 11, 12 of Algorithm 2).\nRoughly, our stopping criterion allows us to make two statements regarding the termination of the algorithm:\n1. When the algorithm terminates, the outlier columns that we have not thresholded will only have small influence on the estimated principal vectors. This is because all points with large influence will be thresholded before the estimate is computed.\n2. The algorithm will not terminate if σ∗k >> ‖N∗‖F : The bound on ‖N∗‖F ensures that not many inlier points can have large influence on estimate of the kth singular vector.\nBy using the above two claims, our analysis shows that TORP-N recovers U∗ up to ∼ ‖N∗‖F error. Time Complexity: Time complexity of each inner iteration of TORP-N is O(ndk). Hence, overall time complexity is O(ndr2), as k can be as large as r. However, using a slightly more complicated algorithm and\nanalysis (see Algorithm 4), we can search for appropriate k using binary search, so the time complexity of the algorithm can be improved to O(ndr log r).\nParameter Estimation: The algorithm requires 3 parameters: rank r, threshold ρ which depends on incoherence µ of L∗ and expressivity parameter η. We can search for these parameters using a coarse-grid search as estimates of these parameters up to constants are enough for our algorithm to succeed albeit with a slightly stricter restriction (by constant factors) on the number of corrupted data points.\nAlgorithm 2 Thresholding based Noisy Outlier Robust PCA (TORP-N)\n1: Input: Corrupted matrix M∗ ∈ Rd×n, Target rank r, Expressivity parameter η, Threshold fraction ρ, Inner iterations T 2: for k = 1 to k = r do 3: C(0) ← 0, τ ← false 4: for t = 0 to t = T do 5: [U (t),Σ(t), V (t)] ← SVDk ( M∗ − C(t) ) , L(t) ← U (t)Σ(t)(V (t))⊤ } Projection onto space of\nlow rank matrices\n6: E ← (Σ(t))−1(U (t))⊤M∗ /* Compute Incoherence */ 7: R ← (I − U (t)(U (t))⊤)M∗ /* Compute residual */\n   Projection onto space of column sparse matrices\n8: CS(t+1) ← HT 2ρ (M∗, E) ∪HT ρ (M∗, R) 9: C(t+1) ← M∗CS(t+1)\n10: nthres ← |{i : ‖Ei‖ ≥ η}| /* Compute high incoherence points */ 11: τ ← τ ∨ (nthres ≥ 2ρn) /* Check termination conditions */ 12: end for 13: if τ then 14: break 15: end if 16: [U,Σ, V ] ← SVDk ( M∗ − C(T+1) ) 17: end for 18: Return: U"
    }, {
      "heading" : "6 Outlier Robust PCA: Gaussian Noise",
      "text" : "In this section, we present our algorithm for the special case of the Outlier Robust PCA problem when inlier points are generated using a standard Gaussian noise model. That is, when D∗ = L∗ + N∗ ∈ Rd×n where each entry of the noise matrix N∗ is sampled i.i.d. from N (0, σ2). Our result for arbitrary N∗ (Theorem 2) estimates U∗ up to ∼ ‖N∗‖F error, which is Ω(σ √ dn) for Gaussian noise. However, using a slight variant\nof Algorithm 2 and exploiting the noise structure, Algorithm 3 is able to estimate U∗ up to σ √ n log d error, which is better than the previous one by a factor of O (d/ log d). At a high level the philosophy of our TORP-G algorithm is similar to TORP, i.e., we iteratively revise estimate of C∗ and the top singular vectors U∗ using SVD and thresholding. That is, we iteratively threshold columns of M∗, that we estimate are corrupted. However, due to Gaussian noise structure our thresholding step is significantly different than that of TORP or TORP-N.\nIn particular, the choice of our thresholding criteria (see lines 10, 12 of Algorithm 3) uses the following two insights:\n1. Length based thresholding (with respect to ζ1—line 10 of Algorithm 3): This thresholding step is\nused to ensure that the noise in each data point is at most O ( σ √ d ) . As the length of random Gaussian vector is at most O ( σ √ d ) with high probability, only a small number of inliers are\nthresholded in this step (Lemma 9).\nAlgorithm 3 Thresholding based Outlier Robust PCA with Gaussian Noise (TORP-G)\n1: Input: Corrupted matrix M∗ ∈ Rd×n, Target rank r, Incoherence Parameter µ, Noise Level σ 2: M ← M∗, τ ← true 3: ζ1 ← σ ( 5 4µ √ r + d 1 2 + 2d 1 4 √ log ( µ2r c2 )) , ζ2 ← σ √ 2r ( 5 4µ+ 2 √ log ( µ2r2d c1 )) 4: C(0) ← 0, CS(0) ← {}, CS(−1) ← {0}, t ← 0 5: while CS(t) 6= CS(t−1) do 6: [U (t),Σ(t), V (t)] ← SVDr+1(M∗ − C(t)), L(t) ← U (t)Σ(t)(V (t))⊤ } Projection onto space of low rank matrices 7: E(t) ← {x : x = U (t)Σ(t)y for some ‖y‖ ≤ 2µ √ r/n} 8: L̂(t) ← PE(t)(L(t)) /* Projection onto incoherent matrices */ 9: I ← { i : ∥∥∥L(t)i − L̂ (t) i ∥∥∥ > ζ2 } /* Points with large influence */\n10: CS(t+1) ← CS(t) ∪ H̃T ζ1 ( L(t) − L̂(t) )\n/* Updating support of outliers */ 11: if |I| ≥ 24nc1µ2dr then 12: CS(t+1) ← CS(t+1) ∪ H̃T ζ2 ( L(t) − L̂(t) )\n   Projection onto space of column sparse matrices /* Update support of outliers */ 13: end if 14: C(t+1) ← M∗CS(t+1) /* Compute Sparse Projection */ 15: t ← t+ 1 16: end while 17: Return: U\n2. Projection based thresholding (with respect to ζ2—line 12 of Algorithm 3): In this step, we threshold points that have large projection along the estimated principal subspace. Note that out of n columns\nof N∗, at most O (\n1 µ2rd\n) fraction of points have projected lengths greater than O ( σ √ log(d) ) along\nany direction (Lemma 12). Thus, chances of a inliers being thresholded in this step is low. On the other hand, any outlier that heavily influences a principal direction will be thresholded by this step.\nAlgorithm 3 provides a detailed pseudo-code of TORP-G. Step 6 of the algorithm computes rank-(r + 1) SVD of the estimate of inlier matrix M∗−C∗. Step 7 defines a set of vectors, whose projection onto singular vectors of L(t) is “typical” for an inlier which is composed of a low-dimensional point perturbed by Gaussian noise vector of length O(σ √ d).\nThis set is used in step 10 to threshold outliers using the hard-thresholding operator H̃T ζ as defined in (2). Next, the set I consists of points which have a large influence on the estimated principal components. In the absence of outliers, the size of this set is bounded by 12nc1µ2dr with high probability. A large deviation in the size of this set indicates the presence of of outliers and the entire set is thresholded.\nNote on Approximate Computation: We would like to note that the projection operator defined in step 8 of the algorithm can be computed efficiently to arbitrary accuracy. A pseudo-code for computing the required projection can be found in Algorithm 5. Algorithm 5 reduces the problem to the univariate problem of finding the root of a monotonically decreasing function in a bounded interval which can be found efficiently via binary search. For the sake of simplicity, we assume that the projection step and the SVD are computed exactly. Our analysis can be extended to the case where the projection and SVD are computed approximately with some added technical difficulty."
    }, {
      "heading" : "7 Proof Overview",
      "text" : "In this section, we provide a brief overview of our analysis for the three main results."
    }, {
      "heading" : "7.1 Noiseless Setting—Theorem 1",
      "text" : "In this section, we present the proof of Theorem 1. Recall that we are given M∗ = D∗ +C∗, where D∗ = L∗ is a rank-r, µ-incoherent matrix and C∗ has at most a fraction of ρ non-zero columns. We can assume with out loss of generality that D∗ and C∗ have disjoint column supports as we can rewrite M∗i , for i ∈ Supp (C∗), as M∗i = D ∗ i + C ∗ i = 0 + (C ∗ i +D ∗ i ) thus absorbing D ∗ i in C ∗ i itself.\nOur proof consists of three main steps. Given any set of columns S and letting [U\\S,Σ\\S , V\\S ] be the top-r SVD of M∗\\S , we establish the following:\nStep 1: Every non-zero column of D∗ has significantly smaller residual when projected onto subspace orthogonal to U\\S than the norm of corrupted columns of M ∗ \\S (Lemma 1), so its likelihood of being\nthresholded (Line 6, 8 of Algorithm 1) is small,\nStep 2: Every non-zero column of D∗ has small incoherence with respect to [U\\S ,Σ\\S , V\\S ] (Lemma 2), i.e., its projection onto U\\S cannot be “too large”. Hence, its likelihood of being thresholded (Line 7,8 of Algorithm 1) is also small,\nStep 3: Any non-zero column of C∗ which has small residual and incoherence compared to those of a non-zero column of D∗ and hence won’t be thresholded by Algorithm 1, has small residual when projected onto U∗ . That is, the column itself is close to subspace spanned by U∗ and hence does not effect estimation of U∗ (Proof of Theorem 1).\nThat is, either a corrupted column will be thresholded or it is close to U∗ while inliers (D∗) have little likelihood of being thresholded (step 1,2 above). We now present the formal statements and their proofs. We start with two lemmata establishing Steps 1,2 above. Detailed proofs of the lemmata are given in Appendix B.1 and B.2, respectively.\nLemma 1. Consider the setting of Theorem 1. Let S ⊂ [n] denote a subset of columns of M∗ such that |S| ≤ 2ρn. Let M∗\\S (L∗\\S) be obtained from M∗ (L∗) by setting the columns corresponding to indices specified in S to 0. Let U\\SΣ\\S(V\\S) ⊤ (U∗\\SΣ ∗ \\S(V ∗ \\S) ⊤) be the rank-r SVD of M∗\\S (L∗\\S), then ∀i:\n∥∥(I − U\\S(U\\S)⊤)L∗i ∥∥ ≤ 33\n32 µ\n√ r\nn\n∥∥∥(I − U∗(U∗)⊤)M∗\\S ∥∥∥\nLemma 2. Under the setting of Lemma 1, we have for every i:\n∥∥∥Σ−1\\SU ⊤ \\SL ∗ i ∥∥∥ ≤ 33 32 µ\n√ r\nn .\nWe now present the proof of Theorem 1 where we illustrate Step 3:\nProof. We will start by showing the quantity ∥∥(I − U∗(U∗)⊤)M (t+1) ∥∥ F decreases at a geometric rate, where M (t+1) = M∗ − C(t+1). Let Q(t) denote the columns of C∗ that are not thresholded in iteration t. Also let S(t) denote the columns of L∗ that are thresholded in iteration t. Let L̃(t+1) := L∗\\S(t) , C̃ (t+1) := C∗ Q(t) , and PU⊥ (M) = (I − U(U)⊤)M . Then, we have: ∥∥∥PU ∗ ⊥ (M (t+1)) ∥∥∥ 2\nF =\n∥∥∥PU ∗ ⊥ (L̃ (t+1) + C̃(t+1)) ∥∥∥ 2\nF =\n∥∥∥PU ∗ ⊥ (C̃ (t+1)) ∥∥∥ 2\nF\n= ∑\nj∈Q(t)\n∥∥∥PU ∗ ⊥ (U (t)Σ(t)W (t) j +R (t) j ) ∥∥∥ 2 ≤ 2 ∑\nj∈Q(t)\n∥∥∥PU ∗ ⊥ (U (t))Σ(t)W (t) j ∥∥∥ 2 + ∥∥∥R(t)j ∥∥∥ 2 , (3)\nwhere W (t) j = (Σ (t))−1(U (t))TC∗j and R (t) j = P U(t) ⊥ (C ∗ j ), ∀j ∈ Q(t). The last inequality follows from triangle inequality and the fact that (a+ b)2 ≤ 2(a2 + b2). Recall, that we threshold a particular column l in iteration t based on ∥∥∥PU(t)⊥ (M∗l ) ∥∥∥ and ∥∥(Σ(t))−1(U (t))⊤M∗l ∥∥. For a particular j ∈ S(t) that wasn’t thresholded in iteration t, we know that there exists a column ij such that ∥∥∥(Σ(t))−1(U (t))⊤L∗ij ∥∥∥ ≥\n∥∥(Σ(t))−1(U (t))⊤C∗j ∥∥. Similarly, there exists a column kj such that∥∥∥PU(t)⊥ (L∗kj ) ∥∥∥ ≥ ∥∥∥PU(t)⊥ (C∗j ) ∥∥∥. From Lemmas 2 and 1, we have:\n∥∥∥W (t)j ∥∥∥ ≤ 33\n32 µ\n√ r\nn\n∥∥∥R(t)j ∥∥∥ ≤ 33\n32 µ\n√ r\nn\n∥∥∥PU ∗\n⊥ (M (t))\n∥∥∥ (4)\nUsing (3) and (4), we have:\n∥∥∥PU ∗ ⊥ (M (t+1)) ∥∥∥ 2\nF ≤ 2\n∑\nj∈S(t)\n( 33\n32\n)2 µ2 r\nn\n∥∥∥PU ∗ ⊥ (U (t))Σ(t) ∥∥∥ 2 + ( 33\n32\n)2 µ2 r\nn\n∥∥∥PU ∗ ⊥ (M (t)) ∥∥∥ 2\n≤ 4 · 9 8 · µ\n2r n · ρn · ‖PU∗⊥ (M (t))‖2 ≤ 1 4\n∥∥∥PU ∗ ⊥ (M (t)) ∥∥∥ 2 ,\nwhere second last inequality follows from |S(t)| ≤ ρn and the last inequality follows from ρ ≤ α ≤ 1128µ2r . By recursively applying the above inequality, we obtain:\n∥∥∥PU ∗ ⊥ (M (T+1)) ∥∥∥ F ≤ ǫ 20n . (5)\nAlso, note that using variational characterization of SVD, we have ‖PU⊥ (M (T+1))‖F ≤ ∥∥PU∗⊥ (M (T+1)) ∥∥ F . Theorem now follows from the following argument:\n∥∥PU⊥ (L∗) ∥∥2 F = ∥∥∥PU⊥ (L̃(T+1)) ∥∥∥ 2\nF +\n∑\ni∈S(T )\n∥∥PU⊥ (L∗i ) ∥∥2 ≤ ∥∥∥PU⊥ (M (T+1)) ∥∥∥ 2\nF +\n∑\ni∈S(T )\n332 322 µ2 r n\n∥∥∥PU⊥ (L̃(T+1)) ∥∥∥ 2\n≤ ∥∥∥PU⊥ (M (T+1)) ∥∥∥ 2\nF + 2ρn\n( 33\n32\n)2 µ2 r\nn\n∥∥∥PU⊥ (M (T+1)) ∥∥∥ 2\nF ≤ ǫ 10n ,\nwhere the first inequality follows from Lemma 6 and using M (T+1) = L̃(T+1) + C̃(T+1), and the fact that L̃(T+1) and C̃(T+1) have different support. The second inequality follows from the fact that at most 2ρ · n points can be thresholded and then using (5)."
    }, {
      "heading" : "7.2 Arbitrary Noise—Theorem 2",
      "text" : "We now briefly discuss the proof of Theorem 2. In fact, we prove a stronger result:\nTheorem 4. Let M∗ = L∗ + C∗ + N∗ such that L∗ satisfies Assumption 1 and C∗ has column sparsity α ≤ 1128µ2r . Furthermore, suppose that ‖N∗‖F ≤ σ∗k 16 for some k ∈ [r]. Then, Algorithm 1 run with ρ = 1128µ2r and η set to 2µ √\nr n with T = log 20‖M∗‖2·n ǫ , returns a subspace U such that:\n∥∥(I − UU⊤)L∗ ∥∥ F ≤ 3 ∥∥(I − U∗k (U∗k )⊤)L∗ ∥∥ F + 9 ‖N∗‖F + ǫ\n10n .\nIntuitively, the proof of Theorem 4 proceeds along the same lines as that of Theorem 1 but requires significantly more careful analysis due to presence of noise and due to the outer loop. For example, due to the presence of noise, we cannot guarantee that Lemma 2, that was critical to proof of Theorem 1, holds for all columns i. We show instead that the number of data points which have a large influence on the top-k\nsingular vectors is bounded (see Lemma 10). This ensures that the algorithm at least reaches the kth stage of the outer iteration before terminating. Similarly, we generalize Lemma 1 to handle N∗ (see Lemma 11). Finally, we present the key lemma that shows that if the algorithm does not terminate in the kth outer iteration, then it would have obtained a good approximation to the top-k principal subspace of L∗.\nLemma 3. Asume the conditions of Theorem 2. Furthermore, assume that Algorithm 2 has not terminated during the kth outer iteration. Then, the iterate U at the end of the kth outer iteration satisfies:\n∥∥(I − UU⊤)L∗ ∥∥ F ≤ 3 ∥∥(I − U∗1:k(U∗1:k)⊤)L∗ ∥∥ F + 9 ‖N∗‖F + ǫ\n10n ,\nwhen Algorithm 2 has been run with parameters ρ = 1128µ2r and η = 2µ √ r n .\nSee Appendix B.5 for a detailed proof. We can now prove Theorem 4 as follows:\nProof. Note that by Lemma 10, the algorithm does not terminate before the completion of kth outer iteration. Now, suppose that the algorithm terminates at some iteration k′ > k. Then, by Lemma 3, we have:\n∥∥PU⊥ (L∗) ∥∥ ≤ 3 ∥∥∥PU ∗ 1:k′−1 ⊥ (L ∗) ∥∥∥ F + 9 ‖N∗‖F + ǫ 10n ≤ 3 ∥∥∥PU ∗ 1:k ⊥ (L ∗) ∥∥∥ F + 9 ‖N∗‖F + ǫ 10n .\nThis concludes the proof of the Theorem."
    }, {
      "heading" : "7.3 Gaussian Noise—Theorem 3",
      "text" : "Our analysis of TORP-G show that the algorithm maintains the following critical invariant with high probability:\nInvariant 1. We assume that the following hold with respect to the two thresholding steps used in Algorithm 3.\n1. With respect to ζ1: If a column i 6∈ Supp (C∗) is thresholded, then the following condition holds:\n‖N∗i ‖ ≥ σ ( √ d+ 2d 1 4 √ log ( µ2r\nc2\n)) .\nand consequently only 3nc22µ2r points are removed in this step.\n2. With respect to ζ2: If a thresholding step occurs due to the second thresholding step with ζ2, then at least half the points thresholded in this step are corrupted points.\nLemma 4. Assume the conditions of Theorem 3. Then, Invariant 1 holds at any point in the running of Algorithm 3 with probability at least 1− δ.\nSee Appendix B.8 for a detailed proof. Our proof then uses the above invariant along with a careful analysis of each of the two thresholding\nsteps (Line 10, 12) to obtain the desired result. See Appendix C for a detailed proof."
    }, {
      "heading" : "8 Conclusions and Future Works",
      "text" : "In this paper, we studied the outlier robust PCA problem. We proposed a novel thresholding based approach that, under standard regularity conditions, can accurately recover the top principal directions of the clean data points, as long as the number of outliers is less than O(1/r) which is information theoretically tight up to constant factors. For noiseless or arbitrary noise case, our algorithms are based on two thresholding operators to detect outliers and leads to better recovery compared to existing methods in essentially the\nsame time as that taken by vanilla PCA. For Gaussian noise, we obtain improved recovery guarantees but at a cost of higher run time.\nThough our bounds have significant improvement over existing ones, they are still weaker than guarantees obtained by vanilla PCA (with out outliers). For instance, for arbitrary noise, our errors are bounded in the Frobenius norm. In contrast, in absence of outliers, SVD can estimate the principal directions in operator norm. A challenging and important open problem is if the principal directions can be estimated in operator norm even in the presence of outliers.\nSimilarly, for Gaussian noise, where each entry has variance σ2, our result obtains an error bound of O(σ √ n) which is significantly better than the Frobenius norm bound we get for arbitrary noise. But again in absence of outliers, SVD can estimate the principal directions exactly asymptotically. So, another open problem is if it is possible to do asymptotically consistent estimation of the principal directions with Gaussian noise in the presence of outliers. Moreover, our algorithm for the Gaussian setting is nearly a factor of n slower than that for vanilla PCA. In order for this to be practical, it is very important to design an algorithm for this setting with nearly the same runtime as that of vanilla PCA."
    }, {
      "heading" : "A Supplementary Results and Preliminaries",
      "text" : "Here, we will state and prove a few results useful in proving our main theorems. We will start by restating Weyl’s perturbation inequality from Bhatia [1997].\nTheorem 5. Let A ∈ Rd×n. Furthermore, let B = A+ E for some matrix E. Then, we have that:\n|σ(A)i − σ(B)i| ≤ ‖E‖ ∀i ∈ min(d, n)\nIn the next lemma, we show that the singular values of the sum of two matrices with disjoint column supports are greater than either of the two matrices individually.\nLemma 5. Let A ∈ Rd×n and B ∈ Rd×n be two matrices with disjoint column support. Then, we have ∀i ∈ min(d, n):\nmax(σi(A), σi(B)) ≤ σi(A+B)\nProof. Let the SVD of A and B be UAΣAV ⊤ A and UBΣBV ⊤ B respectively. The lemma holds for i = 0 as∥∥v⊤(A+B) ∥∥ ≥ max( ∥∥v⊤A ∥∥ , ∥∥v⊤B\n∥∥). For any matrix M , σi(M) = min U∈Rd×(i−1) ∥∥(I − UU⊤)M ∥∥ ∀i > 1. For\nany U , there exist v1 and v2 in Span((UA)[i]) and Span((UB)[i]) respectively and v ⊤ 1 U = v ⊤ 2 U = 0. This is because the rank of Span(U) is at most (i − 1) and Span((UA)[i]) and Span((UB)[i]) are both rank-i subspaces. Now, we have ∥∥v⊤1 (A+B) ∥∥ ≥ ∥∥v⊤1 A ∥∥ ≥ σiA and ∥∥v⊤1 (A+B) ∥∥ ≥ ∥∥v⊤1 B ∥∥ ≥ σi(B). The lemma by using either v1 or v2 for any U .\nThe next lemma shows that an incoherent matrix remains incoherent even if a small number of columns have been set to 0.\nLemma 6. Let L ∈ Rd×n be a µ-column-incoherent, rank-r matrix. Let S ⊂ [n] such that |S| ≤ 132µ2r . Let [U, S, V ] and [U\\S ,Σ\\S , V\\S ] denote the SVDs of L and L\\S respectively. Then, the following hold ∀i ∈ [n]:\nClaim 1: ∥∥e⊤i V\\S ∥∥ ≤ 33 32 µ\n√ r\nn Claim 2:\n31 32 σi(L) ≤ σi(L\\S) ≤ σi(L)\nFurthermore, each column Li∀i ∈ [n] can be expressed as:\nClaim 3: Li = U\\SΣ\\Swi with ‖wi‖ ≤ 33\n32 µ\n√ r\nn\nProof. Let T be defined as the matrix V with the rows in set S set to 0. We will first begin by proving that T is full rank. Let u ∈ Rr and ‖u‖ = 1:\n1 = ‖V u‖ ≥ ‖Tu‖ =\n  n∑\ni=1\n〈u, Vi,:〉2 − ∑\nj∈S 〈u, Vj,:〉2\n  1 2\n≥  1− ∑\nj∈S ‖Vj,:‖2\n  1 2 ≥ ( 1− 1\n32\n) 1 2\nwhere the second inequality is obtained from the bound on |S| and ‖Vj,:‖. Since T and V\\S have the same column space, there exists a matrix R ∈ Rr×r such that TR = V\\S . We know that R is full rank. We will now prove bounds on the singular values of R. For any u ∈ Rr and ‖u‖ = 1\n‖Ru‖ = ‖V Ru‖ ≥ ‖TRu‖ = ∥∥V\\Su ∥∥ = 1 = ∥∥V\\Su ∥∥ = ‖TRu‖ ≥ ( 1− 1\n32\n) 1 2\n‖Ru‖\nFrom this, we obtain the following inequality:\n1 ≤ ‖Ru‖ ≤ ( 1− 1\n32\n)− 12\nFrom this, we have the first claim of the lemma as TR = V\\S . We also know that U\\SΣ\\SV ⊤ \\S = UΣT ⊤. Writing V\\S as TR, we have U\\SΣ\\SR ⊤T⊤ = UΣT⊤. Using the fact that T⊤ is full rank, we have U\\SΣ\\SR ⊤ = UΣ. From this we have that L = UΣV ⊤ = U\\SΣ\\SR\n⊤V ⊤. Choosing wi = (R⊤V ⊤)i, the second claim of the lemma follows.\nFor the final claim of the lemma, note that the singular values of L\\S are the same as the singular values of UΣ(R⊤)−1. We know that σk+1(L\\S) = min\nQ∈Rd×k\n∥∥(I −QQ⊤)UΣ(R⊤)−1 ∥∥. The upper bound follows\nfrom setting Q to be the first k singular vectors of L and our bound on the singular values of R. For the lower bound, consider any Q ∈ Rd×k. Span(Q) is a subspace of rank at most k. Therefore, there exists v ∈ Span(U1) such that ‖v‖ = 1 and v⊤Q = 0. We now have\n∥∥v⊤(I −QQ⊤)UΣ(R⊤)−1 ∥∥ = ∥∥v⊤UΣ(R⊤)−1 ∥∥ ≥ σk+1(L)‖R‖ ≥ 31 32 σk+1(M)\nWhere the last inequality follows from our bounds on the singular values of R and noting that the singular values of R−1 are the inverses of the singular values of R. This proves the third claim of the lemma.\nWe begin by stating a lemma used for bounding the length of Gaussian random vectors from Laurent and Massart [2000]:\nLemma 7. Let Y1, Y2, · · · , Yd be i.i.d Gaussian random variables with mean 0 and variance 1. Let Z = d∑\ni=1\n( Y 2i − 1 ) . Then the following inequality holds for any positive x:\nP ( Z ≥ 2 √ dx+ 2x ) ≤ exp(−x)\nWe will now state the famous Bernstein’s Inequality from Boucheron et al. [2013].\nTheorem 6. Let X1, . . . , Xn be independent real-valued random variables. Assume that there exist positive real numbers ν and c such that n∑\ni=1\nE [ X2i ] ≤ ν and\nn∑\ni=1\nE [ (Xi) q + ] ≤ q!\n2 νcq−2 ∀ q ≥ 3,\nwhere x+ = max(x, 0).\nIf S = n∑\ni=1\n(Xi − E[Xi]), then ∀t ≥ 0, we have:\nP ( S ≥ √ 2νt+ ct ) ≤ exp(−t)\nWe will now restate a lemma for controlling the singular values of a matrix with Gaussian random entries from Vershynin [2010].\nLemma 8. Let A ∈ Rd×n be a random matrix whose entries are independent standard normal random variables. Then, for every t ≥ 0, with probability at least 1− 2 exp ( −t2/2 ) , we have:\n√ n− √ d− t ≤ σmin(A) ≤ σmax(A) ≤ √ n+ √ d+ t\nCorollary 1. Let A ∈ Rd×n be a random matrix whose entries are independent standard normal random variables. For n ≥ 200 ( d+ 2 log ( 2 δ )) , we have:\n0.9 √ n ≤ σmin(A) ≤ σmax(A) ≤ 1.1 √ n\nwith probability at least 1− δ Lemma 9. Let Y1, Y2, . . . , Yn be iid d-dimensional random vectors such that Yi ∼ N (0, I)∀i ∈ [n]. Then, we have for any c2 ≤ 1:\nP (∣∣∣∣∣ { i : ‖Yi‖ ≥ d 1 2 + 2d 1 4 ( log ( 1\nc2\n) + log ( µ2r )) 12 }∣∣∣∣∣ ≥ 3c2n 2µ2r ) ≤ β\nwhen n ≥ 16µ2rc2 log ( 1 β ) .\nProof. Let Y1, . . . , Yn be iid random vectors such that Yi ∼ N (0, I)∀i ∈ [n]. From Lemma 7, we have that:\nP ( ‖Yi‖ ≥ d1/2 + 2d 1 4 ( log ( 1\nc2\n) + log ( µ2r ))1/2 )\n≤ c2 µ2r\nLet p := P ( ‖Yi‖ ≥ d1/2 + 2d 1 4 ( log ( 1 c2 ) + log ( µ2r ))1/2) . Consider random variables Zi∀i ∈ [n] be defined\nsuch that Zi = I [ ‖Yi‖ ≥ d1/2 + 2d 14 ( log ( 1 c2 ) + log ( µ2r ))1/2] . Note that Zi satisfy the conditions of Theorem 6 with ν = np and c = 1. We can now bound the probability that n∑\ni=1\nZi is large by setting\nt = nc216µ2r :\nP ( n∑\ni=1\nZi ≤ 3nc2 2µ2r\n) ≤ P ( n∑\ni=1\nZi ≤ n∑\ni=1\nE [Zi] + nc2 2µ2r\n) ≤ P ( n∑\ni=1\nZi ≤ np+ √ 2npt+ t ) ≤ exp (−t)\nFor our choice of n, the above inequality implies the lemma."
    }, {
      "heading" : "B Proof of Technical Lemmas",
      "text" : ""
    }, {
      "heading" : "B.1 Proof of Lemma 1",
      "text" : "Proof. We prove the lemma through a series of inequalities:\n∥∥∥(I − U\\SU⊤\\S)L∗i ∥∥∥ (ζ1)\n≤ 33 32 µ\n√ r\nn\n∥∥∥(I − U\\SU⊤\\S)L∗\\S ∥∥∥ (ζ2)\n≤ 33 32 µ\n√ r\nn\n∥∥∥(I − U\\S(U\\S)⊤)M∗\\S ∥∥∥\n≤ 33 32 µ\n√ r\nn\n∥∥∥(I − U∗(U∗)⊤)M∗\\S ∥∥∥ ,\nwhere (ζ1) holds from Lemma 6 and (ζ2) follows by using the fact that L ∗ \\S can be obtained by setting a few columns of M∗\\S to 0. The last inequality follows from the fact that U\\S contains the top-r singular vectors of M∗\\S."
    }, {
      "heading" : "B.2 Proof of Lemma 2",
      "text" : "Proof. The lemma can be proved through the following set of inequalities:\n∥∥∥Σ−1\\SU ⊤ \\SL ∗ i ∥∥∥ (ζ1) ≤ ∥∥∥Σ−1\\SU ⊤ \\SU ∗ \\SΣ ∗ \\Sw ∥∥∥ ≤ 33 32 µ\n√ r\nn\n∥∥∥Σ−1\\SU ⊤ \\SL ∗ \\S ∥∥∥\n(ζ2)\n≤ 33 32 µ\n√ r\nn\n∥∥∥Σ−1\\SU ⊤ \\SM ∗ \\S ∥∥∥ ≤ 33 32 µ\n√ r\nn ,\nwhere (ζ1) holds with ‖w‖ ≤ 3332µ √ r n from Lemma 6 and (ζ2) follows from the fact that L ∗ \\S can be obtained from M∗\\S by setting some columns in M ∗ \\S to 0."
    }, {
      "heading" : "B.3 Lemma 10",
      "text" : "Lemma 10. Consider the setting of Theorem 2. Let S ⊂ [n] denote any subset of the columns of M∗ such that |S| ≤ 3ρn. Furthermore, suppose that ‖N∗‖F ≤ σk(L ∗)\n16 for some k ∈ [r]. Let M∗\\S(L∗\\S, N∗\\S, C∗\\S) denote the matrix M∗(L∗, N∗, C∗) projected onto the columns not in S. Let U\\SΣ\\SV ⊤ \\S denote the rank-k ′"
    }, {
      "heading" : "SVD of M∗\\S for some k",
      "text" : "′ ≤ k. Then, we have:\n# ( i : ‖Ei‖ ≥ 2µ √ r\nn\n) ≤ 2ρn,\nwhere E = Σ−1\\SU ⊤ \\SM ∗\nProof. From Lemma 6, we get that σk′(L ∗ \\S) ≥ 3132σk′ (L∗). Along with Theorem 5, we conclude that σk′ (L∗\\S+ N∗\\S) ≥ 78σk′ (L∗). Since the column supports of L∗+N∗ and C∗ are disjoint, we have that σk′(L∗\\S+N∗\\S) ≤ σk′ (M ∗ \\S). That is,\nσk′ (M ∗ \\S) ≥\n7 8 σk′(L ∗). (6)\nWe first bound the quantity ∥∥∥Σ−1\\SU⊤\\SL∗i ∥∥∥ ∀i ∈ [n]:\n∥∥∥Σ−1\\SU ⊤ \\SL ∗ i ∥∥∥ (ζ1) ≤ 33 32 µ\n√ r\nn\n∥∥∥Σ−1\\SU ⊤ \\SL ∗ \\S ∥∥∥ (ζ2) ≤ 33 32 µ\n√ r\nn\n(∥∥∥Σ−1\\SU ⊤ \\S(L ∗ \\S +N ∗ \\S) ∥∥∥+ ∥∥∥Σ−1\\SU ⊤ \\SN ∗ \\S ∥∥∥ )\n(ζ3)\n≤ 33 32 µ\n√ r\nn\n(∥∥∥Σ−1\\SU ⊤ \\SM ∗ \\S ∥∥∥+ ∥∥∥Σ−1\\SU ⊤ \\SN ∗ \\S ∥∥∥ ) (ζ4) ≤ 33 32 µ\n√ r\nn\n( 1 + 1\n14\n) ≤ 9\n8 µ\n√ r\nn , (7)\nwhere ζ1 follows from Lemma 6, ζ2 using triangle inequality, ζ3 using the above given fact that SVDr(M∗\\S) = U\\SΣ\\S(V\\S)\n⊤, ζ4 follows from using (6) with bound on ‖N∗‖F . Suppose ∥∥∥Σ−1\\SU⊤\\S(L∗i +N∗i ) ∥∥∥ ≥ 2µ √ r n for some i. We now have:\n∥∥∥Σ−1\\SU ⊤ \\SN ∗ i ∥∥∥ ≥ 7 8 µ\n√ r\nn .\nSimilarly, using (6), we get that:\n‖N∗i ‖ ≥ 3\n4 µ\n√ r\nn σk′ (L\n∗).\nLet Γ := {i : ∥∥∥Σ−1\\SU⊤\\S(L∗i +N∗i ) ∥∥∥ ≥ 2µ √ r n}. Let N∗Γ denote the matrix N∗ restricted to the set Γ. Then\nwe have, √ |Γ|3\n4 µ\n√ r\nn σk′(L\n∗) ≤ ‖N∗Γ‖ ≤ ‖N∗‖ ≤ 1\n16 σk′(L\n∗).\nThis implies that |Γ| ≤ n144µ2r ≤ ρn. Also, by our assumption, α ≤ ρ, i.e., number of non-zero C∗i is less than ρn. That is, the set {i : ∥∥∥Σ−1\\SU⊤\\SC∗i ∥∥∥ ≥ 2µ √ r n} is of size at most ρn. Using the fact that support of C∗ and L∗ +N∗ is disjoint, we have that the set {i : ∥∥∥Σ−1\\SU⊤\\SM∗i ∥∥∥ ≥ 2µ √ r n} is of size at most 2ρn."
    }, {
      "heading" : "B.4 Proof of Lemma 11",
      "text" : "Lemma 11. Assume the setting of Lemma 10. Let U\\SΣ\\SV ⊤ \\S (U ∗ \\SΣ ∗ \\S(V ∗ \\S) ⊤) be the rank-k SVD of M∗\\S (L∗\\S), then the following holds ∀i, 1 ≤ i ≤ n:\n∥∥∥(I − U\\SU⊤\\S)(L∗i +N∗i ) ∥∥∥ ≤ 33\n32 µ\n√ r\nn\n(∥∥∥(I − U∗1:k(U∗1:k)⊤)M∗\\S ∥∥∥+ ‖N∗‖ ) + ‖N∗i ‖ ."
    }, {
      "heading" : "Proof.",
      "text" : "∥∥∥(I − U\\SU⊤\\S)(L∗i +N∗i ) ∥∥∥ ≤ ‖N∗i ‖+ ∥∥∥(I − U\\SU⊤\\S)L∗i ∥∥∥ (ζ1) ≤ ‖N∗i ‖+ 33\n32 µ\n√ r\nn\n∥∥∥(I − U\\SU⊤\\S)L∗\\S ∥∥∥\n≤ ‖N∗i ‖+ 33\n32 µ\n√ r\nn\n(∥∥∥(I − U\\SU⊤\\S)(L∗\\S +N∗\\S) ∥∥∥+ ∥∥∥N∗\\S ∥∥∥ )\n≤ ‖N∗i ‖+ 33\n32 µ\n√ r\nn\n(∥∥∥(I − U\\SU⊤\\S)M∗\\S ∥∥∥+ ‖N∗‖ )\n≤ ‖N∗i ‖+ 33\n32 µ\n√ r\nn\n(∥∥∥(I − U∗1:k(U∗1:k)⊤)M∗\\S ∥∥∥+ ‖N∗‖ )\nwhere (ζ1) follows from Lemma 6 and the fact that only 3ρn columns are ever thresholded at any stage of the algorithm. The remaining inequalities follow using triangle inequality and M∗\\S = L ∗ \\S +N ∗ \\S along with SVDr(M∗\\S) = U\\SΣ\\S(V\\S)⊤."
    }, {
      "heading" : "B.5 Proof of Lemma 3",
      "text" : "Proof. Let S(t) denote the columns of C∗ that are not thresholded in the tth inner iteration. For each j ∈ S(t), we know that ∥∥(Σ(t))−1(U (t))⊤C∗j ∥∥ ≤ 2µ √ r n from our assumption on the termination of the algorithm. Furthermore, since C∗j is not thresholded, we can associate a unique column ij which is thresholded and\nij 6∈ Supp (C∗) such that ∥∥∥I − U (t)(U (t))⊤M∗ij ∥∥∥ ≥ ∥∥I − U (t)(U (t))⊤M∗j ∥∥. Let yi,t := (U (t))−1(Σ(t))−1M∗i and ri,t := (I − U (t)(U (t))⊤)M∗i , ∀i. Thus we have:\n∥∥yj,t ∥∥ ≤ 2µ\n√ r\nn ,\n∥∥rj,t ∥∥ ≤ ∥∥rij ,t ∥∥ .\nLet Q(t) denote the columns of L∗ that have been thresholded in the tth iteration. Furthermore, we definite the matrices L̃(t+1) := L∗\\Q(t) , Ñ (t+1) := N∗\\Q(t) and C̃ (t+1) := C∗ S(t) . Recall the notation, PU⊥ (M) = (I − UUT )M . We now have for any t ≥ 0: ∥∥∥PU (t+1)\n⊥ (L ∗) ∥∥∥ F = ∥∥∥PU (t+1) ⊥ (L̃ (t+1) + (L∗ − L̃(t+1))) ∥∥∥ F ≤ ∥∥∥PU (t+1) ⊥ (L̃ (t+1)) ∥∥∥ F + ∥∥∥PU (t+1) ⊥ (L ∗ − L̃(t+1)) ∥∥∥ F\n≤ ∥∥∥PU (t+1) ⊥ (L̃ (t+1)) ∥∥∥ F +\n  ∑\ni∈Q(t)\n∥∥∥PU (t+1) ⊥ (L ∗ i ) ∥∥∥ 2\n  1 2 (ζ1) ≤ ∥∥∥PU (t+1) ⊥ (L̃ (t+1)) ∥∥∥ F ( 1 + √ 3ρn 33 32 µ √ r n )\n(ζ2)\n≤ 5 4\n(∥∥∥PU (t+1)\n⊥ (L̃ (t+1) + Ñ (t+1)) ∥∥∥ F + ∥∥∥PU (t+1) ⊥ (Ñ (t+1)) ∥∥∥ F )\n(ζ3)\n≤ 5 4\n(∥∥∥PU (t+1) ⊥ (L̃ (t+1) + Ñ (t+1) + C̃(t+1)) ∥∥∥ F + ∥∥∥Ñ (t+1) ∥∥∥ F )\n(ζ4)\n≤ 5 4\n( P\nU∗1:k ⊥ (L̃ (t+1) + Ñ (t+1) + C̃(t+1)) ) + ∥∥∥Ñ (t+1) ∥∥∥ F (ζ5) ≤ 5 4 (∥∥∥PU ∗ 1:k ⊥ (M (t+1)) ∥∥∥ F + ‖N∗‖F ) , (8)\nwhere (ζ1) follows from Lemma 6, (ζ2) from triangle inequality and bound over ρ, (ζ3) from Lemma 5, (ζ4) from the properties of the SVD and (ζ5) from Lemma 5.\nWe will now show that ∥∥∥PU ∗ 1:k\n⊥ (M (t+1)) ∥∥∥ F decreases at a geometric rate:\n∥∥∥PU ∗ 1:k\n⊥ (M (t+1) ∥∥∥ F (ζ6) ≤ ∥∥∥PU ∗ 1:k ⊥ (L̃ (t+1)) ∥∥∥ F + ‖N∗‖F + (∥∥∥PU ∗ 1:k ⊥ (C̃ (t+1)) ∥∥∥ F )\n(ζ7) ≤ ∥∥∥PU ∗ 1:k ⊥ (L ∗) ∥∥∥ F + ‖N∗‖F +\n  ∑\nj∈S(t−1)\n∥∥∥PU ∗ 1:k\n⊥ ((C̃ (t+1))j)\n∥∥∥ 2  \n(ζ8) ≤ ∥∥∥PU ∗ 1:k ⊥ (L ∗) ∥∥∥ F + ‖N∗‖F +\n 2 ∑\nj∈S(t)\n∥∥∥PU ∗ 1:k\n⊥ (U (t)Σ(t)yj,t\n∥∥∥ 2 + ∥∥∥PU ∗ 1:k\n⊥ (r j,t)\n∥∥∥ 2   1 2\n≤ ∥∥∥PU ∗ 1:k ⊥ (L ∗) ∥∥∥ F + ‖N∗‖F +\n 8ρnµ 2r\nn\n∥∥∥PU ∗ 1:k\n⊥ (U (t)Σ(t))\n∥∥∥ 2 + 2 ∑\nj∈S(t)\n∥∥∥PU ∗ 1:k\n⊥ (r j,t)\n∥∥∥ 2   1 2\n(ζ9) ≤ ∥∥∥PU ∗ 1:k ⊥ (L ∗) ∥∥∥ F + ‖N∗‖F +  1 8 ∥∥∥PU ∗ 1:k ⊥ (U (t)Σ(t)) ∥∥∥ 2 + 2 ∑\nj∈S(t)\n∥∥rij ,t ∥∥2   1 2\n(ζ10) ≤ ∥∥∥PU ∗ 1:k ⊥ (L ∗) ∥∥∥ F + ‖N∗‖F +   1 8 ∥∥∥PU ∗ 1:k ⊥ (U (t)Σ(t)) ∥∥∥ 2\n︸ ︷︷ ︸ Term 1\n+4 ∑\nj∈S(t)\n∥∥∥N∗ij ∥∥∥ 2 + ( 33\n32\n)2 µ2 r\nn\n(∥∥∥PU (t) ⊥ (L̃ (t) + Ñ (t)) ∥∥∥ )2\n︸ ︷︷ ︸ Term 2\n  1 2 , (9)\nwhere (ζ6) follows from triangle inequality, (ζ7) follows from Lemma 5, (ζ8) follows from triangle inequality and the fact that (a+ b)2 ≤ 2a2+2b2, (ζ9) from our previous observations about yj,r and rj,t and (ζ10) from Lemma 10.\nWe will now proceed to bound Term 1 as follows:\n∥∥∥PU ∗ 1:k\n⊥ (U (t)Σ(t))\n∥∥∥ 2\nF\n(ζ11) ≤ ∥∥∥PU ∗ 1:k\n⊥ (M (t))\n∥∥∥ 2\nF , (10)\nwhere (ζ11) follows from considering the full SVD of M (t) and Lemma 5.\nWe now proceed to bound Term 2 as:\n∑\nj∈S(t)\n∥∥∥N∗ij ∥∥∥ 2 + ( 33\n32\n)2 µ2 r\nn\n(∥∥∥PU (t) ⊥ (L̃ (t) + Ñ (t)) ∥∥∥ )2 ≤ ‖N∗‖2F + 9\n8 ρn\nµ2r\nn\n∥∥∥PU (t) ⊥ (M (t)) ∥∥∥ 2\nF\n≤ ‖N∗‖2F + 1\n32\n∥∥∥PU ∗ 1:k\n⊥ (M (t))\n∥∥∥ 2\nF , (11)\nwhere the first inequality follows from Lemma 5 and the second inequality from the fact that U (t+1) are top-k left singular vectors of M (t).\nUsing (9), (10), (11), we have:\n∥∥∥PU ∗ 1:k\n⊥ (M (t+1)) ∥∥∥ F ≤ ∥∥∥PU ∗ 1:k ⊥ (L ∗) ∥∥∥ F + ‖N∗‖F + ( 1 8 ∥∥∥PU ∗ 1:k ⊥ (M (t)) ∥∥∥ 2 F + 4 ‖N∗‖2F + 1 8 ∥∥∥PU ∗ 1:k ⊥ (M (t)) ∥∥∥ 2 F ) 1 2\n≤ ∥∥∥PU ∗ 1:k ⊥ (L ∗) ∥∥∥ F + 3 ‖N∗‖F + 1 2 ∥∥∥PU ∗ 1:k ⊥ (M (t)) ∥∥∥ F ,\nwhere the first inequality follows from Lemma 5 and considering the full SVD of M (t) and the last inequality follows from the fact that √ a+ b+ c ≤ √a+ √ b + √ c.\nBy recursively applying the above inequality, we have: ∥∥∥PU ∗ 1:k\n⊥ (M (T+1)) ∥∥∥ F ≤ 2 ∥∥∥PU ∗ 1:k ⊥ (L ∗) ∥∥∥ F + 6 ‖N∗‖F + ǫ 20n .\nLemma now follows using (8) with the above equation."
    }, {
      "heading" : "B.6 Lemma 12",
      "text" : "Lemma 12. Let Y1, Y2, . . . , Yn be iid d-dimensional random vectors such that Yi ∼ N (0, I)∀i ∈ [n]. Then, we have for any c1 ≤ 1:\nP ( ∃v ∈ Rd, ‖v‖ = 1 s.t ∣∣∣∣∣ { i : 〈v, Yi〉 ≥ 2 ( log ( µ2r ) + log (d) + log ( 1\nc1\n)) 1 2 }∣∣∣∣∣ ≥ 3c1n µ2rd ) ≤ β,\nwhen n ≥ 16µ2rdc1 [ log ( 1 β ) + d log (80d) ] .\nProof. Let v ∈ Rd s.t ‖v‖ = 1. We define the set Sv,θ as follows:\nSv,θ = {u : u ∈ Rd ∧ ‖u‖ = 1 ∧ 〈u, v〉 ≥ cos(θ)}.\nWe now define the set T (v, θ, δ) as: T (v, θ, δ) = {x : x ∈ Rd ∧ ∃u ∈ Sv,θ s.t 〈u, x〉 ≥ δ}.\nNow, let y ∼ N (0, I). Using spherical symmetry of the Gaussian, w.l.o.g. v = e1. We now define the complementary sets Q (ν) and R (ν) as:\nQ (ν) = {x : x ∈ Rd ∧ x1 < ν}, R (ν) = {x : x ∈ Rd ∧ x1 ≥ ν}.\nWe will now bound the probability that y ∈ T (v, θ, δ) for δ = 2 ( log ( µ2r ) + log (d) + log ( 1 c1 ))1/2 and\nθ = csc−1 ( 10(d− 1)1/2 ) .\nP (y ∈ T (e1, θ, δ)) = ∫\nT (e1,θ,δ)\n1 (√\n2π )d exp\n( −‖y‖ 2\n2\n) dy\n=\n∫\nT (e1,θ,δ)∩Q(δ/ √ 2)\n1 (√\n2π )d exp\n( −‖y‖ 2\n2\n) dy +\n∫\nT (e1,θ,δ)∩R(δ/ √ 2)\n1 (√\n2π )d exp\n( −‖y‖ 2\n2\n) dy\n≤ ∫\nR(δ/ √ 2)\n1 (√\n2π )d exp\n( −‖y‖ 2\n2\n) dy +\n∫\nT (e1,θ,δ)∩Q(δ/ √ 2)\n1 (√\n2π )d exp\n( −‖y‖ 2\n2\n) dy\n(ζ1)\n≤ c1 µ2rd +\n∫\nT (e1,θ,δ)∩Q(δ/ √ 2)\n1 (√\n2π )d exp\n( −‖y‖ 2\n2\n) dy,\nwhere (ζ1) follows from the fact that for t ≥ 1, ∞∫ t 1√ 2π exp ( −x22 ) dx ≤ exp ( − t22 ) .\nWe will use M (θ, δ, γ) to denote the set {z : z ∈ Rd−1 ∧ (γ, z) ∈ T (e1, θ, δ)∩Q ( δ/ √ 2 ) }. We can bound\nthe second term as follows:\n∫\nT (e1,θ,δ)∩Q(δ/ √ 2)\n1 (√\n2π )d exp\n( −‖y‖ 2\n2\n) dy\n≤ δ/\n√ 2∫\n−∞\n1√ 2π exp\n( −y 2 1\n2\n)   ∫\nM(θ,δ,y1)\n1 (√\n2π )d−1 exp\n( −‖z‖ 2\n2\n) dz   dy1. (12)\nNow, let z ∈ Rd be such that z1 = y1 ∧ z2:d ∈ M (θ, δ, y1) for some y1 ∈ [−∞, δ/ √ 2]. Therefore, ∃w ∈ Sv,θ such that 〈w, z〉 ≥ δ. We can decompose w into its components along v and orthogonal to it, w = cos(θ′)v+ sin(θ′)v⊥ for some unit vector v⊥ orthogonal to v and some θ′ ∈ [0, θ]. We know that 〈w, z〉 ≥ δ and that 〈w, v〉 ≤ δ/ √ 2. From these two inequalities and using the fact that v = e1, we get:\nsin(θ′) ‖z2:d‖ ≥ sin(θ′) 〈 v⊥, z 〉 ≥ δ − cos(θ′) 〈v, z〉 ≥ δ − cos(θ′) δ√\n2 ≥\n( 1− 1√\n2\n) δ.\nThis allows us to lower bound the length of z2:d by 10(d− 1)1/2 ( 1− 1√\n2\n) δ. For our choice of δ and θ and\nusing Lemma 7, we now get that the inner integration in equation 12 is atmost c1µ2rd . Thus, we have the following bound on P (y ∈ T (e1, θ, δ)):\nP (y ∈ T (e1, θ, δ)) ≤ 2c1 µ2rd . (13)\nLet p be used to denote the value P (y ∈ T (e1, θ, δ)). Now, assume Y1, . . . , Yn are iid random vectors with Yi ∼ N (0, I) ∀i ∈ [n]. Now let Zi be defined such that Zi = I [Yi ∈ T (ei, θ, δ)] ∀i ∈ [n]. Note that Zi is a Bernoulli random variable which is 1 with probability p. It can be seen that Zi satisfy satisfy the conditions of 6 with ν = np and c = 1. Therefore, setting t = nc116µ2rd in Theorem 6, we get:\nP ( n∑\ni=1\nZi ≥ 3nc1 µ2rd\n) ≤ P ( n∑\ni=1\nZi ≥ np+ nc1 µ2rd\n) ≤ P ( n∑\ni=1\nZi ≥ np+ √ 2νt+ t ) ≤ exp (−t) . (14)\nNow, consider the subset K := {x : x ∈ Rd ∧ |xi| ≤ 1∀i ∈ [d]}. Consider a partitioning of K into subsets K (ǫ, j) = {x : x ∈ K∧∀i ∈ [d]jiǫ− 1 ≤ xi ≤ (ji +1)ǫ− 1} where j ∈ J is an index for each of these subsets. Note that for any ǫ, at most (⌈ 2 ǫ ⌉)d such indices are required to ensure that K ⊆ ⋃j∈J K (ǫ, j). Setting ǫ = 140d , we have for any two unit vectors v1 and v2 such that v1, v2 ∈ K (ǫ, j) for some j, ‖v1 − v2‖ ≤ 140d1/2 . From this fact, it can be seen that Equation 14 holds for all unit vectors in K (ǫ, j) with any unit vector v ∈ K (ǫ, j). Therefore, we choose for each subset K (ǫ, j), which contains a unit vector, a unit vector v and take an union bound over all such subsets K (ǫ, j). After doing so we get the following bound:\nP ( ∃v ∈ Rd ∧ ‖v‖ = 1 s.t n∑\ni=1\nZi ≥ 3nc1 µ2rd\n) ≤ (80d)d exp (−t) (ζ2) ≤ β, (15)\nwhere (ζ2) follows from the conditions of the theorem. Thus, we have proved the theorem."
    }, {
      "heading" : "B.7 Lemma 13",
      "text" : "Lemma 13. Assume the conditions of Theorem 3. Let S ⊂ [n] denote any subset such that |S| ≤ 164µ2r . Let M∗\\S(L ∗ \\S, N ∗ \\S, C ∗ \\S) denote the matrices M ∗ (L∗, N∗, C∗) restricted to the columns not in S. Let [U\\S ,Σ\\S , V\\S ] = SVDr+1(M∗\\S). Furthermore, let E = {x : x = U\\SΣ\\Sy for some ‖y‖ ≤ 2µ √ r/n}. Then ∀i, we have: ∥∥PU\\S (L∗i +N∗i )− PE (L∗i +N∗i ) ∥∥ ≤ 33 32 µ √ r n ‖N∗‖+ ∥∥PU\\S (N∗i ) ∥∥ ,\nwhere PU\\S (M) = U\\SU⊤\\SM .\nProof. Let [U∗\\S,Σ ∗ \\S , V ∗ \\S ] = SVDr(L∗\\S). Using Lemma 6, L∗i = L∗\\SV ∗\\Swi for some ‖wi‖ ≤ 3332µ √ r n . Now consider the vector yi := U\\SΣ\\SV ⊤ \\SV ∗ \\Swi. Note that ∥∥∥(Σ\\S)−1U⊤\\Syi ∥∥∥ ≤ 3332µ √ r n . Now, using definition of PE : ∥∥PU\\S (L∗i +N∗i )− PE (L∗i +N∗i ) ∥∥ ≤ ∥∥PU\\S (L∗i +N∗i )− yi ∥∥\n≤ ∥∥PU\\S (N∗i ) ∥∥+ ∥∥∥PU\\S ( L∗\\SV ∗ \\Swi ) − U\\SΣ\\SV ⊤\\SV ∗\\Swi ∥∥∥ (ζ1)\n≤ ∥∥PU\\S (N∗i ) ∥∥+ 33 32 µ\n√ r\nn\n∥∥∥PU\\S ( L∗\\S ) V ∗\\S − PU\\S ( L∗\\S +N ∗ \\S ) V ∗\\S ∥∥∥\n≤ ∥∥PU\\S (N∗i ) ∥∥+ 33 32 µ\n√ r\nn\n∥∥∥PU\\S ( N∗\\S )∥∥∥ ,\nwhere (ζ1) holds from Lemma 6 and the fact that V ∗ \\S has zeros in the rows corresponding to the support of C∗."
    }, {
      "heading" : "B.8 Proof of Lemma 4",
      "text" : "Proof. Note that under the conditions of Theorem 3, the following three conditions hold with probability at least 1− δ.\n#i ( ‖N∗i ‖ ≥ σ ( √ d+ 2d 1 4 √ log ( µ2r\nc2\n))) ≤ 3c2n\nµ2r\n∀ ‖v‖ = 1 #i ( |〈v,N∗i 〉| ≥ 2σ √ log ( µ2r2d\nc1\n)) ≤ 6c1n\nµ2r2d\n0.9σ √ n ≤ ‖N∗‖ ≤ 1.1σ√n\nAlso, for c1 = 1 12288 and c2 = 1 1536 , the invariant implies at most n\n512µ2r clean data points are thresholded at any stage. This allows us to apply Lemma 13 in the subsequent steps.\nWe will prove the lemma by induction on the number of thresholding steps completed so far. Let t denote the number of thresholding steps executed so far.\nBase Case (t = 0): The invariant trivially holds before any data points have been thresholded. Induction Step (t = k + 1): Assuming that the invariant holds after the kth thresholding step, we\nnow have two cases for the (k + 1)th thresholding step:\nCase 1: Thresholding with respect to ζ1. For a column i 6∈ Supp (C∗) thresholded with respect to ζ1, we have from Lemma 13 that:\n‖N∗i ‖ ≥ ζ1 − 33\n32 µ\n√ r\nn ‖N∗‖ = σ\n( 5\n4 µ √ r + d 1 2 + 2d 1 4\n√ log ( µ2r\nc2\n)) − 5\n4 σµ\n√ r\n≥ σ ( d 1 2 + 2d 1 4 √ log ( µ2r\nc2\n)) . (16)\nFrom our choice of ζ1, there are only n\n1024µ2r clean points which satisfy 16.\nCase 2: Thresholding with respect to ζ2. For a column i 6∈ Supp (C∗), it can only be thresholded if the number of columns to be thresholded exceeds 24c1nµ2rd . Note that:\n‖PU(k+1)(N∗i )‖ ≥ ζ2 − 5\n4 σµ\n√ r = 2σ √ 2r √ log ( µ2r2d\nc1\n) .\nNote that U (k+1) is at most a rank 2r subspace. Therefore, ∃j such that: ∣∣∣ 〈 N∗i , U (k+1) j 〉∣∣∣ ≥ 2σ √ log ( µ2r2d\nc1\n) . (17)\nTaking a union bound over all j ∈ [r + 1] and using Lemma 12 for both positive and negative inner product values, we get that at most 12c1nµ2rd clean points satisfy 17. Since, we threshold at least 24c1n µ2rd points, at least half of them must be outliers and hence the invariant holds in the next iteration."
    }, {
      "heading" : "C Gaussian Noise: Proof of Theorem 3",
      "text" : "Proof. We will prove the theorem for c1 = 1 12288 and c2 = 1\n1536 . For our choices of c1 and c2 and n, we have that:\n#i ( ‖N∗i ‖ ≥ σ ( √ d+ 2d 1 4 √ log ( µ2r\nc2\n))) ≤ 3c2n\nµ2r\n∀ ‖v‖ = 1 #i ( |〈v,N∗i 〉| ≥ 2σ √ log ( µ2r2d\nc1\n)) ≤ 6c1n\nµ2r2d\n0.9σ √ n ≤ ‖N∗‖ ≤ 1.1σ√n\nwith probability at least 1− δ from Lemmas 12, 9 and Corollary 1 along with our choice of n. From Lemma 4, we know that Invariant 1 holds at the termination of the algorithm. Therefore, at most\nn 512µ2r inliers are removed (The number of inliers removed is at most αn+ n 1024µ2r ).\nSuppose the algorithm terminated in the T th iteration. Let M := M∗ −C(T ). We will start by making a few observations. The algorithm terminates when no data point is thresholded. Let [U,Σ, V ] = SVDr+1(M). Furthermore, define E = {x : x = UΣy for some ‖y‖ ≤ 2µ √ r/n}. Now, define set A as:\nA := { i : ‖PU ((M)i)− PE((M)i)‖ ≥ σ √ 2r ( 5\n4 µ+ 2 log\n1 2\n( µ2r2d\nc1\n))} ,\nand B as:\nB := { i : ‖PU (Mi)− PE(Mi)‖ ≥ σ ( d 1 2 + 2d 1 4 ( log 1 2 ( µ2r\nc2\n))) + σ 5\n4 µ √ r\n} .\nRecall that we will threshold the columns in A if |A| ≥ 24c1nµ2rd and the columns in B if B is not empty. Therefore, we know that:\n∀i ∈ [n] ‖PU (Mi)− PE(Mi)‖ ≤ σ ( d 1 2 + 2d 1 4 ( log 1 2 ( µ2r\nc2\n))) + σ 5\n4 µ √ r, |A| ≤ 24c1n µ2rd . (18)\nLet S denote the set of data points that have been thresholded when the algorithm terminated, i.e S = CS(T ). Let L = L∗\\S, N = N∗\\S and C = C∗\\S . Additionally, let [U∗\\S ,Σ∗\\S , V ∗\\S ] = SVD(L). Similar to the proofs of Theorems 1 and 2, we start as follows:\n∥∥∥PU1:r⊥ (L∗) ∥∥∥ ≤ (∥∥∥PU1:r⊥ (L) ∥∥∥ 2 + ∑\ni∈S\n∥∥∥PU1:r⊥ (U∗\\SΣ∗\\Swi) ∥∥∥ 2 ) 1 2\nζ1 ≤ (∥∥∥PU1:r⊥ (L) ∥∥∥ 2 + ∑\ni∈S\n9 8 µ2 r n\n∥∥∥PU1:r⊥ (U∗\\SΣ∗\\S) ∥∥∥ 2 ) 1 2 ζ2 ≤ (∥∥∥PU1:r⊥ (L) ∥∥∥ 2 + 3ρn 9\n8 µ2\nr\nn\n∥∥∥PU1:r⊥ (L) ∥∥∥ 2 ) 1 2\n≤ 5 4\n∥∥∥PU1:r⊥ (L) ∥∥∥ ≤ 5\n4\n( ‖N‖+ ∥∥∥PU1:r⊥ (L+N) ∥∥∥ )\nζ3 ≤ 5\n4\n( ‖N∗‖+ ∥∥∥PU1:r⊥ (M) ∥∥∥ ) = 5\n4\n( ‖N∗‖+ ∥∥∥PU1:r⊥ (PU (M)) ∥∥∥ ) , (19)\nζ1 follows using Lemma 6, ζ2 follows using |S| ≤ 2ρn, ζ3 follows using Lemma 5 where the last equality follows from the fact that U consists of the top r + 1 singular vectors of M .\nNow, let Y be an orthogonal basis of the subspace spanned by PU (L). Note that the subspace spanned by Y is at most rank-r. Let O denote the set of corrupted columns that haven’t been thresholded at the\ntermination of the algorithm and let Ol := O ∩A and Os := O\\Ol. We can now bound ∥∥∥PU1:r⊥ (PU (M)) ∥∥∥ as follows:\n∥∥∥PU1:r⊥ (PU (M)) ∥∥∥ ≤ ∥∥P Y⊥ (PU (L +N + C)) ∥∥ ≤ ∥∥P Y⊥ (PU (N + C)) ∥∥ ≤ ‖N‖+ ∥∥P Y⊥ (PU (C)) ∥∥\n≤ ‖N∗‖+   ∑\ni∈Ol\n∥∥P Y⊥ (PU (Ci)) ∥∥2\n︸ ︷︷ ︸ Term 1\n+ ∑\nj∈Os\n∥∥P Y⊥ (PU (Cj)) ∥∥2\n︸ ︷︷ ︸ Term 2\n  1 2 , (20)\nwhere first inequality follows from the fact that U1:r are top singular vectors of PU (M) and second inequality follows from definition of Y .\nWe can now bound Term 1 as follows:\n∑\ni∈Ol\n∥∥P Y⊥ (PU (Ci)) ∥∥2 (ζ1)≤ 2 ∑\ni∈Ol\n∥∥P Y⊥ (PE (Ci)) ∥∥2 + ∥∥P Y⊥ ((PU (Ci)− PE (Ci))) ∥∥2\n≤ 2 ∑\ni∈Ol\n4µ2r\nn\n∥∥P Y⊥ (UΣ) ∥∥2 + ‖PU (Ci)− PE (Ci)‖2 (From Definition of E)\n(ζ2)\n≤ 48c1n µ2rd\n 4µ 2r\nn\n∥∥P Y⊥ (UΣ) ∥∥2 + σ2  4µ2r + 2 ( √ d+ 2d 1 4 √ log ( µ2r\nc2\n))2   \n(ζ3)\n≤ 48c1n µ2rd\n( 4µ2r\nn\n∥∥P Y⊥ (UΣ) ∥∥2 + σ2 ( 4µ2r + 2 ( 2d+ 8d 1 2 log ( µ2r\nc2\n))))\n≤ 48c1n µ2rd\n( 4µ2r\nn\n∥∥P Y⊥ (UΣ) ∥∥2 + σ2 ( 4µ2r + 4d+ 16d 1 2 log ( µ2r\nc2\n)))\n(ζ4)\n≤ 48c1n µ2rd\n( 4µ2r\nn\n∥∥P Y⊥ (UΣ) ∥∥2 + σ2 ( 4µ2r + 4d+ 16d 1 2 log(µ2r) + 16d 1 2 log(1536)\n))\n(ζ5)\n≤ 48c1n µ2rd\n( 4µ2r\nn\n∥∥P Y⊥ (UΣ) ∥∥2 + σ2 ( 24µ2rd+ 120d\n))\n≤ ∥∥P Y⊥ (M) ∥∥2\n32 +\n48c1n\nµ2rd σ2\n( 144µ2rd ) (From Lemma 5)\n≤ ∥∥P Y⊥ (M) ∥∥2\n32 + 0.563σ2n\nwhere (ζ1) and (ζ3) follow from (a+b) 2 ≤ 2a2+2b2, ζ2 follows from (18). (ζ4) and (ζ5) follow from log(x) ≤ x\nand √ x ≤ x for all x ≥ 1.\nWe now bound Term 2 as:\n∑\nj∈Os\n∥∥P Y⊥ (PU (Cj)) ∥∥2 (ζ6)≤ 2 ∑\nj∈Os\n∥∥P Y⊥ (PE (Cj)) ∥∥2 + ∥∥P Y⊥ ((PU (Cj)− PE (Cj))) ∥∥2\n≤ 2 ∑\ni∈Os\n4µ2r\nn\n∥∥P Y⊥ (UΣ) ∥∥2 + ‖PU (Cj)− PE (Cj)‖2 (Definition of E)\n(ζ7) ≤ 2αn ( 4µ2r\nn\n∥∥P Y⊥ (UΣ) ∥∥2 + 2σ2 ( 4µ2r + 8r log ( µ2r2d\nc1\n)))\n(ζ8) ≤ ∥∥P Y⊥ (M) ∥∥2\n32 + 4αnσ2\n( 4µ2r + 8r log(µ2) + 16r log(r) + 8r log(d) + 8r log(12288) )\n(ζ9) ≤ ∥∥P Y⊥ (M) ∥∥2\n32 + 4αnσ2\n( 12µ2r + 24r log(d) + 8r log(12288) ) (ζ10) ≤\n∥∥P Y⊥ (M) ∥∥2\n32 + 0.435σ2n log(d),\nwhere (ζ6) and (ζ7) follow from (a+ b) 2 ≤ 2a2+2b2 along with (18). (ζ8) follows from Lemma 5, (ζ9) follows from the fact that r ≤ d and log(x) ≤ x and (ζ10) follows from assuming log(d) ≥ 1 and µ ≥ 1. From our bounds on Term 1 and Term 2 and (20), we have:\n∥∥∥PU1:r⊥ (PU (M)) ∥∥∥ ≤ ∥∥P Y⊥ (PU (M)) ∥∥ ≤ 4 3 σ √ n log(d).\nTheorem now follows by using the above observation with (19): ∥∥∥PU1:r⊥ (L∗) ∥∥∥ ≤ 6 4 σ √ n+ 5 3 σ √ n log(d) ≤ 4σ √ n log(d)."
    }, {
      "heading" : "D TORP-BIN",
      "text" : "In this section, we propose an improvement to Algorithm 2 which uses binary search instead of a linear scan in the outer iteration. This improves the running time on Algorithm 2 by almost a factor of r."
    }, {
      "heading" : "D.1 Algorithm",
      "text" : "In this section, we present our algorithm (See Algorithm 4) for OR-PCAN which improves the running time of Algorithm 2 by almost a factor of r. The main insight is that inner iteration of Algorithm 2 is independent of the value of k in the outer iteration save for the rank of the projection. In Algorithm 4, we use binary search on k instead of a linear scan which reduces the number of outer iterations from O (r) to O (log r).\nAlgorithm 4 Binary search based TORP (TORP-BIN)\n1: Input: Corrupted matrix M∗ ∈ Rd×n, Target rank r, Expressivity parameter η, Threshold fraction ρ, Number of inner iterations T 2: minK ← 1, maxK ← r 3: while minK ≤ maxK do 4: k ← ⌊minK+maxK2 ⌋ 5: C(0) ← 0, τ ← false 6: for t = 0 to t = T do 7: [U (t),Σ(t), V (t)] ← SVDk ( M∗ − C(t) ) , L(t) ← U (t)Σ(t)(V (t))⊤ } Projection onto space of\nlow rank matrices\n8: E ← (Σ(t))−1(U (t))⊤M∗ /* Compute Incoherence */ 9: R ← (I − U (t)(U (t))⊤)M∗ /* Compute residual */\n   Projection onto space of column sparse matrices\n10: CS(t+1) ← HT 2ρ (M∗, E) ∪HT ρ (M∗, R) 11: C(t+1) ← M∗CS(t+1) 12: nthres ← |{i : ‖Ei‖ ≥ η}| /* Compute high incoherence points */ 13: τ ← τ ∨ (nthres ≥ 2ρn) /* Check termination conditions */ 14: end for 15: if τ then 16: maxK ← k − 1 17: else 18: minK ← k + 1 19: [U,Σ, V ] ← SVDk ( M∗ − C(T+1) ) 20: end if 21: end while 22: Return: U"
    }, {
      "heading" : "D.2 Analysis",
      "text" : "In this section, we will state and prove a theoretical guarantee for Algorithm 4.\nTheorem 7. Assume the conditions of Theorem 2. Furthermore, suppose that ‖N∗‖F ≤ σk(L ∗) 16 for some\nk. Then, for all α ≤ 1128µ2r , Algorithm 4 when run with parameters ρ = 1128µ2r , η = 2µ √ r n and T = log 20nσ1(M)ǫ , returns a subspace U which satisfies:\n∥∥(I − UU⊤)L∗ ∥∥ F ≤ 3 ∥∥(I − U∗1:k(U∗1:k)⊤)L∗ ∥∥ F + 9 ‖N∗‖F + ǫ\n10n .\nProof. We will begin by bounding the running time of the algorithm. Note that because of the binary search, the algorithm will run for at most O (log r) outer iterations.\nLet t denote the number of outer iterations of the algorithm. Let the value of k(maxK, minK) in iteration t be denoted by k(t) (maxK(t), minK(t)). We will first prove the claim that at any point in the running of the algorithm, maxK ≥ k. We will prove the claim via induction on the number of iterations t:\nBase Case: t = 0 The base case is trivially true as maxK = r.\nInduction Step: t = l + 1 Assume that the claim remains true at iteration t = l. In the (l + 1)th iteration, we assume two cases:\nCase 1: The inner iteration finishes with τ = false. In this case, maxK is not updated. So, the claim remains true for t = (l + 1)\nCase 2: The inner iteration finishes with τ = true. In this case, k(t) > k (From Lemma 10 and the termination condition of the inner iteration.). In this iteration, maxK is updated to k(t) − 1 ≥ k. Thus, the claim remains true.\nTherefore, at termination of the algorithm, we have maxK ≥ k. Suppose that the algorithm terminated after iteration T . Note that minK(t) ≤ k ≤ maxK(t)∀0 ≤ t ≤ T . Therefore, we have minK(T+1) = maxK(T+1) + 1. For this to happen, the inner iteration must have run with k(T\n′) = maxK(T+1) with τ = false for some iteration T ′ and also that this is the last such successful iteration as minK is not updated after iteration T ′. Therefore, the algorithm returns the subspace corresponding to k(T\n′) = maxK(T+1) ≥ k. Since the inner iteration is successful for iteration T ′, the Theorem is true from the application of Lemma 3 and noting that kT ′ ≥ k."
    }, {
      "heading" : "E Fast Projection Operator",
      "text" : "In this section, we will describe a fast algorithm to compute the projection operator onto the ellipsoid in Algorithm 2. Formally, we are provided an orthogonal basis U ∈ Rd×r, a positive diagonal matrix Σ, a bound b and a vector x. Let E = {y : y = UΣz for some ‖z‖ ≤ b}. The goal is to compute the projection of the vector x onto the set E ."
    }, {
      "heading" : "E.1 Algorithm",
      "text" : "In this section, we present our algorithm (Algorithm 5) to compute the projection onto the set E . We show that the projection operation boils down to an univariate optimization problem on a monotone function. We then perform binary search on an interval in which the solution is guaranteed to lie."
    }, {
      "heading" : "E.2 Analysis",
      "text" : "Theorem 8. Let U ∈ Rd×r be an orthonormal matrix and Σ ∈ Rr×r be a positive diagonal matrix. Then, for any b ≥ 0, x ∈ Rd and ǫ, the vector w returned by Algorithm 5 satisfies:\n‖w − PE(x)‖ ≤ ǫ\nwhere E := {y : y = UΣz for some ‖z‖ ≤ b}\nProof. We first define the convex optimization problem corresponding to the projection operator PE . Then, we have:\nPE(x) = argmin y ‖x− y‖ s.t y ∈ E\nSince, y ∈ E , a solution to the above optimization problem is equivalent to:\nAlgorithm 5 w = FAST-PR(U,Σ, b, x, ǫ)\n1: Input: Orthogonal Basis U ∈ Rd×r, Positive Diagonal Matrix Σ, Bound b, Projection Vector x, Accuracy Parameter ǫ 2: σmin = min i∈[r] (Σi,i), σmax = max i∈[r] (Σi,i) 3: y ← ΣU⊤x 4: λmin = 0, λmax = ‖y‖ b\n5: T ← log ( λmax √ r‖x‖\nσ2minǫ\n)\n6: for Iteration t = 0 to t = T do 7: λ(t) ← λmin+λmax2 8: z(t) ← (λI +Σ2)−1y 9: if ∥∥z(t) ∥∥ ≤ b then\n10: λmax ← λ(t) 11: else 12: λmin ← λ(t) 13: end if 14: end for 15: Return: UΣz(T )\nPE(x) = argmin z ‖x− UΣz‖2 s.t ‖z‖2 ≤ b2 (21)\nNote that both the constraint and the objective function are convex. Therefore, we can introduce a KKT multiplier λ ≥ 0 and writing down the stationarity conditions of 21, we get:\n2Σ2z + 2λz = 2ΣU⊤x =⇒ z = ( Σ2 + λI )−1 ΣU⊤x\nNow, we just need to ensure that ∥∥∥ ( Σ2 + λI )−1 ΣU⊤x ∥∥∥ ≤ b. Let f(λ) = ∥∥∥ ( Σ2 + λI )−1 ΣU⊤x ∥∥∥ for λ ≥ 0. Let λ∗ be the solution to f(λ) = min( ∥∥Σ−1U⊤x ∥∥ , b). We will first prove that at any point in the running of the algorithm λmax ≥ λ∗ and λmin ≤ λ∗. We prove the claim by induction on the number of iterations t:\nBase Case t = 0: Since λmin = 0, the lower bound holds trivially. That λmax ≥ λ∗ can be proved as follows:\nf(λmax) = ∥∥∥ ( Σ2 + λmaxI )−1 ΣU⊤x ∥∥∥ ≤ min ( ∥∥Σ−1U⊤x ∥∥ , ∥∥ΣU⊤x ∥∥ λmax ) ≤ min (∥∥Σ−1U⊤x ∥∥ , b ) .\nSince, f is a monotonically decreasing function, the claim holds true in the base case.\nInduction Step t = (k + 1): Assume that the claim holds till t = k. We have two cases for iteration k + 1:\nCase 1: λmax ← λ(t+1). In this case, λmin ≤ λ∗ still holds from the inductive hypothesis. For λmax, we have:\nf(λ(t+1)) = ∥∥∥∥ ( Σ2 + λ(t+1)I )−1 ΣU⊤x ∥∥∥∥ ≤ min (∥∥Σ−1U⊤x ∥∥ , f(λ(t+1)) ) ≤ min (∥∥Σ−1U⊤x ∥∥ , b ) ,\nwhere the last inequality holds from the fact that λmax was updated in this iteration. From the monotonicity of f , the induction hypothesis holds in this iteration.\nCase 2: λmin ← λ(t+1). In this case, λmax ≥ λ∗ by the inductive hypothesis. In this case, we have: f(λ(t+1)) ≥ b ≥ f(λ∗). From the monotonicity of f , the induction hypothesis holds in this iteration.\nNote that (λmax − λmin) is halved at each iteration. Therefore, at the termination of the algorithm, we have (λmax − λmin) ≤ σ 2 minǫ√ r‖x‖ . From our claim, this implies that ∣∣λ∗ − λ(T ) ∣∣ ≤ σ 2 minǫ√ r‖x‖ . Note that we can\nwrite PE(x) = UΣ2(λ∗I +Σ2)−1U⊤x. Note that ‖PE(x) − w‖ = ∥∥U⊤(PE(x) − w) ∥∥. We will now bound the element-wise difference between U⊤PE(x) and U⊤w:\n∣∣∣e⊤i Σ2((λ∗I +Σ2)−1 − (λ(T )I +Σ2)−1)U⊤x ∣∣∣ ≤ ‖x‖ ∣∣∣∣σ 2 i ( 1\nλ∗ + σ2i − 1 λ(T ) + σ2i\n)∣∣∣∣\n≤ ‖x‖ σ2i ∣∣∣∣\nλ(T ) − λ∗ (λ∗ + σ2i )(λ (T ) + σ2i )\n∣∣∣∣ ≤ ‖x‖ ∣∣∣∣ λ(T ) − λ∗\nσ2i\n∣∣∣∣ ≤ ǫ√ r .\nBy applying the element-wise bound to Σ2((λ∗I +Σ2)−1 − (λ(T )I +Σ2)−1)U⊤x, we have:\n‖PE(x)− w‖ ≤ √ r\nǫ√ r ≤ ǫ."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "We consider the problem of outlier robust PCA (OR-PCA) where the goal is to recover principal<lb>directions despite the presence of outlier data points. That is, given a data matrix M∗, where (1 − α)<lb>fraction of the points are noisy samples from a low-dimensional subspace while α fraction of the points can<lb>be arbitrary outliers, the goal is to recover the subspace accurately. Existing results for OR-PCA have<lb>serious drawbacks: while some results are quite weak in the presence of noise, other results have runtime<lb>quadratic in dimension, rendering them impractical for large scale applications.<lb>In this work, we provide a novel thresholding based iterative algorithm with per-iteration complexity<lb>at most linear in the data size. Moreover, the fraction of outliers, α, that our method can handle is tight<lb>up to constants while providing nearly optimal computational complexity for a general noise setting. For<lb>the special case where the inliers are obtained from a low-dimensional subspace with additive Gaussian<lb>noise, we show that a modification of our thresholding based method leads to significant improvement<lb>in recovery error (of the subspace) even in the presence of a large fraction of outliers.",
    "creator" : "LaTeX with hyperref package"
  }
}