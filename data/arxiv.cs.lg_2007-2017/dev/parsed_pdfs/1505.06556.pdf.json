{
  "name" : "1505.06556.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Differentially Private Distributed Online Learning",
    "authors" : [ "Chencheng Li" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n50 5.\n06 55\n6v 2\n[ cs\n.L G\n] 2\n3 Ju\nn 20\n15 IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. X, NO. X, JANUARY 20XX 1\nIndex Terms—Distributed Optimization, Online Learning, Differential Privacy, offline learning, mini-batch,\n✦"
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "As the Internet develops rapidly, increasingly more information is put online. For example, in daily life, tens of millions of people on Facebook often share their photos on personal pages and post stories of life in the comments, which makes Facebook process a large scale of data every second. Processing such a large scale of data in an efficient way is a challenging issue. In addition, as an online interaction platform, Internet should offer people a real-time service. This makes Internet companies (e.g., Google, Facebook and YouTube) have to response and update their systems in real time. To provide better services, they need to learn and predict the user behavior based on the past information of users. Hence, the notion “online learning” was introduced by researchers. In early stages, most online learning algorithms proceed in a centralized approach. However, as the data volume grows exponentially large in Big Data era, typical centralized online learning algorithms are no longer capable of processing such large-scale and high-rate online data. Besides, online data collection is inherently decentralized because data sources are often widely distributed in different geographical locations. So it is much more natural to develop a distributed online learning algorithm (DOLA) to solve the problem.\nDuring the learning process, sharing information may leads to privacy breaches. For instance, the hospitals in a city want to conduct a survey (can be regarded as a learning process) of the diseases that citizens are susceptible to. To protect the sensitive information of patients, the hospitals obviously can’t release their cases of illness. Instead, each hospital just can share some limited information with other\n• Chencheng Li, †Corresponding author of this paper, P. Zhou and T. Jiang are with the School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan 430074, China. E-mail: lichencheng@hust.edu.cn, panzhou@hust.edu.cn, tao.jiang@ieee.org • Gong Chen is with the School of Electrical and Computer Engineering, Georgia Tech. E-mail: gong.chen@gatech.edu\nManuscript received XXXXX; revised XXXXX.\nhospitals. However, different patient samples lead to different results. Through analyzing the results, the adversary is able to obtain some sensitive information about certain patients whose cases are only included in one hospital. Faced with this kind of privacy breach, the problem is how we can preserve the privacy of participants in the survey without significantly affecting the accuracy of the survey. To solve this class of problems, we urge to propose a privacypreserving algorithm, which not only effectively processes distributed online learning, but also protects the privacy of the learners.\nIn this paper, we propose a differentially private distributed online learning algorithm with decentralized learners and data sources. The algorithm addresses two issues: 1) distributed online learning; 2) privacy-preserving guarantees. Specifically, we use distributed convex optimization as the distributed online learning model, while use differential privacy [1] to protect the privacy.\nDistributed convex optimization is considered as a consensus problem [2]. To solve this problem, some related works [3, 4, 5] have been done. These papers considered a multiagents network system, where they studied distributed convex optimization for minimizing a sum of convex objective functions. For the convergence of their algorithms, each agent updates the iterates with usual convex optimization method and communicates the iterates to its neighbors. To achieve this goal, a time-variant communication matrix is used to conduct the communications among the agents. The time-variant communication matrix makes the distributed optimization algorithm converge faster and better than the fixed one used in [6]. For our work, the first issue is how the DOLA performs compared with the centralized algorithm. To this end, we use some results of the above works to compute the regret bounds of our DOLA.\nDifferential privacy [1] is a popular privacy mechanism to preserve the privacy of the learners. A lot of progress has been made on differential privacy. This mechanism prevents the adversary from gaining any meaningful information of any individuals. This privacy-preserving method is scalable\nfor large and dynamic dataset. Specifically, it can provide the rigorous and quantitative demonstrations for the risk of a privacy breach in statistical learning algorithms. Many privacy-preserving algorithms [7, 8, 9] have been proposed to use differential privacy to protect sensitive information in the centralized offline learning framework. However, in the distributed learning framework, there is seldom research effort.\nFurthermore, our differentially private DOLA can be used to achieve fast convergence rates for differentially private distributed offline learning algorithm based on [10]. Since the offline learning algorithm has access to all data, the technique of mini-batch [11] is used to reduce the high variance of the differentially private offline learning algorithm. Motivated by [10] and [11], we try to obtain a good utility of the distributed offline learning algorithm while protect the privacy of the learners. More importantly, our differentially private distributed offline learning algorithm guarantees the same level of privacy as the DOLA with less random noise and achieves fast convergence rate.\nFollowing are the main contributions of this paper:\n• We present a DOLA (i.e., Algorithm 1), where each learner updates its learning parameter based on local data source and exchanges information with neighbors. We respectively obtain the classical regret boundsO( √ T ) [12] andO(log T ) [13] for convex and\nstrongly convex objective functions for the algorithm. • To protect the privacy of learners, we make our\nDOLA guarantee ǫ-differential privacy. Interestingly, we find that the private regret bounds has the same order of O( √ T ) and O(log T ) with the non-private ones, which indicates that guaranteeing differential privacy in the DOLA do not significantly hurt the original performance. • We use the differentially private DOLA with good regret bounds to solve differentially private distributed offline learning problems (i.e., Algorithm 2) for the first time. We make Algorithm 2 have tighter utility guarantees than the existing state-of-the-art results while guarantee ǫ-differential privacy. • We use mini-batch to reduce high variance of the differentially private distributed offline learning algorithm and demonstrate that the algorithm using mini-batch guarantees the same level of privacy with less noise.\nThe rest of the paper is organized as follows. Section 2 discusses some related works. Section 3 presents preliminaries for the formal distributed online learning. Section 4 proposes the differentially private distributed online learning algorithm. We discuss the privacy analysis of our DOLA in Section 4.1 and discuss the regret bounds in Section 4.2. In Section 5, we discuss the application of the DOLA to the differentially private distributed offline learning algorithm. Section 5.1 and 5.2 discuss the privacy and the regret respectively. In Section 6, we present simulation results of the proposed algorithms. Finally, Section 7 concludes the paper."
    }, {
      "heading" : "2 RELATED WORK",
      "text" : "Jain et al. [7] studied the differentially private centralized online learning. They provided a generic differentially\nprivate framework for online algorithms. They showed that using their generic framework, Implicit Gradient Descent (IGD) and Generalized Infinitesimal Gradient Ascent (GIGA) can be transformed into differentially private online learning algorithms. Their work motivates our study on the differentially private online learning in distributed scenarios.\nRecently, growing research effort has been devoted to distributed online learning. Yan et al. [6] has proposed a DOLA to handle the decentralized data. A fixed network topology was used to conduct the communications among the learners in their system. They analyzed the regret bounds for convex and strongly convex functions respectively. Further, they studied the privacy-preserving problem, and showed that the communication network made their algorithm have intrinsic privacy-preserving properties. Worse than differential privacy, their privacy-preserving method cannot protect the privacy of all learners absolutely. Because their privacy-preserving properties depended on the connectivity between two nodes, however, all the nodes cannot have the same connectivity in a fixed communication matrix. Besides, Huang et al. [14] is closely related to our work. In their paper, they presented a differentially private distributed optimization algorithm. While guaranteed the convergence of the algorithm, they used differential privacy to protect the privacy of the agents. Finally, they observed that to guarantee ǫ-differential privacy, their algorithm had the accuracy of the order of O( 1\nǫ2 ). Comparing to this\naccuracy, we obtain not only O( 1 ǫ2 ) rates for convex functions, but also O(1 ǫ ) rates for strongly convex functions, if our regret bounds of the differentially private DOLA are converted to convergence rates\nThe method to solve distributed online learning was pioneered in distributed optimization. Hazan has studied online convex optimization in his book [15]. They proposed that the framework of convex online learning is closely tied to statistical learning theory and convex optimization. Duchi et al. [16] developed an efficient algorithm for distributed optimization based on dual averaging of subgradients method. They demonstrated that their algorithm could work, even the communication matrix is random and not fixed. Nedic and Ozdaglar [4] considered a subgradient method for distributed convex optimization, where the functions are convex but not necessarily smooth. They demonstrated that a time-variant communication could ensure the convergence of the distributed optimization algorithm. Ram et al. [3] tried to analyze the influence of stochastic subgradient errors on distributed convex optimization based on a time-variant network topology. They studied the convergence rate of their distributed optimization algorithm. Our work extends the works of Nedic and Ozdaglar [4] and Ram et al. [3]. All these papers have made great contributions to distributed convex optimization, but they did not consider the privacy-preserving problem.\nAs for the study of differential privacy, there has been much research effort being devoted to how differential privacy can be used in existing learning algorithms. For example, Chaudhuri et al. [8] presented the output perturbation and objective perturbation ideas about differential privacy in empirical risk minimization (ERM) classification. They achieved a good utility for ERM algorithmwhile guaranteed\nǫ-differential privacy. Rajkumar and Agarwal [17] extended differentially private ERM classification [8] to differentially private ERM multiparty classification. More importantly, they analyzed the sequential and parallel composability problems while the algorithm guaranteed ǫ-differential privacy. Bassily et al. [18] proposed more efficient algorithms and tighter error bounds for ERM classification on the basis of [8].\nSome papers have discussed the application of online learning with good regret to offline learning. Kakade and Tewari [10] proposed some properties of online learning algorithms if the loss function is Lipschitz and strongly convex. They found that recent online algorithms with logarithmic regret guarantees could help to achieve fast convergence rates for the excess risk with high probability. Subsequently, Jain et al. [7] use the results in [10] to analyze the utility of differentially private offline learning algorithms."
    }, {
      "heading" : "3 PRELIMINARIES",
      "text" : "Notation: Upper case letters (e.g., A or W ) denote matrices or data sets, while lower case letters (e.g., a or w) denote elements of matrices or column vectors. For instance, we denote the i-th learners parameter vector at time t by wit. w[j] denotes the j-th component of a vector w of length N . aij denotes the (i, j)-th element ofA. Unless special remark, ‖·‖ denotes the Euclidean norm ‖w‖ := √∑ i w[i]\n2 and 〈·, ·〉 denotes the inner product 〈x, y〉 = xTy. αt denotes the stepsize.\nCentralized Online Learning: Given the information of the correct results to previous predictions, online learning aims at making a sequence of predictions. Online learning algorithms proceed in rounds. At round t, the learner gets a question xt, taken from a convex set X and should give an answer denoted by pt to this question. Finally, the correct answer yt is given to be compared with pt. Specifically, in online regression problems, xt denotes a vector of features, then pt ← 〈wt, xt〉 is a sequence of linear predictions, and comparing pt with yt leads to the loss function ℓ (wt, xt, yt) (e.g., ℓ (wt, xt, yt) = |〈w, xt〉 − yt|). We let ft(w) := ℓ(w, xt, yt), which is obviously a convex function. According to the definition of online learning regret, the goal of online learning model is to minimize the function:\nRC = T∑\nt=1\nft(wt)− min w∈W\nT∑\nt=1\nft(w), (1)\nwhere W ⊆ Rn. In this paper, distributed online learning model is developed on the basis of the above description. Distributed Convex Optimization: Besides basic assumptions for datasets and objective functions, how conducting the communications among the distributed learners is critical to solve the distributed convex optimization problem in our work. Since the learners exchange information with neighbors while they update local parameters with subgradients, a time-variantm-by-m doubly stochastic matrix At is proposed to conduct the communications. At has a few properties: 1) all elements of At are non-negative and the sum of each row or column is one; 2) aij(t) > 0 means there exists a communication between the i-th and\nj-th learners at round t, while aij(t) = 0 means noncommunication between them; 3) there exists a constant η, 0 < η < 1, such that aij(t) > 0 implies that aij(t) > η.\nFor distributed convex optimization, two assumptions must be made. First, we make the following assumption on the dataset W and the cost functions f it .\nAssumption 1. The set W and the cost functions f it are such that\n(1) The set W is closed and convex subset of Rn. Let\nR ∆ = sup x,y∈W ‖x− y‖ denote the diameter of W .\n(2) The cost functions f it are strongly convex with modulus λ ≥ 0. For all x, y ∈ W , we have 〈 ∇f it , y − x 〉 ≤ f it (y)− f it (x)− λ\n2 ‖y − x‖2. (2)\n(3) The subgradients of f it are uniformly bounded, i.e., there exists L > 0 , for all x ∈ W , we have\n∥∥∇f it (x) ∥∥ ≤ L. (3)\nAssumption (1) guarantees that there exists an optimal solution in our algorithm. Assumptions (2) and (3) help us analyze the convergence of our algorithm.\nTo recall, the learners communicate with neighbors based on the matrix ofAt. Each learner directly or indirectly influences other learners. For a clear description, we denote the communication graph for a learner i at round t by\nG(t)i = {(i, j) : aij(t) > 0}, (4)\nwhere\naij(t) ∈ At.\nIn our algorithm, each learner computes a weighted average [3] of the m learners’ parameters. For the convergence of the DOLA, the weighted average should make each learner have “equal” influence on other learners in long rounds. Then, we make the following assumption about the properties of At.\nAssumption 2. For an arbitrary learner i, there exist a minimal scalar η, 0 < η < 1, and a scalar N such that\n(1) aij(t) > 0 for (i, j) ∈ CG(t+ 1), (2) ∑m j=1 aij(t) = 1 and ∑m i=1 aij(t) = 1, (3) aij(t) > 0 implies that aij(t+ 1) ≥ η, (4) The graph ∪k=1,...NG(t + k)i is strongly connected\nfor all k.\nHere, Assumptions (1) and (2) state that each learner computes a weighted average of the parameters shown in Algorithm 1. Assumption (3) ensures that the influences among the learners are significant. Assumptions (2) and (4) ensure that the m learners are equally influential in a long run. Assumption 2 is crucial to minimize the regret bounds in distributed scenarios.\nDifferential Privacy: Dwork [1] proposed the definition of differential privacy for the first time. Differential privacy makes a data miner be able to release some statistic of its database without revealing sensitive information about a particular value itself. In this paper, we use differential privacy to protect the privacy of learners and give the following definition.\nDefinition 1. Let A denote our differentially private DOLA. Let X = 〈 xi1, x i 2, ..., x i T 〉 be a sequence of questions taken from an arbitrary learner’s local data source. Let W = 〈 wi1, w i 2, ..., w i T 〉 be a sequence of T outputs of the learner and W = A(X ). Then, our algorithm A is ǫ-differentially private if given any two adjacent question sequences X and X ′ that differ in one question entry, the following holds:\nPr [A (X ) ∈ W ] ≤ eǫ Pr [A (X ′) ∈ W ] . (5) This inequality guarantees that whether or not an individual participates in the database, it will not make any significant difference on the output of our algorithm, so the adversary is not able to gain useful information about the individual."
    }, {
      "heading" : "4 DIFFERENTIALLY PRIVATE DISTRIBUTED ONLINE LEARNING",
      "text" : "For differentially private distributed online learning, we assume to have a system ofm online learners, each of them has the independent learning ability. The i-th learner updates its local parameter wit based on its local data points ( xit, y i t )\nwith i ∈ [1,m]. The learner makes the prediction 〈 wit, x i t 〉 at round t , then the loss function f it (w) := ℓ(w, x i t, y i t) is obtained. Even though the m learners are distributed, each learner exchanges information with neighbors. Based on the time-variant matrix At, the learners communicate with different sets of their neighbors at different rounds, which makes them indirectly influenced by other data sources. Specifically, for a learner i, at each round t, it first gets the exchanged parameters and computes the weighted average of them, then updates the local parameter wit with respect to the weighted average bit and the subgradient g i t, finally broadcasts the new local parameter added with a random noise to its neighbors G(t)i. We summarize the algorithm in Algorithm 1.\nBefore we discuss the privacy and utility of Algorithm 1, the regret in distributed setting is given in the following definition.\nDefinition 2. In an online learning algorithm, we assume to have m learners using local data sources. Each learner updates its parameter through a weighted average of the received parameters. Then, we measure the regret of the algorithm as\nRD = T∑\nt=1\nm∑\ni=1\nf it (w j t )− min\nw∈W\nT∑\nt=1\nm∑\ni=1\nf it (w). (6)\nObviously, ft(wt) in (1) is changed to the sum of m learners’ loss function ∑m i=1 f i t (w j t ) in (6). In centralized online learning algorithm, N data points need T = N rounds to be finished, while the distributed algorithm can handlesm×N data points over the same time period. Notice that RD is computed with respect to an arbitrary learner’s parameter wjt [6]. This states that single one learner can measure the regret of the whole system based on its local parameter, even though the learner do not handle all data in the system.\nNext, we analyze the privacy of Algorithm 1 in Section 4.1 and give the regret bounds in Section 4.2.\nAlgorithm 1 Differentially Private Distributed Online Learning\n1: Input: Cost functions f it (w) := ℓ(w, x i t, y i t), i ∈ [1,m]\nand t ∈ [0, T ] ; initial points w10 , ..., wm0 ; double stochastic matrix At = (aij(t)) ∈ Rm×m; maximum iterations T .\n2: for t = 0, ..., T do 3: for each learner i = 1, ...,m do 4: bit = m∑ j=1 aij(t+ 1)(w j t + σ j t ), where σ j t is a Laplace\nnoise vector in Rn\n5: git ← ∇f it (bit) 6: wit+1 = Pro[b i t − αt+1 · git] (Projection onto W ) 7: broadcast the output (wit+1 + σ i t+1) to G(t)i 8: end for 9: end for"
    }, {
      "heading" : "4.1 Privacy Analysis",
      "text" : "As explained previously, exchanging information may cause some privacy breaches, so we have to use differential privacy to protect the privacy. In the view of Algorithm 1, all learners exchange their weighted parameters with neighbors at each round. For preserving-privacy, every exchanged parameter should be made to guarantee differential privacy. To achieve this target, a random noise is added to the parameterwit (see step 7 in Algorithm 1). This method to guarantee differential privacy is known as output perturbation [8]. We have known where to add noise, next we study how much noise to be added.\nDifferential privacy aims at weakening the significantly difference between A (X) and A (X ′). Thus, to show differential privacy, we need to know that how “sensitive” the algorithm A is. Further, according to [1], the magnitude of the noise depends on the largest change that a single entry in data source could have on the output of Algorithm 1; this quantity is referred to as the sensitivity of the algorithm. Then, we define the sensitivity of Algorithm 1 in the following definition.\nDefinition 3 (Sensitivity). Recall in Definition 1, for any X and X ′, which differ in exactly one entry, we define the sensitivity of Algorithm 1 at t-th round as\nS(t)= sup X ,X ′\n‖A (X )−A (X ′)‖1. (7)\nThe above norm is L1-norm. According to the notion of sensitivity, we know that higher sensitivity leads to more noise if the algorithm guarantees the same level of privacy. By bounding the sensitivity S(t), we determine the magnitude of the random noise to guarantee ǫ-differential privacy. We compute the bound of S(t) in the following lemma.\nLemma 1. Under Assumption 1, if the L1-sensitivity of the algorithm is computed as (7), we obtain\nS(t) ≤ 2αt √ nL, (8)\nwhere n denotes the dimensionality of vectors.\nProof. Recall in Definition 1, X and X ′ are any two data sets differing in one entry. wit is computed based on the data set\nX while wit ′ is computed based on the data setX ′. Certainly,\nwe have ‖A (X )−A (X ′)‖1 = ∥∥∥wit − wit ′ ∥∥∥ 1 .\nFor datasets X and X ′ we have wit = Pro [ bit−1 − αtgit−1 ] and wit ′ = Pro [ bit−1 − αtgit−1 ′] .\nThen, we have ∥ ∥ ∥w i\nt − wit ′ ∥ ∥ ∥ 1 = ∥ ∥ ∥Pro [ b i t−1 − αtgit−1 ] − Pro [ b i t−1 − αtgit−1 ′ ]∥ ∥ ∥ 1\n≤ ∥ ∥ ∥(b i t−1 − αtgit−1)− (bit−1 − αtgit−1 ′ ) ∥ ∥ ∥ 1\n= αt ∥ ∥ ∥g i t−1 − git−1 ′ ∥ ∥ ∥ 1 ≤ αt (∥ ∥ ∥g i\nt−1 ∥ ∥ ∥ 1 + ∥ ∥ ∥g i t−1 ′ ∥ ∥ ∥ 1 )\n≤ αt √ n (∥ ∥ ∥g i\nt−1 ∥ ∥ ∥ 2 + ∥ ∥ ∥g i t−1 ′ ∥ ∥ ∥ 2 )\n≤ 2αt √ nL. (9)\nBy Definition 3, we know\nS(t) ≤ ∥∥∥wit − wit ′ ∥∥∥ 1 . (10)\nHence, combining (9) and (10), we obtain (8).\nWe next determine the magnitude of the added random noise due to (10). In step 7 of Algorithm 1, we use σ to denote the random noise. σ ∈ Rn is a Laplace random noise vector drawn independently according to the density function:\nLap (x|µ) = 1 2µ exp\n( −|x|\nµ\n) , (11)\nwhere µ = S (t)/ǫ. We let Lap (µ) denote the Laplace distribution. (8) and (10) show that the magnitude of the added random noise depends on the sensitivity parameters: ǫ, the stepsize αt, the dimensionality of vectors n, and the bounded subgradient L.\nLemma 2. Under Assumption 1 and 2, at the t-th round, the i-th online learner’s output of A, w̃it, is ǫ-differentially private.\nProof. Let w̃it = w i t + σ i t and w̃ i t ′ = wit + σ i t , then by the definition of differential privacy (see Definition 1), w̃it is ǫdifferentially private if\nPr[w̃it ∈ W ] ≤ eǫ Pr[w̃it ′ ∈ W ]. (12)\nFor w ∈ W , we obtain\nPr ( w̃it )\nPr ( w̃it ′ ) =\nn∏\nj=1\n  exp ( − ǫ|w i t [j]−w[j]| S(t) )\nexp ( − ǫ|w i t\n′[j]−w[j]| S(t)\n)  \n= n∏\nj=1\nexp\n  ǫ (∣∣∣wit ′ [j]− w[j] ∣∣∣− ∣∣wit[j]− w[j] ∣∣ )\nS (t)\n\n\n≤ n∏\nj=1\nexp\n  ǫ ∣∣∣wit ′ [j]− wit[j] ∣∣∣ S (t)  \n= exp\n  ǫ ∥∥∥wit ′ − wit ∥∥∥ 1\nS (t)\n\n\n≤ exp (ǫ) , (13)\nwhere the first inequality follows from the triangle inequality, and the last inequality follows from (10).\nMcSherry [19] has proposed that the privacy guarantee does not degrade across rounds as the samples used in the rounds are disjoint. In Algorithm 1, at each round, each learner is given a question xit, then makes the prediction wit. Finally, given the correct answers y i t , each learner can obtain the loss functions f it (w) := ℓ(w, x i t, y i t). In this\nprocess, we regard ( xit, y i t ) as a sample. During the T rounds of Algorithm 1, these samples are disjoint. Therefore, as Algorithm 1 runs, the privacy guarantee will not degrade. Then we obtain the following theorem.\nTheorem 1 (Parallel Composition). On the basis of Definition 1 and 3, under Assumption 1 and Lemma 2, our DOLA (see Algorithm 1) is ǫ-differentially private.\nProof. This proof follows from the theorem 4 of [19]. The probability of the output W (defined in Definition 1) is\nPr [A (X) ∈ W ] = T∏\nt=1\nPr[A(X)t ∈ W ]. (14)\nUsing the definition of differential privacy for each output (see Lemma 2), we have\nT∏\nt=1\nPr[A(X )t ∈ W ]\n≤ T∏\nt=1\nPr[A(X ′)t ∈ W ]× T∏\nt=1\nexp (ǫ× |Xt ⊕X ′t|)\n≤ T∏\nt=1\nPr[A(X ′)t ∈ W ]× exp (ǫ× |X ⊕ X ′|) , (15)\nwhere |X ⊕ X ′| denotes the different entry between X and X ′.\nIntuitively, the above inequality states that the ultimate privacy guarantee is determined by the worst of the privacy guarantees, not the sum T ǫ.\nCombining (8), (11) and Lemma 2, we find that if each round of Algorithm 1 has the privacy guarantee at the same level (ǫ-differential privacy), the magnitude of the noise will decrease as Algorithm 1 runs. That is because the magnitude of the noise depends on the stepsize αt+1, which decreases as the subgradient descends."
    }, {
      "heading" : "4.2 Regret Analysis",
      "text" : "The regret of online learning algorithm represents a sum of mistakes, which are made by the learners during the learning and predicting process. That means if Algorithm 1 runs better and faster, the regret of our distributed online learning algorithm will be lower. In other words, faster convergence rate ensures that the m learners make less mistakes and predict more accurately. Hence, we bound the regret RD through the convergence of w i t in Algorithm 1.\nTo analyze the convergence of wit, we consider the behavior of the time-variant matrix At. Let At be the matrix with (i, j)-th equal to aij (t) in Assumption 2. According to the assumption, At is a doubly stochastic. As mentioned previously, some related works have studied the matrix\nconvergence ofAt. For simplicity, we use one of these results to obtain the following lemma.\nLemma 3 ([3]). We suppose that at each round t, the matrix At satisfies the description in Assumption 2. Then, we have\n(1) lim k→∞\nφ(k, s) = 1 m eeT for all k, s ∈ Z with k ≥ s,\nwhere\nφ(k, s) = A(k)A(k − 1)A · · ·A(s+ 1). (16)\n(2) Further, the convergence is geometric and the rate of convergence is given by\n∣∣∣∣ [ φ(k, s)ij − 1\nm\n]∣∣∣∣ ≤ θβk−s, (17)\nwhere\nθ = ( 1− η\n4m2\n)−2 β = ( 1− η\n4m2\n) 1 N\n.\nLemma 3 will be repeatedly used in the proofs of the following lemmas. Next, we study the convergence of Algorithm 1 in details. We use subgradient descent method to make wit move forward to the theoretically optimal solution. Based on this method, we know that wit+1 is closer to the optimal solution than wit. Besides, we also want to know the difference between two arbitrary learners, but computing\nthe norms ∥∥∥wit − wjt ∥∥∥ makes no sense. Alternatively, we study the behavior of ∥∥wt − wit ∥∥, where for all t, wt is defined by\nwt = 1\nm\nm∑\ni=1\nwit. (18)\nIn the following lemma, we give the bound of∥∥wt − wit ∥∥.\nLemma 4. Under Assumption 1 and 2, for all i ∈ {1, ...,m} and t ∈ {1, ..., T }, we have\n∥∥wt − wit ∥∥ ≤ mLθ\nt−1∑\nk=1\nβt−kαk+θ t−1∑\nk=1\nβt−k m∑\ni=1\n∥∥σik ∥∥+2αtL.\n(19)\nProof. For simplicity, we first study ∥∥wt+1 − wit+1 ∥∥ instead. Define that\ndit+1 = w i t+1 − bit, (20)\nwhere bit is defined in step 4 of Algorithm 1. We next estimate the norm of dit for any t and i. According to the famous non-expansive property of the Euclidean projection onto a closed and convex W , for all x ∈ W , we have\n‖Pro[x]‖ ≤ ‖x‖ . (21)\nBased on (20) and (21), using the definition of bit and g i t in Algorithm 1, we obtain ∥∥dit+1 ∥∥ = ∥∥Pro[bit − αt+1git]− bit ∥∥\n≤ αt+1 ∥∥git ∥∥ ≤ αt+1L. (22)\nWe use (3) in the last step.\nWe conduct the mathematical induction for (20) and use the matrices φ(k, s) defined in (16). We then obtain\nwit+1 =d i t+1 +\nt∑\nk=1\n  m∑\nj=1\n[φ(t+ 1, k)]ijd j k\n \n+ t∑\nk=1\n  m∑\nj=1\n[φ(t+ 1, k)]ijσ j k\n . (23)\nUsing (18) and (20), we rewrite wt+1 as follows\nwt+1 = 1\nm\n( m∑\ni=1\nbit + m∑\ni=1\ndit+1\n)\n= 1\nm\n  m∑\nj=1\nm∑\ni=1\naij(t+ 1)(w i t + σ i t) +\nm∑\ni=1\ndit+1\n \n= 1\nm\n  m∑\ni=1\n  m∑\nj=1\naij(t+ 1)  (wit + σit) + m∑\ni=1\ndit+1\n  .\n(24)\nAccording to Assumption 2, we know ∑m\nj=1 aij(t+ 1) = 1, then simplify wt+1 as\nwt+1 = 1\nm\n( m∑\ni=1\n(wit + σ i t) +\nm∑\ni=1\ndit+1\n)\n= wt + 1\nm\nm∑\ni=1\n(σit + d i t+1). (25)\nFinally, we have\nwt+1 = 1\nm\nt∑\nk=1\nm∑\ni=1\nσik + 1\nm\nt+1∑\nk=1\nm∑\ni=1\ndik. (26)\nUsing (23) and (26), we obtain\n∥∥wt+1 − wit+1 ∥∥= ∥∥∥∥∥ 1 m t∑\nk=1\nm∑\ni=1\nσik + 1\nm\nt+1∑\nk=1\nm∑\ni=1\ndik\n−\n dit+1 + t∑\nk=1\n  m∑\nj=1\n[φ(t+ 1, k)]ijd j k\n\n\n+ t∑\nk=1\n  m∑\nj=1\n[φ(t + 1, k)]ijσ j k\n\n\n\n ∥∥∥∥∥∥\n= ∥∥∥∥∥ t∑\nk=1\nm∑\ni=1\n( 1\nm − [φ (t+ 1, k)]ij\n) ( σik + d i k )\n+\n( 1\nm\nm∑\ni=1\ndit+1 − dit+1 )∥∥∥∥∥ . (27)\nAccording to the triangle inequality in Euclidean geometry, we further have\n∥∥wt+1 − wit+1 ∥∥ ≤ t∑\nk=1\nm∑\ni=1\n∣∣∣∣ 1\nm − [φ (t+ 1, k)]ij\n∣∣∣∣ (∥∥σik ∥∥+ ∥∥dik ∥∥)\n+ 1\nm\nm∑\ni=1\n∥∥dit+1 ∥∥+ ∥∥dit+1 ∥∥ . (28)\nUsing the bound of ∥∥dit+1 ∥∥ in (22) and (17) in Lemma 3, we have\n∥∥wt+1 − wit+1 ∥∥ ≤mLθ t∑\nk=1\nβt+1−kαk\n+ θ t∑\nk=1\nβt+1−k m∑\ni=1\n∥∥σik ∥∥+ 2αt+1L. (29)\nFinally, we obtain (18) based on (29)\nNext we bound the distance ‖wt+1 − w‖2 for an arbitrary w ∈ W . This bound together with Lemma 4 helps to analyze the convergence of our algorithm. In following Lemma 5, 6 and Theorem 2, we denote ft =∑m i=1 f i t for simplicity.\nLemma 5. Under Assumption 1 and 2, for any w ∈ W and for all t, we have\n‖wt+1 − w‖ ≤ (1 + 2αt+1L+ 2L + 2\nm\nm∑\ni=1\n∥∥σit ∥∥\n−2λ) ‖wt − w‖ − 2\nm (ft (wt)− ft (w))\n+ 4L 1\nm\nm∑\ni=1\n∥∥wt − wit ∥∥\n+ ∥∥∥∥∥ 1 m m∑\ni=1\n( σit + d i t+1 ) ∥∥∥∥∥ 2 . (30)\nProof. For any w ∈ W and all t, we use (25) to have\n‖wt+1 − w‖2 = ∥∥∥∥∥wt + 1 m m∑\ni=1\n( σit + d i t+1 ) − w ∥∥∥∥∥\n= ‖wt − w‖2 + ∥∥∥∥∥ 1 m m∑\ni=1\n( σit + d i t+1 ) ∥∥∥∥∥ 2\n+ 2\n〈 1\nm\nm∑\ni=1\n( σit + d i t+1 ) , wt − w 〉 . (31)\nBased on\n‖wt+1 − w‖ − ‖wt − w‖ ≤ (‖wt+1 − w‖ − ‖wt − w‖) (‖wt+1 − w‖+ ‖wt − w‖) = ‖wt+1 − w‖2 − ‖wt − w‖2, (32)\nwe can transform (32) to the following inequality:\n‖wt+1 − w‖ ≤ ∥∥∥∥∥wt + 1 m m∑\ni=1\n( σit + d i t+1 ) − w ∥∥∥∥∥\n= ‖wt − w‖+ ∥∥∥∥∥ 1 m m∑\ni=1\n( σit + d i t+1 ) ∥∥∥∥∥ 2\n+ 2\n〈 1\nm\nm∑\ni=1\n( σit + d i t+1 ) , wt − w 〉 . (33)\nNow we pay attention to\n2\n〈 1\nm\nm∑\ni=1\n( σit + d i t+1 ) , wt − w\n〉\n= − 2 m\nm∑\ni=1\n〈 git, wt − w 〉 + 2\nm\nm∑\ni=1\n〈 git + σ i t + d i t+1, wt − w 〉 ].\n(34)\nFirst, we compute the inner product:\n− 2 m\nm∑\ni=1\n〈 git, wt − w 〉 .\nUsing (2) and (3) in Assumption 1, we first obtain\n− 〈 git, wt − w 〉 = − 〈 git, wt − wit 〉 − 〈 git, w i t − w 〉 ≤ ∥∥git ∥∥ ∥∥wt − wit ∥∥+ f it (w)− f it (wit)− λ ∥∥wit − w ∥∥ = ∥∥git ∥∥ ∥∥wt − wit ∥∥+ f it (wt)− f it (wit)− λ ∥∥wit − w ∥∥\n+ f it (w) − f it (wt) ≤ ∥∥git ∥∥ ∥∥wt − wit ∥∥+ 〈 git, wt − wit 〉 − λ ∥∥wit − wt ∥∥\n− λ ∥∥wit − w ∥∥+ f it (w) − f it (wt) ≤ (∥∥git ∥∥+ ∥∥git ∥∥) ∥∥wt − wit ∥∥\n− λ ‖wt − w‖+ f it (w) − f it (wt) ≤ 2L ‖wt − w‖ − λ ‖wt − w‖ − ( f it (wt)− f it (w) ) . (35)\nAdding up the above inequality over i = 1, ...,m, we can have\n− 2 m\nm∑\ni=1\n〈 git, wt − w 〉\n≤ 4L m\nm∑\ni=1\n∥∥wt − wit ∥∥− 2λ ‖wt − w‖\n− 2 m (ft(wt)− ft(w)) . (36)\nThen, compute the other inner product:\n2\nm\nm∑\ni=1\n〈 git + σ i t + d i t+1, wt − w 〉\n≤ 2 m\nm∑\ni=1\n∥∥git + σit + dit+1 ∥∥ ‖wt − w‖\n≤ 2 m\nm∑\ni=1\n(∥∥git ∥∥+ ∥∥σit ∥∥+ ∥∥dit+1 ∥∥) ‖wt − w‖\n≤ 2 m\nm∑\ni=1\n( αt+1L+ L+ ∥∥σit ∥∥) ‖wt − w‖ . (37)\nIn the last inequality, we use (3) and (16). Combing (33)-(37), we complete the proof.\nBased on Lemma 4 and 5, we give the general regret bound in the following lemma. For simplicity, we let ft =∑m\ni=1 f i t .\nLemma 6. We let w∗ denote the optimal solution computed in hindsight. The regret RD of Algorithm 1 is given by:\nT∑\nt=1\n[ ft(w i t)− ft(w∗) ]\n≤ ( mRL+ 3βθm2L2\n1− β + 13 2 mL2\n) T∑\nt=1\nαt\n+\n( 3βθmL\n1− β + 2L+ 1 2m\n) T∑\nt=1\nm∑\ni=1\n∥∥σit ∥∥\n+ mR\n2 . (38)\nProof. We use (30) in Lemma 5, which contains the term ft(wt) − ft(w), and set w = w∗. Then, we rearrange (30) to have\nft(w i t)− ft(w∗)\n= ft(wt)− ft(w∗) + ft(wit)− ft(wt)\n≤ m 2 (1− 2λ+ 2αt+1L+ 2L+ 2 m\nm∑\ni=1\n∥∥σit ∥∥) ‖wt − w∗‖\n+ 2L m∑\ni=1\n∥∥wt − wit ∥∥− m\n2 ‖wt+1 − w∗‖\n+ m\n2 ∥∥∥∥∥ 1 m m∑\ni=1\n( σit + d i t+1 ) ∥∥∥∥∥ 2 +mL ∥∥wt − wit ∥∥ . (39)\nPlug in the bound of ∥∥wt − wit ∥∥ in Lemma 4, we rewrite (39) as\nft(w i t)− ft(w∗)\n≤ m 2 (1− 2λ+ 2αt+1L+ 2L+ 2 m\nm∑\ni=1\n∥∥σit ∥∥) ‖wt − w∗‖\n− m 2 ‖wt+1 − w∗‖+ 1 2m ∥∥∥∥∥ m∑\ni=1\n( σit + d i t+1 ) ∥∥∥∥∥ 2 + 6αtmL 2\n+ 3θm2L2 t−1∑\nk=1\nβt−kαk + 3θmL t−1∑\nk=1\nβt−k m∑\ni=1\n∥∥σik ∥∥. (40)\nSumming up (40) over t = 1, ..., T , we have\nT∑\nt=1\n[ft(wt)− ft(w∗)]\n≤ m 2\n[ T∑\nt=1\n(1− 2λ+ 2αt+1L+ 2L+ 2\nm\nm∑\ni=1\n∥ ∥ ∥σ i\nt\n∥ ∥ ∥) ‖wt −w∗‖\n︸ ︷︷ ︸\n∈S1\n− T∑\nt=1\n‖wt+1 − w∗‖ ]\n︸ ︷︷ ︸\n∈S1\n+ T∑\nt=1\n[\n1\n2m ∥ ∥ ∥ ∥ ∥ m∑\ni=1\n(\nσ i t + d i t+1 ) ∥ ∥ ∥ ∥ ∥ 2 + 6αtmL 2 ]\n︸ ︷︷ ︸\nS2\n+3θm2L2 T∑\nt=1\nt−1∑\nk=1\nβ t−k αk + 3θmL T∑\nt=1\nt−1∑\nk=1\nβ t−k\nm∑\ni=1\n∥ ∥ ∥σ i\nk\n∥ ∥ ∥\n︸ ︷︷ ︸\nS3\n. (41)\nRecall in Assumption 1, R be the upper bound of the diameter of W and αt+1 < αt, we compute (41) as follows\nS1 = m\n2\nT∑\nt=2\n‖wt − w∗‖ (\n2αt+1L+ 2L− 2λ+ 2 m\nm∑\ni=1\n∥ ∥ ∥σ i\nt\n∥ ∥ ∥\n)\n+ m\n2 ‖w1 − w∗‖ (1 + 2αt+1L+ 2L− 2λ +\n2\nm\nm∑\ni=1\n∥ ∥ ∥σ i\nt\n∥ ∥ ∥)\n− m 2 ‖wT+1 − w∗‖\n≤ mR 2\nT∑\nt=1\n(\n2αtL+ 2\nm\nm∑\ni=1\n∥ ∥ ∥σ i\nt\n∥ ∥ ∥\n)\n+ mR\n2 −mRT (λ− L)\n≤ R T∑\nt=1\n(\nmαtL+ m∑\ni=1\n∥ ∥ ∥σ i\nt\n∥ ∥ ∥\n)\n+ mR\n2 , (42)\nS2 = 3θm 2 L 2\nT∑\nt=1\nt−1∑\nk=1\nβ t−k αk + 3θmL\nT∑\nt=1\nt−1∑\nk=1\nβ t−k\nm∑\ni=1\n∥ ∥ ∥σ i\nk\n∥ ∥ ∥\n≤ 3θm2L2 T∑\nt=1\nαt\nT∑\nk=1\nβ k + 3θmL\nT∑\nk=1\nβ k\nT∑\nt=1\nm∑\ni=1\n∥ ∥ ∥σ i\nt\n∥ ∥ ∥\n≤ 3βθm 2L2\n1− β\nT∑\nt=1\nαt + 3βθmL\n1− β\nT∑\nt=1\nm∑\ni=1\n∥ ∥ ∥σ i\nt\n∥ ∥ ∥, (43)\nS3 = 6mL 2\nT∑\nt=1\nαt + 1\n2m\nT∑\nt=1\n∥ ∥ ∥ ∥ ∥ m∑\ni=1\n(\nσ i t + d i t+1 ) ∥ ∥ ∥ ∥ ∥ 2\n≤ 6mL2 T∑\nt=1\nαt + 1\n2m\nT∑\nt=1\n[\n(2L+ 1)\nm∑\ni=1\n∥ ∥ ∥σ i\nt\n∥ ∥ ∥+m 2 L 2 αt\n]\n≤ 13 2 mL 2\nT∑\nt=1\nαt + 1\n2m\nT∑\nt=1\n[\n(2L+ 1)\nm∑\ni=1\n∥ ∥ ∥σ i\nt\n∥ ∥ ∥\n]\n. (44)\nCombining S1, S2 and S3, we get (38).\nLemma 6 gives the regret bound with respect to the stepsize αt and the noise parameter σ i t . Further, we analyze the regret bounds for convex and strongly convex functions. Besides, we need to figure out the influence that the total noise have on the regret bounds.\nTheorem 2. Based on Lemma 6, if λ > 0 and we set αt = 1 λt , then the expected regret of our DOLA satisfies:\nE\n[ T∑\nt=1\nft(w i t)\n] − T∑\nt=1\nft(w ∗)\n≤ mL λ\n( R+ 3βθmL\n1− β + 13 2 L\n) (1 + logT )\n+\n( 3βθmL\n1− β + 2L+ 1 2m\n) 2 √ 2mnL\nλǫ (1 + logT ) +\nmR\n2 ,\n(45)\nand if λ = 0 and set αt = 1 2 √ t then\nE\n[ T∑\nt=1\nft(w i t)\n] − T∑\nt=1\nft(w ∗)\n≤ mL λ\n( R+ 3βθmL\n1− β + 13 2 L\n)(√ T − 1\n2\n)\n+\n( 3βθmL\n1− β + 2L+ 1 2m\n) 2 √ 2mnL\nǫ\n(√ T − 1\n2\n) + mR\n2 .\n(46)\nProof. First we consider αt = 1 λt , then\nT∑\nt=1\nαt = T∑\nt=1\n1\nλt =\n1\nλ\nT∑\nt=1\n1 t ≤ 1 λ (1 + logT ) . (47)\nSince σit is drawn from Lap (µ) and each component of the vector σit is independent, we have\nT∑\nt=1\nm∑\ni=1\n∥ ∥ ∥σ i\nt\n∥ ∥ ∥ = m T∑\nt=1\n‖σt‖\n= T∑\nt=1\nm\n√\n|σt [1]|2+...+|σt [n]|2\n= m √ n T∑\nt=1\n√\n|σt [j]|2, (48)\nwhere σt [j] denotes an arbitrary component of the vector σt. Under the condition, σt [j] ∼ Lap (µ), we have E [ |σt [j]|2 ] = 2µ2, then\nE\n[ T∑\nt=1\nm∑\ni=1\n∥∥σit ∥∥ ] = E [ m √ n T∑\nt=1\n√ |σt [j]|2\n]\n= T∑\nt=1\nµm √ 2n = T∑\nt=1\nS (t)m √ 2n\nǫ\n≤ 2 √ 2mnL\nǫ\nT∑\nt=1\nαt\n≤ 2 √ 2mnL\nλǫ (1 + logT ) . (49)\nThe last inequality follows form (47). Then, using (47) and (49), we get (45). If λ = 0, and we set αt =\n1 2 √ t , we have\nT∑\nt=1\nαt = T∑\nt=1\n1\n2 √ t ≤\n√ T − 1\n2 . (50)\nUsing (30), we rewrite (29) as\nE\n[ T∑\nt=1\nm∑\ni=1\n∥∥σit ∥∥ ] ≤ 2 √ 2mnL\na\n(√ T − 1\n2\n) . (51)\nNow, using (50) and (51) we get (46).\nAs expected, we respectively obtain the square root regret O( √ T ) and the logarithmic regret O(log T ) of Algorithm 1 in Theorem 2. Intuitively, except for T , the regret bounds are also with respect to the size of distributed network m. More importantly, the total noise added to the outputs has the magnitude of the same order of O( √ T ) and O(log T ). This means that guaranteeing differential privacy has no strong influence on the non-private DOLA. The reason why this happens is that the magnitude of the total noise is with respect to the stepsize αt from (29). It has the similar form as the non-private regret. Thus, the final regret bound with noise has the same order of non-private regret bound."
    }, {
      "heading" : "5 APPLICATION TO PRIVATE DISTRIBUTED OFFLINE LEARNING USING MINI-BATCH",
      "text" : "In Section 4, we proposed a differentially private DOLA with good regret bounds of O( √ T ) and O(log T ). Kakade and Tewari [10] and Jain et al. [7] have both proposed that online learning algorithms with good regret bounds can be used to achieve fast convergence rates for offline learning algorithms. Based on the analysis in [7], we exploit this application in distributed scenarios. Before that, we first discuss the private distributed offline learning using minibatch.\nIn distributed offline learning scenarios, we also assume that there are m offline learners. Each learner can obtain the labelled examples ( e.g., ( xi1, y i 1 ) , ... ( xin, y i n )) from its local data source. Differing from the distributed online learners, the offline learners have the data beforehand. Before we describe the distributed offline learning model, we should\npay attention to how the centralized offline learning model works.\nIn a centralized offline learning model, the classical method of training such a model based on labelled data is by optimizing the following problem:\nw∗ = argmin w∈Rn\n1\nn\nn∑\nk=1\nℓ (w, xk, yk) + ϕ\n2 ‖w‖2, (52)\nwhere ℓ is a convex loss function. According to the different choices of ℓ in machine learning, we can obtain different data mining algorithms. For example, Support Vector Machine (SVM) algorithm comes from ℓ (w, x, y) = max ( 1− ywTx, 0 ) and Logistic Regression algorithm comes from ℓ (w, x, y) = log ( 1 + exp ( −ywTx )) . For solving the problem in (52), stochastic gradient descent (SGD) (mentioned in [11]) was proposed. SGD updates the iterate at round t as:\nwt+1 = wt − αt+1 (∇ℓ (wt, xt, yt) + ϕwt) , (53)\nwhere this iterate is updated based on a single point (xt, yt) sampled randomly from the local data set.\nNext, based on the centralized offline learning model, we build the distributed offline learning model. In distributed model, each learner updates its parameter with subgradient as (53) does. Meanwhile, each learner must exchange information with other learners. Hence, for distributed offline learning we update the iterate as:\nwit+1 = m∑\nj=1\naij(t+ 1)w j t − αt+1 ( git + ϕw i t ) . (54)\nIn offline leaning framework, all data are available beforehand. To handle such massive training points, we use SGD with mini-batch to update the iterate. Using minibatch, we update the iterate at round t on the basis of a subset Ht of examples. This help us process multiple sampled examples instead of a single one at each round. Under this model, our offline learning algorithm runs in a parallel and distributed method. Based on mini-batch, we rewrite (54) as:\nwit+1 = m∑\nj=1\naij (t+ 1) w̃ j t − αt+1 h\n∑\n(xk,yk)∈Ht\ngik, (55)\nwhere h denotes the number of examples included in Ht and w̃jt is defined in Lemma 2. In (55), we compute an average of subgradients of h examples sampled i.i.d. from the local data source.\nAs with the DOLA, exchanging information also leads to a privacy breach in distributed offline learning. Hence, to protect the privacy, we make our distributed offline learning algorithm guarantee ǫ-differential privacy as well. The differentially private method used here is the same with that used in Algorithm 1. Furthermore, mini-batch can weaken the influence of noise on regret bounds when the algorithm guarantees differential privacy. For example, Song et al. [11] demonstrated that differentially private SGD algorithm updated with a single point has high variance and used mini-batch to reduce the variance. In this paper, we also use mini-batch to achieve the same goal.\nTo conclude, we propose a private distributed offline learning algorithm using mini-batch. The algorithm is summarized in Algorithm 2.\nAlgorithm 2 Differentially Private Distributed Offline Learning Using Mini-Batch\n1: Input: Cost functions f it (w) := ℓ(w, x i t, y i t), i ∈ [1,m]\nand t ∈ [0, T ] ; initial points w10 , ..., wm0 ; double stochastic matrix At = (aij(t)) ∈ Rm×m; maximum iterations T h .\n2: for t = 0, ..., T h\ndo 3: for each learner i = 1, ...,m do 4: bit = m∑ j=1 aij(t+ 1)(w j t + σ j t ), where σ j t is a Laplace\nnoise vector in Rn\n5: gik ← ∇f ik(bit), which is computed based on examples (xk, yk) ∈ Ht\n6: wit+1 = Pro [ bit − αt+1 ( ϕwit + 1 h\n∑ (xk,yk)∈Ht gik\n)]\n(Projection onto W ) 7: broadcast the output (wit+1 + σ i t+1) 8: end for 9: end for"
    }, {
      "heading" : "5.1 Privacy analysis for Algorithm 2",
      "text" : "Algorithm 2 guarantees the same level of privacy as Algorithm 1 does. Differing from Algorithm 1, the step 6 in Algorithm 2 computes a average of subgradients. According to the analysis of the sensitivity in Section 4.1, we easily know that the sensitivity of Algorithm 2 must be different from (8). Then, we compute new sensitivity of Algorithm 2 in the following lemma.\nLemma 7. (Sensitivity of Algorithm 2) Under Assumption 1, let all definitions made previously be used here again, the L1-sensitivity of Algorithm 2 is\nS2 (t) ≤ 2αt\n√ nL h . (56)\nWe omit the proof of Lemma 7, which follows along the lines of Lemma 1.\nObviously, Lemma 7 demonstrates that except for the parameters in (6), the magnitude of the sensitivity of Algorithm 2 is with respect to the batch size h. Comparing (56) with (8), we find that the sensitivity of Algorithm 2 is smaller than that of Algorithm 1. (11) shows that lower sensitivity leads to less added noise. So Algorithm 2 can add less random noise to its output while it guarantees the same level of privacy as Algorithm 1.\nTo recall in Lemma 2, we also ensure that the output of Algorithm 2 guarantees ǫ-Differential privacy at each round t. Then, we consider the following lemma.\nLemma 8. At the t-th round, the i-th online learner’s output of Algorithm 2 is ǫ-differentially private.\nThe proof follows along the lines of Lemma 2, and is omitted.\nTo recall, we use mini-batch to reduce the variance. We divide the dataset into batchesH1, ..., Ht, which are disjoint subsets. According to the theory of parallel composition [19] in differential privacy, we know that the privacy guarantee\ndoes not degrade across rounds. Based on this observation, we can obtain the following theorem, which omits the proof.\nTheorem 3. Using Lemma 8 and the theory of parallel composition, Algorithm 2 is ǫ-differentially private."
    }, {
      "heading" : "5.2 Utility analysis for Algorithm 2",
      "text" : "As described, we next use the regret bounds of Algorithm 1 to achieve fast convergence rates for Algorithm 2 based on [10]. Note that the following Lemma 9 and 10 are proposed to prepare for the final result, Theorem 4.\nFor a clear description, we first consider the centralized offline learning. Let X be the domain of samples xt and Dx denotes a distribution over the domain X . Instead of minimizing (1), we bound\nF (w)− min w∈W F (w) , (57)\nwhere F (w) = E [f (w, x, y)] , (x, y) ∼ Dx and w = 1 T ∑T t=1 wt. Then, we obtain the centralized approximation error in the following lemma. Lemma 9 ([10]). Under Assumption 1, let RC be the regret (e.g., say RC ≤ logT ) of centralized online learning algorithm. Then with probability 1− 4γ lnT ,\nF (w)− F (w∗)\n≤ RC T + 4\n√ L2 lnT\nλ √ RC T +max { 16L2 λ , 6 } ln (1/γ) T ,\n(58)\nwhere w∗ ∈ arg min w∈W F (w).\nIntuitively, Lemma 9 relates the online regret to the offline convergence rate. But if we want to have the similar lemma when update the iterate as (55), we must know the new online regret using mini-batch. Dekel et al. [20] demonstrated that the mini-batch update does not improve the regret but also not significantly hurt the update rule. Based on their analysis, we obtain\nRcmb ≤ hRC , (59)\nwhere Rcmb denotes the centralized regret with mini-batch and h is the size of Ht.\nLemma 10. Under Assumption 1, for the centralized offline learning update with mini-batch, if we update the iterate as (55), then with probability 1− 4γ lnT , we have\nFmb (w)− Fmb (w∗)\n≤ h 2RC T + 4\n√ L2 ln (T /h)\nλ\nh √ hRC T\n+max\n{ 16L2\nλ , 6\n} h ln (1/γ)\nT (60)\nProof. Substituting T/h (see step 2 in Algorithm 2) for T in (58) and using Rcmb ≤ hRC , we obtain (60).\nLemma 10 is the utility analysis for the centralized model, while Algorithm 2 is a distributed offline learning algorithm using mini-batch. Next, we analyze the utility of the distributed model on the basis of Lemma 10. Similarly, we shall use the regret of Algorithm 1 to achieve the fast convergence rate for Algorithm 2.\nTheorem 4 (Utility of Algorithm 2). Under Assumption 1, the regret RD of Algorithm 1 can be used to achieve the convergence rate for Algorithm 2. Then, with probability 1− 4γ lnT , we have\nFdmb ( wi ) − Fdmb ( wi ∗)\n≤ h 2RD mT + 4\n√ L2 ln (T /h)\nλ\nh √ hRD/m\nT\n+max\n{ 16L2\nλ , 6\n} h ln (1/γ)\nT , (61)\nwhere Fdmb (w) = Edmb [ f ( wi, x, y )] , wi = 1\nT/h T/h∑ t=1 wit.\nProof. We estimate the convergence rate with respect to an arbitrary learner i. So we use the regret of a single learner, RD/m. Based on (60), we substitute RD/m for RC , then obtain (61).\nBased on [7] and [10], we study the application of regret bounds to offline convergence rates in distributed scenarios. Our work also have the same three significant advantages in [7]. Except for these existing advantages, we find new advantages in distributed scenarios: 1) the corresponding algorithms converge faster; 2) guaranteeing the same level of privacy needs less noise; 3) the noise of same magnitude has less influence on the utility of algorithms."
    }, {
      "heading" : "6 SIMULATIONS",
      "text" : "In this section, we conduct two sets of simulations. One is to study the privacy and regret trade-offs for our DOLA. The other is to illustrate how well the mini-batch performs to reduce high variance of differential privacy in the offline learning algorithm. For our implementations, we have the hinge loss function f it (w) = max ( 1− yit 〈 w, xit 〉) , where\n{( xit, y i t ) ∈ Rn × {±1} } are the data available only to the ith learner. For fast convergence rates, we set the learning rate αt = 1 λt . Furthermore, we do experiments on both synthetic and real datasets. The synthetic data are generated from a unit ball of dimensionality d = 10. We generate a total of 100,000 labeled examples. The real data used in our simulation is a subset of the RCV1 dataset. For a sharp contrast, this subset has the same number of examples with the synthetic data. As shown in Algorithm 1 and 2, the dataset is divided into m subsets. Each node updates the parameter based on its own subset and timely exchanges the updates its parameter to neighbors. Note that at round t, the i-th learner must exchange the parameter wit in strict accordance with Assumption 2. For a good observation, we sum the normalized error bounds (i.e., the “Regret” on yaxis) for both Figure 1 and 2.\nFigure 1 (a) and (b) show the average regret (normalized by the number of iterations) incurred by our DOLA for different level of privacy ǫ on synthetic and RCV1 datasets. Our differentially private DOLA has low-regret even for a little high level of privacy (e.g., ǫ = 0.01). The regret obtained by the non-private algorithm has the lowest regret as expected. More significantly, the regret gets closer to the non-private regret as its privacy preservation is weaker. Figure 1 (c) and (d) show the average regret for different nodes of the online system on the same level of privacy. Clearly, the centralized online learning algorithm (node = 1) has the lowest regret on the level of privacy ǫ = 0.1 and the regret gets lower as its number of nodes is smaller. Furthermore, the regret on synthetic data performs better than that on real data under the same conditions.\nFigure 2 (a) and (b) show the average regret for different batch size on synthetic data. When batch size is one (see Figure 2 (a)), the differentially private regret has higher variance than the non-private regret. However, a modest batch size h = 5, as shown in Figure 2 (b), reduces the vari-\nance of our differentially private distributed offline learning algorithm. The mini-batch technique makes the variance of differentially private distributed offline learning algorithm nearly identical to that of the non-private offline algorithm. Figure 2 (c) and (d) show the same simulation on RCV1 data and obtain the same conclusion with Figure 2 (a) and (b).\nAs we know, the hinge loss ℓ (w) = max ( 1− ywTx, 0 ) leads to the data mining algorithm, SVM. To be more persuasive, we conduct a differentially private distributed SVM and test this algorithm on RCV1 data. Table 1 shows the accuracy for different level of privacy and different number of nodes of algorithm. Intuitively, the centralized non-private model has the highest accuracy 88.74% while the model of 64 nodes at a high level ǫ = 0.01 of privacy has the lowest accuracy 50.36%. Further, we conclude that the accuracy gets higher as the level of privacy is lower or the number of nodes is smaller. This conclusion goes along with Figure 1 and 2."
    }, {
      "heading" : "7 CONCLUSION AND DISCUSSION",
      "text" : "We have proposed a differentially private distributed online learning algorithm. We used subgradient to update the learning parameter and used random doubly stochastic matrix to guide the learners to communicate with others. More importantly, our network topology is time-variant. As expected, we obtained the regret bounds in the order of O( √ T ) and O(log T ). Interestingly, the magnitude of the total noise added to guarantee ǫ-differential privacy also has the order of O( √ T ) and O(log T ) along with the nonprivate regret. Furthermore, we used our private distributed online learning algorithm with good regret bounds to solve the private distributed offline learning problems. In order to reduce high variance of our differentially private algorithm, we use the mini-batch technique to weaken the influence of added noise. This method makes the algorithm guarantee the same level of privacy using less random noise.\nIn this paper, we did not take the delay into consideration. In distributed online learning scenarios, there must exist delays among the nodes when they communicate with others, which is hard to analyze. Because each node has different delay according to its communication graph and the graph is even time-variant. Then, in future work, we hope that distributed online learning with delay can be presented."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "This research is supported by National Science Foundation of China with Grant 61401169."
    } ],
    "references" : [ {
      "title" : "Differential privacy",
      "author" : [ "C. Dwork" ],
      "venue" : "Proceedings of the 33rd international conference on Automata, Languages and Programming-Volume Part II. Springer-Verlag, 2006, pp. 1–12.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Consensus and cooperation in networked multi-agent systems",
      "author" : [ "R. Olfati-Saber", "J. Fax", "R. Murray" ],
      "venue" : "Proceedings of the IEEE, vol. 95, no. 1, pp. 215–233, Jan 2007.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Distributed stochastic subgradient projection algorithms for convex optimization",
      "author" : [ "S. Ram", "A. Nedić", "V. Veeravalli" ],
      "venue" : "Journal of optimization theory and applications, vol. 147, no. 3, pp. 516–545, 2010.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Distributed subgradient methods for multi-agent optimization",
      "author" : [ "A. Nedic", "A. Ozdaglar" ],
      "venue" : "IEEE Transactions on Automatic Control, vol. 54, no. 1, pp. 48–61, 2009.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "On the convergence of decentralized gradient descent",
      "author" : [ "K. Yuan", "Q. Ling", "W. Yin" ],
      "venue" : "arXiv preprint arXiv:1310.7063, 2013.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Distributed autonomous online learning: Regrets and intrinsic privacy-preserving properties",
      "author" : [ "F. Yan", "S. Sundaram", "S. Vishwanathan", "Y. Qi" ],
      "venue" : "IEEE Transactions on Knowledge and Data Engineering, vol. 25, no. 11, pp. 2483–2493, 2013.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Differentially private online learning",
      "author" : [ "P. Jain", "P. Kothari", "A. Thakurta" ],
      "venue" : "arXiv preprint arXiv:1109.0105, 2011.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Differentially private empirical risk minimization",
      "author" : [ "K. Chaudhuri", "C. Monteleoni", "A.D. Sarwate" ],
      "venue" : "The Journal of Machine Learning Research, vol. 12, pp. 1069– 1109, 2011.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Probabilistic inference and differential privacy",
      "author" : [ "O. Williams", "F. McSherry" ],
      "venue" : "Advances in Neural Information Processing Systems, 2010, pp. 2451–2459.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "On the generalization ability of online strongly convex programming algorithms",
      "author" : [ "S.M. Kakade", "A. Tewari" ],
      "venue" : "Advances in Neural Information Processing Systems, 2009, pp. 801–808.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Stochastic gradient descent with differentially private updates",
      "author" : [ "S. Song", "K. Chaudhuri", "A.D. Sarwate" ],
      "venue" : "IEEE Global Conference on Signal and Information Processing, 2013.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Online convex programming and generalized infinitesimal gradient ascent",
      "author" : [ "M. Zinkevich" ],
      "venue" : "ICML, pp. 928–936, 2003.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Logarithmic regret algorithms for online convex optimization",
      "author" : [ "E. Hazan", "A. Agarwal", "S. Kale" ],
      "venue" : "Machine Learning, vol. 69, no. 2-3, pp. 169–192, 2007.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Differentially private distributed optimization",
      "author" : [ "Z. Huang", "S. Mitra", "N. Vaidya" ],
      "venue" : "Proceedings of the 2015 International Conference on Distributed Computing and Networking. ACM, 2015, p. 4.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Online Convex Optimization",
      "author" : [ "E. Hazan" ],
      "venue" : "2015.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Dual averaging for distributed optimization",
      "author" : [ "J.C. Duchi", "A. Agarwal", "M.J. Wainwright" ],
      "venue" : "Communica-  IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. X, NO. X, JANUARY 20XX  13 tion, Control, and Computing (Allerton), 2012 50th Annual Allerton Conference on. IEEE, 2012, pp. 1564–1565.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "A differentially private stochastic gradient descent algorithm for multiparty classification",
      "author" : [ "A. Rajkumar", "S. Agarwal" ],
      "venue" : "International Conference on Artificial Intelligence and Statistics, 2012, pp. 933–941.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Differentially private empirical risk minimization: Efficient algorithms and tight error bounds",
      "author" : [ "R. Bassily", "A. Smith", "A. Thakurta" ],
      "venue" : "arXiv preprint arXiv:1405.7085, 2015.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Privacy integrated queries: an extensible platform for privacy-preserving data analysis",
      "author" : [ "F.D. McSherry" ],
      "venue" : "Proceedings of the 2009 ACM SIGMOD International Conference on Management of data. ACM, 2009, pp. 19– 30.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Specifically, we use distributed convex optimization as the distributed online learning model, while use differential privacy [1] to protect the privacy.",
      "startOffset" : 126,
      "endOffset" : 129
    }, {
      "referenceID" : 1,
      "context" : "Distributed convex optimization is considered as a consensus problem [2].",
      "startOffset" : 69,
      "endOffset" : 72
    }, {
      "referenceID" : 2,
      "context" : "To solve this problem, some related works [3, 4, 5] have been done.",
      "startOffset" : 42,
      "endOffset" : 51
    }, {
      "referenceID" : 3,
      "context" : "To solve this problem, some related works [3, 4, 5] have been done.",
      "startOffset" : 42,
      "endOffset" : 51
    }, {
      "referenceID" : 4,
      "context" : "To solve this problem, some related works [3, 4, 5] have been done.",
      "startOffset" : 42,
      "endOffset" : 51
    }, {
      "referenceID" : 5,
      "context" : "The time-variant communication matrix makes the distributed optimization algorithm converge faster and better than the fixed one used in [6].",
      "startOffset" : 137,
      "endOffset" : 140
    }, {
      "referenceID" : 0,
      "context" : "Differential privacy [1] is a popular privacy mechanism to preserve the privacy of the learners.",
      "startOffset" : 21,
      "endOffset" : 24
    }, {
      "referenceID" : 6,
      "context" : "Many privacy-preserving algorithms [7, 8, 9] have been proposed to use differential privacy to protect sensitive information in the centralized offline learning framework.",
      "startOffset" : 35,
      "endOffset" : 44
    }, {
      "referenceID" : 7,
      "context" : "Many privacy-preserving algorithms [7, 8, 9] have been proposed to use differential privacy to protect sensitive information in the centralized offline learning framework.",
      "startOffset" : 35,
      "endOffset" : 44
    }, {
      "referenceID" : 8,
      "context" : "Many privacy-preserving algorithms [7, 8, 9] have been proposed to use differential privacy to protect sensitive information in the centralized offline learning framework.",
      "startOffset" : 35,
      "endOffset" : 44
    }, {
      "referenceID" : 9,
      "context" : "Furthermore, our differentially private DOLA can be used to achieve fast convergence rates for differentially private distributed offline learning algorithm based on [10].",
      "startOffset" : 166,
      "endOffset" : 170
    }, {
      "referenceID" : 10,
      "context" : "Since the offline learning algorithm has access to all data, the technique of mini-batch [11] is used to reduce the high variance of the differentially private offline learning algorithm.",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 9,
      "context" : "Motivated by [10] and [11], we try to obtain a good utility of the distributed offline learning algorithm while protect the privacy of the learners.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 10,
      "context" : "Motivated by [10] and [11], we try to obtain a good utility of the distributed offline learning algorithm while protect the privacy of the learners.",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 11,
      "context" : "We respectively obtain the classical regret boundsO( √ T ) [12] andO(log T ) [13] for convex and strongly convex objective functions for the algorithm.",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 12,
      "context" : "We respectively obtain the classical regret boundsO( √ T ) [12] andO(log T ) [13] for convex and strongly convex objective functions for the algorithm.",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 6,
      "context" : "[7] studied the differentially private centralized online learning.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "[6] has proposed a DOLA to handle the decentralized data.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 13,
      "context" : "[14] is closely related to our work.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 14,
      "context" : "Hazan has studied online convex optimization in his book [15].",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 15,
      "context" : "[16] developed an efficient algorithm for distributed optimization based on dual averaging of subgradients method.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 3,
      "context" : "Nedic and Ozdaglar [4] considered a subgradient method for distributed convex optimization, where the functions are convex but not necessarily smooth.",
      "startOffset" : 19,
      "endOffset" : 22
    }, {
      "referenceID" : 2,
      "context" : "[3] tried to analyze the influence of stochastic subgradient errors on distributed convex optimization based on a time-variant network topology.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "Our work extends the works of Nedic and Ozdaglar [4] and Ram et al.",
      "startOffset" : 49,
      "endOffset" : 52
    }, {
      "referenceID" : 2,
      "context" : "[3].",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 7,
      "context" : "[8] presented the output perturbation and objective perturbation ideas about differential privacy in empirical risk minimization (ERM) classification.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 16,
      "context" : "Rajkumar and Agarwal [17] extended differentially private ERM classification [8] to differentially private ERM multiparty classification.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 7,
      "context" : "Rajkumar and Agarwal [17] extended differentially private ERM classification [8] to differentially private ERM multiparty classification.",
      "startOffset" : 77,
      "endOffset" : 80
    }, {
      "referenceID" : 17,
      "context" : "[18] proposed more efficient algorithms and tighter error bounds for ERM classification on the basis of [8].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 7,
      "context" : "[18] proposed more efficient algorithms and tighter error bounds for ERM classification on the basis of [8].",
      "startOffset" : 104,
      "endOffset" : 107
    }, {
      "referenceID" : 9,
      "context" : "Kakade and Tewari [10] proposed some properties of online learning algorithms if the loss function is Lipschitz and strongly convex.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 6,
      "context" : "[7] use the results in [10] to analyze the utility of differentially private offline learning algorithms.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 9,
      "context" : "[7] use the results in [10] to analyze the utility of differentially private offline learning algorithms.",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 2,
      "context" : "In our algorithm, each learner computes a weighted average [3] of the m learners’ parameters.",
      "startOffset" : 59,
      "endOffset" : 62
    }, {
      "referenceID" : 0,
      "context" : "Differential Privacy: Dwork [1] proposed the definition of differential privacy for the first time.",
      "startOffset" : 28,
      "endOffset" : 31
    }, {
      "referenceID" : 5,
      "context" : "Notice that RD is computed with respect to an arbitrary learner’s parameter w t [6].",
      "startOffset" : 80,
      "endOffset" : 83
    }, {
      "referenceID" : 7,
      "context" : "This method to guarantee differential privacy is known as output perturbation [8].",
      "startOffset" : 78,
      "endOffset" : 81
    }, {
      "referenceID" : 0,
      "context" : "Further, according to [1], the magnitude of the noise depends on the largest change that a single entry in data source could have on the output of Algorithm 1; this quantity is referred to as the sensitivity of the algorithm.",
      "startOffset" : 22,
      "endOffset" : 25
    }, {
      "referenceID" : 18,
      "context" : "McSherry [19] has proposed that the privacy guarantee does not degrade across rounds as the samples used in the rounds are disjoint.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 18,
      "context" : "This proof follows from the theorem 4 of [19].",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 2,
      "context" : "Lemma 3 ([3]).",
      "startOffset" : 9,
      "endOffset" : 12
    }, {
      "referenceID" : 0,
      "context" : "t=1 m √ |σt [1]|2+.",
      "startOffset" : 12,
      "endOffset" : 15
    }, {
      "referenceID" : 9,
      "context" : "Kakade and Tewari [10] and Jain et al.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 6,
      "context" : "[7] have both proposed that online learning algorithms with good regret bounds can be used to achieve fast convergence rates for offline learning algorithms.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 6,
      "context" : "Based on the analysis in [7], we exploit this application in distributed scenarios.",
      "startOffset" : 25,
      "endOffset" : 28
    }, {
      "referenceID" : 10,
      "context" : "For solving the problem in (52), stochastic gradient descent (SGD) (mentioned in [11]) was proposed.",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 10,
      "context" : "[11] demonstrated that differentially private SGD algorithm updated with a single point has high variance and used mini-batch to reduce the variance.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 18,
      "context" : "According to the theory of parallel composition [19] in differential privacy, we know that the privacy guarantee does not degrade across rounds.",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 9,
      "context" : "2 Utility analysis for Algorithm 2 As described, we next use the regret bounds of Algorithm 1 to achieve fast convergence rates for Algorithm 2 based on [10].",
      "startOffset" : 153,
      "endOffset" : 157
    }, {
      "referenceID" : 9,
      "context" : "Lemma 9 ([10]).",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 6,
      "context" : "Based on [7] and [10], we study the application of regret bounds to offline convergence rates in distributed scenarios.",
      "startOffset" : 9,
      "endOffset" : 12
    }, {
      "referenceID" : 9,
      "context" : "Based on [7] and [10], we study the application of regret bounds to offline convergence rates in distributed scenarios.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 6,
      "context" : "Our work also have the same three significant advantages in [7].",
      "startOffset" : 60,
      "endOffset" : 63
    } ],
    "year" : 2015,
    "abstractText" : "In this paper, we propose a novel distributed online learning algorithm to handle massive data in Big Data era. Comparing to the typical centralized scenario, our proposed distributed online learning has multi-learners. Each learner optimizes its own learning parameter based on local data source and communicates timely with neighbors. We study the regret of the distributed online learning algorithm. However, communications among the learners may lead to privacy breaches. Thus, we use differential privacy to preserve the privacy of the learners, and study the influence of guaranteeing differential privacy on the regret of the algorithm. Furthermore, our online learning algorithm can be used to achieve fast convergence rates for offline learning algorithms in distributed scenarios. We demonstrate that the differentially private offline learning algorithm has high variance, but we can use mini-batch to improve the performance. The simulations show that our proposed theorems are correct and our differentially private distributed online learning algorithm is a general framework.",
    "creator" : "LaTeX with hyperref package"
  }
}