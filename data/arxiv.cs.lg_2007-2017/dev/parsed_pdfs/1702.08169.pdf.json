{
  "name" : "1702.08169.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Communication-efficient Algorithms for Distributed Stochastic Principal Component Analysis",
    "authors" : [ "Dan Garber" ],
    "emails" : [ "dangar@technion.ac.il", "ohad.shamir@weizmann.ac.il", "nati@ttic.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 2.\n08 16\n9v 1\n[ cs\n.L G\n] 2\n7 Fe\nb 20"
    }, {
      "heading" : "1 Introduction",
      "text" : "Principal Component Analysis (PCA) [17, 9, 13] is one of the most celebrated and popular techniques in data analysis and machine learning. For data that consists of N vectors in Rd, x1, ...,xN , with normalized covariance matrix X̂ = 1 N ∑N i=1 xix ⊤ i , The PCA method finds the k-dimensional subspace (which corresponds to the span of the top k principal components) such that the projection of the data onto the subspace has largest variance, i.e., it is the solution to the optimization problem:\nmax W∈Rd×k ,WTW=I\n‖X̂W‖2F . (1)\nPCA is often considered in a statistical setting in which the assumption is that the input vectors are not arbitrary but sampled i.i.d. from some fixed but unknown distribution with certain general characteristics D. Then, it is often of interest to use the observed sample to estimate the top k principal components of the population covariance matrix, rather then that of the sample, which leads to the modified optimization problem:\nmax W∈Rd×k ,WTW=I\n‖Ex∼D [ xx⊤ ] W‖2F . (2)\nOf course the empirical estimation problem (1) and the population estimation problem (2) are well connected, and it is well-known that under mild assumptions on the distribution D and\ngiven a sufficiently large sample, we can guarantee small estimation error in (2) by solving optimization problem (1).\nIn this work we consider the problem of estimating the first principal component (i.e., k = 1) in a statistical and distributed setting. We assume the availability of m machines, each of which stores a sample of n vectors sampled i.i.d from a fixed distribution D over Rd, and we are interested in algorithms that can be applied efficiently to solve Problem (2) for k = 1, with estimation error that approaches that of a centralized algorithm, which has access to all mn samples and does not pay for communication between machines. Indeed, when considering the efficiency of algorithms, we will mainly focus on the amount of communication between machines they require, since this is often the most expensive resource in distributed computing. We note that the i.i.d. assumption is standard in many applications of PCA, and can be leveraged to get more efficient algorithms than when the data partition is arbitrary. Also, we will make a standard assumption that the population covariance matrix has a non-zero additive gap between the first and second eigenvalues, which makes the problem of estimating the leading principal component meaningful.\nA main challenge that often arises in many computational settings of principal components is that it leads to inherently non-convex optimization problems. While many times these problems turn out to admit efficient algorithms, the rich toolbox of optimization and statistical estimation procedures developed for convex problems often cannot be directly applied to problems such as (1) and (2). Instead, one often needs to consider a specialized and more involved analysis, to get analogous convergence results for the PCA problem. This for instance was the case in a recent wave of results that applied concepts such as stochastic gradient updates [4, 20, 11, 3] and variance reduction [19, 21, 6, 7, 2] to the PCA problem. This is also the case in our distributed setting. For instance, [26] proposed communication-efficient algorithms for a distributed statistical estimation settings, similar to ours, but under convexity assumptions. The authors show that under their assumptions, in a wide regime of parameters (namely when the per-machine sample size n is large enough), then a simple averaging of the empirical risk minimizers (ERM), computed locally on each machine, leads to estimation error of the population parameters of the order the centralized ERM solution. While averaging makes perfect sense in a convex setting, it is clear that it can completely fail in a non-convex setting. Indeed, we show that already for the PCA problem with k = 1, simply averaging the local ERM solutions (and normalizing to obtain a unit vector as required), cannot improve significantly over the estimation error of any single machine. We then show that a simple fix to the above scheme, namely correlating the directions of individual ERM solutions, remedies this phenomena and results in estimation error similar to that of the centralized ERM solution. Much like the results of [26], this result only holds in the regime when the per-machine sample size n is sufficiently large. As discussed, due to the inherent non-convexity of the PCA objective, this approach requires a novel analysis tailored to the PCA problem. In this context, we view this work as an initiation of a research effort to understand how to efficiently aggregate statistical estimators in a distributed non-convex setting.\nA second line of results for distributed estimation under convexity assumptions consider iterative algorithms that perform multiple communication rounds and are based on distributed gradient computations (some examples include [23, 27, 14, 22, 10, 18]). The benefit of these methods is that (a) they provide meaningful estimation error guarantees in a much wider regime of parameters than the “one-shot” aggregation methods (namely in terms of the number of samples per machine), and (b), due to their iterative nature, they allow to approximate the centralized ERM solution arbitrary well. Unfortunately, these methods, all of which rely heavily on convexity assumption, cannot be directly applied to the PCA problem. Towards designing efficient distributed iterative methods for our PCA setting, we consider the application of the recently proposed method of Shift-and-Invert power iterations (S&I) for PCA [6, 7]. The S&I method reduces the problem of computing the leading eigenvector of a real positive semidefinite\nmatrix to that of approximately solving a small number (i.e. poly-logarithmic in the problem parameters) of systems of linear equations. These in turn, could be efficiently solved by arbitrary distributed convex solvers. We show that coupling the S&I method with the stochastic pre-conditioning technique for linear systems proposed in [27] and well known fast gradient methods such as the conjugate gradient method, gives state-of-the-art guarantees in terms of communication costs, and provides a significant improvement over distributed variants of classical fast eigenvector algorithms such as power iterations and the faster Lanczos algorithm. Much like its convex counterparts, which only rely on distributed gradient computations and simple vector aggregations, our iterative method only relies on distributed matrix-vector products, i.e., it requires each machine to only send products of its local empirical covariance matrix with some input vector.\nBeyond the results described so far, [15, 5] studied distributed algorithms for PCA in a deterministic setting in which the partition of the data across machines is arbitrary and communication is measured in terms of number of transmitted bits. The approximation guarantees provided in these works are in terms of the projection of the data onto the leading principal components (instead of alignment between the estimate and the optimal solution, studied in this paper). Applying these results to our setting will give a number of communication rounds that scales like poly(ǫ−1δ−1), where ǫ is the desired error and δ is the population eigengap. In our setting, ǫ will scale with the inverse of the size of the sample, i.e., ǫ ≈ (mn)−1, which for these algorithms will result in amount of communication that is polynomial in the size of the data. In contrast, we will be interested in algorithms whose communication costs does not scale with n at all. In this context we note that, by focusing on algorithms that either perform simple aggregation of local ERM solutions, or perform only distributed matrix-vector products with the empirical covariance matrix, we can circumvent the need to measure communication explicitly in terms of the number of bits transmitted, which often burdens the analysis of natural algorithms, such as those proposed here."
    }, {
      "heading" : "2 Preliminaries",
      "text" : ""
    }, {
      "heading" : "2.1 Notation and problem setting",
      "text" : "We write vectors in Rd in boldface lower-case letters (e.g., v), matrices in boldface upper-case letters (e.g., X), and scalars are written as lightface letters (e.g., c). We let ‖ · ‖ denote the standard Euclidean norm for vectors and the spectral norm for matrices.\nWe consider the following statistical distributed setting. Let D be a distribution over vectors in Rd with squared ℓ2 norm at most b, for some b > 0. We consider a setting in which m machines, numbered 1...m, are each given a dataset of n samples drawn i.i.d. from D. We let v1 denote a leading eigenvector of the population covariance matrix X = Ex∼D[xx⊤]. Our goal is to efficiently (mainly in terms of communication) find an estimate w for v1, i.e., a unit vector that maximizes the product (v⊤1 w)\n2 with high probability. Towards this end, we assume that the population covariance matrix X has a non-zero eigengap δ, i.e., δ := λ1(X) − λ2(X) > 0, where λi(·) denotes the ith largest eigenvalue of a symmetric real matrix. Note that δ > 0 is necessary for v1 to be uniquely defined (up to sign).\nIn addition, we let X̂i denote the empirical covariance matrix of the sample stored on\nmachine i for every i ∈ [m], i.e., X̂i = 1n ∑n j=1 x (i) j x (i)⊤ j , where x (i) 1 ...x (i) n are the samples stored on machine i. We let X̂ denote the empirical covariance matrix of the union of points across all machines i.e., X̂ = 1m ∑m i=1 X̂i.\nOur model of communication assumes that the m machines work in rounds during which a central machine (w.l.o.g. machine 1) can send a single vector in Rd to all other machines, or every machine can send either the leading eigenvector of its local empirical covariance matrix, or the product of a single input vector with its local covariance, to machine 1. We will measure\ncommunication complexity in terms of number of such rounds required to achieve a certain estimation error."
    }, {
      "heading" : "2.1.1 The centralized solution",
      "text" : "Our primary benchmark for measuring performance will be the centralized empirical risk minimizer which is the leading eigenvector of the aggregated empirical covariance matrix X̂.\nThe following standard result bounds the error of the centralized ERM.\nLemma 1 (Risk of centralized ERM). Fix p ∈ (0, 1). Suppose that δ > 0 and let v̂1 denote the leading eigenvector of X̂, i.e., v̂1 ∈ argmaxv:‖v‖=1 v⊤X̂v. Then it holds w.p. at least 1− p that\n1− (v⊤1 v̂1)2 ≤ ǫERM(p) := 32b2 ln(d/p)\nmnδ2 . (3)\nLemma 1 is a direct consequence of the following standard concentration argument for random matrices, and the Davis-Kahan sin(θ) theorem (whose proof is given in the appendix for completeness):\nTheorem 1 (Matrix Hoeffding, see [24]). Let D be a distribution over vectors with squared ℓ2 norm at most b, and let X = Ex∼D[xx⊤]. Let X̂ = 1 n ∑n i=1 xix ⊤ i , where x1, ...,xn are sampled i.i.d. from D. Then, it holds that\n∀ǫ > 0 : Pr ( ‖X̂−X‖ ≥ ǫ ) ≤ d · exp ( − ǫ 2n\n16b2\n)\n.\nTheorem 2 (Davis-Kahan sin(θ) theorem). Let X,Y be symmetric real d × d matrices with leading eigenvectors vX and vY respetively. Also, suppose that δ(X) := λ1(X) − λ2(X) > 0. Then it holds that\n1− ( v⊤XvY )2 ≤ 2‖X−Y‖ 2\nδ(X)2 ."
    }, {
      "heading" : "2.2 Informal statement of main results and previous algorithms",
      "text" : "We now informally describe our main results, followed by a detailed description of previous approaches that are directly applicable to our setting. The algorithmic results (both new and old) are summarized in Table 1."
    }, {
      "heading" : "2.2.1 Main results",
      "text" : "Failure of simple averaging of local ERM solutions We show that a natural approach of simply averaging the individual leading eigenvectors of the empirical covariance matrices X̂i (and normalizing the obtain a unit vector) cannot significantly improve (beyond logarithmic factors) over the performance of any of the individual eigenvectors. More concretely, if we let v̂ (i) 1 denote the leading eigenvector of X̂i for any i ∈ [m], and we denote their average by v̄1 = 1 m ∑m i=1 v̂ (i) 1 , then there exists a distribution D over vectors with magnitude O(1) and covariance eigengap δ = 1, such that\n∀m,n : ED [ 1− ( v̄⊤1 v1 ‖v̄1‖ )2 ] = Ω ( 1 n ) ,\nSee Theorem 3 in Section 3 for the complete and formal argument."
    }, {
      "heading" : "A successful single communication round algorithm via correlation of individual",
      "text" : "ERM solutions We show that if prior to averaging the local ERM solutions, as suggested above, we correlate their directions by aligning them according to any single machine (say machine number 1), i.e., we let v̄1 = 1 m ∑m i=1 sign(v̂ (i)⊤ 1 v̂ (1) 1 )v̂ (i) 1 , then this guarantees that for any p ∈ (0, 1), w.p. at least 1− p,\n1− ( v̄⊤1 v1 ‖v̄1‖ )2 = O\n\n\nb2 ln (\ndm p\n)\nδ2mn +\nb4 ln2 (\ndm p\n)\nδ4n2\n\n . (4)\nSee Theorem 4 in Section 3 for the complete and formal result. In particular, in the likely scenario when m = O(d/p) we have that w.p. at least 1 − p,\n1 − ( v̄⊤1 v1/‖v̄1‖ )2 = ǫERM(p)) · O ( 1 +m2 · ǫERM(p) )\n, where ǫERM(p)) is defined in Eq. (3). Another related interpretation of the results is that the bound in Eq. (4) is comparable with ǫERM (up to poly-log factors) when n = Ω ( δ−2b2m ln(dm/p) )\n. We also show a matching lower bound that the bound in Eq. (4) is tight (up to poly-log\nfactors) for this aggregation method, see Theorem 5.\nA multi communication round algorithm We present a distributed algorithm based on the Shift-and-Invert framework for leading eigenvector computation [6, 7] which is applied to explicitly solving the centralized ERM problem. We show that for any p ∈ (0, 1), when mn = Ω(b2 ln(d/p)/δ2) (i.e., when Lemma 3 is meaningful), the algorithm produces a solution w such that w.p. at least 1− p,\n1− (v⊤1 w)2 ≤ ǫERM(p)) · (1 + o(1)) , (5)\nwhere ǫERM(p)) is defined in Eq. (3). The algorithm performs overall Õ( √ bδ−1/2n−1/4) distributed matrix-vector products with the centralized empirical covariance matrix X̂ 1. The Õ(·) notation hides poly-logarithmic factors in 1/p, 1/δ, d, 1/ǫERM(p). See Theorem 6 in Section 4 for the complete and formal result.\nWe note that in particular, under our assumption that mn = Ω̃(b2/δ2), it holds that the number of distributed matrix-vector products is upper bounded by Õ(m1/4). Moreover, in the regime n = Ω(b2δ−2), we can see that the number of distributed matrix-vector products depends only poly-logarithmically on the problem parameters.\nIn general, the sub-constant o(1) factor in (5) could be made arbitrarily small by trading the approximation error with the number of distributed matrix-vector products.\n1i.e., on each round, each machine i sends the product of an input vector in Rd with its local covariance matrix X̂i."
    }, {
      "heading" : "2.2.2 Previous algorithms",
      "text" : "Distributed versions of classical iterative algorithms: Classical fast iterative algorithms for computing the leading eigenvector of a positive semidefinite matrix, such as the well-known Power Method and the Lanczos Algorithm, require iterative multiplications of the input matrix (X̂ in our case) with the current estimate. It is thus straightforward to implement these algorithms in our distributed setting, by multiplying the same vector with the covariance matrices at each machine, and averaging the result. Thus, by well-known convergence guarantees of these two methods, we will have that for a fixed ǫ > 0, these methods produce a unit vector w such that, for any p ∈ (0, 1), 1 − (w⊤v̂1)2 ≤ ǫ w.p. at least 1 − p, after O(λ̂1δ̂−1 ln(d/pǫ)) rounds for the Power Method and O( √\nλ̂1δ̂−1 ln(d/pǫ)) for the Lanczos Algorithm, where λ̂1, δ̂ denote the leading eigenvalue and eigengap of X̂, respectively. Moreover, in the regime of mn in which Lemma 1 is meaningful, we can replace λ̂1, δ̂ with λ1, δ in the above bounds, and the result will still hold with high probability.\nSimple calculations show that in the regime of mn in which Lemma 1 is meaningful, it holds that our Shift-and-Invert-based algorithm outperforms distributed Lanczos (in terms of worst-case guarantees) whenever n = Ω̃(b2/λ21).\n“Hot potato” SGD: Another straightforward approach is to apply a sequential algorithm for direct risk minimization that can process the data-points one by one, such as stochastic gradient descent (SGD), by passing its state from one machine to the next, after completing a full pass over the machine’s data. Clearly, this process of making a full pass over the data of a certain machine before sending the final estimate to the next one, requires overall m communication rounds in order to make a full pass over all mn points. SGD for PCA was studied in several results in recent years [4, 20, 21, 12, 3]. For instance applying the result of [12] in this way will result in a final estimate w satisfying\n1− (w⊤v1)2 = O ( b2 ln d\nδ2mn\n)\nw.p. at least 3/4. (6)\nWe note that in the regime in which the bound in (6) is meaningful it holds that the number of communication rounds of our Shift-and-Invert-based algorithm is upper-bounded by Õ(m1/4) which for sufficiently large m dominates the communication complexity of SGD."
    }, {
      "heading" : "3 Single Communication Round Algorithms via ERM on Each",
      "text" : "Machine\nIn this section we consider distributed algorithms that require only a single round of communication. Naturally for this regime, all algorithms will be based on aggregating the ERM solutions of the individual machines, i.e., each machine i only sends the leading eigenvector of its empirical covariance matrix X̂i to a centralized machine (without loss of generality, machine 1) which it turn combines them to a single unit vector in some manner."
    }, {
      "heading" : "3.1 Simple averaging of eigenvectors fail",
      "text" : "Perhaps the simplest method to aggregate the individual eigenvectors of each machine is to average them, and then normalize to obtain a unit vector. For instance, in the distributed statistical setting considered in [26], in which the objective is strongly convex, it was shown that simply averaging the individual ERM solutions leads, in a meaningful regime of parameters, to estimation error of the order of the centralized ERM solution. However, here we show that for PCA, in which the objective is certainly not convex, this approach fails practically in any\nregime, in the sense that the error of the returned aggregated solution can be no better than that returned by any single machine.\nTheorem 3. There exists a distribution over vectors in R2 with ℓ2 norm bounded by a universal constant for which the eigengap in the covariance matrix is 1 (i.e., δ = 1), such that if each machine i returns an estimate v̂ (i) 1 which is an unbiased leading eigenvector of X̂i (i.e., both outcomes −v̂(i)1 ,+v̂ (i) 1 are equally likely), then the aggregated vector v̄1 = 1 m ∑m i=1 v̂ (i) 1 satisfies\n∀m,n : E [ 1− 〈 v̄1\n‖v̄1‖ ,v1\n〉2 ]\n= Ω(1/n).\nThe proof is given in the appendix."
    }, {
      "heading" : "3.2 Averaging with Sign Fixing",
      "text" : "As evident from the statement of Theorem 3, an important assumption is that each machine produces an unbiased estimate, in the sense that the sign of the outcome is uniform and independent of the other machines. This hints that correlating the signs of the different estimates can circumvent the lower bound result in Theorem 3. It turns out that this is indeed the case, as captured by the following theorem:\nTheorem 4. Let w̃i be the leading eigenvector of X̂i for any i ∈ [m], and consider the unit vector\nw =\n∑m i=1 sign(w̃ ⊤ i w̃1)w̃i\n‖∑mi=1 sign(w̃⊤i w̃1)w̃i‖ . (7)\nThen, for any p ∈ (0, 1), it holds w.p. at least 1− p that\n1− (v⊤1 w)2 = O\n\n\nb2 log (\ndm p\n)\nδ2mn +\nb4 log2 (\ndm p\n)\nδ4n2\n\n .\nFor ease of presentation, throughout the rest of this section we denote the correlated vector ŵi = sign(w̃ ⊤ i w̃1)w̃i for any i ∈ [m].\nThe main step towards proving Theorem 4 is to consider each ŵi as an approximately unbiased perturbation of the true leading eigenvector v1 and to upper bound the magnitude of this perturbation. This is carried out in the following much more general and self-contained lemma, which might be of independent interest.\nLemma 2. Let A be a positive semidefinite matrix with some fixed leading eigenvector v1, a leading eigenvalue λ1 and an eigengap δ := λ1(A) − λ2(A) > 0. Let Â be some positive semidefinite matrix such that ‖Â−A‖ ≤ δ/4. Then there is a unique leading eigenvector v̂1 of Â such that 〈v̂1,v〉 ≥ 0, and\n∥ ∥\n∥ v̂1 − v1 − (λ1I−A)†(Â−A)v1\n∥ ∥ ∥ ≤ c‖Â−A‖ 2\nδ2 ,\nwhere † denotes the pseudo-inverse, and c is a positive numerical constant.\nProof. The proof is based on viewing Â as an unbiased perturbation of the matrix A, and computing a Taylor expansion of v̂1 around v1. For notational convenience, let E = Â − A, and define A(t) = A+ tE for t ∈ [0, 1]. Also, define λ(t) to be the leading eigenvalue of A(t).\nFirst, we note that for any t ∈ [0, 1], A(t) has an eigengap of at least δ/2 between its first two eigenvalues (since by Weyl’s inequality, its eigenvalues are at most ‖tE‖ ≤ ‖E‖ ≤ δ/4 different\nthan A, and we know that A has an eigengap of δ). Therefore, the leading eigenvalue of A(t) is simple. This means that the function v(t), which equals the leading eigenvector of A(t), is uniquely defined up to a sign. This sign will be chosen so that 〈v(t),v1〉 ≥ 0, which makes v(t) unique and well-defined2. By Theorem 1 in [16], we have that both λ(t) and v(t) are infinitely differentiable at any t ∈ [0, 1], and satisfy3\nλ′(t) = v(t)⊤Ev(t) , v′(t) = (λ(t)I −A(t))†Ev(t) .\nWe will also need to bound the second derivative of v(t). By the product rule and the equations above, this derivative equals\nv′′(t) = ∂\n∂t\n( (λ(t)I−A(t))† ) Ev(t) + (λ(t)I −A(t))†E ∂ ∂t v(t)\n= ∂\n∂t\n( (λ(t)I−A(t))† ) Ev(t) + (λ(t)I −A(t))†E(λ(t)I −A(t))†Ev(t). (8)\nTo compute the derivative above, we apply the chain rule. The derivative of a pseudo-inverse B† of a matrix-valued function B = B(t) with respect to t (assumingB and hence its pseudo-inverse is symmetric for all t) is given by (see Theorem 4.3 in [8])\n−B† ( ∂\n∂t B\n)\nB† + ( B† )2\n(\n∂ ∂t B\n) (I −BB†) + (I −B†B) ( ∂\n∂t B\n)\n( B† )2 .\nThis formula is true assuming the rank of B is constant in some open neighborhood of t. Applying this for B = λ(t)I − A(t) (which indeed has a fixed rank of d − 1 by the eigengap assumption), noting that ∥ ∥\n∂ ∂t(λ(t)I −A(t))\n∥ ∥ = ∥ ∥v(t)⊤Ev(t)I −E ∥ ∥ ≤ 2‖E‖, and using the facts that ‖v(t)‖ = 1, ‖I−B†B‖ ≤ 1,‖I−BB†‖ ≤ 1 and ‖(λ(t)I−A(t))†‖ ≤ 2/δ (since the smallest non-zero eigenvalue of λ(t)I−A(t) is at least δ/2), we have that\n∥ ∥ ∥ ∥ ∂\n∂t\n( (λ(t)I−A(t))† )\n∥ ∥ ∥ ∥ ≤ 24 · ‖E‖ δ2 .\nPlugging this into (8), and again using the fact that ‖(λ(t)I −A(t))†‖ ≤ 2/δ, we get that\n∥ ∥v′′(t) ∥ ∥ ≤ c‖E‖ 2\nδ2\nfor some numerical constant c. By a first-order Taylor expansion of v(t) with an explicit remainder term4,\nv(1) = v(0) + v′(0) + 1\n2\n∫ 1\nt=0 (1− t)2v′′(t)dt ,\nwhich by the equations above and the definition of v(t) implies that\nv̂1 = v1 + (λ1I−A)†Ev1 + 1\n2\n∫ 1\nt=0 (1− t)2v′′(t)dt .\n2Note that ties are impossible, since that can only happen if 〈v(t),v1〉 = 0, yet by applying the Davis-Kahan\nsin(θ) theorem (Theorem 2), √ 1− 〈v(t),v1〉2 ≤ 2‖A(t)−A‖ δ ≤ 2‖E‖ δ ≤ 1 2 .\n3Formally speaking, the theorem only ensures v(t), λ(t) exist and are infinitely differentiable in some open neighborhood of t. However, since the result holds for any t ∈ [0, 1], and the proof implies that these functions are unique in each such neighborhood (where the uniqueness of v(t) holds once we fixed the sign as above), it follows that the same holds in all of t ∈ [0, 1].\n4Since v(t),v′(t),v′′(t) are all vectors, this is a direct consequence of the standard Taylor expansion of the scalar function t 7→ v(t)j , mapping t to the j-th coordinate of v(t), using the fact that this mapping is differentiable to any order (see Theorem 1 in [16], and in particular twice continuously differentiable.\nThis implies\n∥ ∥ ∥v̂1 − v1 − (λ1I−A)†Ev1 ∥ ∥ ∥ ≤ 1 2\n∫ 1 t=0 (1− t)2‖v′′(t)‖dt ≤ c‖E‖ 2 2λ2 ∫ 1 t=0 (1− t)2dt,\nwhich is at most c′‖E‖2/λ2 for some appropriate numerical constant c′. Plugging back E = Â−A, the result follows.\nLemma 2 is central to the proof of the following Lemma, of which the proof of Theorem 4 is an easy consequence.\nLemma 3. The following two conditions hold with probability at least 1− p− d exp(−δ2n/cb2), for some numerical constants c, c′ > 0:\n• The leading eigenvalue of every X̂i is simple, i.e., λ1(X̂i)− λ2(X̂i) > 0.\n• Fixing v1, there exist unique leading eigenvectors v̂i1, . . . , v̂im of X̂1, . . . , X̂m, such that maxi ‖v̂i1 − v1‖ ≤ 14 , and\n∥ ∥ ∥ 1\nm\nm ∑\ni=1\nv̂i1 − v1 ∥ ∥ ∥ ≤ c′\n(b2 log(2dm/p)\nδ2n +\n√\nb2 log(2dm/p)\nδ2mn\n)\n.\nProof. Using the matrix Hoeffding inequality (Theorem 1) and a union bound, we that\nPr\n(\n∃i, ‖X̂i −X‖ > δ\n12\n) ≤ md exp ( − δ 2n\nc′b2\n)\n(9)\nfor some constant c′ > 0. Thus, with high probability, maxi ‖X̂i − X‖ ≤ δ/12. By Weyl’s inequality, it follows that the eigenvalues of X and X̂i are at most δ/12 apart, and since X has an eigengap of δ between its two leading eigenvalues, it follows that X̂i has an eigengap of at least δ − δ/12 − δ/12 > 0, which proves the first part of the lemma. To handle the second part, note that by a variant of the Davis-Kahan sinθ theorem (see Corollary 1 in [25]), if maxi ‖X̂i − X‖ ≤ δ/12, then the leading eigenvectors v̂i1 of X̂i (after choosing the sign appropriately, i.e. 〈v̂i1,v1〉 ≥ 0) are all at a distance of at most 1/4 from v1. Moreover, by Lemma 2,\n1\nm\nm ∑\ni=1\n∥ ∥\n∥v̂ i 1 − v1 − (λ1I−X)†(X̂i −X)v1\n∥ ∥ ∥ ≤ c δ2 · 1 m\nm ∑\ni=1\n‖X̂i −X‖2.\nBy the triangle inequality, this implies\n∥ ∥ ∥ ∥ ∥ 1 m m ∑\ni=1\nv̂i1 − v1 − (λ1I−X)† ( 1\nm\nm ∑\ni=1\n(X̂i −X) ) v1\n∥ ∥ ∥ ∥ ∥ ≤ c δ2 · 1 m m ∑\ni=1\n‖X̂i −X‖2,\nand therefore (as ‖v1‖ = 1), ∥\n∥ ∥ ∥ ∥ 1 m\nm ∑\ni=1\nv̂i1 − v1 ∥ ∥ ∥ ∥\n∥ ≤ c δ2 · 1 m\nm ∑\ni=1\n‖X̂i −X‖2 + ∥ ∥ ∥ (λ1I−X)† ∥ ∥ ∥ · ∥ ∥ ∥ ∥\n∥\n1\nm\nm ∑\ni=1\nX̂i −X ∥ ∥ ∥ ∥\n∥\n. (10)\nSince X has an eigengap of δ, it follows that the minimal non-zero eigenvalue of λ1I−X is at least δ, and therefore ∥ ∥(λ1I−X)† ∥\n∥ ≤ 1/δ. As to the other terms, recall that X̂i is the average of n i.i.d. matrices with mean X, and 1m ∑m i=1 X̂i is the average of mn such i.i.d. matrices. Thus,\nby a matrix Hoeffding inequality (Theorem 1) and a union bound, it holds with probability at least 1− p that\n∀i, ‖X̂i −X‖ ≤ c1 √ b2 log(2dm/p)\nn as well as\n∥ ∥ ∥ ∥ ∥ 1 m m ∑\ni=1\nX̂i −X ∥ ∥ ∥ ∥\n∥\n≤ c1 √ b2 log(2dm/p)\nmn\nfor some constant c1. Combining this with (9) using a union bound, and plugging into (10), it follows that with probability at least 1− p− d exp (\n− δ2nc′b2 ) ,\n∥ ∥ ∥ ∥ ∥ 1 m m ∑\ni=1\nv̂i1 − v1 ∥ ∥ ∥ ∥\n∥\n≤ cc 2 1b 2 log(2dm/p)\nδ2n + c1\n√\nb2 log(2dm/p)\nδ2mn .\nSlightly simplifying, the result follows.\nWe can now complete the proof of Theorem 4.\nProof of Thm. 4. The proof is an easy consequence of Lemma 3. Assuming the events in the lemma occur, we have that the leading eigenvalues of X as well as X̂i for all i are simple, hence the leading eigenvectors are all unique up to a sign. In particular, let v1 be the eigenvector closest to w̃1 = ŵ1, with ties broken arbitrarily, so that ‖ŵ1 − v1‖ ≤ ‖ŵ1 + v1‖. This implies that ŵ1 = v̂ 1 1 (where v̂ 1 1 is as defined in Lemma 3), since otherwise, by the inequality above, we would get ‖ − v̂11 − v1‖ ≤ ‖ − v̂11 + v1‖, which implies in turn 〈v̂11,v1〉 ≤ 0, contradicting the fact that ‖v̂11 − v1‖ = √\n2− 2〈v̂1,v1〉 is at most 1/4 by Lemma 3. Having established that ŵ1 = v̂ 1 1, we note that by Lemma 3 and the triangle inequality, for\nany i > 1,\n‖v̂i1 − v̂11‖ ≤ 1\n2 and therefore ‖v̂i1 − ŵ1‖ ≤\n1 2 .\nAs v̂i1, ŵ1 are unit vectors, this implies that ‖v̂i1 − ŵ1‖ < ‖ − v̂i1 − ŵ1‖. Since for any i, we have ŵi ∈ {−v̂i1, v̂i1}, with the sign chosen based on which vector is closest to ŵ1, it follows that ŵi = v̂ i 1 for all i. Applying Lemma 3 with ŵi = v̂ i 1, we get that with probability at least 1− p− d exp ( −δ2n/cb2 ) ,\n∥ ∥ ∥ 1\nm\nm ∑\ni=1\nŵi − v1 ∥ ∥ ∥ ≤ c′\n(b2 log(2dm/p)\nδ2n +\n√\nb2 log(2dm/p)\nδ2mn\n)\n.\nSquaring both sides and using the fact that (x+ y)2 ≤ 2x2 + 2y2, we get that ∥\n∥ ∥\n1\nm\nm ∑\ni=1\nŵi − v1 ∥ ∥ ∥ 2 ≤ 2(c′)2\n(b4 log2(2dm/p)\nδ4n2 +\nb2 log(2dm/p)\nδ2mn\n)\n. (11)\nThis holds with probability at least 1 − p − d exp ( −δ2n/cb2 )\n. To simplify things a bit, note that we can assume d exp(−δ2n/cb2) ≤ p without loss of generality, since otherwise the bound in the displayed equation above is at least a constant and therefore trivially true (holds with probability 1) if we make the constant c′ sufficiently large. Therefore, we can argue that (11) (with an appropriate c′) holds with probability at least 1− 2p. Absorbing the 2 factor into the p term, slightly increasing c′ appropriately, and simplifying a bit, the result finally follows from the simple observation that\n(v⊤1 w) 2 =\n1\n2\n( 2− ‖w − v1‖2 ) ≥ 1 2 ( 2− 2 ∥ ∥ ∥w − 1 m\nm ∑\ni=1\nŵi\n∥ ∥ ∥ 2 − 2 ∥ ∥ ∥ 1\nm\nm ∑\ni=1\nŵi − v1 ∥ ∥ ∥ 2)\n≥ 1− 2 ∥ ∥\n∥\n1\nm\nm ∑\ni=1\nŵi − v1 ∥ ∥ ∥ 2 ,\nwhere the first inequality follows from the triangle inequality and the inequality (a + b)2 ≤ 2a2 + 2b2, and the second inequality follows since v1 is a unit vector, and by definition, w is the unit vector closest to 1m ∑m i=1 ŵi."
    }, {
      "heading" : "3.3 Lower Bound for Sign Fixing",
      "text" : "We now show that the result of Theorem 4 is tight up to poly-logarithmic factors and cannot be improved in general:\nTheorem 5. For any δ ∈ (0, 1) and d > 1, there exist a distribution over vectors in Rd (of norm at most a universal constant) with eigengap δ in the covariance matrix, such that for any number of machines m and for per-machine sample size any n sufficiently larger than 1/δ2, the aggregated vector v̄1 = 1 m ∑m i=1 v̂ (i) 1 (even after sign fixing with the population eigenvector v1) satisfies\nE\n[\n1− 〈 v̄1\n‖v̄1‖ , e1\n〉2 ]\n= Ω\n(\n1\nδ2mn +\n1\nδ4n2\n)\nThe proof is given in the appendix."
    }, {
      "heading" : "4 A Multi-round Algorithm based on Shift-and-Invert Itera-",
      "text" : "tions\nIn this section we move on to consider distributed algorithms that perform multiple communication rounds. The main motivation, beyond improving some poly-logarithmic factors in the estimation error, is to obtain a result that does not require the per-machine sample size n to grow with the number of machines m, as in the result of Theorem 4.\nTowards this end we consider the use of the Shift-and-Invert meta-algorithm, originally described in [6, 7], to explicitly solve the centralized ERM objective, i.e., find a unit vector that is an approximate solution to maxv:‖v‖=1 v ⊤X̂v.\nThroughout this section we let λ̂1, δ̂ denote the leading eigenvalue and eigengap of X̂, respectively. Also, we assume without loss of generality that b = 1 (i.e., all data points lie in the unit Euclidean ball).\nSince our approach is to approximate the population risk by approximating the empirical risk, we state the following simple lemma for completeness (a proof is given in the appendix).\nLemma 4 (Risk of approximated-ERM for PCA). Let w be a unit vector such that (w⊤v̂1)2 ≥ 1 − ǫ, for some fixed ǫ > 0, where v̂1 is the leading eigenvector of X̂. Then it holds that 1− (w⊤v1)2 ≤ 1− (w⊤v̂1)2 + √ 2ǫ."
    }, {
      "heading" : "4.1 The Shift-and-Invert meta-algorithm",
      "text" : "The Shift-and-Invert algorithm [6, 7] efficiently reduces the problem of computing the leading eigenvector of a positive semidefinite matrix X̂ to that of approximately-solving a polylogarithmic number of linear systems, i.e., finding approximate minimizers of convex quadratic optimization problems of the form\nmin z∈Rd\n{Fλ,w(z) := 1 2 z⊤(λI − X̂)z− z⊤w}, (12)\nwhere λ > λ1(X̂) is a shifting parameter. The algorithm is essentially based on applying power iterations to a shifted and inverted matrix (λI−X̂)−1, where the shifting parameter λ is carefully chosen. The algorithm that implements this reduction, originally described in [6], is given below (see Algorithm 1).\nAlgorithm 1 Shift-and-Invert Power Method\n1: Input: estimate δ̃ for the gap δ̂, accuracy ǫ ∈ (0, 1), failure probability p 2: Set: m1 ← ⌈8 ln ( 144d/p2 ) ⌉,m2 ← ⌈32 ln ( 18d p2ǫ ) ⌉ 3: Set: ǫ̃ ← min {\n1 16\n( δ̃/8 )m1+1\n, ǫ4\n( δ̃/8 )m2+1 }\n4: Set: λ(0) ← 1 + δ̃ , ŵ0 ← random unit vector, s ← 0 5: repeat 6: s ← s+ 1 , Ms ← (λ(s−1)I− X̂) 7: for t = 1...m1 do 8: Find an approximate minimizer - ŵt of Fλ(s−1),ŵt−1(z) such that ‖ŵt −M−1s ŵt−1‖ ≤ ǫ̃ 9: end for\n10: ws ← ŵm1/‖ŵm1‖ 11: Find an approximate minimizer - vs of Fλ(s−1),ws(z) such that ‖vs −M−1s ws‖ ≤ ǫ̃ 12: ∆s ← 12 · 1w⊤s vs−ǫ̃ , λ(s) ← λ(s−1) − ∆s 2 13: until ∆s ≤ δ̃ 14: λ(f) ← λ(s) , Mf ← (λ(f)I− X̂) 15: for t = 1...m2 do 16: Find an approximate minimizer - ŵt of Fλ(f),ŵt−1(z) such that ‖ŵt −M−1f ŵt−1‖ ≤ ǫ̃ 17: end for 18: Return: wf ← ŵm2/‖ŵm2‖\nLemma 5 (Efficient reduction of top eigenvector to convex optimization; originally Theorem 4.2 in [6]). Suppose that δ̂ := λ1(X̂)− λ2(X̂) > 0 and suppose that the estimate δ̃ in Algorithm 1 satisfies δ̃ ∈ [δ̂/2, 3δ̂/4]. Then, with probability at least 1− p, Algorithm 1 finds a unit vector wf such that (w ⊤ f v̂1)\n2 ≥ 1 − ǫ, and the total number of optimization problems of the form (12) solved during the run of the algorithm, is upper bounded by O ( ln(d/p) ln(δ̂−1) + ln (\nd pǫ\n))\n.\nMoreover, throughout the run of the algorithm it holds that 1 + δ̂ ≥ λ(s) − λ̂1 = Ω(δ̂).\nRemark: the purpose of the repeat-until loop in Algorithm 1 is to efficiently find a shifting parameter λ(f) such that c1δ̂ ≤ λ(f)− λ̂1 ≤ c2δ̂ for some universal constants c2 > c1 > 0. When n satisfies n = Ω(δ−2 ln(d/p)), it follows that we can directly find (with high probability) such a shifting parameter, by simply estimating λ̂1, δ̂ from the data of a single machine, without any communication overhead. Also, in this regime, instead of taking the vector ŵ0 to be arbitrary, we can take it to be the leading eigenvector of any single machine, since this will already have a constant correlation with v̂1 (with high probability). Thus, for such n, the total number of optimization problems can be reduced to O(ln(p−1ǫ−1)).\nAlgorithm 1 is a meta-algorithm in the sense that the choice of solver for the optimization problems minFλ,w is unspecified, and any solver will do. A simple calculation shows that a naive application of either the conjugate gradient method or Nesterov’s accelerated gradient method to solve these optimization problems in a distributed manner, i.e., the computation of the gradient vector is distributed across machines, will require overall Õ ( √ λ̂1/δ̂ )\ncommunication rounds, which does not give any improvement over the distributed Lanczos approach, described in Subsection 2.2.2. However, this can be substantially improved by taking advantage of the fact that the data on all machines is sampled i.i.d. from the same distribution. In particular, we present below an approach based on applying a pre-conditioner to the optimization Problem (12), in the spirit of the one described in [27]."
    }, {
      "heading" : "4.2 Faster Distributed Approximation of Linear Systems via Local Preconditioning",
      "text" : "Let M = λI − X̂, for some shift parameter λ > λ̂1, and define the pre-conditioning matrix C = (λ + µ)I − X̂1, where µ is required so C is invertible. Consider now solving the following modified quadratic problem:\nF̃λ,w(y) := 1 2 y⊤C−1/2MC−1/2y − y⊤C−1/2w. (13)\nNote that if y∗ is the optimal solution to Problem (13), i.e.,\ny∗ = C1/2M−1C1/2C−1/2w = C1/2M−1w,\nthen z∗ := C−1/2y∗ is the optimal solution to Problem (12). The idea behind choosing C this way is very intuitive. Ideally we could have chosen C = M, making the condition number of F̃λ,w equal to κ(F̃λ,w) = 1, which is the best we can hope for. The problem of course is that this requires us to explicitly compute M−1/2, which is more challenging then just computing the leading eigenvector of X̂. The next best thing is thus to choose C based only on the data available on any single machine, which allows computing C−1/2 without additional communication overhead, and leads to the choice described above. The following lemma, rephrased from [27], quantifies exactly how such a choice of C helps in improving the condition number of the new optimization problem, Problem (13). The proof is given in the appendix.\nLemma 6. Suppose that µ ≥ ‖X̂− X̂1‖. Then, F̃λ,w(y) is 1-smooth and ( λ−λ̂1 (λ−λ̂1)+2µ ) -strongly convex. In particular, The condition number5 κ (\nF̃λ,w\n)\nsatisfies\nκ (\nF̃λ,w\n) ≤ 1 + 2µ λ− λ1(X̂) .\nMoreover, fixing ỹ ∈ Rd, if we let z̃ := C−1/2ỹ, then it holds that\n‖z̃−M−1w‖ ≤ (λ− λ̂1)−1/2‖ỹ −C1/2M−1w‖.\nFinally, for any p ∈ (0, 1), if we set µ = 4 √ ln(d/p)/n, then the above holds with probability at least 1− p, where this probability depends only on the randomness in X̂1."
    }, {
      "heading" : "4.2.1 Solving the pre-conditioned linear systems",
      "text" : "We now discuss the application of gradient-based algorithms for finding an approximate minimizer of the pre-conditioned problem, Problem (13), in our distributed setting. Towards this end we require a distributed implementation for the first-order oracle of F̃λ,w(y) (i.e., computation of the value and gradient vector at a queried point).\nA straight-forward implementation of the first-order oracle in our distributed setting is given in Algorithm 2.\nWe have the following lemma, the proof of which is deferred to the appendix.\nLemma 7. Fix some λ > λ1(X̂) and w ∈ Rd, and let 1 ≥ µ > 0 be as in Lemma 6. Fix ǫ > 0. Consider the following two-step algorithm:\n1. Apply either the conjugate gradient method or Nesterov’s accelerated method with the distributed first-order oracle described in Algorithm 2 to find ỹ ∈ Rd such that F̃λ,w(ỹ)− miny∈Rd F̃λ,w(y) ≤ ǫ′\n5defined as the smoothness parameter divided by the strong-convexity parameter.\nAlgorithm 2 Distributed First-Order Oracle for F̃λ,w(y)\n1: Input: shift parameter λ > 0, regularization parameter µ > 0, vector w ∈ Rd, query vector y ∈ Rd 2: send ỹ := C−1/2y to machines {2, . . . ,m} for C := (λ+ µ)I− X̂1 {executed on machine 1} 3: for i = 1...m do 4: send ∇̃i := X̂iỹ to machine 1 {executed on each machine i} 5: end for 6: aggregate ∇̃ := 1m ∑m\ni=1 ∇̃i {executed on machine 1} 7: compute F̃λ,w(y) = 1 2(λy\n⊤C−1y − y⊤C−1/2∇̃)− y⊤C−1/2w {executed on machine 1} 8: compute ∇F̃λ,w(y) = λC−1y−C−1/2∇̃ −C−1/2w {executed on machine 1} 9: return: (F̃λ,w(y),∇F̃λ,w(y))\n2. Return z̃ = C−1/2ỹ.\nThen, for ǫ′ = ǫ2\n(\n1 + 2µ λ−λ̂1\n)−1 (λ − λ̂1) it holds that ‖z̃− (λI− X̂1)−1w‖ ≤ ǫ, and the total\nnumber distributed matrix-vector products with the empirical covariance matrix X̂ required to compute z̃ is upper-bounded by\nO\n( √\n1 + 2µ(λ− λ̂1)−1 ln (( 1 + 2µ\nλ− λ̂1\n) ‖w‖/[(λ − λ̂1)ǫ] )\n)\n."
    }, {
      "heading" : "4.3 Putting it all together",
      "text" : "We now state our main result for this section, which is a simple consequence of the previous lemmas. The full proof is given in the appendix.\nTheorem 6. Fix ǫ ∈ (0, 1) and p ∈ (0, 1). Suppose that mn = Ω(δ−2 ln(d/p)). Set µ = 4 √\nln(3d/p)/n. Applying the Shift-and-Invert algorithm, Algorithm 1, with the parameters ǫ, p/3, and applying the algorithm in Lemma 7 with the parameter µ, to approximately solve the linear systems, yields with probability at least 1− p a unit vector wf such that (w⊤f v̂1)2 ≥ 1− ǫ, after executing at most\nO\n\n\n√\n√\nln(d/p) δ √ n\n[\nln\n(\nd\npǫ2\n)\nln\n(\n√\nln(d/p) δ2 √ n\n)\n+ ln2 ( d\npǫ2\n)\nln\n(\n1\nδ\n)\n]\n\n = Õ\n(√\n1\nδ √ n\n)\ndistributed matrix-vector products with the empirical covariance matrix X̂.\nRemark: Our approach of using Shift-and-Invert with the preconditioning technique for linear systems is applicable in a much more general setting. Namely, all that is required for the method to obtain accelerated rates over standard algorithms, is (1) a non-zero gap in the aggregated empirical matrix, i.e., δ(X̂) > 0, and (2) that the distance ‖X̂− X̂1‖ admits a non-trivial upper-bound."
    }, {
      "heading" : "5 Experiments",
      "text" : "To validate some of our theoretical findings we conducted experiments with single-round algorithms on synthetic data. We generated synthetic datasets using two distributions. For both distributions we used the covariance matrix X = UΣU⊤ with U being a random d × d orthonormal matrix and Σ is diagonal satisfying: Σ(1, 1) = 1, Σ(2, 2) = 0.8, ∀j ≥ 3 : Σ(j, j) =\n0.9 · Σ(j − 1, j − 1), i.e., δ = 0.2. One dataset was generated according to the normal distributions N (0,X), and for the second datasets we generated samples by taking x = √\n3/2X1/2y where y ∼ U [−1, 1]. In both cases we set d = 300.\nBeyond the single-round algorithms that are based on aggregating the individual ERM solutions described so far, we propose an additional natural aggregation approach, based on aggregating the individual projection matrices. More concretely, letting {v̂(i)1 }mi=1 denote the leading eigenvectors of the individual machines, let P̄1 := 1 m ∑m i=1 v̂ (i) 1 v̂ (i)⊤ 1 . We then take the final estimate w to be the leading eigenvector of the aggregated matrix P̄1. Note that as with the sign-fixing based aggregation, this approach also resolves the sign-ambiguity in the estimates produced by the different machines, which circumvents the lower bound result of Theorem 3.\nFor both datasets we fixed the number of machines to m = 25. We tested the estimation error (i.e., the value 1−(w⊤v1)2 where v1 is the leading eigenvector of X andw is the estimator) of five benchmarks vs. the per-machine sample size n: the centralized solution v̂1, the average of the individual (unbiased) ERM solutions (normalized to unit norm),the average of ERM solutions with sign-fixing, and the leading eigenvector of the averaged projection matrix. We also plotted the average loss of the individual ERM solutions. Results are averaged over 400 independent runs.\nThe results appear in Figure 1. It is observable that the results for both distributions are very similar. We can see that, as our lower bound in Theorem 3 suggests, simply averaging and normalizing the individual ERM solutions has significantly worse performance than the centralized ERM solution. Perhaps surprisingly, the performance of this estimator is even worse than the average error of an estimate computed using only a single machine. We see that both aggregation methods that are based on correlating the individual ERM solutions, namely the sign-fixing-based estimator, and the proposed averaging-of-projections heuristic, are asymptotically consistent with the centralized ERM. In particular, the averaging-of-projections scheme, at least empirically, significantly outperforms the sign-fixing approach, which justifies further theoretical investigation of this heuristic. For the sign fixing approach, we can see that as suggested by our bounds, the estimator is not consistent with the centralized ERM solution for small values of n."
    }, {
      "heading" : "A Proofs Omitted from Section 3",
      "text" : "A.1 Proof of Theorem 3\nProof. Consider the following distribution over R2.\nx = e1 +\n(\nǫ1 ǫ2\n)\n, ǫ1, ǫ2 ∼ U{−1,+1},\nwhere e1 is the first standard basis vector in R 2.\nThe population covariance matrix and the empirical covariance matrix of a sample of size n are clearly given by\nX =\n(\n2 0 0 1\n)\n, X̂(n) =\n(\n2 yn yn 1\n)\n,\nwhere yn is a random variable which is the average of n U{−1,+1} random variables. By elementary calculations we have that the leading eigenvector of X̂(n) is given by\nv̂1 = σ · C(yn) · ( 1, 2yn\n1 + √\n1 + 4y2n\n)\n,\nwhere\nC(yn) :=\n\n1 +\n(\n2yn\n1 + √\n1 + 4y2n\n)2 \n\n−1/2\nis the normalization factor that guarantees that v̂1 is a unit vector. In particular, it holds that 1/ √ 2 ≤ C(yn) ≤ 1. The random variable σ ∼ U{−1,+1} is independent of yn and determines the sign of v̂1, which follows from our assumption that v̂1 is generated by unbiased ERM.\nConsider now the average of m such unit vectors v̂ (1) 1 ..v̂ (m) 1 given by v̄ = 1 m ∑m i=1 v̂ (i) 1 and the normalized estimate v̄1/‖v̄1‖, and recall that the leading eigenvector of the population covariance matrix is e1. It holds that\n〈 v̄1‖v̄1‖ , e1〉2 =\nv̄1(1) 2\nv̄1(1)2 + v̄1(2)2 = 1− v̄1(2)\n2\nv̄1(1)2 + v̄1(2)2 . (14)\nTowards upper-bounding the RHS of (14) in expectation, the main step is to lower bound the random variable |v̄1(2)| using Chebyshev’s inequality.\nIt holds that\nE[|v̄1(2)|] = E [∣ ∣ ∣\n∣\n1\nm v̂ (i) 1 (2)\n∣ ∣ ∣ ∣ ] = E\n\n\n∣ ∣ ∣ ∣ ∣ ∣ 1 m m ∑\ni=1\nσ(i) 2C(y\n(i) n )y (i) n\n1 +\n√\n1 + 4y (i)2 n\n∣ ∣ ∣ ∣ ∣ ∣  \n= (a) E\n\n\n∣ ∣ ∣ ∣ ∣ ∣ 1 m m ∑\ni=1\nσ(i) 2C(y\n(i) n )|y(i)n |\n1 +\n√\n1 + 4y (i)2 n\n∣ ∣ ∣ ∣ ∣ ∣  \n= E{σ(i)}\n\nE{y(i)n }\n\n\n∣ ∣ ∣ ∣ ∣ ∣ 1 m m ∑\ni=1\nσ(i) 2C(y\n(i) n )|y(i)n |\n1 +\n√\n1 + 4y (i)2 n\n∣ ∣ ∣ ∣ ∣ ∣ | {σ(i)}    \n≥ (b) E{σ(i)}\n\n\n∣ ∣ ∣ ∣ ∣ ∣ E{y(i)n }   1 m m ∑\ni=1\nσ(i) 2C(y\n(i) n )|y(i)n |\n1 +\n√\n1 + 4y (i)2 n\n| {σ(i)}\n\n\n∣ ∣ ∣ ∣ ∣ ∣  \n= (c) E{σ(i)}\n[∣\n∣ ∣ ∣ ∣ 1 m\nm ∑\ni=1\nσ(i)\n∣ ∣ ∣ ∣ ∣ ] · Eyn [ 2C(yn)|yn| 1 + √\n1 + 4y2n\n]\n= (d) Θ\n(\n1√ mn\n)\n, (15)\nwhere (a) follows since σ(i)y (i) n ∼ σ(i)|y(i)n | and C(y(i)n )/(1 +\n√\n1 + 4y (i)2 n ) depends only on |y(i)n |,\n(b) follows from the triangle inequality, and (c) follows since {σ(i)}i∈[m] and {y(i)n }i∈[m] are independent random variables. Finally, it is easy to verify that (d) follows since\n∑m i=1 σ (i)/m is the average of m U{−1,+1} random variables and hence its expected absolute value is Θ(1/ √ m). Similarly the expected absolute value of yn is Θ(1/ √ n) and C(yn)/(1 + √\n1 + 4y2n) is lower bounded by a positive constant.\nAlso, observe that\nE[v̄1(2) 2] = E\n[\n(\n1\nm v̂ (i) 1 (2)\n)2 ]\n= 1\nm E[v̂1(2)\n2] = 1\nm E\n\n\n(\n2C(yn)yn\n1 + √\n1 + 4y2n\n)2 \n\n≥ 1 2m E[y2n] = Θ\n(\n1\nmn\n)\n, (16)\nwhere the inequality follows since |yn| ≤ 1 and 1/ √ 2 ≤ C(yn) ≤ 1.\nCombining Eq. (15) and Eq. (16), we have by an application of Chebyshev’s inequality to the random variable |v̄1(2)| that there exists universal constants c1 > 0 such that\nPr\n(\n|v̄1(2)| ≤ 1\nc1 √ mn\n)\n≤ 1 4 . (17)\nAlso, it is easy to verify that\nE[v̄1(1) 2] = O(1/m), E[v̄1(2) 2] = O(1/m).\nThus, by a simple application of Markov’s inequality we have that there exists a universal constant c2 > 0 such that\nPr\n(\nmax{v̄1(1)2, v̄1(2)2} ≥ 1\nc3m\n)\n≤ 1 4 . (18)\nUsing Eq. (14), (17) and (18) we finally have that\nE\n[\n〈 v̄1‖v̄1‖ , e1〉2\n] = 1− E [\nv̄1(2) 2\nv̄1(1)2 + v̄1(2)2\n] = 1−Ω ( 1\nn\n)\n.\nA.2 Proof of Theorem 5\nThe proof is a combination of the following two lemmas, each proves one of the lower bounds. We first state the two lemmas and then prove them.\nLemma 8. For any δ ∈ (0, 1) and d > 1, there exist a distribution over vectors in Rd (of norm at most 2) such that the covariance matrix has eigengap δ, and for any number of machines m and per-machine sample size n, the aggregated vector v̄1 = 1 m ∑m i=1 v̂ (i) 1 (even after sign fixing) satisfies\nE\n[\n1− 〈 v̄1‖v̄1‖ , e1〉2\n]\n= Ω\n(\nmin\n{\n1\nm ,\n1\nδ2mn\n})\n.\nLemma 9. For any δ ∈ (0, 1) and d > 1, there exist a distribution over vectors in Rd (of norm at most 2) with eigengap δ in the covariance matrix, such that for any number of machines m and for per-machine sample size any n sufficiently larger than 1/δ2, the aggregated vector v̄1 = 1 m ∑m i=1 v̂ (i) 1 (even after sign fixing with the population eigenvector v1) satisfies\nE\n[\n1− 〈 v̄1‖v̄1‖ , e1〉2\n]\n= Ω\n(\n1\nδ4n2\n)\n.\nproof of Lemma 8. We will prove the result for d = 2 (i.e. a distribution in R2). This is without loss of generality, since we can always embed the distribution below in Rd for any d > 2 (say, by having all coordinates other than the first two identically zero).\nConsider the distribution defined by the random vector x = √ 1 + δe1 + σe2, where σ is uniformly distributed on {−1,+1}, and e1 = (1, 0), e2 = (0, 1) are the standard basis vectors. Clearly, the population covariance matrix is\nX := E[xx⊤] =\n(\n1 + δ 0 0 1\n)\n,\nwith a leading eigenvector (1, 0). Let us now consider the distribution of the output of a machine i. Given n samples, the empirical covariance matrix is\nX̂(n) =\n(\n1 + δ yn yn 1\n) , yn := √ 1 + δ · 1\nn\nn ∑\ni=1\nǫi,\nwhere ǫi are i.i.d. and uniformly distributed on {−1,+1}. Using a standard formula for the leading eigenvector of a 2 × 2 matrix [1], we have that the leading eigenvector (and hence the output of any machine i) is of the form\nv̂1 = 1 ‖û‖ û where û := ( δ 2 + √ δ2 4 + y2n , yn ) . (19)\nNote that with this formula, the leading eigenvector is always closer to (1, 0) than (−1, 0), and converges to (1, 0) as n → ∞. Thus, we can view the random variable v̂(i) as the output of any machine i, given n samples and after fixing the sign.\nConsider now the average of m such vectors given by v̄ = 1m ∑m i=1 v̂ (i) 1 . Using (19), we have\nthat\nE[v̄1(2) 2] = E\n\n\n(\n1\nm\nm ∑\ni=1\nv̂ (i) 2\n)2 \n = 1\nm2\nm ∑\ni=1\nE[(v̂ (i) 2 )\n2] = 1\nm E[(v̂(2))2]\n= 1\nm E\n\n y2n δ2\n2 + 2y 2 n + δ\n√\nδ2 4 + y 2 n\n\n . (20)\nBy definition of yn and recalling that δ ∈ [0, 1], we have that there exist universal constants c1, c2 > 0 such that with constant probability it holds that c1/n ≥ y2n ≥ c2/n. Using this fact and considering the two cases 1/n ≥ δ2 and 1/n < δ2 in the RHS of Eq. (20) separately, we can see that\nE[v̄1(2) 2] = Ω\n(\n1 m min{1, 1 δ2n } ) . (21)\nUsing Eq. (21) we have that\nE\n[\n〈 v̄1‖v̄1‖ , e1〉2\n]\n= E\n[\nv̄1(1) 2\nv̄1(1)2 + v̄1(2)2\n] = 1− E [\nv̄1(2) 2\nv̄1(1)2 + v̄1(2)2\n]\n≤ 1− E [ v̄1(2) 2 ] = 1− Ω ( min { 1\nm ,\n1\nδ2mn\n})\n,\nwhere the inequality follows since ‖v̄1‖ ≤ 1.\nproof of Lemma 9. As in Lemma 8, we prove the result for d = 2, however, using a different construction. Consider the defined by the random vector\nx = √ 1 + δ · e1 + ξ · e2,\nwhere ξ is an independent random variable defined as:\nξ =\n{ √ 2 w.p. 1/3\n−1/ √ 2 w.p. 2/3\nIt is easy to verify that E[ξ] = 0, E[ξ2] = 1, E[ξ3] = 1/ √ 2. As we shall see, choosing ξ to be asymmetric (as opposed to ǫ in the proof of Lemma 9) will be key to our construction. Clearly, the population covariance and the empirical covariance of a sample of size n are given by we have\nX = E[xx⊤] =\n(\n1 + δ 0 0 1\n)\n, X̂(n) =\n(\n1 + δ yn yn zn\n)\n,\nwhere\nyn := √ 1 + δ · 1\nn\nn ∑\ni=1\nξi , zn := 1\nn\nn ∑\ni=1\nξ2i ,\nwith ξ1, . . . , ξn being i.i.d. copies of the random variable ξ.\nClearly the leading eigenvector of X is e1 = (1, 0). Consider now v̂ (1) 1 , . . . , v̂ (m) 1 to be the\nleading eigenvectors of m i.i.d. empirical covariance matrices of n samples, X̂ (1) (n), . . . , X̂ m) (n), and let v̄1 denote their average after sign-fixings according to the leading eigenvector of the population covariance e1. In the following, we let v̂ i j denote the jth coordinate in the eigenvector v̂ (i) 1 . It holds that\nE\n[\n〈 v̄1‖v̄1‖ , e1〉2\n]\n= E\n[\nv̄1(1) 2\nv̄1(1)2 + v̄1(2)2\n] = 1− E [\nv̄1(2) 2\nv̄1(1)2 + v̄1(2)2\n]\n≤ 1− E [ v̄1(2) 2 ] = 1− E\n\n\n(\n1\nm\nm ∑\ni=1\nsign(v̂i1)v̂ i 2\n)2 \n\n≤ 1− ( 1\nm\nm ∑\ni=1\nE [ sign(v̂i1)v̂ i 2 ]\n)2\n= 1− ( E[sign(v̂11)v̂ 1 2 ] )2 , (22)\nwhere the first inequality follows since ‖v̄1‖ ≤ 1, the second inequality follows from Jensen’s inequality, and the last equality follows from the fact that v̂\n(1) 1 , . . . , v̂ (m) 1 are i.i.d. random vari-\nables. From this chain of inequalities, it follows that it is enough to lower bound ( E[sign(v̂11)v̂ 1 2 ] )2 , where v̂1 is the leading eigenvector computed by machine 1. Let us now consider the distribution of the leading eigenvector of the empirical covariance matrix X̂(n). Using a standard formula for the leading eigenvector of a 2×2 matrix [1], we have that this leading eigenvector v̂1 is proportional to\n\n δ + 1− zn 2 +\n√\n( δ + 1− zn 2\n)2\n+ y2n , yn\n\n (23)\nAssume for now that zn ≤ 1+cδ for some positive constant c to be fixed later (note this happens with arbitrarily high probability as n → ∞, as zn converges to 1 in probability). In that case, the sign of the first coordinate in the formula above is positive, and has the same sign as the first coordinate of the leading eigenvector v1 = (1, 0). Moreover, we know that v̂ (1) 1 must have unit norm, from which follows that\nsign(v̂11) · v̂(1)1 =\n(\nδ+1−zn 2 +\n√\n( δ+1−zn 2 )2 + y2n , yn\n)\n√\ny2n +\n(\nδ+1−zn 2 +\n√\n( δ+1−zn 2 )2 + y2n\n)2 . (24)\nIn particular, letting rn = 1− zn, we have that if rn ≥ −cδ, then\nsign(v̂11) · v̂12 = yn √\ny2n +\n(\nδ+rn 2 +\n√\n(\nδ+rn 2 )2 + y2n\n)2\n= yn √\n√ √ √y2n + ( δ+rn 2 )2\n(\n1 +\n√\n1 + (\n2yn δ+rn\n)2 )2\n. (25)\nTowards using Eq. (22) to derive the lower bound, the main step is to bound the expectation of the RHS of Eq.(25) away from zero. To get an intuition why this is possible, observe that when n → ∞ (in particular, when it is significantly larger than 1/δ2), it holds that\nRHS of (25) ≈ yn√ y2n +Θ(δ 2) ,\nsince in this regime, with high probability, rn << δ and yn << 1. Now comes to play our choice of ξ to be an asymmetric random variable. If, just for sake of intuition, we set n = 1, it is easy to verify that despite the fact that E[yn] = 0, it holds that\nE\n[\nyn √\ny2n +Θ(δ 2)\n]\n= E\n[\nξ √\nξ2 +Θ(δ2)\n]\n< 0.\nNote in particular that taking ξ to be uniformly distributed on {−1,+1}, as in Lemma 8, will still give zero expectation, and hence will not work. We now formalize this intuition. We will use a Taylor expansion of the formula above, in order to bound its expectation (over yn, rn), from which a lower bound on ( E [\nsign(v̂11) · v̂12 ])2 would follow. To that end, define the function\ng(t) = tyn √\n√ √ √(tyn)2 + ( δ+trn 2 )2\n(\n1 +\n√\n1 + (\n2tyn δ+trn\n)2 )2\n, t ∈ [0, 1],\nand note that g(1) equals sign(v̂11) · v̂12 as defined above. By a Taylor expansion, we have\nsign(v̂11) · v̂12 = g(1) = g(0) + g′(0) + 1 2 g′′(0) +\ns3\n6 g′′′(s)\nfor some s ∈ [0, 1]. A tedious calculation of g’s derivatives6 reveals that this implies\nsign(v̂11) · v̂12 = yn δ − rnyn δ2 ±O ( |yn|3 + |rn|3 δ3 ) , (26)\nassuming max{|yn|, |rn|} ≤ cδ for some constant c (hence fixing c we used in our earlier assumptions on rn, zn). To simplify notation, let qn = sign(v̂ 1 1) · v̂12 , let bn = ynδ − rnyn δ2 ±O ( |yn|3+|rn|3 δ3 ) be the expression on the right-hand side of the equation above, and let A be the event that max{|yn|, |rn|} ≤ cδ indeed holds. Also, note that with probability 1, |qn| ≤ 1 and |bn| = O(1/δ3). Thus, by Eq. (26), we have that E[qn|A] = E[bn|A], and therefore\nE[qn] = Pr(¬A) · E[qn|¬A] + Pr(A) · E[qn|A] = Pr(¬A) · E[qn|¬A] + Pr(A) · E[bn|A] = Pr(¬A) · E[qn|¬A] + E[bn]− Pr(¬A) · E[bn|¬A] = E[bn]±O ( Pr(¬A)/δ3 ) .\n6Using MATLAB’s symbolic math toolbox together with some straightforward manual calculations\nPlugging back the definitions of qn, bn, A, we get that\nE [ sign(v̂11) · v̂12 ] = E\n[\nyn δ − rnyn δ2 ±O ( |yn|3 + |rn|3 δ3 )] ±O ( 1 δ3 Pr(max{|yn|, |rn|} > cδ) ) .\nRecalling that yn = √ 1 + δ · 1n ∑n i=1 ξi and rn = 1 − zn = 1 − 1n ∑n i=1 ξ 2 i , where ξi are i.i.d.\ncopies of a zero-mean, bounded random variable satisfying E[ξ3] = 1/ √ 2, and using Hoeffding’s inequality, it is easily verified that the above equals\n0 + √ 1 + δ\n1√ 2δ2n ±O (\n1\n(δ2n)3/2\n) ±O ( 1\nδ3 exp(−Ω(nδ2))\n)\n,\nwhich is Ω ( 1 δ2n ) assuming n is sufficiently larger than 1/δ2. As a result, we get that ( E [ sign(v̂11) · v̂12 ])2\n= Ω (\n1 δ4n2\n)\nas required."
    }, {
      "heading" : "B Proofs Omitted from Section 4",
      "text" : "B.1 Proof of Lemma 4\nProof. Let • denote the standard inner product for matrices, i.e., A •B = Tr(AB⊤). It holds that\n(w⊤v1) 2 = ww⊤ • v1v⊤1 ≥ v̂1v̂⊤1 • v1v⊤1 − ‖ww⊤ − v̂1v̂⊤1 ‖F · ‖v1v⊤1 ‖\n= (w⊤v1) 2 −\n√ 2(1− 1(w⊤v̂1)2) ≥ (w⊤v1)2 − √ 2ǫ.\nB.2 Proof of Lemma 6\nProof. Observe that C = M+ (X̂− X̂1) + µI. Thus, by our assumption on µ it follows that\nM+ 2µI C M. (27)\nSince F̃λ,w(y) is twice differentiable, in order to bound its smoothness and strong-convexity parameters, it suffices to upper bound the largest eigenvalue and lower bound the smallest eigenvalue of its Hessian, respectively.\nThe Hessian of F̃λ,w(y) is given by ∇2F̃λ,w(y) = C−1/2MC−1/2. From Eq. (27) it follows that we can write M = C−∆ where ∆ 0. Thus we have that\nλ1(C −1/2MC−1/2) = λ1(C −1/2(C−∆)C−1/2) ≤ λ1(I) = 1, (28)\nwhere the inequality follows since C−1/2∆C−1/2 is positive semidefinite. Since M,C are invertible and positive definite, Eq. (27) implies that\nM−1 C−1 (M+ 2µI)−1. (29)\nThus we have that\nλd(C −1/2MC−1/2) = λd(M 1/2C−1/2C−1/2MC−1/2C1/2M−1/2) = λd(M 1/2C−1M1/2)\n≥ λd(M1/2(M+ 2µI)−1M1/2) = min i∈[d] { λi(M) λi(M) + 2µ }\n= λd(M)\nλd(M) + 2µ = λ− λ̂1 (λ− λ̂1) + 2µ , (30)\nwhere the first equality follows from matrix similarity and the fact that M,C are invertible, and the first inequality follows from Eq. (29).\nTo prove the second part of the lemma we observe that\n‖z̃−M−1w‖ = ‖C−1/2ỹ −C−1/2C1/2M−1w‖ ≤ ‖C−1/2‖ · ‖ỹ −C1/2M−1w‖ ≤ 1√\nλ− λ1(X̂) ‖ỹ −C1/2M−1w‖,\nwhere the second inequality follows from Eq. (29). Finally, the last part of the lemma follows from a direct application of Theorem 1 to upper bound ‖X− X̂1‖.\nB.3 Proof of Lemma 7\nProof. Let z∗ := (λI− X̂)−1w,y∗ := C1/2(λI− X̂)−1w, and recall that z∗ and y∗ are the global minimizers of Fλ,w(z) and F̃λ,w(y), respectively. Using the results of Lemma 6 we have that\n‖z̃− z∗‖ ≤ (λ− λ̂1)−1/2‖ỹ − y∗‖ ≤ (λ− λ̂1)−1/2 √ 2 ( 1 + 2µ\nλ− λ̂1\n)\nǫ′,\nwhere the second inequality follows from the strong-convexity of F̃λ,w(y). Thus, it suffices to set ǫ′ as stated in the lemma in order to obtain the approximation guarantee for z̃.\nTo upper-bound the total number of communication rounds required to obtain ỹ with the guarantee prescribed in the lemma, we note that both the conjugate gradient method and Nesterov’s accelerated gradient method require\nO\n( √\nβ α ln ( ‖y∗‖/ǫ′ )\n)\n(31)\ncalls to the first-order oracle of F̃λ,w(y) to obtain ỹ satisfying F̃λ,w(ỹ)−miny∈Rd F̃λ,w(y) ≤ ǫ′, where α and β are the strong-convexity and smoothness parameters of F̃λ,w, respectively, and assuming w.l.o.g. that the initial iterate is y0 = ~0. Thus, by our construction of a distributed first-order oracle given in Algorithm 2, we have that the total number of communication rounds is upper bounded by (31). The lemma now follows from noticing that by Lemma 6 we have that β/α = 1 + 2µ\nλ−λ̂1 and that\n‖y∗‖ = ‖C1/2(λI− X̂1)w‖ ≤ λ1(C1/2)(λ− λ̂1)−1‖w‖ = O ( ‖w‖/(λ − λ̂1) ) .\nB.4 Proof of Theorem 6\nProof. Under our assumption that mn = Ω(δ−2 ln(d/p)), the following three events all hold with probability at least 1− p (each of which holds w.p. at least 1− p/3):\n1. the output wf satisfies (w ⊤ f v̂1) 2 ≥ 1 − ǫ (holds w.p. 1− p/3 by applying Lemma 5 with our choice of parameters)\n2. δ̂ = Θ(δ) (by applying Theorem 1)\n3. ‖X̂− X̂1‖ ≤ µ, where µ is as prescribed in the Theorem (by applying Theorem 1)\nThe approximation guarantee of wf follows directly from Lemma 5. It thus remains to upperbound the number of matrix-vector products. Thus, combining Lemmas 5 and 7 we have that when using either the conjugate gradient method or Nesterov’s accelerated method to approximately solve the linear systems in Algorithm 1, as prescribed in Lemma 7, the total number of distributed matrix-vector products with X̂ is:\nO\n(\nln\n(\nd\npǫ\n) · ( √ 1 + 2µ\nδ\n(\nln δ−1 ln\n(\nd\npǫ\n)\n+ ln\n(\n(1 + 2µ/δ)\nδǫ̃\n))\n))\n=\nO\n(\n√\n1 + 2µ\nδ\n( ln δ−1 ln2 ( d\npǫ\n)\n+ ln\n(\nd\npǫ\n)(\nln\n(\n(1 + 2µ/δ)\nδ\n)\n+ ln\n(\n1\nǫ̃\n)))\n)\n=\nO\n(\n√\n1 + 2µ\nδ\n( ln δ−1 ln2 ( d\npǫ\n)\n+ ln\n(\nd\npǫ\n)\nln\n(\n(1 + 2µ/δ)\nδ\n) + ln2 ( d\npǫ\n)\nln\n(\n1\nδ\n))\n)\n,\nwhere the first term in the O(·) in the first row accounts for the total number of instances of Fλ,w(z) needs to be solved, given by the bound in Lemma 5, and the second term in the first row accounts for the communication-complexity of solving each such instance according to Lemma 7. Additionally, we have used Lemma 5 to lower bound λ − λ̂1 = Ω(δ̂), and ǫ̃(ǫ) is as prescribed in Algorithm 1. Finally, we have upper-bounded ln(‖w‖), in all instances of Fλ,w(z) solved throughout the run of the algorithm, by noticing that in all of them it holds that\nln(‖w‖) = O ( ln ( λ(s) − λ̂1)−max{m1,m2} )) = O\n(\nln δ−1 ln\n(\nd\npǫ\n))\n,\nwhere m1,m2 are as prescribed in Algorithm 1, and we have used Lemma 5 again to lower bound λ(s) − λ̂1 = Ω(δ).\nFinally, using Lemma 6, we can set µ = 4 √\nln(3d/p)√ n . Thus, the overall number of communi-\ncation rounds is upper-bound by\nO\n\n\n√\n√\nln(d/p) δ √ n\n(\nln\n(\nd\npǫ2\n)\nln\n(\n√\nln(d/p) δ2 √ n\n)\n+ ln2 ( d\npǫ2\n)\nln\n(\n1\nδ\n)\n)\n\n ."
    }, {
      "heading" : "C Proof of the Davis-Kahan sinθ Theorem",
      "text" : "We prove Theorem 2 in greater generality. In particular, Theorem 2 follows from setting k = 1 in the next theorem.\nTheorem 7 (Davis-Kahan sinθ theorem). Let X,Y be symmetric real d × d matrices and fix k ∈ [d]. Let VX and VY denote d × k matrix whose columns are the top k eigenvectors of X and the matrix whose columns are the top k eigenvectors of Y, respectively. Also, suppose that δk(X) := λk(X)− λk+1(X) > 0. Then it holds that\n‖VXV⊤X −VYV⊤Y‖F ≤ 2 ‖X −Y‖ δk(X) .\nProof. Throughout the proof we denote the projection matrices:\nPX := VXV ⊤ X, P ⊥ X := I−VXV⊤X, PY := VYV⊤Y, P⊥Y := I−VYV⊤Y,\ni.e., PX is the projection matrix onto the top k eigenvectors of X and P ⊥ X is the projection matrix onto the lower d− k eigenvectors, and same goes for PY,P⊥Y. We also let A •B denote the standard inner products between matrices A,B.\nWe can write PY as\nPY = PXPYPX +P ⊥ XPYPX +PXPYP ⊥ X +P ⊥ XPYP ⊥ X. (32)\nObserve that\nPXPYP ⊥ X •X = Tr\n(\nPXPYP ⊥ XX\n) = Tr ( PYP ⊥ XXPX ) = 0, (33)\nwhere the second equality follows from the cyclic property of the trace, and the last equality follows since P⊥XXPX = 0d×d. Using Eq. (32) and (33) we have that\nPY •X = PXPYPX •X+P⊥XPYP⊥X •X = Tr (PXPYPXX) + Tr ( P⊥XPYP ⊥ XX )\n= Tr (PYPXX) + Tr ( P⊥XPYP ⊥ XP ⊥ XX ) ≤ Tr (PYPXX) + Tr ( P⊥XPYP ⊥ X ) · λ1(P⊥XX) = Tr (PYPXX) + λk+1(X) · Tr ( P⊥XPY ) , (34)\nwhere the inequality follows since for any two positive semidefinite matrices A,B it holds that Tr(AB) ≤ Tr(A) · λ1(B) and the fact that P⊥XX is positive semidefinite. The last equality follows since λ1(P ⊥ XX) = λk+1(X). It further holds that\nPY •Y ≥ PX •Y = Tr(PXX) +PX • (Y −X). (35)\nSubtracting Eq. (35) from Eq. (34) we have that\nTr (PYPXX) + λk+1(X) · Tr ( P⊥XPY ) − Tr(PXX)−PX • (Y −X) ≥ PY •X−PY •Y.\nRearranging we have that\nTr ((I −PY)PXX)− λk+1(X) · Tr ( P⊥XPY )\n≤ (Y −X) • (PY −PX) ≤ ‖X−Y‖ · ‖PX −PY‖F . (36)\nIt holds that\nTr ((I −PY)PXX) = Tr (PX(I−PY)PXPXX) ≥ Tr (PX(I−PY)PX) · λk(PXX) = Tr (PX −PYPX)) · λk(X) = (k −PX •PY) · λk(X)\n= λk(X)\n2 ‖PX −PY‖2F . (37)\nFurthermore, it holds that\nTr ( P⊥XPY ) = Tr ((I−PX)PY) = k −PX •PY = 1\n2 ‖PX −PY‖2F . (38)\nPlugging Eq. (37) and (38) into Eq. (36), we have that\n1 2 ‖PX −PY‖2F · (λk(X)− λk+1(X)) ≤ ‖X−Y‖ · ‖PX −PY‖F , (39)\nwhich completes the proof."
    } ],
    "references" : [ {
      "title" : "Even faster SVD decomposition yet without agonizing pain",
      "author" : [ "Zeyuan Allen Zhu", "Yuanzhi Li" ],
      "venue" : "In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2016
    }, {
      "title" : "Fast global convergence of online",
      "author" : [ "Zeyuan Allen Zhu", "Yuanzhi Li" ],
      "venue" : "PCA. CoRR,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2016
    }, {
      "title" : "The fast convergence of incremental PCA",
      "author" : [ "Akshay Balsubramani", "Sanjoy Dasgupta", "Yoav Freund" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2013
    }, {
      "title" : "Optimal principal component analysis in distributed and streaming models",
      "author" : [ "Christos Boutsidis", "David P Woodruff", "Peilin Zhong" ],
      "venue" : "In Proceedings of the 48th Annual ACM SIGACT Symposium on Theory of Computing,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2016
    }, {
      "title" : "Fast and simple pca via convex optimization",
      "author" : [ "Dan Garber", "Elad Hazan" ],
      "venue" : "arXiv preprint arXiv:1509.05647,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2015
    }, {
      "title" : "Faster eigenvector computation via shift-and-invert",
      "author" : [ "Dan Garber", "Elad Hazan", "Chi Jin", "Sham M. Kakade", "Cameron Musco", "Praneeth Netrapalli", "Aaron Sidford" ],
      "venue" : "preconditioning. CoRR,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2016
    }, {
      "title" : "The differentiation of pseudo-inverses and nonlinear least squares problems whose variables separate",
      "author" : [ "Gene H Golub", "Victor Pereyra" ],
      "venue" : "SIAM Journal on numerical analysis,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1973
    }, {
      "title" : "Analysis of a complex of statistical variables into principal components",
      "author" : [ "H. Hotelling" ],
      "venue" : "J. Educ. Psych., 24",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 1933
    }, {
      "title" : "Communication-efficient distributed dual coordinate ascent",
      "author" : [ "Martin Jaggi", "Virginia Smith", "Martin Takác", "Jonathan Terhorst", "Sanjay Krishnan", "Thomas Hofmann", "Michael I Jordan" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2014
    }, {
      "title" : "Matching matrix bernstein with little memory: Near-optimal finite sample guarantees for oja’s algorithm",
      "author" : [ "Prateek Jain", "Chi Jin", "Sham M Kakade", "Praneeth Netrapalli", "Aaron Sidford" ],
      "venue" : "arXiv preprint arXiv:1602.06929,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2016
    }, {
      "title" : "Matching matrix bernstein with little memory: Near-optimal finite sample guarantees for oja’s algorithm",
      "author" : [ "Prateek Jain", "Chi Jin", "Sham M Kakade", "Praneeth Netrapalli", "Aaron Sidford" ],
      "venue" : "arXiv preprint arXiv:1602.06929,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2016
    }, {
      "title" : "Principal component analysis",
      "author" : [ "IT Jolliffe" ],
      "venue" : "2002. Spring-verlag, New York",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Distributed stochastic variance reduced gradient methods",
      "author" : [ "Jason D. Lee", "Tengyu Ma", "Qihang Lin" ],
      "venue" : "CoRR, abs/1507.07595,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2015
    }, {
      "title" : "Improved distributed principal component analysis",
      "author" : [ "Yingyu Liang", "Maria-Florina F Balcan", "Vandana Kanchanapally", "David Woodruff" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2014
    }, {
      "title" : "On differentiating eigenvalues and eigenvectors",
      "author" : [ "Jan R Magnus" ],
      "venue" : "Econometric Theory,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1985
    }, {
      "title" : "On lines and planes of closest fit to systems of points in space",
      "author" : [ "K. Pearson" ],
      "venue" : "Philosophical Magazine, 2(6):559–572",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 1901
    }, {
      "title" : "AIDE: fast and communication efficient distributed optimization",
      "author" : [ "Sashank J. Reddi", "Jakub Konecný", "Peter Richtárik", "Barnabás Póczos", "Alexander J. Smola" ],
      "venue" : null,
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2016
    }, {
      "title" : "A stochastic PCA and SVD algorithm with an exponential convergence rate",
      "author" : [ "Ohad Shamir" ],
      "venue" : "In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2015
    }, {
      "title" : "Convergence of stochastic gradient descent for PCA",
      "author" : [ "Ohad Shamir" ],
      "venue" : "In Proceedings of the 33nd International Conference on Machine Learning,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2016
    }, {
      "title" : "Fast stochastic algorithms for svd and pca: Convergence properties and convexity",
      "author" : [ "Ohad Shamir" ],
      "venue" : "In Proceedings of The 33rd International Conference on Machine Learning,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2016
    }, {
      "title" : "Without-replacement sampling for stochastic gradient methods",
      "author" : [ "Ohad Shamir" ],
      "venue" : "In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2016
    }, {
      "title" : "Communication-efficient distributed optimization using an approximate newton-type method",
      "author" : [ "Ohad Shamir", "Nathan Srebro", "Tong Zhang" ],
      "venue" : "In Proceedings of the 31th International Conference on Machine Learning,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2014
    }, {
      "title" : "User-friendly tail bounds for sums of random matrices",
      "author" : [ "Joel A. Tropp" ],
      "venue" : "Foundations of Computational Mathematics,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2012
    }, {
      "title" : "Samworth. A useful variant of the davis–kahan theorem for statisticians",
      "author" : [ "Yi Yu", "Tengyao Wang", "Richard J" ],
      "venue" : null,
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2015
    }, {
      "title" : "Communication-efficient algorithms for statistical optimization",
      "author" : [ "Yuchen Zhang", "John C Duchi", "Martin J Wainwright" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 15,
      "context" : "1 Introduction Principal Component Analysis (PCA) [17, 9, 13] is one of the most celebrated and popular techniques in data analysis and machine learning.",
      "startOffset" : 50,
      "endOffset" : 61
    }, {
      "referenceID" : 7,
      "context" : "1 Introduction Principal Component Analysis (PCA) [17, 9, 13] is one of the most celebrated and popular techniques in data analysis and machine learning.",
      "startOffset" : 50,
      "endOffset" : 61
    }, {
      "referenceID" : 11,
      "context" : "1 Introduction Principal Component Analysis (PCA) [17, 9, 13] is one of the most celebrated and popular techniques in data analysis and machine learning.",
      "startOffset" : 50,
      "endOffset" : 61
    }, {
      "referenceID" : 2,
      "context" : "This for instance was the case in a recent wave of results that applied concepts such as stochastic gradient updates [4, 20, 11, 3] and variance reduction [19, 21, 6, 7, 2] to the PCA problem.",
      "startOffset" : 117,
      "endOffset" : 131
    }, {
      "referenceID" : 18,
      "context" : "This for instance was the case in a recent wave of results that applied concepts such as stochastic gradient updates [4, 20, 11, 3] and variance reduction [19, 21, 6, 7, 2] to the PCA problem.",
      "startOffset" : 117,
      "endOffset" : 131
    }, {
      "referenceID" : 9,
      "context" : "This for instance was the case in a recent wave of results that applied concepts such as stochastic gradient updates [4, 20, 11, 3] and variance reduction [19, 21, 6, 7, 2] to the PCA problem.",
      "startOffset" : 117,
      "endOffset" : 131
    }, {
      "referenceID" : 1,
      "context" : "This for instance was the case in a recent wave of results that applied concepts such as stochastic gradient updates [4, 20, 11, 3] and variance reduction [19, 21, 6, 7, 2] to the PCA problem.",
      "startOffset" : 117,
      "endOffset" : 131
    }, {
      "referenceID" : 17,
      "context" : "This for instance was the case in a recent wave of results that applied concepts such as stochastic gradient updates [4, 20, 11, 3] and variance reduction [19, 21, 6, 7, 2] to the PCA problem.",
      "startOffset" : 155,
      "endOffset" : 172
    }, {
      "referenceID" : 19,
      "context" : "This for instance was the case in a recent wave of results that applied concepts such as stochastic gradient updates [4, 20, 11, 3] and variance reduction [19, 21, 6, 7, 2] to the PCA problem.",
      "startOffset" : 155,
      "endOffset" : 172
    }, {
      "referenceID" : 4,
      "context" : "This for instance was the case in a recent wave of results that applied concepts such as stochastic gradient updates [4, 20, 11, 3] and variance reduction [19, 21, 6, 7, 2] to the PCA problem.",
      "startOffset" : 155,
      "endOffset" : 172
    }, {
      "referenceID" : 5,
      "context" : "This for instance was the case in a recent wave of results that applied concepts such as stochastic gradient updates [4, 20, 11, 3] and variance reduction [19, 21, 6, 7, 2] to the PCA problem.",
      "startOffset" : 155,
      "endOffset" : 172
    }, {
      "referenceID" : 0,
      "context" : "This for instance was the case in a recent wave of results that applied concepts such as stochastic gradient updates [4, 20, 11, 3] and variance reduction [19, 21, 6, 7, 2] to the PCA problem.",
      "startOffset" : 155,
      "endOffset" : 172
    }, {
      "referenceID" : 24,
      "context" : "For instance, [26] proposed communication-efficient algorithms for a distributed statistical estimation settings, similar to ours, but under convexity assumptions.",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 24,
      "context" : "Much like the results of [26], this result only holds in the regime when the per-machine sample size n is sufficiently large.",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 21,
      "context" : "A second line of results for distributed estimation under convexity assumptions consider iterative algorithms that perform multiple communication rounds and are based on distributed gradient computations (some examples include [23, 27, 14, 22, 10, 18]).",
      "startOffset" : 227,
      "endOffset" : 251
    }, {
      "referenceID" : 12,
      "context" : "A second line of results for distributed estimation under convexity assumptions consider iterative algorithms that perform multiple communication rounds and are based on distributed gradient computations (some examples include [23, 27, 14, 22, 10, 18]).",
      "startOffset" : 227,
      "endOffset" : 251
    }, {
      "referenceID" : 20,
      "context" : "A second line of results for distributed estimation under convexity assumptions consider iterative algorithms that perform multiple communication rounds and are based on distributed gradient computations (some examples include [23, 27, 14, 22, 10, 18]).",
      "startOffset" : 227,
      "endOffset" : 251
    }, {
      "referenceID" : 8,
      "context" : "A second line of results for distributed estimation under convexity assumptions consider iterative algorithms that perform multiple communication rounds and are based on distributed gradient computations (some examples include [23, 27, 14, 22, 10, 18]).",
      "startOffset" : 227,
      "endOffset" : 251
    }, {
      "referenceID" : 16,
      "context" : "A second line of results for distributed estimation under convexity assumptions consider iterative algorithms that perform multiple communication rounds and are based on distributed gradient computations (some examples include [23, 27, 14, 22, 10, 18]).",
      "startOffset" : 227,
      "endOffset" : 251
    }, {
      "referenceID" : 4,
      "context" : "Towards designing efficient distributed iterative methods for our PCA setting, we consider the application of the recently proposed method of Shift-and-Invert power iterations (S&I) for PCA [6, 7].",
      "startOffset" : 190,
      "endOffset" : 196
    }, {
      "referenceID" : 5,
      "context" : "Towards designing efficient distributed iterative methods for our PCA setting, we consider the application of the recently proposed method of Shift-and-Invert power iterations (S&I) for PCA [6, 7].",
      "startOffset" : 190,
      "endOffset" : 196
    }, {
      "referenceID" : 13,
      "context" : "Beyond the results described so far, [15, 5] studied distributed algorithms for PCA in a deterministic setting in which the partition of the data across machines is arbitrary and communication is measured in terms of number of transmitted bits.",
      "startOffset" : 37,
      "endOffset" : 44
    }, {
      "referenceID" : 3,
      "context" : "Beyond the results described so far, [15, 5] studied distributed algorithms for PCA in a deterministic setting in which the partition of the data across machines is arbitrary and communication is measured in terms of number of transmitted bits.",
      "startOffset" : 37,
      "endOffset" : 44
    }, {
      "referenceID" : 22,
      "context" : "(3) Lemma 1 is a direct consequence of the following standard concentration argument for random matrices, and the Davis-Kahan sin(θ) theorem (whose proof is given in the appendix for completeness): Theorem 1 (Matrix Hoeffding, see [24]).",
      "startOffset" : 231,
      "endOffset" : 235
    }, {
      "referenceID" : 4,
      "context" : "A multi communication round algorithm We present a distributed algorithm based on the Shift-and-Invert framework for leading eigenvector computation [6, 7] which is applied to explicitly solving the centralized ERM problem.",
      "startOffset" : 149,
      "endOffset" : 155
    }, {
      "referenceID" : 5,
      "context" : "A multi communication round algorithm We present a distributed algorithm based on the Shift-and-Invert framework for leading eigenvector computation [6, 7] which is applied to explicitly solving the centralized ERM problem.",
      "startOffset" : 149,
      "endOffset" : 155
    }, {
      "referenceID" : 2,
      "context" : "SGD for PCA was studied in several results in recent years [4, 20, 21, 12, 3].",
      "startOffset" : 59,
      "endOffset" : 77
    }, {
      "referenceID" : 18,
      "context" : "SGD for PCA was studied in several results in recent years [4, 20, 21, 12, 3].",
      "startOffset" : 59,
      "endOffset" : 77
    }, {
      "referenceID" : 19,
      "context" : "SGD for PCA was studied in several results in recent years [4, 20, 21, 12, 3].",
      "startOffset" : 59,
      "endOffset" : 77
    }, {
      "referenceID" : 10,
      "context" : "SGD for PCA was studied in several results in recent years [4, 20, 21, 12, 3].",
      "startOffset" : 59,
      "endOffset" : 77
    }, {
      "referenceID" : 1,
      "context" : "SGD for PCA was studied in several results in recent years [4, 20, 21, 12, 3].",
      "startOffset" : 59,
      "endOffset" : 77
    }, {
      "referenceID" : 10,
      "context" : "For instance applying the result of [12] in this way will result in a final estimate w satisfying 1− (wv1) = O ( b2 ln d δ2mn )",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 24,
      "context" : "For instance, in the distributed statistical setting considered in [26], in which the objective is strongly convex, it was shown that simply averaging the individual ERM solutions leads, in a meaningful regime of parameters, to estimation error of the order of the centralized ERM solution.",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 14,
      "context" : "By Theorem 1 in [16], we have that both λ(t) and v(t) are infinitely differentiable at any t ∈ [0, 1], and satisfy3 λ′(t) = v(t)⊤Ev(t) , v′(t) = (λ(t)I −A(t))†Ev(t) .",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 6,
      "context" : "3 in [8]) −B† ( ∂ ∂t B )",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 14,
      "context" : "Since v(t),v(t),v(t) are all vectors, this is a direct consequence of the standard Taylor expansion of the scalar function t 7→ v(t)j , mapping t to the j-th coordinate of v(t), using the fact that this mapping is differentiable to any order (see Theorem 1 in [16], and in particular twice continuously differentiable.",
      "startOffset" : 260,
      "endOffset" : 264
    }, {
      "referenceID" : 23,
      "context" : "To handle the second part, note that by a variant of the Davis-Kahan sinθ theorem (see Corollary 1 in [25]), if maxi ‖X̂i − X‖ ≤ δ/12, then the leading eigenvectors v̂i 1 of X̂i (after choosing the sign appropriately, i.",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 4,
      "context" : "Towards this end we consider the use of the Shift-and-Invert meta-algorithm, originally described in [6, 7], to explicitly solve the centralized ERM objective, i.",
      "startOffset" : 101,
      "endOffset" : 107
    }, {
      "referenceID" : 5,
      "context" : "Towards this end we consider the use of the Shift-and-Invert meta-algorithm, originally described in [6, 7], to explicitly solve the centralized ERM objective, i.",
      "startOffset" : 101,
      "endOffset" : 107
    }, {
      "referenceID" : 4,
      "context" : "1 The Shift-and-Invert meta-algorithm The Shift-and-Invert algorithm [6, 7] efficiently reduces the problem of computing the leading eigenvector of a positive semidefinite matrix X̂ to that of approximately-solving a polylogarithmic number of linear systems, i.",
      "startOffset" : 69,
      "endOffset" : 75
    }, {
      "referenceID" : 5,
      "context" : "1 The Shift-and-Invert meta-algorithm The Shift-and-Invert algorithm [6, 7] efficiently reduces the problem of computing the leading eigenvector of a positive semidefinite matrix X̂ to that of approximately-solving a polylogarithmic number of linear systems, i.",
      "startOffset" : 69,
      "endOffset" : 75
    }, {
      "referenceID" : 4,
      "context" : "The algorithm that implements this reduction, originally described in [6], is given below (see Algorithm 1).",
      "startOffset" : 70,
      "endOffset" : 73
    }, {
      "referenceID" : 4,
      "context" : "2 in [6]).",
      "startOffset" : 5,
      "endOffset" : 8
    } ],
    "year" : 2017,
    "abstractText" : "We study the fundamental problem of Principal Component Analysis in a statistical distributed setting in which each machine out of m stores a sample of n points sampled i.i.d. from a single unknown distribution. We study algorithms for estimating the leading principal component of the population covariance matrix that are both communication-efficient and achieve estimation error of the order of the centralized ERM solution that uses all mn samples. On the negative side, we show that in contrast to results obtained for distributed estimation under convexity assumptions, for the PCA objective, simply averaging the local ERM solutions cannot guarantee error that is consistent with the centralized ERM. We show that this unfortunate phenomena can be remedied by performing a simple correction step which correlates between the individual solutions, and provides an estimator that is consistent with the centralized ERM for sufficiently-large n. We also introduce an iterative distributed algorithm that is applicable in any regime of n, which is based on distributed matrix-vector products. The algorithm gives significant acceleration in terms of communication rounds over previous distributed algorithms, in a wide regime of parameters.",
    "creator" : "dvips(k) 5.996 Copyright 2016 Radical Eye Software"
  }
}