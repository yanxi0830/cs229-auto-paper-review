{
  "name" : "1509.07107.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "On The Direct Maximization of Quadratic Weighted Kappa",
    "authors" : [ "David Vaughn", "Derek Justice" ],
    "emails" : [ "DVAUGHN@MEASINC.COM", "DJUSTICE@MEASINC.COM" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Yet, little is understood about the nature of this metric, its underlying mathematical properties, where it fits among other common evaluation metrics such as mean squared error (MSE) and correlation, or if it can be optimized analytically, and if so, how. Much of this is due to the cumbersome way that this metric is commonly defined.\nIn this paper we first derive an equivalent but much simpler, and more useful, definition for quadratic weighted kappa, and then employ this alternate form to address the above issues."
    }, {
      "heading" : "1. Preliminaries",
      "text" : "Although first developed in the statistical community as a measure of inter-rater agreement, κ has more recently become a popular performance metric in supervised machine learning, specifically in situations where the target (dependent) variable y is a discrete, interval variable (usually drawn from non-negative integers) such as is common in most human rating scales (e.g.“on a scale from 1 to 10”).\nThis differs from the ordinal regression setting, where there only exists an ordering over labels, but no intrinsic or constant length interval between them. Some have argued that the use of quadratic weighted kappa as a metric in the domain of human ratings imposes the erroneous assumption of “equal intervals” where there should be none (how this assumption is expressed in the metric itself will be made\nclear in the following section). For example, when rating student essays on a scale from 1 to 5, the difference between a 1 and a 2 may not be equal to the difference between a 4 and a 5. While this may or may not be true in certain cases, we will not be concerned with that here."
    }, {
      "heading" : "1.1. Standard Definition",
      "text" : "Quadratic weighted kappa, which we write κ to distinguish from linear weighted kappa, was originally developed as a measure of inter-rater agreement. In this scenario, there are two raters, A and B, each associated with a vector of n integer ratings a,b ∈ Ln×1 where L = {1, 2, · · · , `} is a finite set of ` possible values. We seek to quantify the level of agreement between a and b. In order to compute κ(a,b), it is customary to start by computing frequency tables. The observed confusion matrix U = (ui,j) ∈ N`×` is first computed as:\nui,j =\nn∑\nk=1\nI(ak = i) · I(bk = j)\nNext, the expected confusion matrix V = (vi,j) ∈ R`×` is computed by assuming there is no correlation between raters. Under this assumption, we can simply compute V as the outer product between the two rater’s observed marginal distributions, normalized so that V has the same sum as U ( i.e. ∑` i,j=1 vi,j = ∑` i,j=1 ui,j ) :\nV = (U · e)⊗ (U> · e)\nn\nwhere e denotes the all ones vector. Finally, a matrix of weights W = (wi,j) ∈ R`×` is defined as:\nwi,j = (i− j)2 (`− 1)2\nGiven U, V, and W, it is customary to define κ as:\nκ(a,b) = 1− ∑` i,j=1 ui,j · wi,j∑` i,j=1 vi,j · wi,j = 1− 〈U,W〉F〈V,W〉 F (1)\nwhere 〈·, ·〉 F denotes the Frobenius inner product.\nar X\niv :1\n50 9.\n07 10\n7v 1\n[ cs\n.L G\n] 2\n3 Se\np 20\n15"
    }, {
      "heading" : "1.2. Alternate Form",
      "text" : "Assume a data matrix X = [x>1 , · · · ,x>n ]> ∈ Rn×d of n points in d dimensional space, and a vector y ∈ Ln×1 of n integer labels where L = {1, 2, · · · , `} is a finite set of possible labels. We assume there is some true functional relationship yi = f(xi) between each point xi and it’s label yi. The goal is to find a function ŷi = f̂(xi) which approximates the true function as closely as possible. Using this notation, we can begin to rewrite the standard definition of κ by first noting that the numerator simply represents a standard sum of squared errors:\n〈U,W〉 F =\nn∑\nk=1\n(yk − f̂(xk))2 (`− 1)2 = ‖y − ŷ‖2 (`− 1)2\nNext, we note that the denominator can be similarly rewritten in terms of y and ŷ:\n〈V,W〉 F = ‖y‖2 − 2n (y>e)(ŷ>e) + ‖ŷ‖2\n(`− 1)2\nBy combining these expressions we can derive a simplified expression for κ purely in terms of y and ŷ, without the need for contingency tables:\nκ(y, ŷ) = 1− ‖y − ŷ‖ 2\n‖y‖2 − 2n (y>e)(ŷ>e) + ‖ŷ‖2\n= 1− ‖y‖ 2 − 2y>ŷ + ‖ŷ‖2\n‖y‖2 − 2n (y>e)(ŷ>e) + ‖ŷ‖2\n= 2[y − e(y>en )] >ŷ\n‖y‖2 − 2n (y>e)(ŷ>e) + ‖ŷ‖2\nIf we drop the integer constraint on labels then we can assume that y has been centered, which allows further simplification:\nκ(y, ŷ) = 2〈y · ŷ〉\n‖y‖2 + ‖ŷ‖2 (2)\nThroughout the rest of this paper we will assume (without loss of generality) that y is centered."
    }, {
      "heading" : "2. Linear Model",
      "text" : "We consider a simple linear model for y = f(X):\nf(X) = Xα\nand so we seek the function parameter α̂ that maximizes κ(α), where\nκ(α) = 2〈y ·Xα〉\n‖y‖2 + ‖Xα‖2\nWe will rely on the fact that κ(α) is strictly quasiconcave (see appendix for proof), since it is known that any local maximum of a strictly quasiconcave function f is also a global maximum (Avriel, 2003). And so we can apply the method of Lagrange multipliers to find a local maximum, and conclude that this is the global maximum as well. To begin, we can use the variable substitution γ = ‖y‖2 + ‖Xα‖2 to write this as a constrained optimization problem:\nmax α,γ\n2 γ 〈y ·Xα〉\nsubject to γ = ‖y‖2 + ‖Xα‖2\nFrom here we can define the Lagrangian:\nL(α, γ, λ) = 2\nγ 〈y ·Xα〉+ λ(γ − ‖y‖2 − ‖Xα‖2)\n= 2\nγ y>Xα+ λ(γ − y>y −α>X>Xα)\nWe solve by first differentiating w.r.t. γ and setting equal to zero:\n∂L\n∂γ ∣∣∣∣ α̂,γ̂,λ̂\n= − 2 γ̂2 y>Xα̂+ λ̂ = 0\nλ̂ = 2\nγ̂2 y>Xα̂ (3)\nThen w.r.t. α:\n∂L\n∂α ∣∣∣∣ α̂,γ̂,λ̂\n= 2\nγ̂ X>y − 2λ̂X>Xα̂ = 0\nSubstituting in the previous expression (3) for λ̂ yields:\nX>y\nγ̂ − 2y\n>Xα̂ γ̂2 X>Xα̂ = 0\n2y>Xα̂\nγ̂ X>Xα̂ = X>y\nNow substituting back in for γ̂ gives a surprising result:\n2y>Xα̂\n‖y‖2 + ‖Xα̂‖2X >Xα̂ = X>y\nκ̂X>Xα̂ = X>y\nα̂ = 1\nκ̂ (X>X) −1 X>y (4)\nwhere we have simplified by recognizing the expression for κ̂ = κ(α̂). In addition, we recognize the “least-squares” solution α̂`s = (X >X) −1 X>y, the well studied minimizer of the quantity ‖y −Xα‖, a.k.a. the norm of the residual. This means that α̂ (which we will write α̂κ when necessary to avoid confusion) and the least-squares solution α̂`s are related via κ̂ by a surprisingly simple formula:\nα̂κ = 1\nκ̂ α̂`s (5)\nIn order to obtain a closed form solution for α̂κ we must solve for κ̂ solely in terms of X and y. We employ another useful tool from least-squares regression, the “hat” matrix H = X(X>X)−1X>. The hat matrix allows the least-squares labels ŷ`s to be expressed solely in terms of the original labels y:\nŷ`s = Xα̂`s = Hy (6)\nUsing the well-known fact that the hat matrix is both symmetric and idempotent (H>H = H2 = H), we can establish the following identity:\n‖ŷ`s‖2 = (Hy)>(Hy) = y>H>Hy\n= y>Hy\n= y>Xα̂`s = 〈y ·Xα̂`s〉 (7)\nNow we can take the derived expression for α̂κ from (5) and substitute it back into the original expression for κ and apply the above identity (7) to solve for κ̂:\nκ̂ = κ(α̂κ) = 2〈y ·Xα̂κ〉\n‖y‖2 + ‖Xα̂κ‖2 = 2〈y · 1κ̂Xα̂`s〉\n‖y‖2 + ‖ 1κ̂Xα̂`s‖2\n= 2 κ̂ 〈y ·Xα̂`s〉\n‖y‖2 + 1κ̂2 ‖Xα̂`s‖2\n= 2κ̂‖ŷ`s‖2\nκ̂2‖y‖2 + ‖ŷ`s‖2 κ̂2‖y‖2 + ‖ŷ`s‖2 = 2‖ŷ`s‖2\nκ̂ = ‖ŷ`s‖ ‖y‖ (8)\nFinally we can express α̂ solely in terms of X and y:\nα̂κ = 1\nκ̂ α̂`s = ‖y‖ ‖ŷ`s‖ α̂`s = ‖y‖ ‖ŷ`s‖ (X>X) −1 X>y\n=\n(√ y>y\ny>X(X>X) −1 X>y\n) (X>X) −1 X>y\n=\n(√ y>y\ny>Hy\n) (X>X) −1 X>y (9)"
    }, {
      "heading" : "2.1. Regularization",
      "text" : "It is often useful to include a regularization term in the optimization to account for cases where X>X is singular or ill-conditioned, which could lead to a numerically unstable (or non-existent) solution. In addition, it provides a convenient mechanism for manually tuning the bias/variance trade-off. It is easy to accommodate a norm penalty term within the current framework, yielding a slightly modified optimization problem:\nmax α,γ\n2 γ 〈y ·Xα〉 − µ‖α‖2\nsubject to γ = ‖y‖2 + ‖Xα‖2\nwhere µ controls the magnitude of the penalty, and is usually set experimentally (e.g. with cross-validation). This leads to a similarly modified Lagrangian:\nL(α, γ, λ) = 2\nγ y>Xα− µα>α+ λ(γ − y>y −α>X>Xα)\nProceeding in the same manner as the last section, differentiating w.r.t. γ and setting equal to zero yields the same result (3) for λ̂. Differentiating w.r.t. α yields a slightly different result:\n∂L\n∂α ∣∣∣∣ α̂,γ̂,λ̂\n= 2\nγ̂ X>y − 2λ̂X>Xα̂− 2µα̂ = 0\nAgain, substituting in for λ̂ and manipulating the result, we end up with:\nκ̂X>Xα̂+ µγ̂α̂ = X>y\nα̂ = 1\nκ̂ (X>X+\nµγ̂\nκ̂ I) −1 X>y\nIn order to obtain a closed form solution for α̂ all that is important to recognize is that we once again know the optimal solution to within a factor ϕ = 1κ̂ , just as in the regularized case. The only difference is that now, instead of α̂ just being within a scalar multiple of the least-squares solution,\nα̂ is now within a scalar multiple of the regularized least squares solution, i.e. the ridge regression solution α̂rr. As µ is just a regularization tuning parameter, it can simply absorb the κ̂ and γ̂. We make the substitution σ = µγ̂κ̂ to make this more clear:\nα̂κ = 1\nκ̂ (X>X+ σI) −1 X>y =\n1 κ̂ α̂rr (10)\nJust as in the previous case, we can take the derived expression for α̂κ from (10) and substitute it back into the original expression for κ to solve for κ̂:\nκ̂ = κ(α̂κ) = 2〈y ·Xα̂κ〉\n‖y‖2 + ‖Xα̂κ‖2 = 2〈y · 1κ̂Xα̂rr〉\n‖y‖2 + ‖ 1κ̂Xα̂rr‖2\n= 2 κ̂ 〈y ·Xα̂rr〉\n‖y‖2 + 1κ̂2 ‖Xα̂rr‖2 κ̂2‖y‖2 + ‖Xα̂rr‖2 = 2〈y ·Xα̂rr〉\nκ̂ =\n√ 2〈y ·Xα̂rr〉 − ‖Xα̂rr‖2\n‖y‖ (11)\nIn this case the result does not simplify as nicely as in the non-regularized case, due to the fact that Hσ (where Hσ = X(X>X + σI) −1 X> denotes the regularized variant of the “hat” matrix) is no longer idempotent. Still, we can use Hσ to write a somewhat simplified expression for κ̂:\nκ̂ =\n√ 2y>Hσy − y>H2σy\ny>y (12)\nFinally we can express the regularized solution α̂κ in closed form:\nα̂κ = 1\nκ̂ α̂rr\n=\n(√ y>y\n2y>Hσy − y>H2σy\n) (X>X+ σI) −1 X>y\n(13)"
    }, {
      "heading" : "3. Link to Correlation",
      "text" : "There is an interesting link between quadratic weighted kappa and correlation. There are many (equivalent) definitions of correlation, but the simplest is just the normalized dot product between two vectors u,v:\nρ(u,v) = 〈u · v〉 ‖u‖‖v‖\nNow returning to the (non-regularized) κ-optimal solution labels ŷκ = Xα̂κ, we can expand the expression for ρ(y, ŷκ) and apply (7) and (8) to simplify:\nρ(y, ŷκ) = 〈y · ŷκ〉 ‖y‖‖ŷκ|\n= 〈y ·Xα̂κ〉 ‖y‖‖Xα̂κ| = 1 κ̂ 〈y ·Xα̂`s〉 ‖y‖‖ 1κ̂Xα̂`s‖ = 1 κ̂‖ŷ`s‖2\n1 κ̂‖y‖‖ŷ`s‖\n= ‖ŷ`s‖ ‖y‖ = κ(y, ŷκ) = κ̂ . (14)\nAnd so while correlation is, in general, not equivalent to quadratic weighted kappa, they do coincide precisely at the point where kappa is maximized."
    }, {
      "heading" : "Appendix: Proof of Strict Quasiconcavity",
      "text" : "We wish to show that the function\nκ(α) = 2〈y ·Xα〉\n‖y‖2 + ‖Xα‖2\nis strictly quasiconcave. A function f is said to be strictly quasiconcave if, for any two points u,v and any positive weight θ < 1, the following is true (Avriel, 2003):\nf(θu+ (1− θ)v) > min[f(u), f(v)] (A-1)\nLet us assume, without loss of generality, that min[κ(u), κ(v)] = κ(u), which leads us to write:\nκ(v) ≥ κ(u) 2〈y ·Xv〉\n‖y‖2 + ‖Xv‖2 ≥ 2〈y ·Xu〉 ‖y‖2 + ‖Xu‖2\ny>Xv ≥ y>Xu (‖y‖2 + ‖Xv‖2 ‖y‖2 + ‖Xu‖2 ) (A-2)\nReturning to the the definition of strictly quasiconcave and expanding the expression for κ(θu+ (1− θ)v) gives us:\nκ(θu+ (1− θ)v) = 2θy >Xu+ 2(1− θ)y>Xv\n‖y‖2 + ‖θXu+ (1− θ)Xv‖2\nNow, we know that the function h(z) = ‖Xz‖2 is strictly convex (because ‖Xz‖2 = z>X>Xz, where X>X 0 since X>X is assumed non-singular). By the definition of strict convexity, this means:\n‖θXu+ (1− θ)Xv‖2 < θ‖Xu‖2 + (1− θ)‖Xv‖2\nTherefore we can substitute the expression on the right into the previous equation to obtain:\nκ(θu+ (1− θ)v) > 2θy >Xu+ 2(1− θ)y>Xv\n‖y‖2 + θ‖Xu‖2 + (1− θ)‖Xv‖2 (A-3)\nWe can then substitute the expression for y>Xv from the right side of (A-2) into the above, leading to:\n2θy>Xu+ 2(1− θ)y>Xv ‖y‖2 + θ‖Xu‖2 + (1− θ)‖Xv‖2 ≥\n2θy>Xu+ 2(1− θ)y>Xu ( ‖y‖2+‖Xv‖2 ‖y‖2+‖Xu‖2 )\n‖y‖2 + θ‖Xu‖2 + (1− θ)‖Xv‖2\n= 2y>Xu\n( θ + (1− θ) ( ‖y‖2+‖Xv‖2 ‖y‖2+‖Xu‖2 ))\n‖y‖2 + θ‖Xu‖2 + (1− θ)‖Xv‖2\n= 2y>Xu\n( θ(‖y‖2+‖Xu‖2)+(1−θ)(‖y‖2+‖Xv‖2) ‖y‖2+‖Xu‖2 )\n‖y‖2 + θ‖Xu‖2 + (1− θ)‖Xv‖2\n=\n2y>Xu ‖y‖2+‖Xu‖2 (‖y‖2 + θ‖Xu‖2 + (1− θ)‖Xv‖2)\n‖y‖2 + θ‖Xu‖2 + (1− θ)‖Xv‖2 = κ(u) .\nCombining this result with (A-3) we have\nκ(θu+ (1− θ)v) > κ(u)\nor, stated another way\nκ(θu+ (1− θ)v) > min[κ(u), κ(v)]\nwhich is what we were trying to prove."
    } ],
    "references" : [ {
      "title" : "Nonlinear programming: analysis and methods",
      "author" : [ "Avriel", "Mordecai" ],
      "venue" : "Courier Corporation,",
      "citeRegEx" : "Avriel and Mordecai.,? \\Q2003\\E",
      "shortCiteRegEx" : "Avriel and Mordecai.",
      "year" : 2003
    } ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "In recent years, quadratic weighted kappa has been growing in popularity in the machine learning community as an evaluation metric in domains where the target labels to be predicted are drawn from integer ratings, usually obtained from human experts. For example, it was the metric of choice in several recent, high profile machine learning contests hosted on Kaggle : www.kaggle.com/c/asap-aes , www.kaggle.com/c/asap-sas , www.kaggle.com/c/diabeticretinopathy-detection . Yet, little is understood about the nature of this metric, its underlying mathematical properties, where it fits among other common evaluation metrics such as mean squared error (MSE) and correlation, or if it can be optimized analytically, and if so, how. Much of this is due to the cumbersome way that this metric is commonly defined. In this paper we first derive an equivalent but much simpler, and more useful, definition for quadratic weighted kappa, and then employ this alternate form to address the above issues. 1. Preliminaries Although first developed in the statistical community as a measure of inter-rater agreement, κ has more recently become a popular performance metric in supervised machine learning, specifically in situations where the target (dependent) variable y is a discrete, interval variable (usually drawn from non-negative integers) such as is common in most human rating scales (e.g.“on a scale from 1 to 10”). This differs from the ordinal regression setting, where there only exists an ordering over labels, but no intrinsic or constant length interval between them. Some have argued that the use of quadratic weighted kappa as a metric in the domain of human ratings imposes the erroneous assumption of “equal intervals” where there should be none (how this assumption is expressed in the metric itself will be made clear in the following section). For example, when rating student essays on a scale from 1 to 5, the difference between a 1 and a 2 may not be equal to the difference between a 4 and a 5. While this may or may not be true in certain cases, we will not be concerned with that here. 1.1. Standard Definition Quadratic weighted kappa, which we write κ to distinguish from linear weighted kappa, was originally developed as a measure of inter-rater agreement. In this scenario, there are two raters, A and B, each associated with a vector of n integer ratings a,b ∈ Ln×1 where L = {1, 2, · · · , `} is a finite set of ` possible values. We seek to quantify the level of agreement between a and b. In order to compute κ(a,b), it is customary to start by computing frequency tables. The observed confusion matrix U = (ui,j) ∈ N`×` is first computed as:",
    "creator" : "LaTeX with hyperref package"
  }
}