{
  "name" : "1605.02216.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Distributed stochastic optimization for deep learning",
    "authors" : [ "Yann LeCun", "Minjie Wang", "Zhaoguo Wang" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Distributed stochastic optimization for deep learning\nby\nSixin Zhang\nA dissertation submitted in partial fulfillment\nof the requirements for the degree of\nDoctor of Philosophy\nDepartment of Computer Science\nNew York University\nMay, 2016\n——————————–\nYann LeCun\nar X\niv :1\n60 5.\n02 21\n6v 1\n[ cs\n.L G\n] 7\nM ay\n2 01\n6"
    }, {
      "heading" : "Dedication",
      "text" : ""
    }, {
      "heading" : "To my parents and grandparents",
      "text" : "ii"
    }, {
      "heading" : "Acknowledgements",
      "text" : "Almost all the thesis start with a thank you to the thesis advisor, why this is so. Maybe it is the time. The time one spends most with during the PhD. The time the impact from whom will last for long on shaping one’s flavor of the research. My advisor, Yann LeCun, is of no double having a big influence on me. To some extent, the way we interact is like the Elastic Averaging SGD (EASGD) method that we have named together. During my whole PhD, I am very grateful to have the freedom to explore various research subjects. He always has patience to wait, even though sometimes there is no interesting results. It is an experiment. Sometimes, the good result is only one step away, and he has a good feeling to sense that. Sometimes, he is also very strict. Remember when I was preparing the thesis proposal, but was proposing too many directions. Everyone was worried, as the deadline was approaching. To focus and be quick. A lot of the time, the way how we think decides the question we ask and where we go. My advisor’s conceptual way of thinking is still to me a mystery of arts and science.\nI would also like to thank Anna Choromanska for being my collaborator and for helping me at many critical points. When we started to work on the EASGD method, it was not very clear why we gave it a new name. The answer only became sharp when Anna questioned me countlessly why we do what we do. Luckily, we have discovered interesting answers. Chapter 2, Chapter 3 and Chapter 4 of this thesis are based on our joint work.\nChapter 5 is inspired by the discussion with Professor Weinan E, and his students Qianxiao Li and Cheng Tai during my visit to Princeton University. We were trying to use stochastic differential equation to analyze EASGD method. The idea is to get the maximum insight with minimum effort. The results in Chapter 5 share a similar flavor.\nChapter 6 would not come into shape without the helpful discussions with Professor Jinyang Li, and her students Russell Power, Minjie Wang, Zhaoguo Wang, Christopher Mitchell and Yang Zhang. The design and the implementation of the EASGD and EASGD Tree are guided by their valuable experience. Also the numerical results would\niii\nnot be obtained without their feedback, as well as the consistent support of the NYU HPC team, in particular Shenglong Wang.\nI would also like to give special thanks to Professor Rob Fergus, Margaret Wright, and David Sontag for serving in my thesis committee, as well as being my teacher during my PhD studies. Margaret was so eager to read my thesis and asked when do you expect your thesis to converge whenever I have a new version. Rob was so glad to see me when I first came to the CBLL lab, while David is always sharing his knowledge with us during the CBLL talks.\nI also learn a lot from my earlier collaborators, in particular Marco Scoffier, Tom Schaul and Wan Li. Arthur Szlam, Camille Couprie, Clement Farabet, David Eigen, Dilip Krishnan, Joan Bruna, Koray Kavukcuoglu, Matt Zeiler, Olivier Henaff, Pablo Sprechmann, Pierre Sermanet, Ross Goroshin, Xiang Zhang and Y-Lan Boureau are always very mind-refreshing to be around.\nI also appreciate the feedback from many senior researchers on the EASGD method. In particular, Mark Tygert, Samy Bengio, Patrick Combettes, Peter Richtarik, Yoshua Bengio and Yuri Bakhtin.\nThe coach of the NYU running club, Mike Galvan, kicks me up to practice at 6:30am every Monday morning during my last year of the PhD. It gives me a consistency energy to keep running and to finish my PhD.\nAlmost half of my PhD time is spent at the building of Courant Institute. I have learned a lot of mathematics which is still so tremendous to me. The more you write, the less you see. There are a lot of inspiring stories that I would like to write, but maybe I should keep them in mind for the moment.\niv"
    }, {
      "heading" : "Abstract",
      "text" : "We study the problem of how to distribute the training of large-scale deep learning models in the parallel computing environment. We propose a new distributed stochastic optimization method called Elastic Averaging SGD (EASGD). We analyze the convergence rate of the EASGD method in the synchronous scenario and compare its stability condition with the existing ADMM method in the round-robin scheme. An asynchronous and momentum variant of the EASGD method is applied to train deep convolutional neural networks for image classification on the CIFAR and ImageNet datasets. Our approach accelerates the training and furthermore achieves better test accuracy. It also requires a much smaller amount of communication than other common baseline approaches such as the DOWNPOUR method.\nWe then investigate the limit in speedup of the initial and the asymptotic phase of the mini-batch SGD, the momentum SGD, and the EASGD methods. We find that the spread of the input data distribution has a big impact on their initial convergence rate and stability region. We also find a surprising connection between the momentum SGD and the EASGD method with a negative moving average rate. A non-convex case is also studied to understand when EASGD can get trapped by a saddle point.\nFinally, we scale up the EASGD method by using a tree structured network topology. We show empirically its advantage and challenge. We also establish a connection between the EASGD and the DOWNPOUR method with the classical Jacobi and the Gauss-Seidel method, thus unifying a class of distributed stochastic optimization methods.\nv"
    }, {
      "heading" : "Contents",
      "text" : ""
    }, {
      "heading" : "Dedication ii",
      "text" : ""
    }, {
      "heading" : "Acknowledgements iv",
      "text" : ""
    }, {
      "heading" : "Abstract v",
      "text" : ""
    }, {
      "heading" : "List of Figures viii",
      "text" : ""
    }, {
      "heading" : "List of Tables xvi",
      "text" : ""
    }, {
      "heading" : "1 Introduction 1",
      "text" : "1.1 What is the problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 1.2 Formalizing the problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 1.3 An overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5"
    }, {
      "heading" : "2 Elastic Averaging SGD (EASGD) 9",
      "text" : "2.1 Synchronous EASGD . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 2.2 Asynchronous EASGD . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 2.3 Momentum EASGD . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14"
    }, {
      "heading" : "3 Convergence Analysis of EASGD 17",
      "text" : "3.1 Quadratic case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n3.1.1 One-dimensional case . . . . . . . . . . . . . . . . . . . . . . . . . 18 3.1.2 Generalization to multidimensional case . . . . . . . . . . . . . . . 26\n3.2 Strongly convex case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 3.3 Stability of EASGD and ADMM . . . . . . . . . . . . . . . . . . . . . . . 37\nvi"
    }, {
      "heading" : "4 Performance in Deep Learning 43",
      "text" : "4.1 Experimental setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 4.2 Experimental results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 4.3 Further discussion and understanding . . . . . . . . . . . . . . . . . . . . 59\n4.3.1 Comparison of SGD, ASGD, MVASGD and MSGD . . . . . . . . 59 4.3.2 Dependence of the learning rate . . . . . . . . . . . . . . . . . . . . 62 4.3.3 Dependence of the communication period . . . . . . . . . . . . . . 62 4.3.4 The tradeoff between data and parameter communication . . . . . 64 4.3.5 Time speed-up . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66\n4.4 Additional pseudo-codes of the algorithms . . . . . . . . . . . . . . . . . . 67"
    }, {
      "heading" : "5 The Limit in Speedup 71",
      "text" : "5.1 Additive noise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71\n5.1.1 SGD with mini-batch . . . . . . . . . . . . . . . . . . . . . . . . . 72 5.1.2 Momentum SGD . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73 5.1.3 EASGD and EAMSGD . . . . . . . . . . . . . . . . . . . . . . . . 78\n5.2 Multiplicative noise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89\n5.2.1 SGD with mini-batch . . . . . . . . . . . . . . . . . . . . . . . . . 89 5.2.2 Momentum SGD . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92 5.2.3 EASGD and EAMSGD . . . . . . . . . . . . . . . . . . . . . . . . 95\n5.3 A non-convex case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109"
    }, {
      "heading" : "6 Scaling up Elastic Averaging SGD 112",
      "text" : "6.1 EASGD Tree . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112\n6.1.1 The algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113 6.1.2 The result . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117\n6.2 Unifying EASGD and DOWNPOUR . . . . . . . . . . . . . . . . . . . . . 130"
    }, {
      "heading" : "7 Conclusion 134",
      "text" : "Bibliography 138\nvii"
    }, {
      "heading" : "List of Figures",
      "text" : "2.1 The big picture of EASGD. . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n3.1 Theoretical mean squared error (MSE) of the center x̃ in the quadratic\ncase, with various choices of the learning rate η (horizontal within each block), and the moving rate β = pα (vertical within each block), the number of processors p = {1, 10, 100, 1000, 10000} (vertical across blocks), and the time steps t = {1, 2, 10, 100,∞} (horizontal across blocks). The MSE is plotted in log scale, ranging from 10−3 to 103 (from deep blue to red). The dark red (i.e. on the upper-right corners) indicates divergence. . 22\n3.2 The largest absolute eigenvalue of the linear map F = F p3 ◦F p2 ◦F p1 ◦ . . . ◦\nF 13 ◦F 12 ◦F 11 as a function of η ∈ (0, 10−2) and ρ ∈ (0, 10) when p = 3 and p = 8. To simulate the chaotic behavior of the ADMM algorithm, one may pick η = 0.001 and ρ = 2.5 and initialize the state s0 either randomly or with λi0 = 0, x i 0 = x̃0 = 1000, ∀i = 1, . . . , p. Figure should be read in color. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\n3.3 Instability of ADMM in the round-robin scheme. Pick p = 3, η = 0.001,\nρ = 2.5 and initialize the state s0 with λ i 0 = 0, x i 0 = x̃0 = 1000,∀i = 1, . . . , p. The x-axis is the time step t, the y-axis is the (one-dimensional) value of the center variable x̃t. . . . . . . . . . . . . . . . . . . . . . . . . 42\n4.1 Training and test loss and the test error for the center variable versus a\nwallclock time for communication period τ = 1 on CIFAR dataset with the 7-layer convolutional neural network. p = 4. . . . . . . . . . . . . . . . 49\nviii\n4.2 Training and test loss and the test error for the center variable versus a\nwallclock time for communication period τ = 4 on CIFAR dataset with the 7-layer convolutional neural network. p = 4. . . . . . . . . . . . . . . . 50\n4.3 Training and test loss and the test error for the center variable versus a\nwallclock time for communication period τ = 16 on CIFAR dataset with the 7-layer convolutional neural network. p = 4. . . . . . . . . . . . . . . . 51\n4.4 Training and test loss and the test error for the center variable versus a\nwallclock time for communication period τ = 64 on CIFAR dataset with the 7-layer convolutional neural network. p = 4. . . . . . . . . . . . . . . . 52\n4.5 Training and test loss and the test error for the center variable versus a\nwallclock time with the number of local workers p = 4 for parallel methods on CIFAR with the 7-layer convolutional neural network. . . . . . . . . . 53\n4.6 Training and test loss and the test error for the center variable versus a\nwallclock time with the number of local workers p = 8 for parallel methods on CIFAR with the 7-layer convolutional neural network. . . . . . . . . . 54\n4.7 Training and test loss and the test error for the center variable versus\na wallclock time with the number of local workers p = 16 for parallel methods on CIFAR with the 7-layer convolutional neural network. . . . . 55\n4.8 Training and test loss and the test error for the center variable versus a\nwallclock time with the number of local workers p = 4 on ImageNet with the 11-layer convolutional neural network. . . . . . . . . . . . . . . . . . . 56\n4.9 Training and test loss and the test error for the center variable versus a\nwallclock time with the number of local workers p = 8 on ImageNet with the 11-layer convolutional neural network. . . . . . . . . . . . . . . . . . . 57\n4.10 Convergence of the training and test loss (negative log-likelihood) and\nthe test error (original and zoomed) computed for the center variable as a function of wallclock time for SGD, ASGD, MVASGD and MSGD (p = 1) on the CIFAR experiment. . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\nix\n4.11 Convergence of the training and test loss (negative log-likelihood) and\nthe test error (original and zoomed) computed for the center variable as a function of wallclock time for SGD, ASGD, MVASGD and MSGD (p = 1) on the ImageNet experiment. . . . . . . . . . . . . . . . . . . . . . . . . . 61\n4.12 Convergence of the training loss (negative log-likelihood, original) and\nthe test error (zoomed) computed for the center variable as a function of wallclock time for EAMSGD and EASGD run with different values of η on the CIFAR experiment. p = 16, τ = 10. . . . . . . . . . . . . . . . . . 63\n4.13 Convergence of the training loss (negative log-likelihood, original) and\nthe test error (zoomed) computed for the center variable as a function of wallclock time for EASGD and EAMSGD (p = 16, η = 0.01, β = 0.9, δ = 0.99) on the CIFAR experiment with various communication period τ and learning rate decay γ. The learning rate is decreased gradually over time based each local worker’s own clock t with ηt = η/(1 + γt) 0.5. . . . . . . . 65\n4.14 The wall clock time needed to achieve the same level of the test error\nthr as a function of the number of local workers p on the CIFAR dataset. From left to right: thr = {21%, 20%, 19%, 18%}. Missing bars denote that the method never achieved specified level of test error. . . . . . . . . . . . 68\n4.15 The wall clock time needed to achieve the same level of the test error thr\nas a function of the number of local workers p on the ImageNet dataset. From left to right: thr = {49%, 47%, 45%, 43%}. Missing bars denote that the method never achieved specified level of test error. . . . . . . . . . . . 69\n5.1 The largest absolute eigenvalue of the matrix M (sp(M)) in Equation 5.6\nas a function of the learning rate η ∈ (0, 2) and the momentum rate δ ∈ (−1, 1). h = 1. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77\n5.2 The largest absolute eigenvalue of the matrix M in Equation 5.12 as a\nfunction of the learning rate η ∈ (0, 2) and the moving rate α ∈ (−1, 1). h = 1 and β = 0.9. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82\nx\n5.3 Three independent simulations of EASGD using the elastic averaging α =\nβ/p and the optimal α given in Equation 5.17. The x-axis is the time step t in the EASGD updates of Equation 5.9. The y-axis is the squared distance of the center variable to the optimum zero, i.e. x̃2t . We have chosen h = 1, σ = 10−2, p = 4, η = 0.1 and β = 0.9. . . . . . . . . . . . . 83\n5.4 The absolute value of the eigenvalues of z1, z2 and z3 in Equation 5.19.\nηh = 0.1 and β = 0.9. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85\n5.5 The absolute value of the eigenvalues of z1, z2 and z3 in Equation 5.19.\nηh = 1.5 and β = 0.9. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85\n5.6 The largest absolute eigenvalue of the matrix Mp in Equation 5.18 as a\nfunction of the learning rate η ∈ (0, 2) and the moving rate α ∈ (−1, 1). h = 1 and β = 0.9. Note that we computed this spectrum using p = 2, as we have discussed in the text, it is independent of the choice of p for p > 1. 86\n5.7 Three independent simulations of EASGD using the elastic averaging α =\nβ/p and the optimal α given in Equation 5.17. The x-axis is the time step t in the EASGD updates of Equation 5.9. The y-axis is the squared distance of the center variable to the optimum zero, i.e. x̃2t . We have chosen h = 1, σ = 10−2, p = 4, η = 1.5 and β = 0.9. . . . . . . . . . . . . 87\n5.8 The largest absolute eigenvalue of the matrix Mp in Equation 5.20 as a\nfunction of the learning rate η ∈ (0, 2) and the moving rate α ∈ (−1, 1). h = 1, β = 0.9 and δ = 0.99. Note that we computed this spectrum again using p = 2, as we have discussed in the text, it is independent of the choice of p for p > 1. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90\n5.9 The probability density function of the Gamma distribution Γ(λ, ω). The\nx-axis and y-axis are both in log-scale to enlarge the singularity at zero and the decay of the tail toward infinity. . . . . . . . . . . . . . . . . . . . 93\n5.10 The largest absolute eigenvalue of the matrix M in Equation 5.30 as a\nfunction of the learning rate η ∈ (0, 1) and the momentum rate δ ∈ (−1, 1). λ = 0.5, ω = 0.5. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96\nxi\n5.11 The largest absolute eigenvalue of the matrix M in Equation 5.30 as a\nfunction of the learning rate η ∈ (0, 1) and the momentum rate δ ∈ (−1, 1). λ = 1, ω = 1. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97\n5.12 The largest absolute eigenvalue of the matrix M in Equation 5.30 as a\nfunction of the learning rate η ∈ (0, 1) and the momentum rate δ ∈ (−1, 1). λ = 2, ω = 2. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98\n5.13 The largest absolute eigenvalue of the matrix M in Equation 5.30 as\na function of the momentum rate δ ∈ (−1, 1) at η = λω+1 . (λ, ω) ∈ {(0.5, 0.5), (1, 1), (2, 2)}. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99\n5.14 The largest absolute eigenvalue of the matrix M in Equation 5.30 as a\nfunction of the input Gamma distribution Γ(λ, ω) in the range ω ∈ (0, 100) and the λ ∈ (0, 100). (η, δ) ∈ {(1, 0), (0.1, 0), (0.1, 0.9)}. . . . . . . . . . . . 100\n5.15 The largest absolute eigenvalue of the matrix M in Equation 5.34 as a\nfunction of the learning rate η ∈ (0, 1) and the number of workers p ∈ [1, 64]. λ = 0.5, ω = 0.5, β = 0.9, α = β/p. . . . . . . . . . . . . . . . . . . 103\n5.16 The largest absolute eigenvalue of the matrix M in Equation 5.34 as a\nfunction of the learning rate η ∈ (0, 1) and the number of workers p ∈ [1, 64]. λ = 1, ω = 1, β = 0.9, α = β/p. . . . . . . . . . . . . . . . . . . . . 104\n5.17 The largest absolute eigenvalue of the matrix M in Equation 5.34 as a\nfunction of the learning rate η ∈ (0, 1) and the number of workers p ∈ [1, 64]. λ = 2, ω = 2, β = 0.9, α = β/p. . . . . . . . . . . . . . . . . . . . . 105\n5.18 The largest absolute eigenvalue of the matrix M in Equation 5.34 as a\nfunction of the learning rate η ∈ (0, 2) and the number of workers p ∈ [1, 64]. λ = 10, ω = 10, β = 0.9, α = β/p. The minimal sp(M) = 0.0868 is achieved at p = 29 and η = 0.8929. . . . . . . . . . . . . . . . . . . . . . 106\n5.19 The largest absolute eigenvalue of the matrix M in Equation 5.34 as a\nfunction of the learning rate η ∈ (0, 1) and the moving rate α ∈ (−1, 1). λ = 0.5, ω = 0.5, β = 0.9, p = 100. The minimal sp(M) = 0.5024 is achieved at η = 0.4343 and α = 0.2525. . . . . . . . . . . . . . . . . . . . 108\nxii\n5.20 The smallest eigenvalue of the Hessian matrix H in Equation 5.38 as a\nfunction of the penalty term ρ, evaluated at the critical point x = √ 1− ρ, y = −√1− ρ, z = 0. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\n6.1 The behavior of EASGD Tree in Algorithm 6. . . . . . . . . . . . . . . . . 115 6.2 The two communication schemes of the EASGD Tree. . . . . . . . . . . . 116 6.3 EASGD Tree on CIFAR-lowrank using the first communication scheme\nwith τ1 = 10 and τ2 = 100. Training loss and error, test loss and error of the root node versus a wallclock time. We run this experiment six times independently (labeled with a,b,c,d,e,f) with a same random initialization. p = 256, d = 16, η = 5e− 3, α = 0.9/(d+ 1). . . . . . . . . . . . . . . . . 120\n6.4 EASGD Tree on CIFAR-lowrank using the second communication scheme\nwith τu = 8 and τd = 80. Training loss and error, test loss and error of the root node versus a wallclock time. We run this experiment six times independently (labeled with a,b,c,d,e,f) with a same random initialization. p = 256, d = 16, η = 5e− 3, α = 0.9/(d+ 1). . . . . . . . . . . . . . . . . 121\n6.5 EASGD Tree on CIFAR-lowrank using the first communication scheme\nwith τ1 = 1 and τ2 = 10. Training loss and error, test loss and error of the root node versus a wallclock time. We run this experiment six times independently (labeled with a,b,c,d,e,f) with a same random initialization. p = 256, d = 16, η = 5e− 2, α = 0.9/(d+ 1). The momentum rate δ = 0. . 122\n6.6 EASGD Tree on CIFAR-lowrank using the first communication scheme\nwith τ1 = 1 and τ2 = 10. Training loss and error, test loss and error of the root node versus a wallclock time. We run this experiment six times independently (labeled with a,b,c,d,e,f) with a same random initialization. p = 256, d = 16, η = 5e− 3, α = 0.9/(d+ 1). The momentum rate δ = 0.9. 123\nxiii\n6.7 EASGD Tree on CIFAR-lowrank using the first communication scheme\nwith τ1 = 1 and τ2 = 10. Training loss and error, test loss and error of the root node versus a wallclock time. We run this experiment six times independently (labeled with a,b,c,d,e,f) with a same random initialization. p = 256, d = 16, η = 5e− 4, α = 0.9/(d+ 1). The momentum rate δ = 0.99. 124\n6.8 EASGD Tree on CIFAR-lowrank using the second communication scheme\nwith τu = 1 and τd = 10. Training loss and error, test loss and error of the root node versus a wallclock time. We run this experiment six times independently (labeled with a,b,c,d,e,f) with a same random initialization. p = 256, d = 16, η = 5e− 2, α = 0.9/(d+ 1). The momentum rate δ = 0. . 125\n6.9 EASGD Tree on CIFAR-lowrank using the second communication scheme\nwith τu = 1 and τd = 10. Training loss and error, test loss and error of the root node versus a wallclock time. We run this experiment six times independently (labeled with a,b,c,d,e,f) with a same random initialization. p = 256, d = 16, η = 5e− 3, α = 0.9/(d+ 1). The momentum rate δ = 0.9. 126\n6.10 EASGD Tree on CIFAR-lowrank using the second communication scheme\nwith τu = 1 and τd = 10. Training loss and error, test loss and error of the root node versus a wallclock time. We run this experiment six times independently (labeled with a,b,c,d,e,f) with a same random initialization. p = 256, d = 16, η = 5e− 4, α = 0.9/(d+ 1). The momentum rate δ = 0.99. 127\n6.11 EASGD Tree on CIFAR-lowrank using the first and second communica-\ntion scheme with momentum. Training loss and error, test loss and error of the root node versus a wallclock time. The curve e in Figure 6.5, the curve e in Figure 6.6, the curve a in Figure 6.7, the curve b in Figure 6.8, the curve b in Figure 6.9 and the curve d in Figure 6.10 are selected and plotted. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128\nxiv\n6.12 Best test performance of DOWNPOUR (p=16), EASGD (p=16), and\nEASGD Tree (p=256) on CIFAR-lowrank. Training loss and error, test loss and error (of the center variable or the root node) versus a wallclock time. The momentum is not applied (δ = 0). . . . . . . . . . . . . . . . . 129\nxv"
    }, {
      "heading" : "List of Tables",
      "text" : "4.1 Learning rates explored for each method shown in Figure 4.1, 4.2, 4.3\nand 4.4 (CIFAR experiment). . . . . . . . . . . . . . . . . . . . . . . . . . 48\n4.2 Learning rates explored for each method shown in Figure 4.5, 4.6 and 4.7\n(CIFAR experiment). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\n4.3 Learning rates explored for each method shown in Figure 4.8 and 4.9\n(ImageNet experiment). . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\n4.4 Approximate computation time, data loading time and parameter com-\nmunication time [sec] for DOWNPOUR (top line for τ = 1) and EASGD (the time breakdown for EAMSGD is almost identical) (bottom line for τ = 10). Left time corresponds to CIFAR experiment and right table corresponds to ImageNet experiment. The computation and the communication time may have some overlap, due to the MPI implementation. . . 66\nxvi\nChapter 1"
    }, {
      "heading" : "Introduction",
      "text" : ""
    }, {
      "heading" : "1.1 What is the problem",
      "text" : "The subject of this thesis is on how to parallelize the training of large deep learning models that use a form of stochastic gradient descent (SGD) [9].\nThe classical SGD method processes each data point sequentially on a single processor. As an optimization method, it often exhibits fast initial convergence toward the local optimum as compared to the batch gradient method [30]. As a learning algorithm, it often leads to better solutions in terms of the test accuracy [30, 23].\nHowever, as the size of the dataset explodes [22], the amount of time taken to go through each data point sequentially becomes prohibitive. To meet this challenge, many distributed stochastic optimization methods have been proposed in literature, e.g. the mini-batch SGD, the asynchronous SGD, and the ADMM -based methods.\nMeanwhile, the scale and the complexity of the deep learning models is also growing to adapt to the growth of the data. There have been attempts to parallelize the training for large-scale deep learning models on thousands of CPUs, including the Google’s Distbelief system [16]. But practical image recognition systems consist of large-scale convolutional\n1\nneural networks trained on a few GPU cards sitting in a single computer [27, 49]. The main challenge is to devise parallel SGD algorithms to train large-scale deep learning models that yield a significant speedup when run on multiple GPU cards across multiple machines. To date, the AlphaGo system is trained using 50 GPUs for a few weeks [52].\nTo solve such large-scale optimization problem, one line of research is to fill-in the gap between the stochastic gradient descent method (SGD) and the batch gradient method [40]. The batch gradient method evaluates the gradient (in a batch) using all the data points while the stochastic gradient descent method uses only a single data point to estimate the gradient of the objective function. The mini-batch SGD, i.e. sampling a subset of the data points to estimate the full gradient, is one possibility to fill-in this gap [14, 17, 25, 31]. Larger mini-batch size reduces the variance of the stochastic gradient. Consequently, one can use a larger learning rate to gain a faster convergence in training. However, to implement it efficiently requires very skillful engineering efforts in order to minimize the communication overhead, which is difficult in particular for training large deep learning models on GPUs [25, 15]. It is even observed that in deep learning problems, using too large mini-batch size may lead to solutions of very poor test accuracy [62].\nAnother possibility is to use the asynchronous stochastic gradient descent methods [8, 29, 63, 2, 46, 16]. The idea is similar to the mini-batch SGD, i.e. to distribute the computation of the gradients to the local workers (processors) and collect them by the master (a parameter server), but asynchronously. The advantage of using asynchronous communication is that it allows local workers to communicate with the master at different time intervals, and thus it can significantly reduce the waiting time spent on the synchronization. The tradeoff is that asynchronous behavior results in large communication delay, which can in turn slow down the convergence rate [62].\nThe DOWNPOUR method belongs to the above class of asynchronous SGD methods, and is proposed for training deep learning models [16] . The main ingredient of the DOWNPOUR method is to reduce the (gradient) communication overhead by running\n2\nthe SGD method on each local worker for multiple steps instead of just one single step. This idea resembles the incremental gradient method [6]. The merit is that each local worker can spend more time on the computation than on the communication. The disadvantage, however, is that the method is not very stable when the (gradient) communication period is large. We shall discuss this phenomenon further in Chapter 4.\nThe mini-batch SGD and the asynchronous SGD methods that we have discussed so far can be implemented in a distributed computing environment [34, 26]. However, conceptually it is still centralized as there is a master server which is needed to collect all the gradients and store the latest model parameter. There’s a class of the distributed optimization methods which is conceptually decentralized. It is based on the idea of consensus averaging [57]. A classical problem is to compute the average value of the local clocks in a sensor network (aka. clock synchronization) [41]. In such setting, one needs to consider how to optimize the design of the averaging network [59] and to analyze the convergence rate on various networks subject to link failure [37, 19].\nThe ADMM (Alternating Direction Method of Multipliers) [11] method can also be used to solve the consensus averaging problem above. The basic idea is to decompose the objective function into several smaller ones so that each one can be solved separately in parallel and then be combined into one solution. In some sense, it is more effective because the dual Lagrangian update is used to close the primal-dual gap associated with the consensus constraints (i.e. to reach the consensus). ADMM is also generalized to the stochastic and the asynchronous setting for solving large-scale machine learning problems [42, 61]. Nevertheless, the consensus can be harder to reach, as the oscillations from the stochastic sampling and the asynchronous behavior need to be absorbed (averaged out) by the whole system (for the constraints to be satisfied).\nIn this thesis, we explore another dimension of such possibility. Unlike the mini-batch SGD and the asynchronous SGD method, we would like to maintain the stochastic nature (oscillation) of the SGD method as the number of workers grows. To accelerate\n3\nthe training of the large-scale deep learning models, in particular under communication constraints, we study instead a weak consensus reaching problem. We use a central variable to average out the noise, but we only maintain a weak coupling (consensus) between the local variables and the central variable. The goal is thus very different to the consensus averaging method and the ADMM method."
    }, {
      "heading" : "1.2 Formalizing the problem",
      "text" : "Consider minimizing a function F (x) in a parallel computing environment [7] with p ∈ N workers and a master. In this thesis we focus on the stochastic optimization problem of the following form\nmin x F (x) := E[f(x, ξ)], (1.1)\nwhere x is the model parameter to be estimated and ξ is a random variable that follows the probability distribution P over Ω such that F (x) = ∫\nΩ f(x, ξ)P(dξ). The optimization\nproblem in Equation 1.1 can be reformulated as follows\nmin x1,...,xp,x̃ p∑ i=1 E[f(xi, ξi)] + ρ 2 ‖xi − x̃‖2, (1.2)\nwhere each ξi follows the same distribution P (thus we assume each worker can sample the entire dataset). In this thesis we refer to xi’s as local variables and we refer to x̃ as a center variable.\nThe problem of the equivalence of these two objectives is studied in the literature and is known as the augmentability or the global variable consensus problem [24, 11]. The quadratic penalty term ρ in Equation 1.2 is expected to ensure that local workers will not fall into different attractors that are far away from the center variable.\nWe will focus on the problem of reducing the parameter communication overhead between the master and the local workers [50, 16, 60, 43, 48]. The problem of data communication\n4\nwhen the data is distributed among the workers [7, 5] is a more general problem and is not addressed in this work. We however emphasize that our problem setting is still highly non-trivial under the communication constraints due to the existence of many local optima [13].\nWe remark further a connection between the quadratic penalty term in Equation 1.2 and the Moreau-Yosida regularization [33] in convex optimization used to smooth the non-smooth part of the objective function. When using the rectified linear units [35] in deep learning models, the objective function also becomes non-smooth. This suggests that it may indeed be a good idea to use such a quadratic penalty term."
    }, {
      "heading" : "1.3 An overview",
      "text" : "We now give an overview of the main focus and results of the following chapters.\nChapter 2 introduces the Elastic Averaging SGD (EASGD) method. We first discuss the basic idea and motivate EASGD. We then propose synchronous EASGD and discuss its connection with the classical Jacobi method in the numerical analysis. We further propose an asynchronous extension of EASGD and discuss how the asynchronous EASGD can be thought of as an perturbation of the synchronous EASGD. We end up with combing EASGD with the classical momentum method, notably the Nesterov ’s momentum, giving the EAMSGD algorithm.\nChapter 3 provides the convergence analysis of the synchronous EASGD algorithm. The analysis is focused on the convergence of the center variable. We discuss a onedimensional quadratic objective function first. We show that the variance of the center variable tends to zero as the number of workers grows. Meanwhile, the variance of the local variables keeps increasing. We also introduce a double averaging sequence computing the time average of the center variable and show that it is asymptotically optimal. We further extend our analysis to the strongly convex case, and discusses the\n5\ntightness of the bound we have obtained compared to the quadratic case. Finally, we study the stability condition of EASGD and ADMM method in the round-robin scheme. We compute numerically the stability region for the ADMM method and show that it can be unstable when the quadratic penalty term is relatively small. This is in contrast to the stability region of the EASGD method.\nChapter 4 is an empirical study on the performance of various serial and parallel stochastic optimization methods for training deep convolutional neural networks on the CIFAR and ImageNet dataset. We first describe the experimental setup, in particular how we preprocess and sample the dataset by the local workers in parallel. We then compare the performance of EASGD and its momentum variant EAMSGD with the SGD and DOWNPOUR methods (including their averaging and momentum variants). We find that EASGD is very robust when the communication period is large, which is not the case for DOWNPOUR. Furthermore, EAMSGD (the combination of EASGD and Nesterov’s momentum method) achieves the best performance measured both in the wallclock time and in the smallest achievable test error. The smallest achievable test error is further improved as we gradually increase the communication period and the number of processors. We provide further empirical results on the dependency of the learning rate and the communication period. To choose a proper communication period, we also give an explicit analysis in terms of the bandwidth requirement for the data communication and the parameter communication in the ImageNet case.\nChapter 5 studies the limit in speedup of various stochastic optimization methods. We would like to know what would be the limit in theory if we were given an infinite amount of processors. We study first the asymptotic phase of these methods using an additive noise model (a one-dimensional quadratic objective with Gaussian noise). The asymptotic variance can be obtained explicitly, and it can be reduced by either the mini-batch SGD or the EASGD method. However, the SGD method using momentum can strictly increase this asymptotic variance. On the other hand, we seek the optimal momentum rate such that the momentum SGD method converges the fastest. We find that this op-\n6\ntimal momentum rate can either be positive or negative. We ask a similar question for the EASGD method by fixing the moving rate of the center variable and then optimize the moving rate of the local variable. We find that the optimal moving (average) rate of the EASGD method is either zero or negative. The surprising connection between these two results is that they are obtained based on a nearly same proof. We perform a similar moment analysis on the EAMSGD method, and find (but only numerically) that the optimal moving rate can be either positive or negative, depending on the choice of the learning rate.\nWe then move on to study the initial phase of these methods based on a multiplicative noise model. We introduce the Gamma distribution to parametrize the spread the input data distribution. We obtain the optimal learning rate for the mini-batch SGD method, and discuss how fast the optimal rate of convergence varies with the mini-batch size. We find that if the input data distribution has a large spread (to be made precise in Chapter 5), then one can gain more effective speedup by using mini-batch SGD. For the momentum SGD method, we observe that using momentum can slow down the optimal convergence rate, but it can accelerate the convergence when the learning rate is chosen to be sub-optimal. For the EASGD method, we observe a quite different picture to the mini-batch SGD. There is an optimal number of workers that EASGD method will achieve the best convergence rate. We also perform an asymptotic analysis when the number of workers is infinite, and show that the stability region can still be enlarged if the spread of the input data distribution is large.\nWe finally discuss a non-convex case to understand when EASGD can get trapped by a saddle point. This is the phenomenon that we have observed in Chapter 4 when the communication period is too large. We find that if the quadratic penalty term is smaller than a critical value, then the local variables can stay on both sides of a saddle point, and it is a stable configuration. This suggests that EASGD can spend a lot of time in such configuration if the coupling between the master and the workers is too weak.\n7\nChapter 6 attempts to scale up the EASGD method to a larger number of processors. Due to the communication constraints, we propose a tree extension of the EASGD method. The leaf node of the tree performs the gradient descent locally and from time to time performs the elastic averaging with their parent. The root node of the tree tracks the spatial average of the variable of its children, and in turn the average of all the leaf nodes. We perform an empirical study with two different communication schemes. The first scheme exploits the fact that faster communication can be achieved at the bottom layer (between the leaf nodes and their parent) than the upper layers. The second scheme uses a faster upward communication rate and a slower downward communication rate so that the root node can be informed of the latest information from the bottom as quick as possible. We observe that the first scheme gives better training speedup, while the second scheme gives better test accuracy. One difference compared to the asynchronous EASGD experiment in Chapter 4 is that the communication protocol between the tree nodes is fully asynchronous so as to maximize the I/O throughput.\nWe end up the Chapter 6 by establishing a connection between the DONWPOUR and EASGD methods. For clarity, we focus on the synchronous scenario. These two methods can be unified by a same equation once we transform EASGD from the Jacobi form into the Gauss-Seidel form. The difference between the two is the choice of the moving rates. A further stability analysis shows that DONWPOUR has a very singular region for these rates which is separated from EASGD when the number of processors is large.\nThe last chapter concludes the thesis with a reprise of this overview, together with some open questions and directions to follow in the future. We have also made an open source project named mpiT on github to facilitate the communication using MPI under Torch, including our implementation of DOWNPOUR and EASGD.\n8\nChapter 2"
    }, {
      "heading" : "Elastic Averaging SGD (EASGD)",
      "text" : "In this chapter we introduce the Elastic Averaging SGD method (EASGD) and its variants. EASGD is motivated by the quadratic penalty method [40], but is re-interpreted as a parallelized extension of the averaging SGD method [45]. The basic idea is to let each worker maintain its own local parameter, and the communication and coordination of work among the local workers is based on an elastic force which links the parameters they compute with a center variable stored by the master. The center variable is updated as a moving average where the average is taken in time and also in space over the parameters computed by local workers. The local variables are updated with the SGD-based methods, so as to keep the oscillations during the training process.\nWe discuss first the synchronous EASGD in Section 2.1. Then in Section 2.2 we discuss how to extend the synchronous EASGD to the asynchronous scenario. Finally in Section 2.3 we combine EASGD with two classical momentum methods in the first-order convex optimization literature, i.e. the heavy-ball method and the Nesterov ’s momentum method, to accelerate EASGD. The big picture is illustrated in Figure 2.1.\n9\n2.1 Synchronous EASGD\nThe EASGD updates captured in resp. Equation 2.1 and 2.2 are obtained by taking the gradient descent step on the objective in Equation 1.2 with respect to resp. variable xi and x̃,\nxit+1 = x i t − η(git(xit) + ρ(xit − x̃t)) (2.1)\nx̃t+1 = x̃t + η p∑ i=1 ρ(xit − x̃t), (2.2)\nwhere git(x i t) denotes the stochastic gradient of F with respect to x i evaluated at iteration t, xit and x̃t denote respectively the value of variables x i and x̃ at iteration t, and η is the learning rate.\nThe update rule for the center variable x̃ takes the form of moving average where the\n10\naverage is taken over both space and time. Denote α = ηρ and β = pα, then Equation 2.1 and 2.2 become\nxit+1 = x i t − ηgit(xit)− α(xit − x̃t) (2.3) x̃t+1 = (1− β)x̃t + β ( 1\np p∑ i=1 xit\n) . (2.4)\nNote that choosing β = pα leads to an elastic symmetry in the update rule, i.e. there exists a symmetric force equal to α(xir − x̃t) between the update of each xi and x̃. It gives us a simple and intuitive reason for the the algorithm’s stability over the ADMM method as will be explained in Section 3.3. However, this relation (β = pα) is by no means optimal as we shall see through our analysis in Chapter 5.\nWe interpret our synchronous EASGD as an approximate model for the asynchronous EASGD. Thus in order to minimize the staleness [25] of the difference xit−x̃t between the center and the local variable for the asynchronous EASGD (described in Algorithm 1), the update for the master in Equation 2.4 involves xit instead of x i t+1. On the other hand, we can also think of our synchronous EASGD update rules (defined by Equation 2.3 and 2.4) as a Jacobi method [47]. We could have proposed a Gauss-Seidel version of the synchronous EASGD by successively updating the local and center variables with local averaging, local gradient descent, and then the global averaging. This possibility will be made precise in Chapter 6.\nNote also that α = ηρ, where the magnitude of ρ (the quadratic penalty term in Equation 1.2) represents the amount of exploration we allow in the model. In particular, small ρ allows for more exploration as it allows xi’s to fluctuate further from the center x̃.\nThe distinctive idea of EASGD is to allow the local workers to perform more exploration (small ρ) and the master to perform exploitation. This approach differs from other settings explored in the literature [16, 4, 8, 36, 29, 2, 46, 63], and focus on how fast the center variable converges.\n11\n2.2 Asynchronous EASGD\nWe discussed the synchronous update of EASGD algorithm in the previous section, where the workers update the local variables in parallel such that the ith worker reads the current value of the center variable and use it to update local variable xi using Equation 2.1. All workers share the same global clock. The master has to wait for the xi updates from all p workers before being allowed to update the value of the center variable x̃ according to Equation 2.2.\nIn this section we propose its asynchronous variant. The local workers are still responsible for updating the local variables xi’s, whereas the master is updating the center variable x̃. Each worker maintains its own clock ti, which starts from 0 and is incremented by 1 after each stochastic gradient update of xi as shown in Algorithm 1. The master performs an update whenever the local workers finished τ steps of their gradient updates, where we refer to τ as the communication period. As can be seen in Algorithm 1, whenever τ divides the local clock of the ith worker, the ith worker communicates with the master and requests the current value of the center variable x̃. The worker then waits until the master sends back the requested parameter value, and computes the elastic difference α(x − x̃) (this entire procedure is captured in step a) in Algorithm 1). The elastic difference is then sent back to the master (step b) in Algorithm 1) who then updates x̃.\nNote that the asynchronous behavior described above is partially asynchronous [7]. As in the beginning of each communication period, each worker needs to read the latest parameter from the master (blocking) and then sends the elastic difference back (blocking). Although this only involves local synchronization, one can avoid this synchronization cost by using a fully asynchronous protocol such that no waiting is necessary. The fully asynchronous protocol may however increase the network traffic and the delay in the parameter communication.We shall be more precise on this fully asynchronous protocol when we discuss the EASGD Tree algorithm in Chapter 6 (Section 6.1).\nRecall that we have chosen the Jacobi form in our synchronous EASGD update rules\n12\n(Equation 2.3 and 2.4) as an approximate model for the asynchronous behavior. It suggests another more efficient way to realize the partially asynchronous protocol as follows. At the beginning of each communication period, each local worker sends (nonblocking) its parameter to the master, and the master will send (non-blocking) back the elastic difference once having received that local worker’s parameter. During that period of time, the local worker’s computation can still make progress. At the end of that communication period (i.e. all the τ gradient updates have completed), each local worker will read (blocking) the elastic difference sent from the master, and then apply it. On the master side, it can either sum the p elastic differences altogether in one step as in the synchronous case or make an update whenever sending an elastic difference.\nThe communication period τ controls the frequency of the communication between every local worker and the master, and thus the trade-off between exploration and exploitation. We show demonstrate empirically in Chapter 4 (Section 4.3.3) that in deep learning problems, too large or too small communication period can both hurt the performance.\n13\nAlgorithm 1: Asynchronous EASGD: Processing by worker i and the master\nInput: learning rate η, moving rate α,\ncommunication period τ ∈ N\nInitialize: x̃ is initialized randomly, xi = x̃,\nti = 0\nRepeat\nx← xi if (τ divides ti) then\na) xi ← xi − α(x− x̃) b) x̃ ← x̃ + α(x− x̃)\nend xi ← xi − ηgi ti (x) ti ← ti + 1\nUntil forever\n2.3 Momentum EASGD\nThe momentum EASGD (EAMSGD) is a variant of our Algorithm 1 and is captured in Algorithm 2. It is based on the Nesterov ’s momentum scheme [39, 28, 55], where the update of the local worker of the form captured in Equation 2.1 is replaced by the following update\nvit+1 = δv i t − ηgit(xit + δvit) (2.5) xit+1 = x i t + v i t+1 − ηρ(xit − x̃t),\nwhere δ is the momentum rate. Note that when δ = 0 we recover the original EASGD algorithm.\n14\nThe idea of momentum is to accelerate the slow components in the gradient descent method. The tradeoff is that we may slow down the components which were originally fast. We shall give an explicit example to illustrate this tradeoff in Chapter 5 (Section 5.2.2).\nIn literature, there’s another well-known momentum variant called heavy-ball method (aka Polyak ’s method) [44]. The analysis of its global convergence property is still a very challenging problem in convex optimization literature [21]. If we were to combine it with EASGD, we would have the following update\nvit+1 = δv i t − ηgit(xit) (2.6) xit+1 = x i t + v i t+1 − ηρ(xit − x̃t).\nNote that in both cases, we do not add the momentum to the center variable. One reason is that the momentum method has an error accumulation effect [18]. Due to the stochastic noise in the gradient, using momentum can actually result in higher asymptotic variance (see [32] and our discussion in Section 5.1.2). The role of the center variable is indeed to reduce the asymptotic variance.\n15\nAlgorithm 2: Asynchronous EAMSGD: Processing by worker i and the master\nInput: learning rate η, moving rate α,\ncommunication period τ ∈ N, momentum term δ\nInitialize: x̃ is initialized randomly, xi = x̃,\nvi = 0, ti = 0\nRepeat\nx← xi if (τ divides ti) then\na) xi ← xi − α(x− x̃) b) x̃ ← x̃ + α(x− x̃)\nend vi ← δvi − ηgi ti (x+ δvi) xi ← xi + vi ti ← ti + 1\nUntil forever\n16\nChapter 3"
    }, {
      "heading" : "Convergence Analysis of EASGD",
      "text" : "In this chapter, we provide the convergence analysis of the synchronous EASGD algorithm with constant learning rate. The analysis is focused on the convergence of the center variable to the optimum. We discuss one-dimensional quadratic case first (Lemma 3.1.1), then we introduce a double averaging sequence and prove that it is asymptotically optimal. For this, we provide two distinct proofs (one in Lemma 3.1.2 for the one-dimensional case, and the other in Lemma 3.1.3 for the multidimensional case). We extend the analysis to the strongly convex case as stated in Theorem 3.2.1. Finally, we provide stability analysis of the asynchronous EASGD and ADMM methods in the round-robin scheme in Section 3.3."
    }, {
      "heading" : "3.1 Quadratic case",
      "text" : "Our analysis in the quadratic case extends the analysis of ASGD in [45]. Assume each of the p local workers xit ∈ Rn observes a noisy gradient at time t ≥ 0 of the linear form given in Equation 3.1.\ngit(x i t) = Ax i t − b− ξit, i ∈ {1, . . . , p}, (3.1)\n17\nwhere the matrix A is positive-definite (each eigenvalue is strictly positive) and {ξit}’s are i.i.d. random variables, with zero mean and positive-definite covariance matrix Σ. Let x∗ denote the optimum solution, where x∗ = A−1b ∈ Rn."
    }, {
      "heading" : "3.1.1 One-dimensional case",
      "text" : "In this section we analyze the behavior of the mean squared error (MSE) of the center variable x̃t, where this error is denoted as E[‖x̃t − x∗‖2], as a function of t, p, η, α and β, where β = pα. Note that the MSE error can be decomposed as (squared) bias and variance1: E[‖x̃t − x∗‖2] = ‖E[x̃t − x∗]‖2 +V[x̃t−x∗]. For one-dimensional case (n = 1), we assume A = h > 0 and Σ = σ2 > 0. Lemma 3.1.1. Let x̃0 and {xi0}i=1,...,p be arbitrary constants, then\nE[x̃t − x∗] = γt(x̃0 − x∗) + γt − φt γ − φ αu0, (3.2)\nV[x̃t − x∗] = p2α2η2 (γ − φ)2 ( γ2 − γ2t 1− γ2 + φ2 − φ2t 1− φ2 − 2 γφ− (γφ)t 1− γφ ) σ2 p , (3.3)\nwhere u0 = ∑p i=1(x i 0 − x∗ − α1−pα−φ(x̃0 − x∗)), a = ηh + (p + 1)α, c2 = ηhpα, γ = 1− a− √ a2−4c2 2 , and φ = 1− a+ √ a2−4c2 2 .\nIt follows from Lemma 3.1.1 that for the center variable to be stable the following has to hold\n− 1 < φ < γ < 1. (3.4)\nIt can be verified that φ and γ are the two zero-roots of the polynomial in λ: λ2 − (2− a)λ+ (1− a+ c2). Recall that φ and λ are the functions of η and α. Thus (see proof in Section 3.1.1)\n• γ < 1 iff c2 > 0 (i.e. η > 0 and α > 0).\n• φ > −1 iff (2− ηh)(2− pα) > 2α and (2− ηh) + (2− pα) > α. 1In our notation, V denotes the variance.\n18\n• φ = γ iff a2 = 4c2 (i.e. ηh = α = 0).\nThe proof the above Lemma is based on the diagonalization of the linear gradient map (this map is symmetric due to the relation β = pα). The stability analysis of the asynchronous EASGD algorithm in the round-robin scheme is similar due to this elastic symmetry.\nProof. Substituting the gradient from Equation 3.1 into the update rule used by each local worker in the synchronous EASGD algorithm (Equation 2.3 and 2.4) we obtain\nxit+1 = x i t − η(Axit − b− ξit)− α(xit − x̃t), (3.5)\nx̃t+1 = x̃t + p∑ i=1 α(xit − x̃t), (3.6)\nwhere η is the learning rate, and α is the moving rate. Recall that α = ηρ and A = h.\nFor the ease of notation we redefine x̃t and x i t as follows:\nx̃t , x̃t − x∗ and xit , xit − x∗.\nWe prove the lemma by explicitly solving the linear equations 3.5 and 3.6. Let xt = (x1t , . . . , x p t , x̃t) T . We rewrite the recursive relation captured in Equation 3.5 and 3.6 as simply\nxt+1 = Mxt + bt,\nwhere the drift matrix M is defined as\nM =  1− α− ηh 0 ... 0 α 0 1− α− ηh 0 ... α ... 0 ... 0 ... 0 ... 0 1− α− ηh α\nα α ... α 1− pα\n ,\n19\nand the (diffusion) vector bt = (ηξ 1 t , . . . , ηξ p t , 0) T .\nNote that one of the eigenvalues of matrix M , that we call φ, satisfies (1 − α − ηh − φ)(1 − pα − φ) = pα2. The corresponding eigenvector is (1, 1, . . . , 1,− pα1−pα−φ)T . Let\nut be the projection of xt onto this eigenvector. Thus ut = ∑p i=1(x i t − α1−pα−φ x̃t). Let\nfurthermore ξt = ∑p i=1 ξ i t. Therefore we have\nut+1 = φut + ηξt. (3.7)\nBy combining Equation 3.6 and 3.7 as follows\nx̃t+1 = x̃t + p∑ i=1 α(xit − x̃t) = (1− pα)x̃t + α(ut + pα 1− pα− φx̃t)\n= (1− pα+ pα 2\n1− pα− φ)x̃t + αut = γx̃t + αut,\nwhere the last step results from the following relations: pα 2\n1−pα−φ = 1 − α − ηh − φ and\nφ+ γ = 1− α− ηh+ 1− pα. Thus we obtained\nx̃t+1 = γx̃t + αut. (3.8)\nBased on Equation 3.7 and 3.8, we can then expand ut and x̃t recursively,\nut+1 = φ t+1u0 + φ t(ηξ0) + . . .+ φ 0(ηξt), (3.9)\nx̃t+1 = γ t+1x̃0 + γ t(αu0) + . . .+ γ 0(αut). (3.10)\nSubstituting u0, u1, . . . , ut, each given through Equation 3.9, into Equation 3.10 we obtain\nx̃t = γ tx̃0 + γt − φt γ − φ αu0 + αη t−1∑ l=1 γt−l − φt−l γ − φ ξl−1. (3.11)\nTo be more specific, the Equation 3.11 is obtained by interchanging the order of sum-\n20\nmation,\nx̃t+1 = γ t+1x̃0 + t∑ i=0 γt−i(αui)\n= γt+1x̃0 + t∑ i=0 γt−i(α(φiu0 + i−1∑ l=0 φi−1−lηξl))\n= γt+1x̃0 + t∑ i=0 γt−iφi(αu0) + t−1∑ l=0 t∑ i=l+1 γt−iφi−1−l(αηξl) = γt+1x̃0 + γt+1 − φt+1\nγ − φ (αu0) + t−1∑ l=0 γt−l − φt−l γ − φ (αηξl).\nSince the random variables ξl are i.i.d, we may sum the variance term by term as follows\nt−1∑ l=0 ( γt−l − φt−l γ − φ )2 = t−1∑ l=0 γ2(t−l) − 2γt−lφt−l + φ2(t−l) (γ − φ)2\n= 1 (γ − φ)2 ( γ2 − γ2(t+1) 1− γ2 − 2 γφ− (γφ)t+1 1− γφ + φ2 − φ2(t+1) 1− φ2 ) .\n(3.12)\nNote that E[ξt] = ∑p i=1 E[ξit] = 0 and V[ξt] = ∑p i=1 V[ξit] = pσ2. These two facts, the equality in Equation 3.11 and Equation 3.12 can then be used to compute E[x̃t] and V[x̃t] as given in Equation 3.2 and 3.3 in Lemma 3.1.1."
    }, {
      "heading" : "Visualizing Lemma 3.1.1",
      "text" : "In Figure 3.1, we illustrate the dependence of MSE on β, η and the number of processors p over time t. We consider the large-noise setting where x̃0 = x i 0 = 1, h = 1 and σ = 10. The MSE error is color-coded such that the deep blue color corresponds to the MSE equal to 10−3, the green color corresponds to the MSE equal to 1, the red color corresponds to MSE equal to 103 and the dark red color corresponds to the divergence of algorithm EASGD (condition in Equation 3.4 is then violated). The plot shows that we can achieve significant variance reduction by increasing the number of local workers\n21\n22\np. This effect is less sensitive to the choice of β and η for large p."
    }, {
      "heading" : "Condition in Equation 3.4",
      "text" : "We are going to show that\n• γ < 1 iff c2 > 0 (i.e. η > 0 and β > 0).\n• φ > −1 iff (2− ηh)(2− β) > 2β/p and (2− ηh) + (2− β) > β/p.\n• φ = γ iff a2 = 4c2 (i.e. ηh = β = 0).\nRecall that a = ηh + (p + 1)α, c2 = ηhpα, γ = 1 − a− √ a2−4c2 2 , φ = 1 − a+ √ a2−4c2 2 , and β = pα. We have\n• γ < 1⇔ a− √ a2−4c2 2 > 0⇔ a > √ a2 − 4c2 ⇔ a2 > a2 − 4c2 ⇔ c2 > 0. • φ > −1⇔ 2 > a+ √ a2−4c2 2 ⇔ 4− a > √ a2 − 4c2 ⇔ 4− a > 0, (4− a)2 > a2− 4c2 ⇔\n4− a > 0, 4− 2a+ c2 > 0⇔ 4 > ηh+ β + α, 4− 2(ηh+ β + α) + ηhβ > 0.\n• φ = γ ⇔ √ a2 − 4c2 = 0⇔ a2 = 4c2.\nThe next corollary is a consequence of Lemma 3.1.1. As the number of workers p grows, the averaging property of the EASGD can be characterized as follows Corollary 3.1.1. Let the Elastic Averaging relation β = pα and the condition 3.4 hold, then\nlim p→∞ lim t→∞\npE[(x̃t − x∗)2] = βηh (2− β)(2− ηh) · 2− β − ηh+ βηh β + ηh− βηh · σ2 h2 .\nProof. Note that when β is fixed, limp→∞ a = ηh+ β and c 2 = ηhβ. Then limp→∞ φ = min(1−β, 1−ηh) and limp→∞ γ = max(1−β, 1−ηh). Also note that using Lemma 3.1.1\n23\nwe obtain\nlim t→∞\nE[(x̃t − x∗)2] = β2η2 (γ − φ)2 ( γ2 1− γ2 + φ2 1− φ2 − 2γφ 1− γφ ) σ2 p\n= β2η2 (γ − φ)2 ( γ2(1− φ2)(1− φγ) + φ2(1− γ2)(1− φγ)− 2γφ(1− γ2)(1− φ2) (1− γ2)(1− φ2)(1− γφ) ) σ2 p = β2η2 (γ − φ)2 ( (γ − φ)2(1 + γφ) (1− γ2)(1− φ2)(1− γφ) ) σ2 p = β2η2\n(1− γ2)(1− φ2) · 1 + γφ 1− γφ · σ2 p .\nCorollary 3.1.1 is obtained by plugging in the limiting values of φ and γ.\nThe crucial point of Corollary 3.1.1 is that the MSE in the limit t→∞ is in the order of 1/p which implies that as the number of processors p grows, the MSE will decrease for the EASGD algorithm. Also note that the smaller the β is (recall that β = pα = pηρ), the more exploration is allowed (small ρ) and simultaneously the smaller the MSE is.\nThe next lemma (Lemma 3.1.2) shows that EASGD algorithm achieves the highest possible rate of convergence when we consider the double averaging sequence (similarly to [45]) {z1, z2, . . . } defined as below\nzt+1 = 1\nt+ 1 t∑ k=0 x̃k. (3.13)\nLemma 3.1.2 (Weak convergence). If the condition in Equation 3.4 holds, then the normalized double averaging sequence defined in Equation 3.13 converges weakly to the normal distribution with zero mean and variance σ2/ph2,\n√ t(zt − x∗) ⇀ N (0, σ2\nph2 ), t→∞. (3.14)\nProof. As in the proof of Lemma 3.1.1, for the ease of notation we redefine x̃t and x i t as follows:\nx̃t , x̃t − x∗ and xit , xit − x∗.\n24\nAlso recall that {ξit}’s are i.i.d. random variables (noise) with zero mean and the same positive definite covariance matrix Σ 0. We are interested in the asymptotic behavior of the double averaging sequence {z1, z2, . . . } defined as\nzt+1 = 1\nt+ 1 t∑ k=0 x̃k. (3.15)\nRecall the Equation 3.11 from the proof of Lemma 3.1.1 (for the convenience it is provided below):\nx̃k = γ kx̃0 + αu0 γk − φk γ − φ + αη k−1∑ l=1 γk−l − φk−l γ − φ ξl−1,\nwhere ξt = ∑p i=1 ξ i t. Therefore t∑ k=0 x̃k = 1− γt+1 1− γ x̃0 + αu0 1 γ − µ ( 1− γt+1 1− γ − 1− φt+1 1− φ ) + αη t−1∑ l=1 t∑ k=l+1 γk−l − φk−l γ − φ ξl−1\n= O(1) + αη t−1∑ l=1 1 γ − φ ( γ 1− γt−l 1− γ − φ 1− φt−l 1− φ ) ξl−1\nNote that the only non-vanishing term (in weak convergence) of 1/ √ t ∑t\nk=0 x̃k as t→∞\nis\n1√ t αη t−1∑ l=1 1 γ − φ ( γ 1− γ − φ 1− φ ) ξl−1. (3.16)\nAlso recall that V[ξl−1] = pσ2 and\n1\nγ − φ\n( γ\n1− γ − φ 1− φ\n) =\n1 (1− γ)(1− φ) = 1 ηhpα .\nTherefore the expression in Equation 3.16 is asymptotically normal with zero mean and variance σ2/ph2.\n25"
    }, {
      "heading" : "3.1.2 Generalization to multidimensional case",
      "text" : "The asymptotic variance in the Lemma 3.1.2 is optimal with any fixed η and β for which Equation 3.4 holds. The next lemma (Lemma 3.1.3) extends the result in Lemma 3.1.2 to the multi-dimensional setting. Lemma 3.1.3 (Weak convergence). Let h denotes the largest eigenvalue of A. If (2 − ηh)(2 − β) > 2β/p, (2 − ηh) + (2 − β) > β/p, η > 0 and β > 0, then the normalized double averaging sequence converges weakly to the normal distribution with zero mean and the covariance matrix V = A−1Σ(A−1)T ,\n√ tp(zt − x∗) ⇀ N (0, V ), t→∞. (3.17)\nProof. Since A is symmetric, one can use the proof technique of Lemma 3.1.2 to prove Lemma 3.1.3 by diagonalizing the matrix A. This diagonalization essentially generalizes Lemma 3.1.1 to the multidimensional case. We will not go into the details of this proof as we will provide a simpler way to look at the system. As in the proof of Lemma 3.1.1 and Lemma 3.1.2, for the ease of notation we redefine x̃t and x i t as follows:\nx̃t , x̃t − x∗ and xit , xit − x∗.\nLet the spatial average of the local parameters at time t be denoted as yt where yt = 1 p ∑p i=1 x i t, and let the average noise be denoted as ξt, where ξt = 1 p ∑p i=1 ξ i t. Equations 3.5 and 3.6 can then be reduced to the following\nyt+1 = yt − η(Ayt − ξt) + α(x̃t − yt), (3.18) x̃t+1 = x̃t + β(yt − x̃t). (3.19)\nWe focus on the case where the learning rate η and the moving rate α are kept constant over time2. Recall β = pα and α = ηρ.\n2As a side note, notice that the center parameter x̃t is tracking the spatial average yt of the local\n26\nLet’s introduce the block notation Ut = (yt, x̃t), Ξt = (ηξt, 0), M = I − ηL and\nL =  A+ αη I −αη I −βη I β η I  . From Equations 3.18 and 3.19 it follows that Ut+1 = MUt + Ξt. Note that this linear system has a degenerate noise Ξt which prevents us from directly applying results of [45]. Expanding this recursive relation and summing by parts, we have\nt∑ k=0 Uk = M 0U0 +\nM1U0 +M 0Ξ0 + M2U0 +M 1Ξ0 +M 0Ξ1 +\n... M tU0 +M t−1Ξ0 + · · ·+M0Ξt−1.\nBy Lemma 3.1.4, ‖M‖2 < 1 and thus\nM0 +M1 + · · ·+M t + · · · = (I −M)−1 = η−1L−1.\nSince A is invertible, we get\nL−1 =  A−1 αβA−1 A−1 ηβ + α βA −1  , thus\n1√ t t∑ k=0 Uk = 1√ t U0 + 1√ t ηL−1 t∑ k=1 Ξk−1 − 1√ t t∑ k=1 Mk+1Ξk−1.\nparameters with a non-symmetric spring in Equation 3.18 and 3.19. To be more precise note that the update on yt+1 contains (x̃t− yt) scaled by α, whereas the update on x̃t+1 contains −(x̃t− yt) scaled by β. Since α = β/p the impact of the center x̃t+1 on the spatial local average yt+1 becomes more negligible as p grows.\n27\nNote that the only non-vanishing term of 1√ t ∑t k=0 Uk is 1√ t (ηL)−1 ∑t k=1 Ξk−1, thus in weak convergence we have\n1√ t (ηL)−1 t∑ k=1\nΞk−1 ⇀ N ( 0\n0\n ,  V V\nV V ), (3.20) where V = A−1Σ(A−1)T .\nLemma 3.1.4. If the following conditions hold:\n(2− ηh)(2− pα) > 2α\n(2− ηh) + (2− pα) > α\nη > 0\nα > 0\nthen ‖M‖2 < 1.\nProof. The eigenvalue λ of M and the (non-zero) eigenvector (y, z) of M satisfy\nM  y z  = λ  y z  . (3.21) Recall that\nM = I − ηL =  I − ηA− αI αI βI I − βI  . (3.22) From the Equations 3.21 and 3.22 we obtain\n y − ηAy − αy + αz = λyβy + (1− β)z = λz . (3.23)\n28\nSince (y, z) is assumed to be non-zero, we can write z = βy/(λ + β − 1). Then the Equation 3.23 can be reduced to\nηAy = (1− α− λ)y + αβ λ+ β − 1y. (3.24)\nThus y is the eigenvector of A. Let λA be the eigenvalue of matrix A such that Ay = λAy. Thus based on Equation 3.24 it follows that\nηλA = (1− α− λ) + αβ\nλ+ β − 1 . (3.25)\nEquation 3.25 is equivalent to\nλ2 − (2− a)λ+ (1− a+ c2) = 0, (3.26)\nwhere a = ηλA + (p + 1)α, c 2 = ηλApα. It follows from the condition in Equation 3.4 that −1 < λ < 1 iff η > 0, β > 0, (2−ηλA)(2−β) > 2β/p and (2−ηλA)+(2−β) > β/p. Let h denote the maximum eigenvalue of A and note that 2−ηλA ≥ 2−ηh. This implies that the condition of our lemma is sufficient.\nAs in Lemma 3.1.2, the asymptotic covariance in the Lemma 3.1.3 is optimal, i.e. meets the Fisher information lower-bound. The fact that this asymptotic covariance matrix V does not contain any term involving ρ is quite remarkable, since the penalty term ρ does have an impact on the condition number of the Hessian in Equation 1.2."
    }, {
      "heading" : "3.2 Strongly convex case",
      "text" : "We now extend the above proof ideas to analyze the strongly convex case, in which the noisy gradient git(x) = ∇F (x)− ξit has the regularity that there exists some 0 < µ ≤ L, for which µ ‖x− y‖2 ≤ 〈∇F (x)−∇F (y), x− y〉 ≤ L ‖x− y‖2 holds uniformly for any x ∈ Rd, y ∈ Rd. The noise {ξit}’s is assumed to be i.i.d. with zero mean and bounded\n29\nvariance E[ ∥∥ξit∥∥2] ≤ σ2.\nTheorem 3.2.1. Let at = E ∥∥∥1p∑pi=1 xit − x∗∥∥∥2, bt = 1p∑pi=1 E∥∥xit − x∗∥∥2, ct = E ‖x̃t − x∗‖2, γ1 = 2η µL µ+L and γ2 = 2ηL(1− 2 √ µL µ+L ). If 0 ≤ η ≤ 2µ+L(1− α), 0 ≤ α < 1 and 0 ≤ β ≤ 1 then  at+1 bt+1\nct+1\n ≤  1− γ1 − γ2 − α γ2 α 0 1− γ1 − α α\nβ 0 1− β\n  at bt\nct\n+  η2 σ 2 p η2σ2\n0\n .\nProof. The idea of the proof is based on the point of view in Lemma 3.1.3, i.e. how close the center variable x̃t is to the spatial average of the local variables yt = 1 p ∑p i=1 x i t. To further simplify the notation, let the noisy gradient be ∇f it,ξ = git(xit) = ∇F (xit) − ξit, and ∇f it = ∇F (xit) be its deterministic part. Then EASGD updates can be rewritten as follows,\nxit+1 = x i t − η∇f it,ξ − α(xit − x̃t), (3.27) x̃t+1 = x̃t + β(yt − x̃t). (3.28)\nWe have thus the update for the spatial average,\nyt+1 = yt − η 1\np p∑ i=1 ∇f it,ξ − α(yt − x̃t). (3.29)\nThe idea of the proof is to bound the distance ‖x̃t − x∗‖2 through ‖yt − x∗‖2 and 1 p ∑p i ∥∥xit − x∗∥∥2. We start from the following estimate for the strongly convex function [38],\n〈∇F (x)−∇F (y), x− y〉 ≥ µL µ+ L ‖x− y‖2 + 1 µ+ L ‖∇F (x)−∇F (y)‖2 .\n30\nSince ∇f(x∗) = 0, we have\n〈 ∇f it , xit − x∗ 〉 ≥ µL µ+ L ∥∥xit − x∗∥∥2 + 1µ+ L ∥∥∇f it∥∥2 . (3.30) From Equation 3.27 the following relation holds,\n∥∥xit+1 − x∗∥∥2 = ∥∥xit − x∗∥∥2 + η2 ∥∥∇f it,ξ∥∥2 + α2 ∥∥xit − x̃t∥∥2 − 2η 〈 ∇f it,ξ, xit − x∗ 〉 − 2α 〈 xit − x̃t, xit − x∗\n〉 + 2ηα 〈 ∇f it,ξ, xit − x̃t 〉 . (3.31)\nBy the cosine rule (2 〈a− b, c− d〉 = ‖a− d‖2−‖a− c‖2 + ‖c− b‖2−‖d− b‖2), we have\n2 〈 xit − x̃t, xit − x∗ 〉 = ∥∥xit − x∗∥∥2 + ∥∥xit − x̃t∥∥2 − ‖x̃t − x∗‖2 . (3.32)\nBy the Cauchy-Schwarz inequality, we have\n〈 ∇f it , xit − x̃t 〉 ≤ ∥∥∇f it∥∥ ∥∥xit − x̃t∥∥ . (3.33)\nCombining the above estimates in Equations 3.30, 3.31, 3.32, 3.33, we obtain\n∥∥xit+1 − x∗∥∥2 ≤ ∥∥xit − x∗∥∥2 + η2 ∥∥∇f it − ξit∥∥2 + α2 ∥∥xit − x̃t∥∥2 − 2η ( µL\nµ+ L ∥∥xit − x∗∥∥2 + 1µ+ L ∥∥∇f it∥∥2 ) + 2η 〈 ξit, x i t − x∗ 〉 − α\n( ∥∥xit − x∗∥∥2 + ∥∥xit − x̃t∥∥2 − ‖x̃t − x∗‖2 ) + 2ηα\n∥∥∇f it∥∥ ∥∥xit − x̃t∥∥ − 2ηα 〈ξit, xit − x̃t〉 . (3.34) Choosing 0 ≤ α < 1, we can have this upper-bound for the terms α2\n∥∥xit − x̃t∥∥2 − α ∥∥xit − x̃t∥∥2 + 2ηα ∥∥∇f it∥∥ ∥∥xit − x̃t∥∥ = −α(1 − α) ∥∥xit − x̃t∥∥2 + 2ηα ∥∥∇f it∥∥ ∥∥xit − x̃t∥∥ ≤ η2α 1−α ∥∥∇f it∥∥2 by applying −ax2 +bx ≤ b24a with x = ∥∥xit − x̃t∥∥. Thus we can further bound\n31\nEquation 3.34 with\n∥∥xit+1 − x∗∥∥2 ≤ (1− 2η µLµ+ L − α)∥∥xit − x∗∥∥2 + (η2 + η2α1− α − 2ηµ+ L)∥∥∇f it∥∥2 − 2η2 〈 ∇f it , ξit 〉 + 2η 〈 ξit, x i t − x∗ 〉 − 2ηα 〈 ξit, x i t − x̃t 〉 (3.35)\n+ η2 ∥∥ξit∥∥2 + α ‖x̃t − x∗‖2 (3.36)\nAs in Equation 3.35 and 3.36, the noise ξit is zero mean (Eξit = 0) and the variance of the noise ξit is bounded (E ∥∥ξit∥∥2 ≤ σ2), if η is chosen small enough such that η2+ η2α1−α− 2ηµ+L ≤ 0, then\nE ∥∥xit+1 − x∗∥∥2 ≤ (1− 2η µLµ+ L − α)E∥∥xit − x∗∥∥2 + η2σ2 + αE ‖x̃t − x∗‖2 .(3.37)\nNow we apply similar idea to estimate ‖yt − x∗‖2. From Equation 3.29 the following relation holds,\n‖yt+1 − x∗‖2 = ‖yt − x∗‖2 + η2 ∥∥∥∥∥1p p∑ i=1 ∇f it,ξ ∥∥∥∥∥ 2 + α2 ‖yt − x̃t‖2\n− 2η 〈 1\np p∑ i=1 ∇f it,ξ, yt − x∗ 〉 − 2α 〈yt − x̃t, yt − x∗〉\n+ 2ηα\n〈 1\np p∑ i=1 ∇f it,ξ, yt − x̃t 〉 . (3.38)\nBy 〈\n1 p ∑p i=1 ai, 1 p ∑p j=1 bj 〉 = 1p ∑p i=1 〈ai, bi〉 − 1p2 ∑ i>j 〈ai − aj , bi − bj〉, we have\n〈 1\np p∑ i=1 ∇f it , yt − x∗ 〉 = 1 p p∑ i=1 〈 ∇f it , xit − x∗ 〉 − 1 p2 ∑ i>j 〈 ∇f it −∇f jt , xit − xjt 〉 . (3.39)\nBy the cosine rule, we have\n2 〈yt − x̃t, yt − x∗〉 = ‖yt − x∗‖2 + ‖yt − x̃t‖2 − ‖x̃t − x∗‖2 . (3.40)\n32\nDenote ξt = 1 p ∑p i=1 ξ i t, we can rewrite Equation 3.38 as\n‖yt+1 − x∗‖2 = ‖yt − x∗‖2 + η2 ∥∥∥∥∥1p p∑ i=1 ∇f it − ξt ∥∥∥∥∥ 2 + α2 ‖yt − x̃t‖2\n− 2η 〈 1\np p∑ i=1 ∇f it − ξt, yt − x∗ 〉 − 2α 〈yt − x̃t, yt − x∗〉\n+ 2ηα\n〈 1\np p∑ i=1 ∇f it − ξt, yt − x̃t 〉 . (3.41)\nBy combining the above Equations 3.39, 3.40 with 3.41, we obtain\n‖yt+1 − x∗‖2 = ‖yt − x∗‖2 + η2 ∥∥∥∥∥1p p∑ i=1 ∇f it − ξt ∥∥∥∥∥ 2 + α2 ‖yt − x̃t‖2\n− 2η ( 1\np p∑ i=1 〈 ∇f it , xit − x∗ 〉 − 1 p2 ∑ i>j 〈 ∇f it −∇f jt , xit − xjt 〉 ) (3.42)\n+ 2η 〈ξt, yt − x∗〉 − α(‖yt − x∗‖2 + ‖yt − x̃t‖2 − ‖x̃t − x∗‖2)\n+ 2ηα\n〈 1\np p∑ i=1 ∇f it − ξt, yt − x̃t 〉 . (3.43)\nThus it follows from Equation 3.30 and 3.43 that\n‖yt+1 − x∗‖2 ≤ ‖yt − x∗‖2 + η2 ∥∥∥∥∥1p p∑ i=1 ∇f it − ξt ∥∥∥∥∥ 2 + α2 ‖yt − x̃t‖2\n− 2η1 p p∑ i=1 ( µL µ+ L ∥∥xit − x∗∥∥2 + 1µ+ L ∥∥∇f it∥∥2 ) + 2η 1\np2 ∑ i>j 〈 ∇f it −∇f jt , xit − xjt 〉 + 2η 〈ξt, yt − x∗〉 − α(‖yt − x∗‖2 + ‖yt − x̃t‖2 − ‖x̃t − x∗‖2)\n+ 2ηα\n〈 1\np p∑ i=1 ∇f it − ξt, yt − x̃t 〉 . (3.44)\n33\nRecall yt = 1 p ∑p i=1 x i t, we have the following bias-variance relation,\n1 p p∑ i=1 ∥∥xit − x∗∥∥2 = 1p p∑ i=1 ∥∥xit − yt∥∥2 + ‖yt − x∗‖2 = 1p2 ∑ i>j ∥∥∥xit − xjt∥∥∥2 + ‖yt − x∗‖2 , 1\np p∑ i=1 ∥∥∇f it∥∥2 = 1p2 ∑ i>j ∥∥∥∇f it −∇f jt ∥∥∥2 + ∥∥∥∥∥1p p∑ i=1 ∇f it ∥∥∥∥∥ 2 . (3.45)\nBy the Cauchy-Schwarz inequality, we have\nµL\nµ+ L ∥∥∥xit − xjt∥∥∥2 + 1µ+ L ∥∥∥∇f it −∇f jt ∥∥∥2 ≥ 2 √ µL µ+ L 〈 ∇f it −∇f jt , xit − xjt 〉 . (3.46)\nCombining the above estimates in Equations 3.44, 3.45, 3.46, we obtain\n‖yt+1 − x∗‖2 ≤ ‖yt − x∗‖2 + η2 ∥∥∥∥∥1p p∑ i=1 ∇f it − ξt ∥∥∥∥∥ 2 + α2 ‖yt − x̃t‖2\n− 2η ( µL\nµ+ L ‖yt − x∗‖2 +\n1\nµ+ L ∥∥∥∥∥1p p∑ i=1 ∇f it ∥∥∥∥∥ 2)\n+ 2η ( 1− 2 √ µL\nµ+ L\n) 1\np2 ∑ i>j 〈 ∇f it −∇f jt , xit − xjt 〉 + 2η 〈ξt, yt − x∗〉 − α(‖yt − x∗‖2 + ‖yt − x̃t‖2 − ‖x̃t − x∗‖2)\n+ 2ηα\n〈 1\np p∑ i=1 ∇f it − ξt, yt − x̃t 〉 . (3.47)\nSimilarly if 0 ≤ α < 1, we can have this upper-bound for the terms α2 ‖yt − x̃t‖2 − α ‖yt − x̃t‖2 + 2ηα ∥∥∥1p∑pi=1∇f it∥∥∥ ‖yt − x̃t‖ ≤ η2α1−α ∥∥∥1p∑pi=1∇f it∥∥∥2 by applying −ax2 +\n34\nbx ≤ b24a with x = ‖yt − x̃t‖. Thus we have the following bound for the Equation 3.47\n‖yt+1 − x∗‖2 ≤ (1− 2η µL µ+ L − α) ‖yt − x∗‖2 + (η2 +\nη2α\n1− α − 2η µ+ L ) ∥∥∥∥∥1p p∑ i=1 ∇f it ∥∥∥∥∥ 2\n− 2η2 〈 1\np p∑ i=1 ∇f it , ξt 〉 + 2η 〈ξt, yt − x∗〉 − 2ηα 〈ξt, yt − x̃t〉\n+ 2η ( 1− 2 √ µL\nµ+ L\n) 1\np2 ∑ i>j 〈 ∇f it −∇f jt , xit − xjt 〉 + η2 ‖ξt‖2 + α ‖x̃t − x∗‖2 . (3.48)\nSince 2 √ µL µ+L ≤ 1, we need also bound the nonlinear term 〈 ∇f it −∇f jt , xit − xjt 〉 ≤\nL ∥∥∥xit − xjt∥∥∥2. Recall the bias-variance relation 1p∑pi=1 ∥∥xit − x∗∥∥2 = 1p2 ∑i>j ∥∥∥xit − xjt∥∥∥2+ ‖yt − x∗‖2. The key observation is that if 1p ∑p i=1\n∥∥xit − x∗∥∥2 remains bounded, then larger variance ∑ i>j\n∥∥∥xit − xjt∥∥∥2 implies smaller bias ‖yt − x∗‖2. Thus this nonlinear term can be compensated.\nAgain choose η small enough such that η2 + η 2α 1−α − 2η µ+L ≤ 0 and take expectation in Equation 3.48,\nE ‖yt+1 − x∗‖2 ≤ (1− 2η µL µ+ L − α)E ‖yt − x∗‖2\n+ 2ηL ( 1− 2 √ µL\nµ+ L\n)( 1\np p∑ i=1 E ∥∥xit − x∗∥∥2 − E ‖yt − x∗‖2)\n+ η2 σ2\np + αE ‖x̃t − x∗‖2 . (3.49)\nAs for the center variable in Equation 3.28, we apply simply the convexity of the norm ‖·‖2 to obtain\n‖x̃t+1 − x∗‖2 ≤ (1− β) ‖x̃t − x∗‖2 + β ‖yt − x∗‖2 . (3.50)\nCombining the estimates from Equations 3.37, 3.49, 3.50, and denote at = E ‖yt − x∗‖2,\n35\nbt = 1 p ∑p i=1 E ∥∥xit − x∗∥∥2, ct = E ‖x̃t − x∗‖2, γ1 = 2η µLµ+L , γ2 = 2ηL(1− 2√µLµ+L ), then  at+1 bt+1\nct+1\n ≤  1− γ1 − γ2 − α γ2 α 0 1− γ1 − α α\nβ 0 1− β\n  at bt\nct\n+  η2 σ 2 p η2σ2\n0\n ,\nas long as 0 ≤ β ≤ 1, 0 ≤ α < 1 and η2 + η2α1−α − 2η µ+L ≤ 0, i.e. 0 ≤ η ≤ 2µ+L(1− α). The above theorem captures the bias-variance tradeoff of the spatial average 1p ∑p i=1 x i t of the local variables (the at), with respect to the averaged mean squared error of each local variable (the bt). The center variable x̃t is tracking 1 p ∑p i=1 x i t over time (the ct).\nTo get an upper bound on the rate of convergence for x̃t, we need to assume the matrix M to be positive, and its spectral norm to be smaller than one. Here\nM =  1− γ1 − γ2 − α γ2 α 0 1− γ1 − α α\nβ 0 1− β\n .\nWe have three eigenvalues of M as follows:\nλ1 = 1− α− γ1 − γ2, λ2 = 1 + 1\n2 (−α− β − γ1 +\n√ (α+ β + γ1)2 − 4βγ1),\nλ3 = 1 + 1\n2 (−α− β − γ1 −\n√ (α+ β + γ1)2 − 4βγ1).\nUnder the conditions of the above theorem (0 ≤ η ≤ 2µ+L(1 − α), 0 ≤ α < 1 and 0 ≤ β ≤ 1), we still need to assume λ1 ≥ 0 so that M is positive. Since γ1 = 2η µLµ+L ≥ 0 and γ2 = 2ηL(1− 2 √ µL µ+L ) ≥ 0, we deduce that λ1 ≤ 1. We can also verify that λ3 ≤ λ2 ≤ 1. Thus for the stability we only need λ3 ≥ −1.\nFor λ1 > 0, we get the condition 0 < η < 1−α\n2µL µ+L\n+2L(1− 2 √ µL µ+L ) . For λ3 > −1, we have the\n36\ncondition η < µ+LµL (1− α2−β ). When µ = L, these two conditions mean 0 < η < 1−αL and 0 < η < 2L(1 − α2−β ). On the other hand, when µ = 0, we have 0 < η < 1−α2L . In either case, our method operates in the under-damping (no oscillations) region.\nWith the above conditions, we can now ask what is the asymptotic variance of at, bt and ct. By solving the fixed point equation (a∞, b∞, c∞) ′ = M(a∞, b∞, c∞) ′+(η2 σ 2 p , η 2σ2, 0)′, we obtain\na∞ = c∞ = α/p+ γ1/p+ γ2 γ1(α+ γ1 + γ2) η2σ2,\nb∞ = α/p+ γ1 + γ2 γ1(α+ γ1 + γ2) η2σ2.\nIf µ = L, then γ2 = 0, we indeed get the asymptotic variance c∞ of order σ 2/p. This order matches our quadratic case analysis above. However, if µ << L, then γ1 will be close to zero, and we don’t see in this upper bound the benefit of variance reduction by increasing p (number of workers). It would be interesting to find a non-quadratic example such that this can actually happen.\n3.3 Stability of EASGD and ADMM\nIn this section we study the stability of the asynchronous EASGD and ADMM methods in the round-robin scheme [29]. We first state the updates of both algorithms in this setting, and then we study their stability. We will show that in the one-dimensional quadratic case, ADMM algorithm can exhibit chaotic behavior, leading to exponential divergence. The analytic condition for the ADMM algorithm to be stable is still unknown, while for the EASGD algorithm it is very simple.\nIn our setting, the ADMM method [11, 61, 42] involves solving the following minimax\n37\nproblem3,\nmax λ1,...,λp min x1,...,xp,x̃ p∑ i=1 F (xi)− λi(xi − x̃) + ρ 2 ‖xi − x̃‖2, (3.51)\nwhere λi’s are the Lagrangian multipliers. The resulting updates of the ADMM algorithm in the round-robin scheme are given next. Let t ≥ 0 be a global clock. At each t,\nwe linearize the function F (xi) with F (xit)+ 〈 ∇F (xit), xi − xit 〉 + 12η ∥∥xi − xit∥∥2 as in [42]. The updates become\nλit+1 =  λ i t − (xit − x̃t) if mod (t, p) = i− 1;\nλit if mod (t, p) 6= i− 1. (3.52)\nxit+1 =  xit−η∇F (xit)+ηρ(λit+1+x̃t) 1+ηρ if mod (t, p) = i− 1;\nxit if mod (t, p) 6= i− 1. (3.53)\nx̃t+1 = 1\np p∑ i=1 (xit+1 − λit+1). (3.54)\nEach local variable xi is periodically updated (with period p). First, the Lagrangian multiplier λi is updated with the dual ascent update as in Equation 3.52. It is followed by the gradient descent update of the local variable as given in Equation 3.53. Then the center variable x̃ is updated with the most recent values of all the local variables and Lagrangian multipliers as in Equation 3.54. Note that since the step size for the dual ascent update is chosen to be ρ by convention [11, 61, 42], we have re-parametrized the Lagrangian multiplier to be λit ← λit/ρ in the above updates.\nThe EASGD algorithm in the round-robin scheme is defined similarly and is given below\nxit+1 =  x i t − η∇F (xit)− α(xit − x̃t) if mod (t, p) = i− 1;\nxit if mod (t, p) 6= i− 1. (3.55)\nx̃t+1 = x̃t + ∑\ni: mod (t,p)=i−1\nα(xit − x̃t). (3.56)\n3The convergence analysis in [61] is based on the assumption that “At any master iteration, updates from the workers have the same probability of arriving at the master.”, which is not satisfied in the round-robin scheme.\n38\nAt time t, only the i-th local worker (whose index i− 1 equals t modulo p) is activated, and performs the update in Equations 3.55 which is followed by the master update given in Equation 3.56.\nWe will now focus on the one-dimensional quadratic case without noise, i.e.\nF (x) = x2\n2 , x ∈ R.\nFor the ADMM algorithm, let the state of the (dynamical) system at time t be st = (λ1t , x 1 t , . . . , λ p t , x p t , x̃t) ∈ R2p+1. The local worker i’s updates in Equations 3.52, 3.53, and 3.54 are composed of three linear maps which can be written as st+1 = (F i 3 ◦ F i2 ◦ F i1)(st). For simplicity, we will only write them out below for the case when i = 1 and p = 2:\nF 11=  1 −1 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0\n0 0 0 0 1\n , F 12=  1 0 0 0 0 ηρ 1+ηρ 1−η 1+ηρ 0 0 ηρ 1+ηρ 0 0 1 0 0 0 0 0 1 0\n0 0 0 0 1\n , F 13=  1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0\n− 1p 1p − 1p 1p 0\n .\nFor each of the p linear maps, it’s possible to find a simple condition such that each map, where the ith map has the form F i3 ◦ F i2 ◦ F i1, is stable (the absolute value of the eigenvalues of the map are smaller or equal to one). However, when these non-symmetric maps are composed one after another as follows F = F p3 ◦ F p2 ◦ F p1 ◦ . . . ◦ F 13 ◦ F 12 ◦ F 11 , the resulting map F can become unstable! (more precisely, some eigenvalues of the map can sit outside the unit circle in the complex plane).\nWe now present the numerical conditions for which the ADMM algorithm becomes unstable in the round-robin scheme for p = 3 and p = 8, by computing the largest absolute eigenvalue of the map F . Figure 3.2 summarizes the obtained result. We also illustrate this unstable behavior in Figure 3.3\n39\nOn the other hand, the EASGD algorithm involves composing only symmetric linear maps due to the elasticity. Let the state of the (dynamical) system at time t be st = (x1t , . . . , x p t , x̃t) ∈ Rp+1. The activated local worker i’s update in Equation 3.55 and the master update in Equation 3.56 can be written as st+1 = F i(st). In case of p = 2, the map F 1 and F 2 are defined as follows\nF 1=  1− η − α 0 α 0 1 0\nα 0 1− α\n, F 2=  1 0 0 0 1− η − α α\n0 α 1− α  For the composite map F p ◦ . . .◦F 1 to be stable, the condition that needs to be satisfied is actually the same for each i, and is furthermore independent of p (since each linear map F i is symmetric). It essentially involves the stability of the 2× 2 matrix\n 1− η − α α α 1− α  , whose two (real) eigenvalues λ satisfy (1 − η − α − λ)(1 − α − λ) = α2. The resulting stability condition (|λ| ≤ 1) is simple and given as\n0 ≤ η ≤ 2, 0 ≤ α ≤ 4− 2η 4− η .\n40\n41\n42\nChapter 4"
    }, {
      "heading" : "Performance in Deep Learning",
      "text" : "In this chapter, we compare empirically the performance in deep learning of asynchronous EASGD and EAMSGD with the parallel method DOWNPOUR and the sequential method SGD, as well as their averaging and momentum variants.\nAll the parallel comparator methods are listed below1:\n• DOWNPOUR [16], the detail and the pseudo-code of the implementation are de-\nscribed in Section 4.4 (Algorithm 3).\n• Momentum DOWNPOUR (MDOWNPOUR), where the Nesterov’s momentum\nscheme is applied to the master’s update (note it is unclear how to apply it to the local workers or for the case when τ > 1). The pseudo-code is described in Section 4.4 (Algorithms 4 and 5). • A method that we call ADOWNPOUR, where we compute the average over time\nof the center variable x̃ as follows: zt+1 = (1 − αt+1)zt + αt+1x̃t, and αt+1 = 1t+1 is a moving rate, and z0 = x̃0. The t denotes the master clock, which is initialized to 0 and incremented every time the center variable x̃ is updated. • A method that we call MVADOWNPOUR, where we compute the moving average 1We have compared asynchronous ADMM [61] with EASGD in our setting as well, the performance is nearly the same. However, ADMM ’s momentum variant is not as stable when using large communication period τ .\n43\nof the center variable x̃ as follows: zt+1 = (1 − α)zt + αx̃t, and the moving rate α was chosen to be constant, and z0 = x̃0. The t denotes the master clock and is defined in the same way as for the ADOWNPOUR method.\nAll the sequential comparator methods (p = 1) are listed below:\n• SGD [9] with constant learning rate η. • Momentum SGD (MSGD) [55] with constant (Nesterov’s) momentum rate δ. • ASGD [45] with moving rate αt+1 = 1t+1 . • MVASGD [45] with moving rate α set to a constant.\nWe perform experiments on two benchmark datasets: CIFAR-10 (we refer to it as CIFAR)2 and ImageNet ILSVRC 2013 (we refer to it as ImageNet)3. We focus on the image classification task with deep convolutional neural networks. We first explain the experimental setup in Section 4.1 and then present the main experimental results in Section 4.2. We present further experimental results in Section 4.3 and discuss the effect of the averaging, the momentum, the learning rate, the communication period, the data and parameter communication tradeoff, and finally the speedup."
    }, {
      "heading" : "4.1 Experimental setup",
      "text" : "For all our experiments we use a GPU-cluster interconnected with InfiniBand. Each node has 4 Titan GPU processors where each local worker corresponds to one GPU processor. The center variable of the master is stored and updated on the centralized parameter server [16]. Our implementation is available at https://github.com/sixin-zh/mpiT.\nTo describe the architecture of the convolutional neural network, we will first introduce a notation. Let (c, x, y) denotes the size of the input image to each layer, where c is the number of color channels and (x, y) denotes the horizontal and the vertical dimension of\n2Downloaded from http://www.cs.toronto.edu/~kriz/cifar.html. 3Downloaded from http://image-net.org/challenges/LSVRC/2013.\n44\nthe input. Let C denotes the fully-connected convolutional operator and let R denotes the rectified linear non-linearity (relu, c.f. [35]), P denotes the max pooling operator, L denotes the linear operator and D denotes the dropout operator with rate equal to 0.5 and S denotes the the softmax nonlinearity. We use the cross-entropy loss for the classification.\nFor the ImageNet experiment we use the similar approach to [49] with the following 11-layer convolutional neural network: (3, 221, 221) C,R−−−−−→\n(7,7,2,2) (96, 108, 108) P−−−−−→ (3,3,3,3)\n(96, 36, 36) C,R−−−−−→\n(5,5,1,1) (256, 32, 32) P−−−−−→ (2,2,2,2) (256, 16, 16) C,R−−−−−→ (3,3,1,1) (384, 14, 14) C,R−−−−−→ (2,2,1,1)\n(384, 13, 13) C,R−−−−−→\n(2,2,1,1) (256, 12, 12) P−−−−−→ (2,2,2,2) (256, 6, 6) L,R,D−−−−→ 0.5 (4096, 1, 1) L,R,D−−−−→ 0.5\n(4096, 1, 1) L,S−−→ (1000, 1, 1).\nFor the CIFAR experiment we use the similar approach to [58] with the following 7-layer convolutional neural network: (3, 28, 28) C,R−−−−−→\n(5,5,1,1) (64, 24, 24) P−−−−−→ (2,2,2,2) (64, 12, 12) C,R−−−−−→ (5,5,1,1)\n(128, 8, 8) P−−−−−→\n(2,2,2,2) (128, 4, 4) C,R−−−−−→ (3,3,1,1) (64, 2, 2) L,R,D−−−−→ 0.5 (256, 1, 1) L,S−−→ (10, 1, 1).\nNote that the numbers below the rightarrow of the C and P operator represent the kernel size (first horizontal and then vertical), the stride size (first horizontal and then vertical) and the padding size (if exists, first horizontal and then vertical) on each of the two sides of the image. The number below the rightarrow of the D operator emphasizes the dropout rate 0.5 [54].\nIn our experiments, all the methods we run use the same initial parameter chosen randomly, except that we set all the biases to zero for CIFAR case and to 0.1 for ImageNet case. This parameter is used to initialize the master and all the local workers4. We add l2-regularization λ 2 ‖x‖ 2 to the loss function F (x). For ImageNet we use λ = 10−5 and for CIFAR we use λ = 10−4. We also compute the stochastic gradient using mini-batches of sample size 128.\n4On the contrary, initializing the local workers and the master with different random seeds ’traps’ the algorithm in the symmetry breaking phase.\n45"
    }, {
      "heading" : "Data preprocessing",
      "text" : "For the ImageNet experiment, we re-size each RGB image so that the smallest dimension is 256 pixels. We also re-scale each pixel value to the interval [0, 1]. We then extract random crops (and their horizontal flips) of size 3 × 221 × 221 pixels and present these to the network in mini-batches of size 128.\nFor the CIFAR experiment, we use the original RGB image of size 3 × 32 × 32. As before, we re-scale each pixel value to the interval [0, 1]. We then extract random crops (and their horizontal flips) of size 3× 28× 28 pixels and present these to the network in mini-batches of size 128.\nThe training and test loss and the test error are only computed from the center patch (3 × 28 × 28) for the CIFAR experiment and the center patch (3 × 221 × 221) for the ImageNet experiment.\nData prefetching (Sampling the dataset by the local workers in parallel)\nWe will now explain precisely how the dataset is sampled by each local worker as uniformly and efficiently as possible. The general parallel data loading scheme on a single machine is as follows: we use k CPUs, where k = 8, to load the data in parallel. Each data loader reads from the memory-mapped (mmap) file a chunk of c raw images (preprocessing was described in the previous subsection) and their labels (for CIFAR c = 512 and for ImageNet c = 64). For the CIFAR, the mmap file of each data loader contains the entire dataset whereas for ImageNet, each mmap file of each data loader contains different 1/k fractions of the entire dataset. A chunk of data is always sent by one of the data loaders to the first worker who requests the data. The next worker requesting the data from the same data loader will get the next chunk. Each worker requests in total k data chunks from k different data loaders and then process them before asking for new data chunks. Notice that each data loader cycles5 through the\n5Its advantage is observed in [10].\n46\ndata in the mmap file, sending consecutive chunks to the workers in order in which it receives requests from them. When the data loader reaches the end of the mmap file, it selects the address in memory uniformly at random from the interval [0, s], where s = (number of images in the mmap file modulo mini-batch size), and uses this address to start cycling again through the data in the mmap file. After the local worker receives the k data chunks from the data loaders, it shuffles them and divides it into mini-batches of size 128."
    }, {
      "heading" : "4.2 Experimental results",
      "text" : "For all experiments in this section we use EASGD with β = 0.9 and α = β/p, for all momentum-based methods we set the momentum term δ = 0.99 and finally for MVADOWNPOUR we set the moving rate to α = 0.001. We start with the experiment on CIFAR dataset with p = 4 local workers running on a single computing node.\nFor all the methods, we examined the communication periods from the following set τ = {1, 4, 16, 64}. For each method we examined a wide range of learning rates. The learning rates explored in all experiments are summarized in Table 4.1, 4.2 and 4.3. The CIFAR experiment was run 3 times independently from the same random initialization and for each method we report its best performance measured by the smallest achievable test error.\nFrom the results in Figure 4.1, 4.2, 4.3 and 4.4, we conclude that all DOWNPOUR-based methods achieve their best performance (test error) for small τ (τ ∈ {1, 4}), and become highly unstable for τ ∈ {16, 64}. While EAMSGD significantly outperforms comparator methods for all values of τ by having faster convergence. It also finds better-quality solution measured by the test error and this advantage becomes more significant for τ ∈ {16, 64}. Note that the tendency to achieve better test performance with larger τ is also characteristic for the EASGD algorithm. We remark that if the stochastic gradient is sparse, DOWNPOUR empirically performs well with large communication period [20].\n47\n49\n50\n51\n52\n53\n54\n55\n56\n57\nWe next explore different number of local workers p from the set p = {4, 8, 16} for the CIFAR experiment, and p = {4, 8} for the ImageNet experiment6. For the ImageNet experiment we report the results of one run with the best setting we have found. EASGD and EAMSGD were run with τ = 10 whereas DOWNPOUR and MDOWNPOUR were run with τ = 1.\nFor the CIFAR experiment, the results are in Figure 4.5, 4.6 and 4.7. EAMSGD achieves significant accelerations compared to other methods, e.g. the relative speedup for p = 16 (the best comparator method is then MSGD) to achieve the test error 21% equals 11.1. It’s noticeable that the smallest achievable test error by either EASGD or EAMSGD decreases with larger p. This can potentially be explained by the fact that larger p allows for more exploration of the parameter space. In the next section, we discuss further the trade-off between exploration and exploitation as a function of the learning rate (section 4.3.2) and the communication period (section 4.3.3).\nFor the ImageNet experiment, the results are in Figure 4.8 and 4.9. The difficulty in this task is that we need to manually reduce the learning rate, otherwise the training loss will stagnate. Thus our initial learning rate is decreased twice over time, by a factor of 5 and then 2, when we observe that the online predictive loss [12] stagnates. EAMSGD again achieves significant accelerations compared to other methods, e.g. the relative speedup for p = 8 (the best comparator method is then DOWNPOUR) to achieve the test error 49% equals 1.8, and simultaneously it reduces the communication overhead (DOWNPOUR uses communication period τ = 1 and EAMSGD uses τ = 10). However, there’s an annealing effect here in the sense that depending on the time the learning rate is reduced, the final test performance can be quite different. This makes the performance comparison difficult to define. In general, this is also a difficulty in comparing the NPhard problem solvers.\n6For the ImageNet experiment, the training loss is measured on a subset of the training data of size 50,000.\n58"
    }, {
      "heading" : "4.3 Further discussion and understanding",
      "text" : "4.3.1 Comparison of SGD, ASGD, MVASGD and MSGD\nFor comparison we also report the performance of MSGD which outperformed SGD, ASGD and MVASGD on the test dataset. Recall that the way we compare the performance between different methods is based on the smallest achievable test error. Since the test dataset is fixed a prior, we may have the tendency to overfit this test dataset. Indeed, as we shall see in the Figure 4.10. One could use cross-validation to remedy this, we however emphasize that the point here is not to seek the best possible test accuracy, but to see all the possibilities that we can find, i.e. the richness of the dynamics arising from the neural network. We are aware that we could not exhaust all the possibilities.\nFigure 4.10 shows the convergence of the training and test loss (negative log-likelihood) and the test error computed for the center variable as a function of wallclock time for SGD, ASGD, MVASGD and MSGD (p = 1) on the CIFAR experiment. We observe that the final test performance of ASGD and MSGD are quite close to each other. But ASGD is much faster from the beginning. This explains why we do not see much speedup of the EASGD method (e.g. in Figure 4.1, 4.2, 4.3) and can sometimes be even slower (e.g. in Figure 4.4). It is caused by the sensitivity of the test performance to the choice of the learning rate. We shall discuss this phenomenon further in the next section 4.3.2.\nFigure 4.11 shows the convergence of the training and test loss (negative log-likelihood) and the test error computed for the center variable as a function of wallclock time for SGD, ASGD, MVASGD and MSGD (p = 1) on the ImageNet experiment. Note that for all CIFAR experiments we always start the averaging for the ADOWNPOUR and ASGD methods from the very beginning of each experiment. But for the ImageNet experiments we start the averaging for the ASGD and MVASGD at the first time when we reduce the learning rate. We have tried to start the averaging from the right beginning, both of the training and test performance are poor and they look very similar to the\n59\n60\n61\nASGD curve in Figure 4.11. The big difference between ASGD and MVASGD is quite striking and worth further study."
    }, {
      "heading" : "4.3.2 Dependence of the learning rate",
      "text" : "This section discusses the dependence of the trade-off between exploration and exploitation on the learning rate. We compare the performance of respectively EAMSGD and EASGD for different learning rates η when p = 16 and τ = 10 on the CIFAR experiment. We observe in Figure 4.12 that higher learning rates η lead to better test performance for the EAMSGD algorithm which potentially can be justified by the fact that they sustain higher fluctuations of the local workers. We conjecture that higher fluctuations lead to more exploration and simultaneously they also impose higher regularization. This picture however seems to be opposite for the EASGD algorithm for which larger learning rates hurt the performance of the method and lead to overfitting. Interestingly in this experiment for both EASGD and EAMSGD algorithm, the learning rate for which the best training performance was achieved simultaneously led to the worst test performance."
    }, {
      "heading" : "4.3.3 Dependence of the communication period",
      "text" : "This section discusses the dependence of the trade-off between exploration and exploitation on the communication period. We observe in Figure 4.13 that EASGD algorithm exhibits very similar convergence behavior when τ = 1 up to even τ = 1000 for the CIFAR experiment, whereas EAMSGD can get trapped at a quite high energy level (of the objective) when τ = 100. This trapping behavior is due to the non-convexity of the objective function. It can be avoided by gradually decreasing the learning rate, i.e. increasing the penalty term ρ (recall α = ηρ), as shown in Figure 4.13. In contrast, the EASGD algorithm does not seem to get trapped by any saddle point at all along its trajectory.\n62\n63\nThe performance7 of EASGD being less sensitive to the communication period compared to EAMSGD is another striking observation.\nIt’s also very important to notice the tail behavior of the asynchronous EASGD method, i.e. what would happen if some local worker had finished the gradient updates and stopped the communication with the master. In the EAMSGD case in Figure 4.13, we see that the final training loss and test error can both become worse. This is due to the situation that some of the local workers have stopped earlier than the others, so that the averaging effect on the center variable is diminished."
    }, {
      "heading" : "4.3.4 The tradeoff between data and parameter communication",
      "text" : "In addition, we report in Table 4.4 the breakdown of the total running time for EASGD when τ = 10 (the time breakdown for EAMSGD is almost identical) and DOWNPOUR when τ = 1 into computation time, data loading time and parameter communication time. For the CIFAR experiment the reported time corresponds to processing 400 × 128 data samples whereas for the ImageNet experiment it corresponds to processing 1024× 128 data samples. For τ = 1 and p ∈ {8, 16} we observe that the communication time accounts for significant portion of the total running time whereas for τ = 10 the communication time becomes negligible compared to the total running time (recall that based on previous results EASGD and EAMSGD achieve best performance with larger τ which is ideal in the setting when communication is time-consuming).\nWe shall now examine the data communication cost in detail. Let’s focus on the ImageNet case. Based on the Table 4.4, for each single GPU (p = 1, τ = 1), it takes around 1248 seconds to process 1024 mini-batches of size 128. This is approximately processing one mini-batch per second. Each mini-batch consists of 128 × 3 × 221 × 221 pixels. If each 7Compared to all earlier results, the experiment in this section is re-run three times with a new random seed and with faster cuDNN package on two Tesla K80 nodes (developer.nvidia.com/cuDNN and github.com/soumith/cudnn.torch). Also to clarify, the random initialization we use is by default in Torch’s implementation. All our methods are implemented in Torch (torch.ch). The Message Passing Interface implementation MVAPICH2 (mvapich.cse.ohio-state.edu) is used for the GPU-CPU communication.\n64\n65\npixel value is represented by one byte, then each mini-batch is around 18 MB. As the whole dataset has around 1,300,000 images, it would take 174 GB. In fact, we have compressed the images as JPEG, so that the whole dataset is only around 36 GB. Thus we gain a compression ratio around 1/5, i.e. we can assume each mini-batch size is 18/5 = 3.6 MB. The required data communication rate is thus 3.6 MB/sec. On the other hand, the DOWNPOUR method with τ = 1 requires communicating the whole model parameter per mini-batch. As the parameter size is around 233 MB, we need the network bandwidth at least 233 MB/sec per local worker (we have not even accounted for the gradient communication, which can double this cost). The parameter communication cost is thus at least 66 times of the data communication cost. In the ImageNet case, having access to the full dataset by the local workers is indeed a good tradeoff."
    }, {
      "heading" : "4.3.5 Time speed-up",
      "text" : "In Figure 4.14 and 4.15, we summarize the wall clock time needed to achieve the same level of the test error for all the methods in the CIFAR and ImageNet experiment as a function of the number of local workers p. For the CIFAR (Figure 4.14) we examined the following levels: {21%, 20%, 19%, 18%} and for the ImageNet (Figure 4.15) we examined: {49%, 47%, 45%, 43%}. If some method does not appear on the figure for a given test error level, it indicates that this method never achieved this level. For the CIFAR experiment we observe that from among EASGD, DOWNPOUR and MDOWNPOUR methods, the EASGD method needs less time to achieve a particular level of test error.\n66\nWe observe that with higher p each of these methods does not necessarily need less time to achieve the same level of test error. This seems counter intuitive though recall that the learning rate for the methods is selected based on the smallest achievable test error. For larger p smaller learning rates were selected than for smaller p which explains our results. Meanwhile, the EAMSGD method achieves significant speed-up over other methods for all the test error levels. For the ImageNet experiment we observe that all methods outperform MSGD and furthermore with p = 4 or p = 8 each of these methods requires less time to achieve the same level of test error."
    }, {
      "heading" : "4.4 Additional pseudo-codes of the algorithms",
      "text" : ""
    }, {
      "heading" : "DOWNPOUR pseudo-code",
      "text" : "Algorithm 3 captures the pseudo-code of the implementation of DOWNPOUR used in this paper. Similar to the asynchronous behavior of EASGD that we have described in Chapter 2 (Section 2.2), DOWNPOUR method also performs several steps of local gradient updates by each worker i before pushing back the accumulated gradients vi to the center variable. To be more precise, at the beginning of each period, the i-th worker reads a new center variable x̃ from the parameter server. Then it performs τ local SGD steps from the new x̃. All the gradients are accumulated (added) and at the end of that period, the total sum vi is pushed (added) back to the parameter server. The center variable is updated by summing the accumulated gradients from any of the local workers. Notice that we do not use any adaptive learning scheme as having been done in [16]."
    }, {
      "heading" : "MDOWNPOUR pseudo-code",
      "text" : "Algorithms 4 and 5 capture the pseudo-codes of the implementation of momentum DOWNPOUR (MDOWNPOUR) used in this paper. Algorithm 4 describes the behavior of each local worker and Algorithm 5 describes the behavior of the master. Note that\n67\n.\n68\n.\n69\nAlgorithm 3: DOWNPOUR: Processing by worker i and the master\nInput: learning rate η, communication period τ ∈ N Initialize: x̃ is initialized randomly, xi = x̃, vi = 0, ti = 0 Repeat if (τ divides ti) then\nx̃ ← x̃ + vi xi ← x̃ vi ← 0\nend xi ← xi − ηgi\nti (xi)\nvi ← vi − ηgi ti (xi) ti ← ti + 1\nUntil forever\nunlike the DOWNPOUR method, we do not use the communication period τ . This is because the Nesterov’s momentum is applied to the center variable. Each worker reads an interpolated variable x̃ + δv from the master, and then sends back the stochastic gradient evaluated at that point. In case p = 1, MDOWNPOUR is equivalent to the MSGD method.\nAlgorithm 4: MDOWNPOUR: Processing by worker i\nInitialize: xi = x̃ Repeat Receive x̃+ δv from the master: xi ← x̃+ δv Compute gradient gi = gi(xi) Send gi to the master Until forever\nAlgorithm 5: MDOWNPOUR: Processing by the master\nInput: learning rate η, momentum term δ Initialize: x̃ is initialized randomly, vi = 0, Repeat Receive gi\nv ← δv − ηgi x̃← x̃+ v Send x̃+ δv\nUntil forever\n70\nChapter 5"
    }, {
      "heading" : "The Limit in Speedup",
      "text" : "This chapter studies the limitation in speedup of several stochastic optimization methods: the mini-batch SGD, the momentum SGD and the EASGD method. In Section 5.1, we study first the asymptotic phase of these methods using an additive noise model. The continuous-time SDE (stochastic differential equation) approximation of its SGD update is an Ornstein Uhlenbeck process. Then we study the initial phase of these methods using a multiplicative noise model in Section 5.2. The continuous-time SDE approximation of its SGD update is a Geometric Brownian motion. In Section 5.3, we study the stability of the critical points of a simple non-convex problem and discuss when the EASGD method can get trapped by a saddle point."
    }, {
      "heading" : "5.1 Additive noise",
      "text" : "We (re-)study the simple additive noise model: one-dimensional quadratic objective with Gaussian noise (as in Section 3.1.1). The objective function evaluated at state x ∈ R is defined to be the average loss of the quadratic form (hx− ξ)2, i.e.\nmin x∈R\nE[(hx− ξ)2]. (5.1)\n71\nHere h > 0 is a scalar, and the expectation is taken over the random variable ξ, which follows a Gaussian distribution. For simplicity, we assume further that ξ is zero mean and has a constant variance σ2 > 0.\n5.1.1 SGD with mini-batch\nThe update rule for the SGD method for solving the problem in Equation 5.1 is\nxt+1 = xt − η(hxt − ξt), (5.2)\nwhere x0 is the initial starting point.\nNotice that in the continuous-time limit, i.e. for small η, we can approximate the process in Equation 5.2 by an Ornstein Uhlenbeck process [32] as follows,\ndX(t) = −hX(t)dt+√ησdB(t).\nIn the discrete-time case, the bias term (the first-order moment Ext) in Equation 5.2 will decrease at a linear rate 1− ηh, i.e.\nExt+1 = (1− ηh)Ext.\nThe second-order moment Ex2t changes as\nEx2t+1 = (1− ηh)2Ex2t + η2σ2. (5.3)\nThus the variance will increase as\nVxt+1 = Ex2t+1 − (Ext+1)2 = (1− ηh)2Ex2t + η2σ2 − (1− ηh)2(Ext)2\n= (1− ηh)2Vxt + η2σ2,\n72\nwith Vx0 = 0. As t → ∞, we get Vx∞ = η 2\n1−(1−ηh)2σ 2. If we use mini-batch of size p,\nthe variance of the noise ξt is then reduced by p, and this asymptotic variance becomes\nη2 1−(1−ηh)2 σ2 p . The convergence rate of the bias term, 1− ηh, is however not improved by increasing the mini-batch size p.\n5.1.2 Momentum SGD\nWe now study the update rule for the MSGD (Nesterov’s momentum) method for solving Equation 5.1,\nvt+1 = δvt − η(h(xt + δvt)− ξt), xt+1 = xt + vt+1, (5.4)\nwhere x0 is the initial starting point and v0 = 0 is the initial velocity set to zero.\nLet δh = δ(1− ηh), ηh = ηh, then Equation 5.4 is equivalent to\nvt+1 = δhvt − ηhxt + ηξt, xt+1 = xt + vt+1.\nIn case that δh is chosen independently of h, the update is no different to the heavy ball method [44]. But the momentum term δh above equals δ(1− ηh), thus it implicitly depends on h. As we usually choose δ to be smaller than one, δh is upper bounded by 1− ηh. This fact saves us from the variance explosion as δ tends to one.\nMore precisely, the second-order moment equation can be computed as follows,\nv2t+1 = (δhvt − ηhxt + ηξt)2\n= δ2hv 2 t + η 2 hx 2 t + η 2ξ2t − 2δhηhvtxt + 2δhηvtξt − 2ηhηxtξt x2t+1 = (xt + vt+1) 2 = x2t + 2vt+1xt + v 2 t+1\nvt+1xt = (δhvt − ηhxt + ηξt)xt = δhvtxt − ηhx2t + ηxtξt vt+1xt+1 = vt+1(xt + vt+1) = vt+1xt + v 2 t+1.\n(5.5)\n73\nNow taking expectation on both sides of the Equation 5.5, we obtain the following recursive relation Ev2t+1 Evt+1xt+1\nEx2t+1\n =  δ2h −2δhηh η2h δ2h δh(1− 2ηh) −ηh(1− ηh)\nδ2h 2δh(1− ηh) (1− ηh)2  ︸ ︷︷ ︸\nM\n Ev2t Evtxt\nEx2t\n+  η2σ2 η2σ2\nη2σ2\n . (5.6)\nTo see the asymptotic behavior, assume v2∞ = limt→∞ Ev2t , vx∞ = limt→∞ Evtxt, and x2∞ = limt→∞ Ex2t . Then by solving v2∞ vx∞\nx2∞\n =  δ2h −2δhηh η2h δ2h δh(1− 2ηh) −ηh(1− ηh)\nδ2h 2δh(1− ηh) (1− ηh)2\n  v2∞ vx∞\nx2∞\n+  η2σ2 η2σ2\nη2σ2\n ,\nwe obtain\nv2∞ = 2\n(1− δh)(2(1 + δh)− ηh) η2σ2,\nvx∞ = 1\n(1− δh)(2(1 + δh)− ηh) η2σ2,\nx2∞ = 1 + δh\nηh(1− δh)(2(1 + δh)− ηh) η2σ2. (5.7)\nFrom Equation 5.7, we see that for the asymptotic variance x2∞ to be strictly positive, we should assume −1 < δh < 1 and 0 < ηh < 2(1 + δh). Moreover, compared to the asymptotic variance of SGD (the case that δ = 0), we can check, for example, that in the region ηh ∈ (0, 1) and δh ∈ (0, 1), the asymptotic variance of MSGD is always larger.\nOn the other hand, the above condition −1 < δh < 1 and 0 < ηh < 2(1 + δh) is also the condition for the matrix M in Equation 5.6 to remain (strictly) stable, i.e. the largest absolute eigenvalue is (strictly) smaller than one. In fact, we have three eigenvalues for\n74\nthe matrix M as follows,\nz1 = δh, z2 = (1−ηh)2−2ηhδh+δ2h− √ ((1−ηh)2−2ηhδh+δ2h)2−4δ 2 h\n2 ,\nz3 = (1−ηh)2−2ηhδh+δ2h+\n√ ((1−ηh)2−2ηhδh+δ2h)2−4δ 2 h\n2 .\n(5.8)\nLet 2b = z2 + z3 = (1− ηh)2− 2ηhδh + δ2h and c = z2z3 = δ2h. Then z2 and z3 are the two roots (in z) of z2 − 2bz + c = 0. In general, if z2, z3 are real-valued, i.e. b2 − c > 0, then we have two cases:\n• b > √c implies z3 = b+ √ b2 − c > √c > z2 = b− √ b2 − c > 0, • b < −√c implies z2 = b− √ b2 − c < −√c < z3 = b+ √ b2 − c < 0.\nOn the other hand, if z2, z3 are not real-valued, i.e. b 2 − c ≤ 0, then |z3| = |z1| = |z2| = √ c. Recall that |z1| = |δh| = √ c. We see thus for the (strict) stability of the matrix M in Equation 5.6, we need √ c = |δh| < 1 and |z3| < 1 because the condition b < − √ c is never satisfied for any real δh. The condition for |z3| = b+ √ b2 − c < 1 is that 2b− c < 1, i.e. 0 < ηh < 2(1 + δh).\nMoreover, as in [32], we can try to minimize |z3| with respect to δ such that the rate of convergence of the second order moment is maximized for a given η. We shall prove that the minimal |z3| is achieved at δh = ( √ ηh − 1)2. In fact, it happens when b = √ c, i.e. the optimal rate is at the edge where the eigenvalues transit from real-valued to the complex-valued. Notice that b = √ c gives us two positive solutions: δh = ( √ ηh − 1)2 and δh = ( √ ηh + 1) 2. Since b = 12 [(δh − ηh)2 + 1− 2ηh] is a quadratic function in δh, the fact that b = √ c has two positive solutions means that the quadratic function intersects twice with the line δh in the first orthant. If ηh ≥ 1/4, the first intersection point is to the left of the minimum of b, and the second intersection point is to the right of the minimum, i.e. ( √ ηh − 1)2 ≤ ηh < ( √ ηh + 1) 2. But if 0 < ηh < 1/4, both intersection points will be both to the right of the minimum of b, i.e. ηh < ( √ ηh− 1)2 < ( √ ηh + 1) 2.\n75\nNevertheless, in either case, we can show b > √ c whenever δh < ( √ ηh − 1)2. Thus, in the range δh < ( √ ηh − 1)2, we have z3 > √ c > z2 > 0. We thus only need to find the minimum of z3 in the range δh ∈ (−1, ( √ ηh − 1)2], because |z3| = δh is monotonically increasing in the range 1 > δh > ( √ ηh − 1)2. We now show that the minimal value of z3 is ( √ ηh − 1)2 in this range. In fact, if ηh ≥ 1/4, b is monotonically decreasing in this range, thus z3 ≥ b reaches its minimum at δh = ( √ ηh − 1)2 with z3 = b. If 0 < ηh < 1/4, b firstly decreases for δh < ηh, then increases for ηh ≤ δh ≤ (1 − √ ηh) 2. We show that z3 is still monotonically decreasing in these two ranges of δh. We check whether ∂z3∂δh = (δh − ηh)(1 + b√ b2−c)− δh 1√ b2−c < 0 holds. For δh < ηh, this is equivalent to z3 > δh δh−ηh ; for ηh < δh ≤ (1− √ ηh) 2, this is equivalent to z3 < δh δh−ηh . The former one is true in the range 0 < δh < ηh, because z3 is always positive. One can check that this is still true in the range −1 < δh < 0. The latter one is equivalent to b+ √ b2 − c < δhδh−ηh . Taking square on both sides, we can check that b < δhδh−ηh and b 2 − c < ( δhδh−ηh − b) 2, based on 0 < δh − ηh < 1− 2 √ ηh < 1.\nThus we conclude that for a fixed ηh such that 0 < ηh < 2(1 + δh), the minimal |z3| over −1 < δh < 1 is obtained at\nδh = ( √ ηh − 1)2,\nwith the minimal value z3 = b + √ b2 − c = b = √c = δh = ( √ ηh − 1)2. Compared to the rate (1− ηh)2 in the SGD case (Equation 5.3), MSGD can indeed help if ηh is small enough (usually ηh = ηh is close to the inverse of the condition number of the Hessian in higher dimensional case). To check our above reasoning, we have computed numerically the spectral norm of the matrix M , sp(M), which is given in Figure 5.1. Note that when ηh > 1, we have the optimal momentum rate being negative, i.e. δ = ( √ ηh−1)2\n(1−ηh) < 0.\nPerhaps the most special behavior for MSGD is that when the momentum rate δ gets close to 1, the asymptotic variance of x2∞ in Equation 5.7 still remains bounded, i.e. x2∞ = 1+1−ηh\nη2h(2(1+1−ηh)−ηh) η2σ2 = 2−ηh4−3ηh σ2 h2 for δ = 1 (or δh = 1 − ηh). This contrasts to\nthe behavior of the heavy ball method, whose asymptotic variance tends to infinity as\n76\n77\nδ → 1 [32].\n5.1.3 EASGD and EAMSGD\nWe can now study the moment equation for the EASGD and EAMSGD method. For EASGD, we have the following update rules\nxit+1 = x i t − η(hxit − ξit) + α(x̃t − xit), x̃t+1 = x̃t + β( 1 p ∑p i=1 x i t − x̃t).\n(5.9)\nDenote yt = 1 p ∑p i=1 x i t and ξt = 1 p ∑p i=1 ξ i t, then Equation 5.9 can be reduced to\nyt+1 = yt − η(hyt − ξt) + α(x̃t − yt), x̃t+1 = x̃t + β(yt − x̃t). (5.10)\nSimilar to MSGD, the second-order moment equation for Equation 5.10 is as follows,\ny2t+1 = ((1− ηh− α)yt + αx̃t + ηξt)2\n= (1− ηh− α)2y2t + α2x̃2t + η2ξ2t + 2α(1− ηh− α)ytx̃t + 2η(1− ηh− α)ytξt + 2αηx̃tξt,\nyt+1x̃t+1 = ((1− ηh− α)yt + αx̃t + ηξt)((1− β)x̃t + βyt)\n= (1− ηh− α)βy2t + (1− ηh− α)(1− β)ytx̃t + αβytx̃t + α(1− β)x̃2t + ηξt((1− β)x̃t + βyt),\nx̃2t+1 = (1− β)2x̃2t + β2y2t + 2β(1− β)ytx̃t.\n(5.11)\n78\nTaking expectation on both sides of the Equation 5.11, we get  Ey2t+1 Eyt+1x̃t+1\nEx̃2t+1\n =  (1− ηh− α)2 2α(1− ηh− α) α2 (1− ηh− α)β (1− ηh− α)(1− β) + αβ α(1− β)\nβ2 2β(1− β) (1− β)2  ︸ ︷︷ ︸\nM\n Ey2t Eytx̃t\nEx̃2t\n+  η2 σ 2 p 0\n0\n .\n(5.12)\nSimilar to the analysis of MSGD, we get the following asymptotic variance,\ny2∞ = limt→∞ Ey2t = (2− β)(1− β)ηh + β(2− α− β) ηh[(2− β)(2− ηh)− 2α][α+ β + ηh(1− β)] η2σ2 p , (5.13)\nyx̃∞ = limt→∞ Eytx̃t = β((2− β)(1− ηh)− α) ηh[(2− β)(2− ηh)− 2α][α+ β + ηh(1− β)] η2σ2 p ,\nx̃2∞ = lim t→∞\nEx̃2t = −β(1− β)ηh + β(2− α− β) ηh[(2− β)(2− ηh)− 2α][α+ β + ηh(1− β)] η2σ2 p . (5.14)\nFor the above asymptotic variance to be positive, in particular x2∞, we can assume\nηh > 0,\nβ > 0,\n(2− β)(2− ηh)− 2α > 0, 2−α−β−ηh+βηh α+β+ηh(1−β) > 0.\n(5.15)\nMoreover, if 0 < β < 1, the asymptotic variance of the center variable x̃2∞ is strictly smaller than that of the spatial average y2∞ (by comparing Equation 5.13 and 5.14). Interestingly, if β > 1, it becomes strictly bigger.\nNote that it’s still not very clear whether Equation 5.15 is the necessary and sufficient condition for the matrix M (in Equation 5.12) to be (strictly) stable. However, if we look back to the earlier result in Section 3.1.1 about the condition in Equation 3.4, they are nearly the same, except maybe for the last formula. The last formula in Equation 5.15 reads 0 < α+ β + ηh − βηh < 2. The left inequality implies βηh < α+ β + ηh. Based on\n79\nthe third formula in Equation 5.15, we have thus α + β + ηh < 2 + βηh 2 < 2 + α+β+ηh 2 . This gives us the last formula for the condition in Equation 3.4, which is α+β+ ηh < 4. Conversely, assuming α > 0 (as assumed in Section 3.1.1 for β = pα), then one can check that α + β + ηh < 4 implies the last formula in Equation 5.15, which also reads α− 1 < (ηh − 1)(β − 1) < α+ 1.\nPerhaps the most striking observation is that the optimal α such that the convergence rate (of the moment Equation 5.12) is maximized turns out to be negative, given ηh > 0 and β > 0 fixed.\nThe three eigenvalues of the matrix M in Equation 5.12 are\nz1= −α+ (1− ηh)(1− β), z2= b− √ b2 − c, z3= b+ √ b2 − c,\n(5.16)\nwhere b = 12 [(α − (1 − ηh − β))2 + 1 − 2βηh], c = [α − (1 − ηh)(1 − β)]2 = z21 . Denote α′ = z1 = −α+ (1− ηh)(1− β), then b = 12 [(α′− ηhβ)2 + 1− 2βηh] and c = (α′)2. Using exactly the same analysis and the result from the MSGD case, we get that the minimal |z3| over −1 < α′ < 1 is obtained at\nα′ = ( √ βηh − 1)2,\nwhich is equivalent to\nα = −( √ β −√ηh)2. (5.17)\nThe situation that the coupling constant α being negative, while β being positive suggests a very different perspective to understand EASGD. It seems that our earlier condition β = pα is unnecessary and is sub-optimal in this case. To check our above results, we have computed numerically the spectral norm of the matrix M for a fixed β = 0.9, which is given in Figure 5.2. However, we are in danger this time if we were to simulate\n80\nEASGD using the optimal α given in Equation 5.17. In Figure 5.3, we illustrate an unstable behavior in such optimal case. The reason is that our above analysis is based on the reduced Equation 5.10, rather than the original Equation 5.9.\nIn the original Equation 5.9, we have the following form of the drift matrix\nMp =  1− α− ηh 0 ... 0 α 0 1− α− ηh 0 ... α ... 0 ... 0 ... 0 ... 0 1− α− ηh α\nβ′ β′ ... β′ 1− β\n , (5.18)\nwhose first p rows correspond to the local workers’ updates, and the last row correspond to the master’s update. The p + 1 eigenvalues Mp can be computed recursively as follows: let Ip+1(z) = det(Mp − z), then we have Ip+1(z) = (1 − α − ηh − z)Ip(z) − αβ′(1− α− ηh − z)p−1 = · · · = (1− α− ηh − z)p−1[(1− β − z)(1− α− ηh − z)− pαβ′]. Notice that β′ = β/p, thus we have two eigenvalues which do not depend on p, i.e. (1− β − z)(1− α− ηh− z)− αβ = 0, and an extra eigenvalue which only shows up for p > 1, i.e. z = 1 − α − ηh. This extra eigenvalue z = 1 − α − ηh is completely ignored in our reduced Equation 5.10. Then what is the optimal α for the matrix Mp instead? The three eigenvalues of the matrix Mp in Equation 5.18 are\nz1= 1− α− ηh, z2= b− √ b2 − c, z3= b+ √ b2 − c,\n(5.19)\nwhere b = 12(2− β − ηh − α), c = (1− ηh)(1− β)− α.\nGiven ηh > 0 and β > 0 fixed, we shall prove that\n• if β > ηh: the optimal α = 0. • if β < ηh: the optimal α = −( √ β −√ηh)2.\n81\n82\n83\nThe proof idea is similar to the MSGD case, so we shall be more brief. Let’s focus on the variable c = (1− ηh)(1− β)− α instead of the variable α itself. Rewrite Equation 5.19 with z1 = c+β(1− ηh), b = 12(c−βηh + 1). We find that for b2 > c, we only need c > c2 or c < c1, where c1 = ( √ βηh − 1)2, and c2 = ( √ βηh + 1) 2. We also observe that the line z1 as a function of c intersects with the quadratic curve z2 or z3 at c0 = (1− ηh)(1− β) (i.e. at α = 0). At c = c0, z1 = 1 − ηh, z2 = 1 − 12(β + ηh) − 12 |β − ηh|, and z3 = 1 − 12(β + ηh) + 12 |β − ηh|. If β > ηh, then z3 = z1 at c = c0. Otherwise z2 = z1 at c = c0. We can check that the negative optimal α = −( √ β −√ηh)2 is given at c = c1, where z2 and z3 meet and transit from real-valued to complex-valued. Note that z1 is an increasing function of c, and if it intersects with z3, the optimal α becomes 0 (rather than being negative). They are illustrated in Figure 5.4 and 5.5 as a function of α under the two conditions, i.e. β > ηh and β < ηh. We also computed numerically the spectral norm of the matrix Mp for a fixed β = 0.9, which is given in Figure 5.6. Finally, we show in Figure 5.7 the optimal case of EASGD under the condition β < ηh.\nWe have studied the the second-order moment equation 5.12 of the reduced system (Equation 5.10), and the first-order moment equation 5.18 of the original system (Equation 5.9) for the EASGD method. We found that the reduced system can lose critical information about the stability of the original system. However, the eigenvalues are still closely related in the first-order moment matrix Mp and the second-order moment M , i.e. the z2 and z3 in Equation 5.16 and 5.19. Thus for the EAMSGD method, we shall only focus on the first-order moment equation. Its asymptotic variance, which can obtained from the second-order moment equation, is rather complicated and will not be discussed.\nFor EAMSGD, we have the following update rules\nvit+1 = δv i t − η(h(xit + δvit)− ξit), xit+1 = x i t + v i t+1 + α(x̃t − xit), x̃t+1 = x̃t + β( 1 p ∑p i=1 x i t − x̃t).\n84\n85\n86\n87\nWe have the following form of the drift matrix for the first-order moment equation,\nMp =  δh −ηh 0 0 ... ... 0 δh 1− ηh − α 0 0 ... ... α 0 0 δh −ηh ... ... 0 0 0 δh 1− ηh − α ... ... α ... ... ... ... ... ... ... ... ... ... ... ... ... ...\n0 β′ 0 β′ ... ... 1− β\n , (5.20)\nsuch that [v1t+1, x 1 t+1, v 2 t+1, x 2 t+1, . . . , x̃t+1] T = Mp[v 1 t , x 1 t , v 2 t , x 2 t , . . . , x̃t] T . Recall that δh = δ(1 − ηh), ηh = ηh, and β′ = β/p. We can compute the eigenvalues of the above drift matrix Mp recursively, and one can check that they are again independent of the choice of p for p > 1, as in the EASGD case. More precisely, we have\ndet|Mp − z| = (u(z))p−1v(z) = 0, (5.21)\nwhere u(z) = z2 − (1− η− α− δ)z + δ(1− α) and v(z) = (δ− z)(1− η− α− z)(1− β − z) + ηδ(1− β − z)− αβ(δ − z).\nThe solution of the Equation 5.21 is quite complicated as it involves a third-order polynomial which is irreducible. It would be difficult to obtain the optimal δ or α as before. So we computed numerically the spectral norm of the matrix Mp in Equation 5.20 for a fixed β = 0.9 and δ = 0.99 (which we used in the experiment in Chapter 4). It is given in Figure 5.8. This result suggests that the optimal α increases as the η (or ηh as h = 1) decreases. Recall our result of MSGD in Figure 5.1: the optimal δ increases as η decreases. This is consistent with the choice of the learning rate and the momentum rate scheduling in the Nesterov’s optimal methods in literature [28]. We have seen that there’s an intimate connection between MSGD and EASGD when we were studying the optimal momentum rate δ and the optimal moving rate α. It suggests that we may also\n88\nfind interesting rate scheduling for EASGD, as well as EAMSGD based on the convex analysis. However, we should be careful as the optimal α for EASGD is either zero or negative, while for EAMSGD it can be positive as well."
    }, {
      "heading" : "5.2 Multiplicative noise",
      "text" : "In this section, we study a multiplicative noise model, which attempts to capture the initial behavior of the stochastic optimization method. It is complementary to the additive noise model, which captures the asymptotic behavior.\nOur starting point is the following linear regression problem\nmin a∈R\nE[(v − au)2],\nwhere the expectation E is taken over the joint distribution of (u, v). If the input data (u, v) satisfies v = a∗u, we may reduce the above problem to the following onedimensional case,\nmin x∈R\nE[(xu)2]. (5.22)\nAn interesting perspective of this problem is that we can assume that u2 of the input data follows a Gamma distribution Γ(λ, ω), with mean λ/ω and variance λ/ω2. Note that if u follows a Gaussian distribution with mean zero and variance σ2, then λ = 1/2 and ω = 1/(2σ2).\n5.2.1 SGD with mini-batch\nGiven x0 ∈ R, the mini-batch SGD method for solving the multiplicative noise problem in Equation 5.22 is as follows,\nxt+1 = xt − ηu2txt, (5.23)\nwhere η > 0, and ut is an i.i.d input process.\n89\n90\nAs we assumed that u2t follow a Gamma distribution Γ(λ, ω), it’s easy to verify that if we use mini-batch of size p, Equation 5.23 becomes\nxt+1 = xt − η 1\np p∑ i=1 (uit) 2xt, (5.24)\nand this mini-batch 1p ∑p i=1(u i t) 2 also follows a Gamma distribution Γ(pλ, pω), since the mean of the mini-batch is not changed and the variance is divided by p.\nWe also notice that in the continuous-time limit, i.e. for small η, we can approximate the process in Equation 5.24 by a Geometric Brownian motion [32] as follows,\ndX(t) = −λ ω X(t)dt+ √ η\n√ λ\npω2 X(t)dB(t).\nAn interesting behavior of the geometric Brownian Motion is that its variance can explode (go to infinity), yet the path of the stochastic process still converges almost surely towards zero. In such case, we can observe along the path a few extreme large values. These extreme values may however cause trouble to the stability of the nonlinear dynamics in the training of deep learning models. So our goal here is to control this variance by studying again the second-order moment equation.\nGoing back to the discrete process defined by Equation 5.24. Denote ξt = 1 p ∑p i=1(u i t) 2, we have\nx2t+1 = (1− ηξt)2x2t . (5.25)\nTaking expectation on both sides of the Equation 5.25, we obtain\nE[x2t+1] = (1− 2η pλ\npω + η2\npλ(pλ+ 1)\n(pω)2 )E[x2t ] = (1− 2η\nλ ω + η2 λ(pλ+ 1) pω2 )E[x2t ]. (5.26)\nThe rate of convergence defined by the above second-moment equation 5.26 is 1−2η λω + η2 λ(pλ+1) pω2 . It is monotone decreasing as p increases and will saturate at a limit 1−2η λω +\n91\nη2 λ 2 ω2 = (1 − η λω )2. Given p, we can optimize this convergence rate with respect to the choice of the learning rate η. This optimal learning rate is given by\nηp = pω\npλ+ 1 =\nω\nλ+ 1/p . (5.27)\nFrom this formula, we can see that as p evolves, ηp will change a lot if λ is much smaller than 1/p. In other words, if λ is very big compared to 1/p, then the change in ηp is not that much as we increases the mini-batch size p. Thus we can expect that the speedup we would gain depends heavily on the distribution of the input data, in particular the shape parameter λ of the Gamma distribution. Suppose for p = 1, the ξt follows the Gamma distribution Γ(λ, ω). Its probability density function is ω λ\nΓ(ω)ξ λ−1e−ωξ.\nWe illustrate this probability density function in Figure 5.9 for the standard Gaussian case (λ = 1/2, ω = 1/2) with p = 1, p = 2 and p = 4. We see that when λ is smaller than one, the probability density function has a pole at zero and has also a slower decay (heavier tail) toward infinity. By increasing the mini-batch size p, the probability density function becomes more and more concentrated near its mean. Thus the mini-batch is more effective for such input distribution with very large spread (i.e. small λ).\n5.2.2 Momentum SGD\nGiven x0 ∈ R and v0 = 0, the MSGD method for solving the multiplicative noise problem in Equation 5.22 is as follows,\nvt+1 = δvt − ηξt(xt + δvt), xt+1 = xt + vt+1, (5.28)\nwhere ξt follows a Gamma distribution Γ(λ, ω).\nDenote δt = δ(1−ηξt) and ηt = ηξt, the second-order moment equation can be computed\n92\n93\nas follows,\nv2t+1 = (δ(1− ηξt)vt − ηξtxt)2 = (δtvt − ηtxt)2\n= δ2t v 2 t − 2ηtδtvtxt + η2t x2t ,\nvt+1xt = (δtvt − ηtxt)xt = δtvtxt − ηtx2t ,\nx2t+1 = (xt + vt+1) 2 = x2t + 2vt+1xt + v 2 t+1\n= x2t + 2(δtvtxt − ηtx2t ) + δ2t v2t − 2ηtδtvtxt + η2t x2t , = δ2t v 2 t + (1− 2ηt + η2t )x2t + (2δt − 2ηtδt)vtxt,\nvt+1xt+1 = vt+1(xt + vt+1) = vt+1xt + v 2 t+1\n= δtvtxt − ηtx2t + δ2t v2t − 2ηtδtvtxt + η2t x2t = δ2t v 2 t + (−ηt + η2t )x2t + δt(1− 2ηt)vtxt.\n(5.29)\nTaking expectation on both sides of the Equation 5.29, we obtain the following recursive relation (Ev2t+1,Ex2t+1,Evt+1xt+1)T = M(Ev2t ,Ex2t ,Evtxt)T , where\nM =  δ2(1− 2ηu1 + η2u2) η2u2 −2δη(u1 − ηu2) δ2(1− 2ηu1 + η2u2) 1− 2ηu1 + η2u2 2δ(1− ηu1)− 2δη(u1 − ηu2)\nδ2(1− 2ηu1 + η2u2) −ηu1 + η2u2 δ(1− ηu1)− 2δη(u1 − ηu2)\n , (5.30)\nu1 = λ ω and u2 = λ(λ+1) ω2 .\nNow we can compare the numerical spectral norm of the matrix M for various choices of the input data distribution parameterized by λ and ω. In Figure 5.10, we illustrate the standard Gaussian case, i.e. uit follows i.i.d. N (0, 1) in Equation 5.24. In Figure 5.11 and 5.12 we illustrate the resulting effect if we use mini-batch of size p = 2 and p = 4. Recall that for δ = 0, we have the optimal learning rate in Equation 5.27. It achieves nearly the optimal convergence rate (smallest sp(M)) in these Figures. We also observe that the optimal momentum rate is actually 0 at these optimal learning rates. One can check such observation in Figure 5.13. Thus contrary to our earlier MSGD results in the additive noise case (Figure 5.1), the momentum can slow down the optimal convergence\n94\nrate. However, we can yet ask, given η and δ fixed, what is the region of input data distribution λ and ω such that the momentum helps. We find that the answer is for relatively small slope λω (c.f. Figure 5.14). This phenomenon of acceleration is consistent with our earlier analysis of MSGD in the additive noise case (c.f. Figure 5.1).\n5.2.3 EASGD and EAMSGD\nWe now focus on the EASGD updates for solving the problem in Equation 5.22,\nxit+1 = x i t − ηξitxit + α(x̃t − xit), x̃t+1 = x̃t − βp ∑p i=1(x̃t − xit), (5.31)\nwhere ξit follows a Gamma distribution Γ(λ, ω).\nThe second-order moment equation can be computed as follows,\n(xit+1) 2 = (1− α− ηξit)2(xit)2 + 2α(1− α− ηξit)x̃txit + α2(x̃t)2, (x̃t+1) 2 = (1− β)2(x̃t)2 + 2β(1− β)(1p ∑p i=1 x̃tx i t) + β2 p2 ∑p i=1,j=1 x i tx j t ,\nx̃t+1x i t+1 = (1− β)(1− α− ηξit)x̃txit + α(1− β)(x̃t)2 + (1− α− ηξit)βp ∑p j=1 x i tx j t , xit+1x j t+1 = (1− α− ηξit)(1− α− ηξjt )xitxjt + α2(x̃t)2\n+ α(1− α− ηξit)x̃txit + α(1− α− ηξjt )x̃txjt , i 6= j.\n(5.32)\n95\n96\n97\n98\n99\n100\nTaking expectation on both sides of the Equation 5.32, we get\nE(xit+1)2 = [(1− α− η λω )2 + η2 λω2 ]E(xit)2 + 2α(1− α− η λω )E(x̃txit) + α2E(x̃t)2, E(x̃t+1)2 = (1− β)2E(x̃t)2 + 2β(1− β)1p ∑p i=1 E(x̃txit) + β2 p2 ∑p i=1,j=1 E(xitx j t ),\nE(x̃t+1xit+1) = (1− β)(1− α− η λω )E(x̃txit) + α(1− β)E(x̃t)2\n+ (1− α− η λω ) β p ∑p j=1 E(xitx j t ) + αβ( 1 p ∑p j=1 E(x̃tx j t )),\nE(xit+1x j t+1) = (1− α− η λω )(1− α− η λω )E(xitx j t ) + α 2E(x̃t)2\n+ α(1− α− η λω )E(x̃txit) + α(1− α− η λω )E(x̃tx j t ), i 6= j.\n(5.33)\nWe can reduce the above system (Equation 5.33) into a closed form by introducing\nat = E[(x̃t)2], bt = 1\np p∑ i=1 E[(xit)2], ct = 1 p p∑ i=1 E[x̃txit], dt = 1 p2 p∑ i=1 p∑ j=1 E[xitx j t ].\nWe get (at+1, bt+1, ct+1, dt+1) T = M(at, bt, ct, dt) T , where\nM =  (1− β)2 0 2β(1− β) β2 α2 (1− α− η λω )2 + η2 λω2 2α(1− α− η λω ) 0 α(1− β) 0 (1− β)(1− α− η λω ) + αβ (1− α− η λω )β\nα2 η2 λ pω2\n2α(1− α− η λω ) (1− α− η λω )2\n .\n(5.34)\nWe will analyze the spectral norm of the matrix M in the limit p goes to infinity. We focus on the question that whether the stability region (i.e. the spectral norm be smaller than one) with respect to the learning rate η will be enlarged for large p.\nCase I: α = β/p\nThe characteristic polynomial of M in Equation 5.34 can be written as\nP (z) = [z−(1−β)2][z−(1−β)(1−η λ ω )][z−(1−2η λ ω +η2 λ(λ+ 1) ω2 )][z−(1−η λ ω )2]+Oz( 1 p ),\n101\nwhere Oz( 1 p) is the remaining polynomial in z of order 1 p and higher. The four leading order eigenvalues of P (z) are z1 = (1−β)2, z2 = (1−β)(1−η λω ), z3 = (1−2η λω+η2 λ(λ+1) ω2 ), z4 = (1− η λω )2. We need necessarily β ∈ (0, 2), and in such case the stability condition for η is the same as the SGD case (c.f. Equation 5.26), i.e. |1 − 2η λω + η2 λ(λ+1) ω2 | < 1. Thus the stability region of EASGD remains the same as SGD in the limit p goes to infinity, i.e.\n0 < η < 2ω\nλ+ 1 .\nIn the Figure 5.15, 5.16, 5.17 and 5.18, we illustrate the stability region for different λ, ω and p. Compared with the MSGD method in the Figure 5.13, EASGD improves its optimal convergence rate:\n• λ = 0.5, ω = 0.5: sp(M) = 0.5742 at p = 6 and η = 0.3814 for EASGD vs.\nsp(M) = 0.6667 for MSGD with η = λω+1 = 1 3 and δ = 0.\n• λ = 1, ω = 1: sp(M) = 0.4317 at p = 7 and η = 0.5225 vs. sp(M) = 0.5 for MSGD\nwith η = λω+1 = 1 2 and δ = 0.\n• λ = 2, ω = 2: sp(M) = 0.2945 at p = 9 and η = 0.6647 vs. sp(M) = 0.3333 for\nMSGD with η = λω+1 = 2 3 and δ = 0.\nNote that EASGD ’s optimal convergence rate is achieved for some finite p. This is in contrast to the mini-batch SGD method (c.f. Equation 5.27).\nCase II: α is independent of β and p\nThe characteristic polynomial of M of Equation 5.34 can be written as\nP (z) = (z − z1)(z − z2)(z − z3)(z − z4) +Oz( 1\np ),\nwhere z1 = (1 − β)(1 − η λω ) − α, z2 = (1 − α)2 − 2(1 − α)η λω + η2 λ(λ+1) ω2 , z3 + z4 = 2 + α2 − (2− β)β − (2− η λω )η λω − 2α(1− β − η λω ), z3z4 = z21 .\n102\n103\n104\n105\n106\nThese four eigenvalues depend on λ, ω, β, α, p and η. It’s not as clear here on how to choose an optimal α. We can however still find the stability region with respect of η in the limit p→∞. We ask given λ, ω, β and α, what is the range of η such that the four eigenvalues lie on the unit circle of the complex plane. We also observe that the optimal α can be positive for large p, in contrast to the additive noise case (c.f. Figure 5.19 and 5.6). Thus to simplify our analysis, we only consider η > 0, α ∈ (0, 1) and β ∈ (0, 1). It turns out that in this case,\n• |z1| < 1: 0 < η < 2−α−β1−β ωλ . • |z2| < 1: 0 < η < ω(1−α)λ+1 + ω 2 λ(λ+1) √ λ2 ω2 + λ ω2 (2α− α2). • |z3| < 1 and |z4| < 1: 0 < η < 4−2α−2β2−β ωλ .\nWe can also check that 2−α−β1−β ω λ > 4−2α−2β 2−β ω λ , thus we only need to maximize the min0<α<1{4−2α−2β2−β ωλ , ω(1−α) λ+1 + ω λ(λ+1) √ λ2 + λ(2α− α2)}. We now prove that the maximum is achieved at α = 1 − √ λ. In fact, this is the maximum of the second term, and at that α, the first term equals 4−2α−2β2−β ω λ = 2+2 √ λ−2β 2−β ω λ , and the second term equals\n√ λ λ+1(ω + ω λ ) = ω√ λ . The first term being larger than the second term, thus we conclude that the maximal value ω√ λ is indeed achieved at α = 1− √ λ. Based on the Equation 5.27, the stability region for SGD is 0 < η < 2ωλ+1 ; for minibatch SGD with p → ∞, it is 0 < η < 2ωλ . For EASGD, it is 0 < η < ω√λ , with p → ∞ and α = 1 − √ λ. In case λ = 0.5, ω = 0.5, we can check from Figure 5.19 that α = 1− √ 0.5 = 0.2929 achieves the widest range of η ∈ (0, √ 0.5) for M to be stable.\nFor the EAMSGD method, it’s much more difficult to analyze the second-order moment equation. It involves a linear system of nine variables: at = E(x̃2t ), bt = 1p ∑\ni E(xit)2, ct = 1 p ∑ i E(x̃txit), dt = 1 p2 ∑ i,j E(xitx j t ), et = 1 pE(x̃tv i t), ft = 1 p2 ∑ i,j E(vitv j t ), gt = 1 pE(v i t) 2, ht = 1 p ∑ i E(xitvit), kt = 1 p2 ∑ i,j E(xitv j t ). The linear matrix is quite complicated, so we will not present it here.\n107\n108"
    }, {
      "heading" : "5.3 A non-convex case",
      "text" : "Here’s an amusing non-convex case which sheds some light on when EASGD will work and when it will not work. Recall our formalization of the problem in Chapter 1, Equation 1.2:\nmin x1,...,xp,x̃ p∑ i=1 E[f(xi, ξi)] + ρ 2 ‖xi − x̃‖2,\nwhere we refer to xi’s as local variables and we refer to x̃ as a center variable.\nIf f(xi, ξi) = 14(1−(xi)2)2, which is deterministic (independent of ξi) and one-dimensional. We see that f has two minimum xi = 1 and xi = −1. It also has a saddle point at xi = 0. We ask how large ρ should be such that the xi’s will have a common minimum. Let’s assume that p = 2, we see that if ρ = 0, then x1 = −1 and x2 = 1 can be a stable solution. This is the scenario that the ’elasticity’ of EASGD is broken. We have observed a related phenomenon in EAMSGD when the communication period is too large (τ big) in the Figure 4.13 of Chapter 4. More formally, we have the following objective for p = 2 workers,\nmin x,y,z∈R\n1 4 (1− x2)2 + 1 4 (1− y2)2 + ρ 2 ‖x− z‖2 + ρ 2 ‖y − z‖2. (5.35)\nThe partial derivative of our objective in Equation 5.35 with respect to x, y and z is:\n∂ ∂x = (x 2 − 1)x+ ρ(x− z) ∂ ∂y = (y\n2 − 1)y + ρ(y − z) ∂ ∂z = ρ(z − x) + ρ(z − y).\n(5.36)\nThe question is that what are all the critical points of the objective in Equation 5.35. By setting the derivative to zero in Equation 5.36, they should satisfy z = x+y2 and\n(x2 − 1)x+ ρ(x− x+y2 ) = 0 (y2 − 1)y + ρ(y − x+y2 ) = 0. (5.37)\n109\nObserving three special cases: x = y = z = 1, x = y = z = −1 and x = y = z = 0. We can guess that x = −y is also a solution. In fact, they should satisfy z = 0, (x2 − 1)x + ρ(x − 0) = 0 and (y2 − 1)y + ρ(y − 0) = 0. We have thus either x = 0 or x2 = 1 − ρ (resp. y = 0 or y2 = 1 − ρ). If ρ < 1, then we indeed find a real critical point x = √ 1− ρ, y = −√1− ρ, z = 0. Is this critical point stable? To answer this, we compute the Hessian of our objective in Equation 5.35 with respect to x, y and z:\nH =  3x2 − 1 + ρ 0 −ρ 0 3y2 − 1 + ρ −ρ\n−ρ −ρ 2ρ.\n . (5.38)\nEvaluating this Hessian at the critical point x = √ 1− ρ, y = −√1− ρ, z = 0, we can compute its smallest eigenvalue and see when it is positive definite. Figure 5.20 shows that the smallest eigenvalue is always positive in the range ρ ∈ (0, 2/3). This suggests that this critical point can indeed be stable, i.e. it is a local optimum introduced by EASGD when the penalty term ρ is small enough.\nFor completeness, we now prove that all the critical points are either of the form x = y or of the form x = −y. In fact, adding the two Equations in 5.37 gives us (x2− 1)x+ (y2− 1)y = 0, i.e. x3 + y3 = x+ y = (x2 − xy + y2)(x+ y). Subtracting these two Equations gives us x3−x−y3 +y+ρ(x−y) = 0, i.e. x3−y3 = (1−ρ)(x−y) = (x2 +xy+y2)(x−y). If x 6= y and x 6= −y, then x2 − xy + y2 = 1\nx2 + xy + y2 = 1− ρ. (5.39)\nAdding and subtracting again the two Equations in 5.39, we obtain x2 + y2 = 1− ρ2 , and xy = −ρ2 . There’s no real solution satisfying these two conditions for any ρ > 0.\n110\n111\nChapter 6"
    }, {
      "heading" : "Scaling up Elastic Averaging SGD",
      "text" : "This chapter discusses how to scale up the EASGD method to hundreds and thousands of processors. In Section 6.1, we first propose a tree-structured extension of the EASGD method called EASGD Tree. The basic idea and the design principle are discussed in Section 6.1.1, and the numerical results are presented in Section 6.1.2. We present two different communication schemes for the EASGD Tree method. As we had seen the advantage of EAMSGD, we also accelerate EASGD Tree with Nesterov ’s momentum method. In Section 6.2, we unify the DONWPOUR method and the EASGD method by considering a Gauss-Seidel reformulation of the EASGD update rules in the synchronous scenario. This unification suggests the possibility of using both DONWPOUR and EASGD under the EASGD Tree. It also suggests that in-between the DONWPOUR and the EASGD method there may be some even better method.\n6.1 EASGD Tree\nThe original motivation of EASGD Tree is to run SGD at multiple time scales, where each scale corresponds to the use of a different learning rate. It naturally gives rise to a hierarchical tree structured organization of the processors. In literature, the tree\n112\nidea has shown up in various contexts. For example, the tree was used to scale up the asynchronous SGD method by aggregating the delayed gradients computed by the intermediate nodes and the leaf nodes [2]. It can also be used to efficiently implement the Broadcast/AllReduce operation as in MPI [1, 62]. The benefit of using a tree is that the number of links to connect a very large number of nodes is minimal. The trade-off is that the connectivity of the whole tree is not robust to the link failure. However, the tree structure has its own charm for its simplicity. The main theoretical challenge is to understand its convergence property in terms of the root of the tree. This still remains open."
    }, {
      "heading" : "6.1.1 The algorithm",
      "text" : "We start from introducing the EASGD Tree algorithm. There are two important aspects that can guide us to design such an algorithm, one is the computation, the other is the communication.\n• Do we need intermediate node to perform the gradient computation? We had hoped\nthat each tree node runs SGD in parallel with a smaller and smaller learning rates as the depth of the tree node decreases (from bottom to top). Each intermediate node (i.e. except the leaf node) is expected to average out the noise of its children. The root can thus achieve the smallest variance. One benefit that each node performs its own gradient computation is that it can reduce the bias of the spatial averaging of its children away from the local optimum. The disadvantage is that each node will have the overhead for both computation and communication. In case each tree node uses only one CPU core, it needs to tradeoff the computation speed for the communication throughput. In addition, in the HPC environment (where we run our experiments), we have not observed any visible performance improvement for the intermediate node to perform the gradient computation. It’s also complicated to choose a proper learning rate decay for different levels of the\n113\ntree nodes. On the other hand, if we were to remove the intermediate node’s gradient computation, we can still preserve the multi-scale variance reduction property using the elastic averaging. Moreover, the root node of the tree behaves as the center variable of the EASGD method. Thus in the following we only discuss the EASGD Tree method without intermediate node’s gradient computation.\n• There are at least two different ways to design the communication protocol. One is\nlocally synchronous, the other is fully asynchronous. We observe that the local synchronization protocol (as in our asynchronous EASGD in Section 2.2) may cause extra waiting time for the parent and the children nodes, but it can give better convergence property (due to a smaller parameter staleness). The fully asynchronous protocol can in theory hide completely this waiting time by allowing each node asynchronously broadcasts its parameter to its neighbours without blocking any computation. In the scale that we simulate the EASGD Tree, we have to use the fully asynchronous protocol as it saves significantly the time which is spent on the chain of waiting. In our HPC environment, the fully asynchronous protocol works well if the network link is stable over time.\n• The communication cost is different within a machine and across a machine. We\nwould thus structure the tree nodes from bottom-up, i.e. the leaf nodes are allocated on one machine so that their variance can be averaged out the quickest possible. The intermediate nodes including the root of the tree will then have to communicate across the machine boundary (in our HPC environment, there are 20 physical CPU cores per machine). These communication should happen less frequently. We will thus introduce two levels of the communication periods. The first communication period is between the leaf nodes and their parents. The second (smaller) one is between the intermediate nodes. We will also distinguish the push up (toward parent) period and the push down (toward children) period. With this, we can trade-off more push up communication for less push down communication.\n114\nBased on the above discussion, we now present the EASGD Tree in Algorithm 6. Figure 6.1 illustrates this. This generic pseudocode does not distinguish the leaf node, the intermediate node and the root node, so let’s describe their difference first. As in EASGD, each node starts with the same initial parameter x0 and sets its local clock ti to 0. It performs non-blocking read (Irecv) and non-blocking send (Isend) with its parent and children (if any). Every τup local steps, it sends its parameter to its parent. Every τdown local steps (number of local gradient updates), it sends its parameter to the d children. We denote d to be the degree of the d-ary tree we consider here. Each node also needs to check if there’s any new arrival of the parameter from its parent or its d children. This needs to be interleaved with the gradient computation to achieve high I/O throughput. On receiving a new parameter, a moving average with the moving rate α is performed. If it is the leaf node, then a stochastic gradient descent step with learning rate η is performed per local step; otherwise it replaces the gradient descent step with a while loop, looping for roughly the same amount of time as a gradient step, just to apply\n115\nthe moving average on any new parameter arrival. Note that for each node, they can use different communication period τup and τdown. We discuss two such possible schemes below. They are illustrated in Figure 6.2.\nThe first scheme is based on the idea of multi-scale averaging. We would like to have a fast averaging process at the bottom level, and a slower averaging process at the top and intermediate level. Let τ1 be the fast communication period between the leaf nodes and their parents. Let τ2 be the slow communication period between the intermediate nodes. For the leaf nodes, i.e. without child, we set τup = τ1, τdown = NaN . For their parents (which should sit on the same machine as their children), we set τup = τ2, τdown = τ1. For the rest of the intermediate nodes, except for the root node, we set τup = τdown = τ2. Lastly, for the root node, we set τup = NaN , τdown = τ2.\nThe second scheme is to mimic the behavior of EASGD for β being large and α being small (c.f. Equation 2.3 and 2.4). We use a large τup to represent large β, and a small τdown to represent small α. For simplicity, we set each tree node with the same τup = τu and τdown = τd, except for the leaf node with τdown = NaN and the root node with τup = NaN .\n116\nThere’s a subtle difference between EASGD and EASGD Tree. As we discussed in Section 2.2, the design of EASGD follows the Jacobi form. In EASGD Tree, we use instead the Gauss-Seidel form to perform the moving average. This is because the time when the parameter will arrive is not predictable in general, so we’d rather perform the update just-in-time. Nevertheless, we should be careful to avoid performing the moving average during the gradient update. Moreover, there’s a deeper connection between the Jacobi/Gauss-Seidel method and EASGD/DOWNPOUR method that we shall discuss in the next Section 6.2. Thus EASGD Tree is somewhere in-between EASGD and DOWNPOUR.\nAlgorithm 6: EASGD Tree Processing by worker i, with its parent p(i) and children {c(i, j)|j = 1, . . . , d}. Input: learning rate η, moving rate α, communication period τup and τdown ∈ N Initialize: xi = x0, t i = 0\nIrecv xp(i) from parent p(i) Irecv xc(i,j) from children {c(i, j)|j = 1, . . . , d} Repeat if (τup divides t\ni) then Isend xi to parent p(i)\nend if (τdown divides t\ni) then Isend xi to children {c(i, j)|j = 1, . . . , d}\nend if there is new arrival of xp(i) then\nxi ← xi + α(xp(i) − xi) end if there is new arrival of xc(i,j) then\nxi ← xi + α(xc(i,j) − xi) end xi ← xi − η∇f(xi, ξi\nti )\nti ← ti + 1 Until forever"
    }, {
      "heading" : "6.1.2 The result",
      "text" : "In this section, we show empirically the advantage and the challenge to scale up the EASGD Tree to a few hundreds of CPU cores. Since we are limited by the number\n117\nGPUs available, all the results are run on CPUs. The experiment setup is thus different to our earlier results in Section 4.1. We report the results based on CIFAR-10 dataset using a low-rank convolutional neural-network model [56]. The low-rank convolution approximation saves us a lot of CPU computation time, so we can simulate the long term behavior of the algorithm within 12 hours. We do not use any data augmentation as we did in Chapter 4, because it requires extra CPUs to pre-process the input data. Each data point is sampled uniformly random with replacement (different to what we did in Section 4.1). As in [56], we center the input pixel values by the mean and the standard deviation for each of the three channels using the training data. We also add l2-regularization λ 2 ‖x‖ 2 to the loss function with λ = 0.0001.\nFor consistency, we describe the detailed model1 using the notation in Section 4.1: (3, 32, 32) C−−−−−−−→\n(1,5,1,1,0,2) (10, 32, 32) C,R−−−−−−−→ (5,1,1,1,2,0) (96, 32, 32) P−−−−−→ (2,2,2,2) (96, 16, 16) C−−−−−−−→ (1,5,1,1,0,2)\n(51, 16, 16) C,R−−−−−−−→\n(5,1,1,1,2,0) (128, 16, 16) P−−−−−→ (2,2,2,2) (128, 8, 8) C−−−−−−−→ (1,5,1,1,0,2) (51, 8, 8) C,R−−−−−−−→ (5,1,1,1,2,0)\n(256, 8, 8) P−−−−−→\n(2,2,2,2) (256, 4, 4) C,R−−−−−−−→ (1,1,1,1,0,0) (64, 4, 4) D,L,R−−−−→ 0.5 (256, 1, 1) D,L,S−−−−→ (10, 1, 1).\nIn the following experiments, we call CIFAR-lowrank the experiment that is just described. We use a d-ary tree with degree d = 16 and p = 256 leaf nodes running on 16 machines. For simplicity, we set the moving rate α at each node to be a constant 0.9/(16 + 1). First we examine the two communication schemes that we have discussed above. Figure 6.3 shows the performance of the first communication scheme using period τ1 = 10 and τ2 = 100. Figure 6.4 shows the performance of the second communication scheme using τu = 8 and τd = 80. We see that the first communication scheme achieves a faster convergence in terms of the training loss. All the six runs are nearly the same at the beginning. However, five runs out of the six have diverged in the middle of the training. We have chosen a quite large learning rate which can trigger this instability. The second communication scheme is more stable. Only one out of the six runs has diverged. It has a slower convergence in terms of the training loss, but it consistently\n1see also in https://github.com/chengtaipu/lowrankcnn/blob/master/cifar/models/baseline_ lowrank_cudnn.lua\n118\nachieves a lower test error (the smallest one is 15.32%, compared to the smallest test error of the first scheme which is 16.86%). Also notice that there are two runs with a slower initial convergence rate. This may be related to the network traffic.\nNext we examine the effect of the momentum when using a mini-batch size of 16. We use mini-batch as we have observed that the momentum can help more when the stochastic gradient variance is smaller (c.f. also Figure 5.14). Moreover, it would be nice to see whether using a relatively small mini-batch size can still hurt the test performance. As in EAMSGD, we can apply Nesterov’s momentum to each of the leaf node during the gradient computation. Compared to the earlier results (without using mini-batch) in Figure 6.3 and 6.4, we increase the total training time from 3 hours to 12 hours. This is because we do not have more CPU resources to parallelize the mini-batch computation. It takes 0.13sec/step in the minibatch case vs. 0.01sec/step in the case without using mini-batch.\nIn Figure 6.5, 6.6 and 6.7, we report the results of the momentum in the first communication scheme. We see that without using momentum (i.e. δ = 0), the training process is still not very stable as before. Three out of the six runs have diverged. Using momentum δ = 0.9 allows us to reduce the learning rate by a factor of ten (see also the discussion of Figure 5.14), and gives a much more stable training process. Also they look almost identical, except for the two curves marked in e and f. This difference is due to a longer initialization of all the nodes. Using momentum δ = 0.99 is also very stable, though the results are more varied.\nIn Figure 6.8, 6.9 and 6.10, we report the results of the momentum in the second communication scheme. Without using momentum is still not as stable. The curve a and c have stopped in the middle without any indication of divergence. But they did have diverged and ended with NaN. Nevertheless, the curve b gives a smallest test error 15.4%. The second communication scheme again leads to better test performance. Using momentum δ = 0.9 gives a smallest test error 14.91% in curve b. Using momentum δ = 0.99 also\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\ngives a smallest test error 14.91% in curve d. The most surprising observation is that the curve a has a large peak between hour 8 and 9. We have investigated the log and found that it is correlated with the network traffic.\nWe can now compare all the momentum results by selecting the best test performance curve in previous results. They are summarized in Figure 6.11. It’s very clear that the two communication schemes give very different performance. The first scheme gives better training performance, while the second scheme gives better test performance. The momentum no longer plays a significant role to determine the test performance as in EAMSGD, but it is still helpful. To explain why still remains very open.\nFor completeness, we compare EASGD, DOWNPOUR with p = 16, and the EASGD Tree with p = 256. Figure 6.12 shows the best test performance for each method, selected among a wide range of the learning rates.\n6.2 Unifying EASGD and DOWNPOUR\nIt’s beneficial to write down the updates of the DOWNPOUR method (Algorithm 3 in Section 4.4) in a synchronous scenario. For simplicity, let’s assume that the communication period τ = 1. The global clock t is shared among the local workers. Every step DOWNPOUR performs the following updates to solve the original Problem 1.1 in Chapter 1:\nxit+1 = x̃t − η∇f(x̃t, ξit), x̃t+1 = x̃t + ∑p i=1(x i t+1 − x̃t).\n(6.1)\nNow compare with the EASGD updates (from Equation 2.4 in Chapter 2):\nxit+1 = x i t − η∇f(xit, ξit)− α(xit − x̃t), x̃t+1 = x̃t + β p ∑p i=1(x i t − x̃t).\n(6.2)\nWe realize that it’s possible to unify these two methods by drawing a parallel between\n130\nthe classical Jacobi and Gauss-Seidel methods. To motivate this connection, let’s look back at the convergence result of EASGD. Lemma 3.1.1 indicates that the divergence can happen if the condition 3.4 is not satisfied. One might wonder why there’s such a stability condition. After all, EASGD performs only the gradient descent and the moving averages. Each of these operations is contractive on its own! A short answer is that EASGD is formalized in the Jacobi form rather than the Gauss-Seidel form in order to capture the delay of the parameter communication. We can transform the EASGD ’s Jacobi form in Equation 6.2 to a Gauss-Seidel form as follows:\nxit+1/2 = x i t − α(xit − x̃t),\nxit+1 = x i t+1/2 − η∇f(xit+1/2, ξit+1/2), x̃t+1 = x̃t + β p ∑p i=1(x i t+1 − x̃t).\n(6.3)\nThe first and the third step in Equation 6.3 is contractive if we choose α ∈ (0, 2) and β ∈ (0, 2). Thus the stability condition 3.4 is too much a constraint in this case. Notice that the DOWNPOUR method in Equation 6.1 is equivalent to the EASGD ’s GaussSeidel form in Equation 6.3 with α = 1 and β = p. It is in this sense that DOWNPOUR and EASGD get connected! However, β = p can be very large, how can it be stable?\nFor stability analysis, we consider again the quadratic case where ∇f(x, ξ) = x. We can reduce the Equation 6.3 by introduing x̄t = 1 p ∑p i=1 x i t as follows:\nx̄t+1/2 = x̄t − α(x̄t − x̃t),\nx̄t+1 = x̄t+1/2 − ηx̄t+1/2, x̃t+1 = x̃t + β(x̄t+1 − x̃t).\n(6.4)\nThis is a linear system which can be written as (x̄t+1, x̃t+1) T = M(x̄t, x̃t) T , where\nM =  1 0 β 1− β   1− η 0 0 1   1− α α 0 1  . (6.5)\n131\nWe evaluate the two eigenvalues z1 and z2 of this matrix M , which should satisfy the equation z2 − 2bz + c = 0. One can check that\nb = 12(2− β − η − α(1− β)(1− η)), c = (1− η)(1− α)(1− β). (6.6)\nWe need a basic lemma to study the stability, i.e. when the absolute value of z1 and z2 are smaller than one. Lemma 6.2.1. Let z1 and z2 be the two roots of the polynomial z 2 − 2bz + c = 0 with real coefficients b ∈ R and c ∈ R, then the sufficient and necessary condition for |z1| < 1 and |z2| < 1 is\n− 1 < c < 1, c > 2b− 1, c > −2b− 1. (6.7)\nProof. There are two cases to consider. The first one is the real-valued case, i.e. b2 > c. The second one is the imaginary case, i.e. b2 ≤ c. If b2 ≤ c, then |z1| = |z2| = c2 < 1 is equivalent to −1 < c < 1. If b2 > c then we can suppose z1 = b + √ b2 − c and z2 = b − √ b2 − c. The condition −1 < z2 < z1 < 1 is equivalent to c > 2b − 1, b < 1, c > −2b − 1 and b > −1. Combining the condition from the two cases gives us the Equation 6.7.\nApplying the Lemma 6.7 to the above case in Equation 6.6, we obtain the stability condition for the reduced linear system 6.4 as below,\n−1 < c < 1, 0 < βη < 2(c+ 1),\nwhere c = (1− η)(1− α)(1− β). It’s easy to verify this since 2b = c+ 1− βη.\nWe summarize the obtained stability condition (using Mathematica) in the following theorem. Theorem 6.2.1. Assume η > 0, α > 0 and β > 0, the stability condition for the linear system 6.4 is equivalent to union of the following set of conditions:\n132\n• 0 < η < 1, 0 < β < 1 and 0 < α < c2,\n• 0 < η < 1, β = 1 and α > 0,\n• 0 < η < 1, 1 < β ≤ 2 and 0 < α < c3,\n• 0 < η < 1, 2 < β < 4/η and c2 < α < c3,\n• η = 1, 0 < β < 2 and α > 0,\n• 1 < η < 2, 0 < β < 1 and 0 < α < c3,\n• 1 < η < 2, β = 1 and α > 0,\n• 1 < η < 2, 1 < β < 2 and 0 < α < c2,\n• 2 ≤ η ≤ 4, 0 < β < 1 and c2 < α < c3,\n• η > 4, 0 < β < 4/η and c2 < α < c3,\nwhere\nc2 = 4− 2β − 2η + βη 2− 2β − 2η + 2βη , c3 = −β − η + βη 1− β − η + βη .\nThere is only one condition above which satisfies the DOWNPOUR method when p > 2. This condition is 0 < η < 1, 2 < β < 4/η and c2 < α < c3. It means that if β = p is very large, we need to use quite small learning rate such that 0 < η < 4/p. Moreover, α has to stay very close to 1 (i.e. forgetting all the local memory). This is because c2 tends to 1 as β = p tends to infinity (assume βη < 4 always holds); and c3 tends to 1 as well. For example, this condition does not hold for α = 0.9 and β = 16, for any η > 0. This is rather surprising: the stability condition for α = 1 and β = 16 is 0 < η < 1/8. However, there’s no η > 0 for α = 0.9 and β = 16 to be stable. This suggest DOWNPOUR is a very peculiar singleton in the class of the methods defined by Equation 6.3. EASGD has much more flexibilities as it operates in the region where β ∈ (0, 2). In practice, we have observed that the case β = 1, 0 < η < 2, α > 0 works well. It is very dangerous to operate in the region of η > 2 when α is small.\n133\nChapter 7"
    }, {
      "heading" : "Conclusion",
      "text" : "In this thesis, we focused on the problem of training large-scale deep learning models in a parallel and distributed computing environment.\nOur starting point is a reformulation of the global variable consensus problem into an unconstrained optimization problem using a quadratic penalty between each local variable and the center variable. By reinterpreting the gradient of the quadratic penalty as an averaging process, we introduced the Elastic Averaging SGD (EASGD) method. The EASGD maintains the stochastic nature of the sequential SGD method through a weak coupling between the local variables and the center variable. Each local variable is optimized using an SGD-based method. The averaging process attracts the local variable and the center variable toward each other so that all of them move toward a local optimum. The center variable can slowly track the spatial average of the local variables, thus having a smaller variance. We then discussed how to extend the EASGD method to the asynchronous scenario, and how to accelerate it using the momentum.\nWe proceeded to study the convergence rate the EASGD method in terms of the center variable. In the synchronous scenario, we discussed the variance reduction effect of the EASGD method by increasing the number of local variables, for both quadratic and strongly-convex case. We also introduced a double averaging sequence in the spirit of\n134\nthe Polyak ’s averaging and showed that it is indeed asymptotically optimal. We then compared the stability region of the EASGD method with the ADMM method in the round-robin scheme, where we have found quite unusual instability region for the ADMM method.\nHaving studied the (local) convergence property of the EASGD method, we applied its asynchronous extension to train deep learning models. We focused on the CIFAR-10 and ImageNet ILSVRC 2013 dataset for image classification in a supervised learning setting. The (global) convergence behavior of the EASGD method was simulated starting from a random initialization. We found that the EASGD method accelerates the training of the baseline SGD method. Moreover, it is very communication efficient compared to the asynchronous SGD method DOWNPOUR. After combining with Nesterov ’s momentum method, we obtained EAMSGD and it achieved even better test accuracy. We also discussed the tradeoff between the data communication and the parameter communication. In our cases, it turned out to save the total bandwidth a lot by using more data communication for less parameter communication (recall EASGD method needs to sample the whole dataset in order to avoid the bias of the stochastic gradient).\nBased on the empirical results we have obtained, we asked what is the limit of the speedup by increasing the number of processors. We studied first an additive noise model to capture the asymptotic behavior of various stochastic optimization method, then we proposed a multiplicative noise model to capture the initial behavior of these methods. For the additive noise case, we first studied the SGD method with mini-batch. We saw that increasing the mini-batch size (i.e. number of processors) can reduce the asymptotic variance, but it is not able to increase the convergence speed. We then studied the momentum SGD in the Nesterov ’s form, and showed that faster convergence speed is possible, but that will lead to larger asymptotic variance. We have also generalised the EASGD method by decoupling (de-symmetrize) the elastic averaging between the center variable and the local variables, and found that the EASGD method can behavior like a momentum SGD with a negative moving average rate in some optimal sense (i.e.\n135\nthe center variable pushes the local variables instead of pulling). For the multiplicative noise case, we have found that SGD method with mini-batch can greatly improved the initial convergence speed by choosing the optimal (larger) learning rate. However, the momentum SGD is not as effective in this case. The EASGD method is also slower than SGD with mini-batch, but its stability region is still improved by increasing the number of processors. We showed that given an infinite number of processors, the improvement for stability region of EASGD depends on the spread of the Gamma distribution of the input data. The larger the spread, the more the improvement.\nBack to our starting point, we studied an interesting non-convex case for the EASGD method based on the an unconstrained objective we have introduced. We have discussed the stability of the critical points. We found that when the quadratic penalty is too small (i.e. the coupling between the center variable and the local variable is too weak), the EASGD method can introduce a stable local optimum trapped by a saddle point.\nKnowing the theoretical limitation of the speedup, we scale up the EASGD by using a tree structured network topology. We showed empirically that EASGD Tree can further accelerate the training speed, but no longer in a linear speedup region. This is somehow predicted by our analysis in the multiplicative noise model. The surprising observation is EASGD Tree can still yield better test accuracy when the local variables fluctuate further from the center variable. Moreover, inspired by the design of the EASGD Tree in the asynchronous scenario, we carefully distinguished the difference between the Jacobi form of EASGD with its Gauss-Seidel form, which in turn unified the EASGD and the DOWNPOUR method in the synchronous scenario.\nWe conclude with a few open problems from convex optimization, non-convex optimization, distributed and parallel computing, statistical and deep learning.\n• The smoothing property of EASGD for non-smooth convex optimization. We have\ndiscussed the connection between the quadratic penalty term in Equation 1.2 and the Moreau-Yosida regularization. Can one expect a good rate of convergence?\n136\n• The connection between EAMSGD and Katyusha [3]. Katyusha is an accelerated\nvariance reduction method. The speedup of its acceleration is dragged back by a center variable so as to achieve a smaller variance. This resembles the role of the center variable in EAMSGD.\n• In the simulation of EASGD Tree, we have seen that network congestion can occur.\nIs there a good way to detect when the network becomes slow so as to avoid the potential congestion? This is an adaptive learning rate problem. It is not very simple as we have seen in Chapter 5 that the optimal moving rate can sometime be positive, but sometime be negative.\n• The convergence analysis of the EASGD Tree is still missing. The basic tool [57]\nbased on the mean-field method (i.e. treating each node the same) seems to be too macroscopic to capture the global (e.g. multi-scale variance reduction) behavior of the EASGD Tree.\n• The description of the asynchronous behavior using vector-clock [53]. Although we\nhave unified EASGD and DOWNPOUR in the synchronous scenario, it is still not clear how to measure their behavior in the asynchronous scenario and in the realtime workloads. The idea of using vector-clock from the distributed computing is worthing exploring in the future.\n• How non-convex is the energy landscape? A related question may be what to\ndo when EASGD is trapped by a saddle point. Also why there is a such a big difference between ASGD and MVASGD as we have studied in Figure 4.11?\n• What is the role of the oscillation in deep learning to achieve better test perfor-\nmance? We have observed that both EAMSGD and EASGD Tree can achieve better test performance when the oscillation of the local variables is sufficiently large. Looking back to the Dropout [54] and DropConnect [58] regularization, is there any deeper reason in common [51]?\n137"
    } ],
    "references" : [ {
      "title" : "A reliable effective terascale linear learning system",
      "author" : [ "A. Agarwal", "O. Chapelle", "M. Dud́ık", "J. Langford" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2014
    }, {
      "title" : "Distributed delayed stochastic optimization",
      "author" : [ "A. Agarwal", "J.C. Duchi" ],
      "venue" : "In NIPS, pages 873–881,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2011
    }, {
      "title" : "Katyusha: Accelerated variance reduction for faster sgd",
      "author" : [ "Z. Allen-Zhu" ],
      "venue" : "arXiv preprint arXiv:1603.05953,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2016
    }, {
      "title" : "Towards an optimal stochastic alternating direction method of multipliers",
      "author" : [ "S. Azadi", "S. Sra" ],
      "venue" : "In ICML,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2014
    }, {
      "title" : "Scaling up machine learning: Parallel and distributed approaches",
      "author" : [ "R. Bekkerman", "M. Bilenko", "J. Langford" ],
      "venue" : null,
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2011
    }, {
      "title" : "A new class of incremental gradient methods for least squares problems",
      "author" : [ "D.P. Bertsekas" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1997
    }, {
      "title" : "Parallel and Distributed Computation",
      "author" : [ "D.P. Bertsekas", "J.N. Tsitsiklis" ],
      "venue" : null,
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1989
    }, {
      "title" : "Asynchronous stochastic approximations",
      "author" : [ "V. Borkar" ],
      "venue" : "SIAM Journal on Control and Optimization,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1998
    }, {
      "title" : "Online algorithms and stochastic approximations. In Online Learning and Neural Networks",
      "author" : [ "L. Bottou" ],
      "venue" : null,
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1998
    }, {
      "title" : "Curiously fast convergence of some stochastic gradient descent algorithms. Unpublished open problem offered to the attendance of the SLDS",
      "author" : [ "L. Bottou" ],
      "venue" : null,
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2009
    }, {
      "title" : "Distributed optimization and statistical learning via the alternating direction method of multipliers",
      "author" : [ "S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein" ],
      "venue" : "Found. Trends Mach. Learn.,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2011
    }, {
      "title" : "On the generalization ability of on-line learning algorithms",
      "author" : [ "N. Cesa-Bianchi", "A. Conconi", "C. Gentile" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2004
    }, {
      "title" : "The loss surfaces of multilayer networks",
      "author" : [ "A. Choromanska", "M.B. Henaff", "M. Mathieu", "G.B. Arous", "Y. LeCun" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2015
    }, {
      "title" : "Better mini-batch algorithms via accelerated gradient methods",
      "author" : [ "A. Cotter", "O. Shamir", "N. Srebro", "K. Sridharan" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2011
    }, {
      "title" : "Distributed deep learning using synchronous stochastic gradient descent",
      "author" : [ "D. Das", "S. Avancha", "D. Mudigere", "K. Vaidynathan", "S. Sridharan", "D. Kalamkar", "B. Kaul", "P. Dubey" ],
      "venue" : "arXiv preprint arXiv:1602.06709,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2016
    }, {
      "title" : "Large scale distributed deep networks",
      "author" : [ "J. Dean", "G. Corrado", "R. Monga", "K. Chen", "M. Devin", "Q. Le", "M. Mao", "M. Ranzato", "A. Senior", "P. Tucker", "K. Yang", "A. Ng" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2012
    }, {
      "title" : "Optimal distributed online prediction using mini-batches",
      "author" : [ "O. Dekel", "R. Gilad-Bachrach", "O. Shamir", "L. Xiao" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2012
    }, {
      "title" : "First-order methods of smooth convex optimization with inexact oracle",
      "author" : [ "O. Devolder", "F. Glineur", "Y. Nesterov" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2014
    }, {
      "title" : "Dual averaging for distributed optimization: convergence analysis and network scaling",
      "author" : [ "J.C. Duchi", "A. Agarwal", "M.J. Wainwright" ],
      "venue" : "Automatic control, IEEE Transactions on,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2012
    }, {
      "title" : "Distributed deep learning for answer selection",
      "author" : [ "M. Feng", "B. Xiang", "B. Zhou" ],
      "venue" : "CoRR, abs/1511.01158,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2015
    }, {
      "title" : "Global convergence of the heavy-ball method for convex optimization",
      "author" : [ "E. Ghadimi", "H.R. Feyzmahdavian", "M. Johansson" ],
      "venue" : "In Control Conference (ECC), 2015 European,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2015
    }, {
      "title" : "The unreasonable effectiveness of data",
      "author" : [ "A. Halevy", "P. Norvig", "F. Pereira" ],
      "venue" : "Intelligent Systems, IEEE,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2009
    }, {
      "title" : "Train faster, generalize better: Stability of stochastic gradient descent",
      "author" : [ "M. Hardt", "B. Recht", "Y. Singer" ],
      "venue" : "arXiv preprint arXiv:1509.01240,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2015
    }, {
      "title" : "Optimization theory: the finite dimensional case",
      "author" : [ "M.R. Hestenes" ],
      "venue" : null,
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 1975
    }, {
      "title" : "More effective distributed ml via a stale synchronous parallel parameter server",
      "author" : [ "Q. Ho", "J. Cipar", "H. Cui", "S. Lee", "J.K. Kim", "P.B. Gibbons", "G.A. Gibson", "G. Ganger", "E.P. Xing" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2013
    }, {
      "title" : "Deepspark: Spark-based deep learning supporting asynchronous updates and caffe compatibility",
      "author" : [ "H. Kim", "J. Park", "J. Jang", "S. Yoon" ],
      "venue" : "arXiv preprint arXiv:1602.08191,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2016
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "A. Krizhevsky", "I. Sutskever", "G.E. Hinton" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2012
    }, {
      "title" : "An optimal method for stochastic composite optimization",
      "author" : [ "G. Lan" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2012
    }, {
      "title" : "Slow learners are fast",
      "author" : [ "J. Langford", "A. Smola", "M. Zinkevich" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2009
    }, {
      "title" : "Efficient backprop",
      "author" : [ "Y.A. LeCun", "L. Bottou", "G.B. Orr", "K.-R. Müller" ],
      "venue" : "In Neural networks: Tricks of the trade,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2012
    }, {
      "title" : "Efficient mini-batch training for stochastic optimization",
      "author" : [ "M. Li", "T. Zhang", "Y. Chen", "A.J. Smola" ],
      "venue" : "In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2014
    }, {
      "title" : "Dynamics of stochastic gradient algorithms",
      "author" : [ "Q. Li", "C. Tai", "W. E" ],
      "venue" : "CoRR, abs/1511.06251,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2015
    }, {
      "title" : "A universal catalyst for first-order optimization",
      "author" : [ "H. Lin", "J. Mairal", "Z. Harchaoui" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2015
    }, {
      "title" : "Sparknet: Training deep networks in spark",
      "author" : [ "P. Moritz", "R. Nishihara", "I. Stoica", "M.I. Jordan" ],
      "venue" : "arXiv preprint arXiv:1511.06051,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2015
    }, {
      "title" : "Rectified linear units improve restricted boltzmann machines",
      "author" : [ "V. Nair", "G.E. Hinton" ],
      "venue" : "In Proceedings of the 27th International Conference on Machine Learning",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2010
    }, {
      "title" : "Distributed asynchronous incremental subgradient methods",
      "author" : [ "A. Nedić", "D. Bertsekas", "V. Borkar" ],
      "venue" : null,
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2001
    }, {
      "title" : "Cooperative distributed multi-agent optimization",
      "author" : [ "A. Nedi", "A. Ozdaglar" ],
      "venue" : "Convex Optimization in Signal Processing and Communications,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 2009
    }, {
      "title" : "Introductory lectures on convex optimization, volume 87",
      "author" : [ "Y. Nesterov" ],
      "venue" : "Springer Science & Business Media,",
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 2004
    }, {
      "title" : "Smooth minimization of non-smooth functions",
      "author" : [ "Y. Nesterov" ],
      "venue" : "Math. Program.,",
      "citeRegEx" : "39",
      "shortCiteRegEx" : "39",
      "year" : 2005
    }, {
      "title" : "Numerical Optimization, Second Edition",
      "author" : [ "J. Nocedal", "S. Wright" ],
      "venue" : null,
      "citeRegEx" : "40",
      "shortCiteRegEx" : "40",
      "year" : 2006
    }, {
      "title" : "Convergence speed in distributed consensus and averaging",
      "author" : [ "A. Olshevsky", "J.N. Tsitsiklis" ],
      "venue" : "SIAM review,",
      "citeRegEx" : "41",
      "shortCiteRegEx" : "41",
      "year" : 2011
    }, {
      "title" : "Stochastic alternating direction method of multipliers",
      "author" : [ "H. Ouyang", "N. He", "L. Tran", "A. Gray" ],
      "venue" : "In Proceedings of the 30th International Conference on Machine Learning,",
      "citeRegEx" : "42",
      "shortCiteRegEx" : "42",
      "year" : 2013
    }, {
      "title" : "Gpu asynchronous stochastic gradient descent to speed up neural network training",
      "author" : [ "T. Paine", "H. Jin", "J. Yang", "Z. Lin", "T. Huang" ],
      "venue" : "arXiv preprint arXiv:1312.6186,",
      "citeRegEx" : "43",
      "shortCiteRegEx" : "43",
      "year" : 2013
    }, {
      "title" : "Introduction to optimization",
      "author" : [ "B.T. Polyak" ],
      "venue" : "Optimization Software New York,",
      "citeRegEx" : "44",
      "shortCiteRegEx" : "44",
      "year" : 1987
    }, {
      "title" : "Acceleration of stochastic approximation by averaging",
      "author" : [ "B.T. Polyak", "A.B. Juditsky" ],
      "venue" : "SIAM Journal on Control and Optimization,",
      "citeRegEx" : "45",
      "shortCiteRegEx" : "45",
      "year" : 1992
    }, {
      "title" : "Hogwild: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent",
      "author" : [ "B. Recht", "C. Re", "S.J. Wright", "F. Niu" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "46",
      "shortCiteRegEx" : "46",
      "year" : 2011
    }, {
      "title" : "Iterative methods for sparse linear systems",
      "author" : [ "Y. Saad" ],
      "venue" : null,
      "citeRegEx" : "47",
      "shortCiteRegEx" : "47",
      "year" : 2003
    }, {
      "title" : "1-bit stochastic gradient descent and application to data-parallel distributed training of speech dnns",
      "author" : [ "F. Seide", "H. Fu", "J. Droppo", "G. Li", "D. Yu" ],
      "venue" : "In Interspeech",
      "citeRegEx" : "48",
      "shortCiteRegEx" : "48",
      "year" : 2014
    }, {
      "title" : "Overfeat: Integrated recognition, localization and detection using convolutional networks",
      "author" : [ "P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. LeCun" ],
      "venue" : "In International Conference on Learning Representations (ICLR2014),",
      "citeRegEx" : "49",
      "shortCiteRegEx" : "49",
      "year" : 2014
    }, {
      "title" : "Fundamental limits of online and distributed algorithms for statistical learning and estimation",
      "author" : [ "O. Shamir" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "50",
      "shortCiteRegEx" : "50",
      "year" : 2014
    }, {
      "title" : "Oscillation helps to get division right",
      "author" : [ "D.J. Sherratt" ],
      "venue" : "Proceedings of the National Academy of Sciences,",
      "citeRegEx" : "51",
      "shortCiteRegEx" : "51",
      "year" : 2016
    }, {
      "title" : "Mastering the game of go with deep neural networks and tree search",
      "author" : [ "D. Silver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre", "G. van den Driessche", "J. Schrittwieser", "I. Antonoglou", "V. Panneershelvam", "M. Lanctot" ],
      "venue" : null,
      "citeRegEx" : "52",
      "shortCiteRegEx" : "52",
      "year" : 2016
    }, {
      "title" : "An efficient implementation of vector clocks",
      "author" : [ "M. Singhal", "A. Kshemkalyani" ],
      "venue" : "Information Processing Letters,",
      "citeRegEx" : "53",
      "shortCiteRegEx" : "53",
      "year" : 1992
    }, {
      "title" : "Dropout: A simple way to prevent neural networks from overfitting",
      "author" : [ "N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "54",
      "shortCiteRegEx" : "54",
      "year" : 2014
    }, {
      "title" : "On the importance of initialization and momentum in deep learning",
      "author" : [ "I. Sutskever", "J. Martens", "G. Dahl", "G. Hinton" ],
      "venue" : "In ICML,",
      "citeRegEx" : "55",
      "shortCiteRegEx" : "55",
      "year" : 2013
    }, {
      "title" : "Convolutional neural networks with low-rank regularization",
      "author" : [ "C. Tai", "T. Xiao", "X. Wang", "W. E" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "56",
      "shortCiteRegEx" : "56",
      "year" : 2016
    }, {
      "title" : "Distributed asynchronous deterministic and stochastic gradient optimization algorithms",
      "author" : [ "J.N. Tsitsiklis", "D.P. Bertsekas", "M. Athans" ],
      "venue" : "American Control Conference,",
      "citeRegEx" : "57",
      "shortCiteRegEx" : "57",
      "year" : 1984
    }, {
      "title" : "Regularization of neural networks using dropconnect",
      "author" : [ "L. Wan", "M.D. Zeiler", "S. Zhang", "Y. LeCun", "R. Fergus" ],
      "venue" : "In ICML,",
      "citeRegEx" : "58",
      "shortCiteRegEx" : "58",
      "year" : 2013
    }, {
      "title" : "Distributed average consensus with least-meansquare deviation",
      "author" : [ "L. Xiao", "S. Boyd", "S.-J. Kim" ],
      "venue" : "Journal of Parallel and Distributed Computing,",
      "citeRegEx" : "59",
      "shortCiteRegEx" : "59",
      "year" : 2007
    }, {
      "title" : "Multi-gpu training of convnets",
      "author" : [ "O. Yadan", "K. Adams", "Y. Taigman", "M. Ranzato" ],
      "venue" : "In Arxiv,",
      "citeRegEx" : "60",
      "shortCiteRegEx" : "60",
      "year" : 2013
    }, {
      "title" : "Asynchronous distributed admm for consensus optimization",
      "author" : [ "R. Zhang", "J. Kwok" ],
      "venue" : "Proceedings of the 31st International Conference on Machine Learning",
      "citeRegEx" : "61",
      "shortCiteRegEx" : "61",
      "year" : 2014
    }, {
      "title" : "Staleness-aware async-sgd for distributed deep learning",
      "author" : [ "W. Zhang", "S. Gupta", "X. Lian", "J. Liu" ],
      "venue" : "arXiv preprint arXiv:1511.05950,",
      "citeRegEx" : "62",
      "shortCiteRegEx" : "62",
      "year" : 2015
    }, {
      "title" : "Parallelized stochastic gradient descent",
      "author" : [ "M. Zinkevich", "M. Weimer", "A. Smola", "L. Li" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "63",
      "shortCiteRegEx" : "63",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "34 as a function of the learning rate η ∈ (0, 1) and the number of workers p ∈ [1, 64].",
      "startOffset" : 79,
      "endOffset" : 86
    }, {
      "referenceID" : 0,
      "context" : "34 as a function of the learning rate η ∈ (0, 1) and the number of workers p ∈ [1, 64].",
      "startOffset" : 79,
      "endOffset" : 86
    }, {
      "referenceID" : 0,
      "context" : "34 as a function of the learning rate η ∈ (0, 1) and the number of workers p ∈ [1, 64].",
      "startOffset" : 79,
      "endOffset" : 86
    }, {
      "referenceID" : 0,
      "context" : "34 as a function of the learning rate η ∈ (0, 2) and the number of workers p ∈ [1, 64].",
      "startOffset" : 79,
      "endOffset" : 86
    }, {
      "referenceID" : 8,
      "context" : "The subject of this thesis is on how to parallelize the training of large deep learning models that use a form of stochastic gradient descent (SGD) [9].",
      "startOffset" : 148,
      "endOffset" : 151
    }, {
      "referenceID" : 29,
      "context" : "As an optimization method, it often exhibits fast initial convergence toward the local optimum as compared to the batch gradient method [30].",
      "startOffset" : 136,
      "endOffset" : 140
    }, {
      "referenceID" : 29,
      "context" : "As a learning algorithm, it often leads to better solutions in terms of the test accuracy [30, 23].",
      "startOffset" : 90,
      "endOffset" : 98
    }, {
      "referenceID" : 22,
      "context" : "As a learning algorithm, it often leads to better solutions in terms of the test accuracy [30, 23].",
      "startOffset" : 90,
      "endOffset" : 98
    }, {
      "referenceID" : 21,
      "context" : "However, as the size of the dataset explodes [22], the amount of time taken to go through each data point sequentially becomes prohibitive.",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 15,
      "context" : "There have been attempts to parallelize the training for large-scale deep learning models on thousands of CPUs, including the Google’s Distbelief system [16].",
      "startOffset" : 153,
      "endOffset" : 157
    }, {
      "referenceID" : 26,
      "context" : "neural networks trained on a few GPU cards sitting in a single computer [27, 49].",
      "startOffset" : 72,
      "endOffset" : 80
    }, {
      "referenceID" : 48,
      "context" : "neural networks trained on a few GPU cards sitting in a single computer [27, 49].",
      "startOffset" : 72,
      "endOffset" : 80
    }, {
      "referenceID" : 51,
      "context" : "To date, the AlphaGo system is trained using 50 GPUs for a few weeks [52].",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 39,
      "context" : "To solve such large-scale optimization problem, one line of research is to fill-in the gap between the stochastic gradient descent method (SGD) and the batch gradient method [40].",
      "startOffset" : 174,
      "endOffset" : 178
    }, {
      "referenceID" : 13,
      "context" : "sampling a subset of the data points to estimate the full gradient, is one possibility to fill-in this gap [14, 17, 25, 31].",
      "startOffset" : 107,
      "endOffset" : 123
    }, {
      "referenceID" : 16,
      "context" : "sampling a subset of the data points to estimate the full gradient, is one possibility to fill-in this gap [14, 17, 25, 31].",
      "startOffset" : 107,
      "endOffset" : 123
    }, {
      "referenceID" : 24,
      "context" : "sampling a subset of the data points to estimate the full gradient, is one possibility to fill-in this gap [14, 17, 25, 31].",
      "startOffset" : 107,
      "endOffset" : 123
    }, {
      "referenceID" : 30,
      "context" : "sampling a subset of the data points to estimate the full gradient, is one possibility to fill-in this gap [14, 17, 25, 31].",
      "startOffset" : 107,
      "endOffset" : 123
    }, {
      "referenceID" : 24,
      "context" : "However, to implement it efficiently requires very skillful engineering efforts in order to minimize the communication overhead, which is difficult in particular for training large deep learning models on GPUs [25, 15].",
      "startOffset" : 210,
      "endOffset" : 218
    }, {
      "referenceID" : 14,
      "context" : "However, to implement it efficiently requires very skillful engineering efforts in order to minimize the communication overhead, which is difficult in particular for training large deep learning models on GPUs [25, 15].",
      "startOffset" : 210,
      "endOffset" : 218
    }, {
      "referenceID" : 61,
      "context" : "It is even observed that in deep learning problems, using too large mini-batch size may lead to solutions of very poor test accuracy [62].",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 7,
      "context" : "Another possibility is to use the asynchronous stochastic gradient descent methods [8, 29, 63, 2, 46, 16].",
      "startOffset" : 83,
      "endOffset" : 105
    }, {
      "referenceID" : 28,
      "context" : "Another possibility is to use the asynchronous stochastic gradient descent methods [8, 29, 63, 2, 46, 16].",
      "startOffset" : 83,
      "endOffset" : 105
    }, {
      "referenceID" : 62,
      "context" : "Another possibility is to use the asynchronous stochastic gradient descent methods [8, 29, 63, 2, 46, 16].",
      "startOffset" : 83,
      "endOffset" : 105
    }, {
      "referenceID" : 1,
      "context" : "Another possibility is to use the asynchronous stochastic gradient descent methods [8, 29, 63, 2, 46, 16].",
      "startOffset" : 83,
      "endOffset" : 105
    }, {
      "referenceID" : 45,
      "context" : "Another possibility is to use the asynchronous stochastic gradient descent methods [8, 29, 63, 2, 46, 16].",
      "startOffset" : 83,
      "endOffset" : 105
    }, {
      "referenceID" : 15,
      "context" : "Another possibility is to use the asynchronous stochastic gradient descent methods [8, 29, 63, 2, 46, 16].",
      "startOffset" : 83,
      "endOffset" : 105
    }, {
      "referenceID" : 61,
      "context" : "The tradeoff is that asynchronous behavior results in large communication delay, which can in turn slow down the convergence rate [62].",
      "startOffset" : 130,
      "endOffset" : 134
    }, {
      "referenceID" : 15,
      "context" : "The DOWNPOUR method belongs to the above class of asynchronous SGD methods, and is proposed for training deep learning models [16] .",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 5,
      "context" : "This idea resembles the incremental gradient method [6].",
      "startOffset" : 52,
      "endOffset" : 55
    }, {
      "referenceID" : 33,
      "context" : "The mini-batch SGD and the asynchronous SGD methods that we have discussed so far can be implemented in a distributed computing environment [34, 26].",
      "startOffset" : 140,
      "endOffset" : 148
    }, {
      "referenceID" : 25,
      "context" : "The mini-batch SGD and the asynchronous SGD methods that we have discussed so far can be implemented in a distributed computing environment [34, 26].",
      "startOffset" : 140,
      "endOffset" : 148
    }, {
      "referenceID" : 56,
      "context" : "It is based on the idea of consensus averaging [57].",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 40,
      "context" : "clock synchronization) [41].",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 58,
      "context" : "In such setting, one needs to consider how to optimize the design of the averaging network [59] and to analyze the convergence rate on various networks subject to link failure [37, 19].",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 36,
      "context" : "In such setting, one needs to consider how to optimize the design of the averaging network [59] and to analyze the convergence rate on various networks subject to link failure [37, 19].",
      "startOffset" : 176,
      "endOffset" : 184
    }, {
      "referenceID" : 18,
      "context" : "In such setting, one needs to consider how to optimize the design of the averaging network [59] and to analyze the convergence rate on various networks subject to link failure [37, 19].",
      "startOffset" : 176,
      "endOffset" : 184
    }, {
      "referenceID" : 10,
      "context" : "The ADMM (Alternating Direction Method of Multipliers) [11] method can also be used to solve the consensus averaging problem above.",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 41,
      "context" : "ADMM is also generalized to the stochastic and the asynchronous setting for solving large-scale machine learning problems [42, 61].",
      "startOffset" : 122,
      "endOffset" : 130
    }, {
      "referenceID" : 60,
      "context" : "ADMM is also generalized to the stochastic and the asynchronous setting for solving large-scale machine learning problems [42, 61].",
      "startOffset" : 122,
      "endOffset" : 130
    }, {
      "referenceID" : 6,
      "context" : "2 Formalizing the problem Consider minimizing a function F (x) in a parallel computing environment [7] with p ∈ N workers and a master.",
      "startOffset" : 99,
      "endOffset" : 102
    }, {
      "referenceID" : 23,
      "context" : "The problem of the equivalence of these two objectives is studied in the literature and is known as the augmentability or the global variable consensus problem [24, 11].",
      "startOffset" : 160,
      "endOffset" : 168
    }, {
      "referenceID" : 10,
      "context" : "The problem of the equivalence of these two objectives is studied in the literature and is known as the augmentability or the global variable consensus problem [24, 11].",
      "startOffset" : 160,
      "endOffset" : 168
    }, {
      "referenceID" : 49,
      "context" : "We will focus on the problem of reducing the parameter communication overhead between the master and the local workers [50, 16, 60, 43, 48].",
      "startOffset" : 119,
      "endOffset" : 139
    }, {
      "referenceID" : 15,
      "context" : "We will focus on the problem of reducing the parameter communication overhead between the master and the local workers [50, 16, 60, 43, 48].",
      "startOffset" : 119,
      "endOffset" : 139
    }, {
      "referenceID" : 59,
      "context" : "We will focus on the problem of reducing the parameter communication overhead between the master and the local workers [50, 16, 60, 43, 48].",
      "startOffset" : 119,
      "endOffset" : 139
    }, {
      "referenceID" : 42,
      "context" : "We will focus on the problem of reducing the parameter communication overhead between the master and the local workers [50, 16, 60, 43, 48].",
      "startOffset" : 119,
      "endOffset" : 139
    }, {
      "referenceID" : 47,
      "context" : "We will focus on the problem of reducing the parameter communication overhead between the master and the local workers [50, 16, 60, 43, 48].",
      "startOffset" : 119,
      "endOffset" : 139
    }, {
      "referenceID" : 6,
      "context" : "when the data is distributed among the workers [7, 5] is a more general problem and is not addressed in this work.",
      "startOffset" : 47,
      "endOffset" : 53
    }, {
      "referenceID" : 4,
      "context" : "when the data is distributed among the workers [7, 5] is a more general problem and is not addressed in this work.",
      "startOffset" : 47,
      "endOffset" : 53
    }, {
      "referenceID" : 12,
      "context" : "We however emphasize that our problem setting is still highly non-trivial under the communication constraints due to the existence of many local optima [13].",
      "startOffset" : 152,
      "endOffset" : 156
    }, {
      "referenceID" : 32,
      "context" : "2 and the Moreau-Yosida regularization [33] in convex optimization used to smooth the non-smooth part of the objective function.",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 34,
      "context" : "When using the rectified linear units [35] in deep learning models, the objective function also becomes non-smooth.",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 39,
      "context" : "EASGD is motivated by the quadratic penalty method [40], but is re-interpreted as a parallelized extension of the averaging SGD method [45].",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 44,
      "context" : "EASGD is motivated by the quadratic penalty method [40], but is re-interpreted as a parallelized extension of the averaging SGD method [45].",
      "startOffset" : 135,
      "endOffset" : 139
    }, {
      "referenceID" : 24,
      "context" : "Thus in order to minimize the staleness [25] of the difference xt−x̃t between the center and the local variable for the asynchronous EASGD (described in Algorithm 1), the update for the master in Equation 2.",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 46,
      "context" : "4) as a Jacobi method [47].",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 15,
      "context" : "This approach differs from other settings explored in the literature [16, 4, 8, 36, 29, 2, 46, 63], and focus on how fast the center variable converges.",
      "startOffset" : 69,
      "endOffset" : 98
    }, {
      "referenceID" : 3,
      "context" : "This approach differs from other settings explored in the literature [16, 4, 8, 36, 29, 2, 46, 63], and focus on how fast the center variable converges.",
      "startOffset" : 69,
      "endOffset" : 98
    }, {
      "referenceID" : 7,
      "context" : "This approach differs from other settings explored in the literature [16, 4, 8, 36, 29, 2, 46, 63], and focus on how fast the center variable converges.",
      "startOffset" : 69,
      "endOffset" : 98
    }, {
      "referenceID" : 35,
      "context" : "This approach differs from other settings explored in the literature [16, 4, 8, 36, 29, 2, 46, 63], and focus on how fast the center variable converges.",
      "startOffset" : 69,
      "endOffset" : 98
    }, {
      "referenceID" : 28,
      "context" : "This approach differs from other settings explored in the literature [16, 4, 8, 36, 29, 2, 46, 63], and focus on how fast the center variable converges.",
      "startOffset" : 69,
      "endOffset" : 98
    }, {
      "referenceID" : 1,
      "context" : "This approach differs from other settings explored in the literature [16, 4, 8, 36, 29, 2, 46, 63], and focus on how fast the center variable converges.",
      "startOffset" : 69,
      "endOffset" : 98
    }, {
      "referenceID" : 45,
      "context" : "This approach differs from other settings explored in the literature [16, 4, 8, 36, 29, 2, 46, 63], and focus on how fast the center variable converges.",
      "startOffset" : 69,
      "endOffset" : 98
    }, {
      "referenceID" : 62,
      "context" : "This approach differs from other settings explored in the literature [16, 4, 8, 36, 29, 2, 46, 63], and focus on how fast the center variable converges.",
      "startOffset" : 69,
      "endOffset" : 98
    }, {
      "referenceID" : 6,
      "context" : "Note that the asynchronous behavior described above is partially asynchronous [7].",
      "startOffset" : 78,
      "endOffset" : 81
    }, {
      "referenceID" : 38,
      "context" : "It is based on the Nesterov ’s momentum scheme [39, 28, 55], where the update of the local worker of the form captured in Equation 2.",
      "startOffset" : 47,
      "endOffset" : 59
    }, {
      "referenceID" : 27,
      "context" : "It is based on the Nesterov ’s momentum scheme [39, 28, 55], where the update of the local worker of the form captured in Equation 2.",
      "startOffset" : 47,
      "endOffset" : 59
    }, {
      "referenceID" : 54,
      "context" : "It is based on the Nesterov ’s momentum scheme [39, 28, 55], where the update of the local worker of the form captured in Equation 2.",
      "startOffset" : 47,
      "endOffset" : 59
    }, {
      "referenceID" : 43,
      "context" : "In literature, there’s another well-known momentum variant called heavy-ball method (aka Polyak ’s method) [44].",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 20,
      "context" : "The analysis of its global convergence property is still a very challenging problem in convex optimization literature [21].",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 17,
      "context" : "One reason is that the momentum method has an error accumulation effect [18].",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 31,
      "context" : "Due to the stochastic noise in the gradient, using momentum can actually result in higher asymptotic variance (see [32] and our discussion in Section 5.",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 44,
      "context" : "Our analysis in the quadratic case extends the analysis of ASGD in [45].",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 44,
      "context" : "2) shows that EASGD algorithm achieves the highest possible rate of convergence when we consider the double averaging sequence (similarly to [45]) {z1, z2, .",
      "startOffset" : 141,
      "endOffset" : 145
    }, {
      "referenceID" : 44,
      "context" : "Note that this linear system has a degenerate noise Ξt which prevents us from directly applying results of [45].",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 37,
      "context" : "We start from the following estimate for the strongly convex function [38], 〈∇F (x)−∇F (y), x− y〉 ≥ μL μ+ L ‖x− y‖ + 1 μ+ L ‖∇F (x)−∇F (y)‖ .",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 28,
      "context" : "In this section we study the stability of the asynchronous EASGD and ADMM methods in the round-robin scheme [29].",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 10,
      "context" : "In our setting, the ADMM method [11, 61, 42] involves solving the following minimax",
      "startOffset" : 32,
      "endOffset" : 44
    }, {
      "referenceID" : 60,
      "context" : "In our setting, the ADMM method [11, 61, 42] involves solving the following minimax",
      "startOffset" : 32,
      "endOffset" : 44
    }, {
      "referenceID" : 41,
      "context" : "In our setting, the ADMM method [11, 61, 42] involves solving the following minimax",
      "startOffset" : 32,
      "endOffset" : 44
    }, {
      "referenceID" : 41,
      "context" : "At each t, we linearize the function F (xi) with F (xt)+ 〈 ∇F (xt), xi − xt 〉 + 1 2η ∥∥xi − xit∥∥2 as in [42].",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 10,
      "context" : "Note that since the step size for the dual ascent update is chosen to be ρ by convention [11, 61, 42], we have re-parametrized the Lagrangian multiplier to be λt ← λt/ρ in the above updates.",
      "startOffset" : 89,
      "endOffset" : 101
    }, {
      "referenceID" : 60,
      "context" : "Note that since the step size for the dual ascent update is chosen to be ρ by convention [11, 61, 42], we have re-parametrized the Lagrangian multiplier to be λt ← λt/ρ in the above updates.",
      "startOffset" : 89,
      "endOffset" : 101
    }, {
      "referenceID" : 41,
      "context" : "Note that since the step size for the dual ascent update is chosen to be ρ by convention [11, 61, 42], we have re-parametrized the Lagrangian multiplier to be λt ← λt/ρ in the above updates.",
      "startOffset" : 89,
      "endOffset" : 101
    }, {
      "referenceID" : 60,
      "context" : "The convergence analysis in [61] is based on the assumption that “At any master iteration, updates from the workers have the same probability of arriving at the master.",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 15,
      "context" : "All the parallel comparator methods are listed below1: • DOWNPOUR [16], the detail and the pseudo-code of the implementation are described in Section 4.",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 60,
      "context" : "• A method that we call MVADOWNPOUR, where we compute the moving average We have compared asynchronous ADMM [61] with EASGD in our setting as well, the performance is nearly the same.",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 8,
      "context" : "All the sequential comparator methods (p = 1) are listed below: • SGD [9] with constant learning rate η.",
      "startOffset" : 70,
      "endOffset" : 73
    }, {
      "referenceID" : 54,
      "context" : "• Momentum SGD (MSGD) [55] with constant (Nesterov’s) momentum rate δ.",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 44,
      "context" : "• ASGD [45] with moving rate αt+1 = 1 t+1 .",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 44,
      "context" : "• MVASGD [45] with moving rate α set to a constant.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 15,
      "context" : "The center variable of the master is stored and updated on the centralized parameter server [16].",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 34,
      "context" : "[35]), P denotes the max pooling operator, L denotes the linear operator and D denotes the dropout operator with rate equal to 0.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 48,
      "context" : "For the ImageNet experiment we use the similar approach to [49] with the following 11-layer convolutional neural network: (3, 221, 221) C,R −−−−−→ (7,7,2,2) (96, 108, 108) P −−−−−→ (3,3,3,3) (96, 36, 36) C,R −−−−−→ (5,5,1,1) (256, 32, 32) P −−−−−→ (2,2,2,2) (256, 16, 16) C,R −−−−−→ (3,3,1,1) (384, 14, 14) C,R −−−−−→ (2,2,1,1) (384, 13, 13) C,R −−−−−→ (2,2,1,1) (256, 12, 12) P −−−−−→ (2,2,2,2) (256, 6, 6) L,R,D −−−−→ 0.",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 57,
      "context" : "For the CIFAR experiment we use the similar approach to [58] with the following 7-layer convolutional neural network: (3, 28, 28) C,R −−−−−→ (5,5,1,1) (64, 24, 24) P −−−−−→ (2,2,2,2) (64, 12, 12) C,R −−−−−→ (5,5,1,1) (128, 8, 8) P −−−−−→ (2,2,2,2) (128, 4, 4) C,R −−−−−→ (3,3,1,1) (64, 2, 2) L,R,D −−−−→ 0.",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 53,
      "context" : "5 [54].",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 0,
      "context" : "We also re-scale each pixel value to the interval [0, 1].",
      "startOffset" : 50,
      "endOffset" : 56
    }, {
      "referenceID" : 0,
      "context" : "As before, we re-scale each pixel value to the interval [0, 1].",
      "startOffset" : 56,
      "endOffset" : 62
    }, {
      "referenceID" : 9,
      "context" : "Notice that each data loader cycles5 through the Its advantage is observed in [10].",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 19,
      "context" : "We remark that if the stochastic gradient is sparse, DOWNPOUR empirically performs well with large communication period [20].",
      "startOffset" : 120,
      "endOffset" : 124
    }, {
      "referenceID" : 11,
      "context" : "Thus our initial learning rate is decreased twice over time, by a factor of 5 and then 2, when we observe that the online predictive loss [12] stagnates.",
      "startOffset" : 138,
      "endOffset" : 142
    }, {
      "referenceID" : 15,
      "context" : "Notice that we do not use any adaptive learning scheme as having been done in [16].",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 31,
      "context" : "2 by an Ornstein Uhlenbeck process [32] as follows,",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 43,
      "context" : "In case that δh is chosen independently of h, the update is no different to the heavy ball method [44].",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 31,
      "context" : "Moreover, as in [32], we can try to minimize |z3| with respect to δ such that the rate of convergence of the second order moment is maximized for a given η.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 31,
      "context" : "δ → 1 [32].",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 27,
      "context" : "This is consistent with the choice of the learning rate and the momentum rate scheduling in the Nesterov’s optimal methods in literature [28].",
      "startOffset" : 137,
      "endOffset" : 141
    }, {
      "referenceID" : 31,
      "context" : "24 by a Geometric Brownian motion [32] as follows,",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 0,
      "context" : "34 as a function of the learning rate η ∈ (0, 1) and the number of workers p ∈ [1, 64].",
      "startOffset" : 79,
      "endOffset" : 86
    }, {
      "referenceID" : 0,
      "context" : "34 as a function of the learning rate η ∈ (0, 1) and the number of workers p ∈ [1, 64].",
      "startOffset" : 79,
      "endOffset" : 86
    }, {
      "referenceID" : 0,
      "context" : "34 as a function of the learning rate η ∈ (0, 1) and the number of workers p ∈ [1, 64].",
      "startOffset" : 79,
      "endOffset" : 86
    }, {
      "referenceID" : 0,
      "context" : "34 as a function of the learning rate η ∈ (0, 2) and the number of workers p ∈ [1, 64].",
      "startOffset" : 79,
      "endOffset" : 86
    }, {
      "referenceID" : 1,
      "context" : "For example, the tree was used to scale up the asynchronous SGD method by aggregating the delayed gradients computed by the intermediate nodes and the leaf nodes [2].",
      "startOffset" : 162,
      "endOffset" : 165
    }, {
      "referenceID" : 0,
      "context" : "It can also be used to efficiently implement the Broadcast/AllReduce operation as in MPI [1, 62].",
      "startOffset" : 89,
      "endOffset" : 96
    }, {
      "referenceID" : 61,
      "context" : "It can also be used to efficiently implement the Broadcast/AllReduce operation as in MPI [1, 62].",
      "startOffset" : 89,
      "endOffset" : 96
    }, {
      "referenceID" : 55,
      "context" : "We report the results based on CIFAR-10 dataset using a low-rank convolutional neural-network model [56].",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 55,
      "context" : "As in [56], we center the input pixel values by the mean and the standard deviation for each of the three channels using the training data.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 2,
      "context" : "• The connection between EAMSGD and Katyusha [3].",
      "startOffset" : 45,
      "endOffset" : 48
    }, {
      "referenceID" : 56,
      "context" : "The basic tool [57] based on the mean-field method (i.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 52,
      "context" : "• The description of the asynchronous behavior using vector-clock [53].",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 53,
      "context" : "Looking back to the Dropout [54] and DropConnect [58] regularization, is there any deeper reason in common [51]?",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 57,
      "context" : "Looking back to the Dropout [54] and DropConnect [58] regularization, is there any deeper reason in common [51]?",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 50,
      "context" : "Looking back to the Dropout [54] and DropConnect [58] regularization, is there any deeper reason in common [51]?",
      "startOffset" : 107,
      "endOffset" : 111
    } ],
    "year" : 2016,
    "abstractText" : "We study the problem of how to distribute the training of large-scale deep learning models in the parallel computing environment. We propose a new distributed stochastic optimization method called Elastic Averaging SGD (EASGD). We analyze the convergence rate of the EASGD method in the synchronous scenario and compare its stability condition with the existing ADMM method in the round-robin scheme. An asynchronous and momentum variant of the EASGD method is applied to train deep convolutional neural networks for image classification on the CIFAR and ImageNet datasets. Our approach accelerates the training and furthermore achieves better test accuracy. It also requires a much smaller amount of communication than other common baseline approaches such as the DOWNPOUR method. We then investigate the limit in speedup of the initial and the asymptotic phase of the mini-batch SGD, the momentum SGD, and the EASGD methods. We find that the spread of the input data distribution has a big impact on their initial convergence rate and stability region. We also find a surprising connection between the momentum SGD and the EASGD method with a negative moving average rate. A non-convex case is also studied to understand when EASGD can get trapped by a saddle point. Finally, we scale up the EASGD method by using a tree structured network topology. We show empirically its advantage and challenge. We also establish a connection between the EASGD and the DOWNPOUR method with the classical Jacobi and the Gauss-Seidel method, thus unifying a class of distributed stochastic optimization methods.",
    "creator" : "LaTeX with hyperref package"
  }
}