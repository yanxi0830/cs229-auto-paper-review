{
  "name" : "1701.07953.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "The Price of Differential Privacy For Online Learning",
    "authors" : [ "Naman Agarwal", "Karan Singh" ],
    "emails" : [ "namana@cs.princeton.edu,", "karans@cs.princeton.edu," ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 1.\n07 95\n3v 1\n[ cs\n.L G\n] 2\n7 Ja\nn 20\n√ T )1 regret bounds. In the full-information\nsetting, our results demonstrate that (ε, δ)-differential privacy may be ensured for free – in particular, the regret bounds scale as O( √ T ) + Õ (\n1 ε log 1 δ\n)\n. For bandit linear optimization, and as a special case, for non-stochastic multi-armed bandits, the proposed algorithm achieves a regret of O (√\nT log T ε log 1 δ\n) , while the previously best known bound was Õ ( T 3 4\nε\n)\n."
    }, {
      "heading" : "1 Introduction",
      "text" : "In the paradigm of online learning, a learning algorithm makes a sequence of predictions given the (possibly incomplete) knowledge of the correct answers for the past queries. In contrast to statistical learning, online learning algorithms typically offer distribution-free guarantees, that is no distributional assumption is made on the environment. Consequently, online learning algorithms are well suited to dynamic and adversarial environments, where real-time learning from changing data is essential. While statistical (batch) learning algorithms produce a single predictor as the output, an online learning algorithmmust output a predictor for every time step. As a result, ensuring differential privacy of every data point for online learning is challenging because the change in the data point supplied to the algorithm at the tth time step alters the prediction for all time steps that follow. In this paper, we design differentially private algorithms for online linear optimization with near-optimal regret, both in the full information and partial information (bandit) settings. This result improves the best known regret bounds for a number of important online learning problems – including prediction from expert advice and non-stochastic multi-armed bandits."
    }, {
      "heading" : "1.1 Full-Information Setting: Differential Privacy for Free",
      "text" : "For the full-information setting, we design (ε, δ)-differentially private algorithms with regret bounds that scale as O( √ T ) + Õ (\n1 ε log 1 δ\n)\n, resolving an open question posed in [TS13]. A decomposition of the bound on the regret of this form implies that when ε ≥ 1√\nT we have no\nadditional regret, i.e. differential privacy is free. Moreover even when ε ≤ 1√ T we show a sub constant regret per round as compared to the vacuous constant regret per round guarantee provided by existing results.\n∗namana@cs.princeton.edu, Computer Science, Princeton University †karans@cs.princeton.edu, Computer Science, Princeton University 1Here the Õ(·) notation hides polylog(T ) factors.\nAs a concrete example, for the setting of prediction from expert advice with N experts and\nT rounds of play, we show that it is possible to achieve Õ (√\nN logN ε log 1 δ +\n√ T logN ) regret\nwhile promising (ε, δ)-differential privacy. When ε = ω ( √\nN T log 1 δ\n)\n, the leading term of the\nregret bound for the proposed algorithm is same as that for the best possible non-private regret bound of O( √ T logN) (upto polylog(T ) factors). In contrast, the previously best known regret bound for the problem was Õ (√\nNT ε\n)\n[TS13]. Our result improves on the previous known both\nin terms of dependency on the differential privacy parameter ε and the number of experts N . While the previous works[JKT12, TS13] approach the question of regret minimization using the follow-the-regularized-leader (FTRL) paradigm, our algorithm is based on the follow-theperturbed-leader (FTPL) algorithm [KV05], where we demonstrate that the noise required to achieve low regret also ensures differential privacy and vice versa. This novel observation permits our analysis to decompose the regret bound in the desired form."
    }, {
      "heading" : "1.2 Bandit Feedback: Reduction to the Non-private Setting",
      "text" : "In the partial-information setting of bandit feedback, the online learning algorithm only gets to observe the loss (or the reward) of the prediction it prescribed. We outline a reduction technique that translates a non-private bandit algorithm to a differentially private bandit algorithm, while retaining the Õ( √ T ) dependency of the regret bound on the number of rounds of play for a number of important problems. This allows us to derive the first (ε, δ)-differentially private algorithm for bandit linear optimization, using the algorithm for the non-private problem from [AHR12]. This positively answers a question from [TS13] asking if Õ( √ T ) regret is attainable for differentially private linear bandits. An important case of the general bandit linear optimization framework is the non-stochastic multi-armed bandits problem[BCB+12], with applications for website optimization, personalized medicine, advertisement placement and recommendation systems. Here, the proposed (ε, δ)differentially private algorithm (for N arms) enjoys a regret of Õ (√\nNT logN ε log 1 δ\n)\n, improving\non the previously best attainable regret of Õ ( NT 3 4\nε\n)\n[TS13]."
    }, {
      "heading" : "2 Model and Preliminaries",
      "text" : "This section introduces the model of online (linear) learning, the distinction between full and partial feedback scenarios, and the notion of differential privacy in this model.\nFull information Setting: Online linear optimization [H+16, SS11] involves repeated decision making over T rounds of play. At the beginning of every round (say round t), the algorithm chooses a point in xt ∈ X , where X ⊆ RN is a (compact) convex set. Subsequently, it observes the loss lt ∈ Y ⊆ RN and suffers a loss of 〈lt, xt〉. The measure of success of such an algorithm, after T rounds of play, is defined though regret, defined as\nRegret(T ) = E\n[ T ∑\nt=1\n〈lt, xt〉 −min x∈K\nT ∑\nt=1\n〈lt, x〉 ]\nwhere the expectation is over the randomness of the algorithm. In particular, achieving a sub-linear regret (o(T )) corresponds to doing almost as good (averaging across T rounds) as the fixed decision with the least loss in hindsight. In the non-private setting, a number of algorithms have been devised to achieve O( √ T ) regret, with additional dependence on the properties of the specific decision X and loss Y sets. (See [H+16] for a survey of results.) An important special case of the above framework is prediction with expert advice. Here the underlying decision is the simplex X = ∆N = {x ∈ Rn : xi ≥ 0, ∑n\ni=1 xi = 1} and the loss vectors are constrained to the unit cube Y = {lt ∈ RN : ‖lt‖∞ ≤ 1}. The role of the algorithm in this case can be interpreted as picking one of the N experts to follow it ∈ [N ] for every round of play. Also lt ∈ RN can be viewed as the loss on round t corresponding to each expert. In each round, the algorithm suffers a loss that equals the loss of the expert it chose to follow in that round.\nPartial Information Setting: In the setting of bandit feedback, the critical difference is that the algorithm only gets to observe the value 〈lt, xt〉, in contrast to the complete loss vector lt ∈ RN in the full information scenario. Therefore, the only feedback the algorithm receives is the value of the loss it incurs for the decision it takes. This makes designing algorithms for this feedback model challenging. Nevertheless for the general problem of bandit online optimization, [AHR08] introduced a computationally efficient algorithm that achieves an optimal dependence of the incurred regret of O( √ T ) on the number of rounds of play.\nThe non-stochastic multi-armed bandit problem is the bandit version of the prediction with expert advice framework – namely, in every round, the algorithm only observes the loss it incurs.\nDifferential Privacy: Differential Privacy[DMNS06] is a rigorous framework for establishing guarantees on privacy loss, that admits a number of desirable properties such as graceful degradation of guarantees under composition and robustness to linkage acts [DR+14].\nDefinition 2.1 ((ε, δ)-Differential Privacy). A randomized online learning algorithm A on the action set X and the loss set Y is (ε, δ)-differentially private if for any two sequence of loss vectors L = (l1, . . . lT ) ⊆ YT and L′ = (l′1, . . . l′T ) ⊆ YT differing in at most one vector – that is to say ∃t0 ∈ [T ], ∀t ∈ [T ]− {t0}, lt = l′t – for all S ⊆ X T , it holds that\nP(A(L) ∈ S) ≤ eεP(A(L′) ∈ S) + δ (1)\nFor an algorithm to satisfy the above stated definition is to certify that it is not possible to meaningfully infer the presence or absence of a specific loss vector from the sequence of outputs produced by the algorithm.\nNotation: We define ‖Y‖p = max{‖lt‖p : lt ∈ Y}, ‖X‖p = max{‖x‖p : x ∈ X}, and M = maxl∈Y,x∈X |〈l, x〉|, where ‖ · ‖p is the lp norm. By Holder’s inequality, it is easy to see that M ≤ ‖Y‖p‖X‖q for all p, q ≥ 1 with 1p + 1q = 1."
    }, {
      "heading" : "3 FTPL-based Algorithms: Differential Privacy for Free",
      "text" : "In this section, we outline algorithms based on the follow-the-perturbed-leader template[KV05]. FTPL-based algorithms ensure low-regret by perturbing the cumulative sum of loss vectors with noise from a suitably chosen distribution. We show that the noise added in the process of FTPL is sufficient to ensure differential privacy. More concretely, for the full-information setting, we establish that the regret guarantees obtained scale as O( √ T ) + Õ(1ε log 1 δ ).\nAlgorithm 1 FTPL Template for OLO – A(D, T ) on the action set X , the loss set Y. 1: Initialize an empty binary tree B to compute differentially private estimates of\n∑t s=1 ls.\n2: Sample n10, . . . n ⌈log T ⌉ 0 independently from D. 3: L̃0 ← ∑⌈log T⌉ i=1 n i 0. 4: for t = 1 to T do 5: Choose xt = argminx∈X 〈x, L̃t−1〉. 6: Observe the loss vector lt ∈ Y, and suffer a loss of 〈lt, xt〉. 7: (L̃t, B) ← TreeBasedAggregation(lt, B, t,D, T ). 8: end for\nAlgorithm 2 TreeBasedAggregation(lt, B, t,D, T ) 1: (L̃′t, B) ← PrivateSum(lt, B, t,D, T ) – Algorithm 5 ([JKT12]) with the noise added at each\nnode – be it internal or leaf – sampled independently from the distribution D. 2: st ← the binary representation of t as a string. 3: Find the minimum set S of already populated nodes in B that can compute ∑ts=1 ls. 4: Define (and note) Q = |S| ≤ ⌈log T ⌉. Define rt = ⌈log T ⌉ −Q. 5: Sample n1t , . . . n rt t independently from D. 6: L̃t ← L̃′t + ∑rt i=1 n i t. 7: Output: (L̃t, B)."
    }, {
      "heading" : "3.1 Proof of (ε, δ)-Differential Privacy",
      "text" : "To make formal claims about the quality of privacy, we ensure input differential privacy for the algorithm – that is, we ensure that the sequence of all partial sums of the loss vectors\n( ∑t s=1 ls : t ∈ [T ]) is (ε, δ)-differentially private. This certifies that the sequence of choices made by the algorithm (across all T rounds of play) (it : t ∈ [T ]) is (ε, δ)-differentially private.\nTheorem 3.1 (Privacy Guarantees with Gaussian Noise). Choose any σ2 ≥ ‖Y‖ 2 2\nε2 log 2 T log2 log Tδ .\nWhen Algorithm 1 A(D, T ) is run with D = N (0, σ2IN ), the following claims hold true: • Privacy: The sequence (L̃t : t ∈ [T ]) is (ε, δ)-differentially private. • Distribution: ∀t ∈ [T ], L̃t ∼ N ( ∑t s=1 ls, ⌈logT ⌉σ2IN ).\nProof. By Theorem 9 ([JKT12]), we have that the sequence (L̃′t : t ∈ [T ]) is (ε, δ)-differentially private. Now the sequence (L̃t : t ∈ [T ]) is (ε, δ)-differentially private because differential privacy is immune to post-processing[DR+14].\nNote that the PrivateSum algorithm adds exactly |S| independent draws from the distribution D to ∑ts=1 ls, where S is the minimum set of already populated nodes in the tree that can compute the required prefix sum. Due to Line 6, it is made certain that every prefix sum released is a sum of the true prefix sum and ⌈logT ⌉ independent draws from D.\nFinally, it suffices to note that the outputs of Algorithm 1 are strictly determined by the prefix sum estimates produced by TreeBasedAggregation. Again by post-processing theorem, (ε, δ)-differential privacy of Algorithm 1 is guaranteed."
    }, {
      "heading" : "3.2 Regret Bounds for FTPL",
      "text" : "While follow-the-perturbed-leader[KV05] has been typically analyzed subject to either uniform or expoenetial noise, recently [ALST14] presented the analysis of FTPL-based algorithms in the form of a general framework that accommodates varied noise structures. For a number of subcases of online linear optimization, with different X and Y, [ALST14] established that Gaussian noise achieves minimax optimal regret (upto multiplicative constants).\nWe utilize these results from [ALST14] to prove low-regret bounds for Algorithm 1. We make a couple of observations before applying these results. Firstly, Algorithm 1 calls the argmin oracle on the perturbed estimates of\n∑t s=1 ls, namely L̃t. Since these are distributed\nas L̃′t ∼ N ( ∑t s=1 ls, ⌈logT ⌉σ2IN), this is equivalent to executing Algorithm 1 ([ALST14]) with Gaussian smoothing with the smoothing parameter η = σ √ logT . This realization allows us to measure regret with respect to the true loss vectors, in spite of the proposed algorithm operating on perturbed estimates of the prefix sums. Secondly, note that the perturbations added to the prefix sums will (almost surely) change every round, even while being drawn from a time-invariant distribution. In typical instantiations of the FTPL framework, the noise added is either same over time[KV05] or is sampled independently every round[HP05]. In our setting, neither of the statements hold – since randomness introduced at any internal node in the PrivateSum algorithm can be reused across multiple rounds – hence, from the point of view of the FTPL algorithm, the noises sampled across time steps demonstrate some non-zero, yet non-trivial correlations. However, it is crucial to observe that, for the analysis of [ALST14] (in fact, for [KV05] too) to succeed, it is sufficient that the noise is sampled from the same distribution (not necessarily independently) for all time steps.\nRemark 3.2. In fact, the requirement that the noise is sampled from the same distribution can be relaxed too. Using a different magnitude of distribution in each round corresponds to a time-variant smoothing parameter. The results from [ALST14] hold in that setting as well. We do not delve into this point here to ease the presentation.\nTheorem 3.3 (Prediction with Expert Advice.). For the setting of prediction with expert advice, X = ∆N = {x ∈ Rn : xi ≥ 0, ∑n i=1 xi = 1} and Y = {lt ∈ Rn : ‖lt‖∞ ≤ 1}. Choosing\nσ = max{ √\nT log T , √ N ε logT log log T δ } and D = N (0, σ2IN ), we have\nRegretA(D,T )(T ) ≤ O ( √ T logN +\n√ N logN\nε log1.5 T log\nlogT\nδ\n)\n(2)\nProof. Due to Theorem 8 ([ALST14]), it holds that\nRegretA(D,T )(T ) ≤ √ logN\n(\nσ √ logT + T\nσ √ logT\n)\n(3)\nSubstituting the proposed value of σ, we obtain the stated claim.\nTheorem 3.4 (OLO over Euclidean Balls.). For the setting of online linear optimization over euclidean balls, X = {x ∈ Rn : ‖x‖2 ≤ 1} and Y = {lt ∈ Rn : ‖lt‖2 ≤ 1}. Choosing σ = max{ √\nT 2N log T , 1 ε logT log log T δ } and D = N (0, σ2IN ), we have\nRegretA(D,T )(T ) ≤ O ( √ T +\n√ N\nε log1.5 T log\nlogT\nδ\n)\n(4)\nProof. Due to Theorem 11 ([ALST14]), it holds that\nRegretA(D,T )(T ) ≤ σ √ N logT + T\n2σ √ N logT\n(5)\nSubstituting the proposed value of σ, we obtain the stated claim.\nRemark 3.5. In this case – online linear optimization over euclidean balls, it is in fact possible to show that as long as √\nT 2N log T ≥ 1ε logT log log T δ , the proposed algorithm achieves the exact\nminimax optimal regret. See Section 4.2 ([ALST14]) for more details.\nTheorem 3.6 (General OLO). Let ‖X‖2 = supx∈X ‖x‖2 and ‖Y‖2 = suplt∈Y ‖lt‖2. Choosing σ = max{‖Y‖2 √\nT√ N log T , √ N ε logT log log T δ } and D = N (0, σ2IN), we have\nRegretA(D,T )(T ) ≤ O ( N 1 4 ‖X‖2‖Y‖2 √ T + N‖X‖2 ε log1.5 T log logT δ )\n(6)\nProof. As a consequence of Corollary 4 ([ALST14]) and Lemma 13 ([ALST14]), it is true that\nRegretA(D,T )(T ) ≤ σ √ N logT‖X‖2 + T ‖X‖2‖Y‖22 2σ √ logT\n(7)\nSubstituting the proposed value of σ, we obtain the stated claim.\nWhile Theorem 3.6 is valid for all instances of online linear optimization and achieves O( √ T )\nregret, it yields sub-optimal dependence on the dimension of the problem.\nAlgorithm 3 A′(A,D) – Reduction to the Non-private Setting for Bandit Feedback Input: Online Algorithm A, Noise Distribution D. 1: for t = 0 to T do 2: Receive x̃t ∈ X from A and output x̃t. 3: Receive a loss value 〈lt, x̃t〉 from the adversary. 4: Sample Zt ∼ D. 5: Forward 〈lt x̃t〉+ 〈Zt, x̃t〉 as input to A. 6: end for"
    }, {
      "heading" : "4 Bandit Feedback: Reduction to the Non-private Setting",
      "text" : "We begin by describing an algorithmic reduction that takes as input a non-private bandit algorithm and translates it into an algorithm for (ε, δ)-differentially private bandit algorithm. The reduction works in a straight-forward manner by adding the requisite magnitude of Gaussian noise to ensure differential privacy. For the rest of this section for ease of exposition we will assume that both T and N are sufficiently large.\nTheorem 4.1 (Privacy Guarantees). Let it be that each loss vector lt is constrained to be within the set Y ⊆ RN , such that maxt,l∈Y | 〈l,x̃t〉‖x̃t‖∞ | ≤ B. For D = N (0, σ 2 IN ) where σ 2 = B 2 ε2 log 2 1 δ , the sequence of outputs (x̃t : t ∈ [T ]) produced by the Algorithm A′(A,D) is (ε, δ)-differentially private.\nProof. Consider a pair of sequence of loss vectors that differ at exactly one time step – say L = (l1, . . . lt0 . . . , lT ) and L\n′ = (l1, . . . , l′t0 , . . . lT ). Since the prediction of produced by the algorithm at time step any time t can only depend on the loss vectors in the past (l1, . . . lt−1), it is clear that the distribution of the output of the algorithm for the first t0 rounds (x̃1, . . . x̃t0) is unaltered. We claim that ∀I ⊆ R, it holds that\nP(〈lt0 + Zt0 , x̃t0〉 ∈ I) ≤ eεP(〈l′t0 + Zt0 , x̃t0〉 ∈ I) + δ\nBefore we justify the claim, let us see how this implies that desired statement. Too see this, note that conditioned on the value fed to the inner algorithm A at time t0, the distribution of all outputs produced by the algorithm are completely determined since the feedback to the algorithm at other time steps (discounting t0) stays the same (in distribution). By the above discussion, it is sufficient to demonstrate (ε, δ)-differential privacy for each input fed (as feedback) to the algorithm A.\nFor the sake of analysis, define lFictt as follows. If x̃t = 0, define l Fict t = 0 ∈ RN . Else, define lFictt ∈ RN to be such that (lFictt )i = 〈lt,x̃t〉x̃i if and only if i = argmaxi∈[d]|x̃i| and 0 otherwise, where argmax breaks ties arbitrarily. Define l̃Fictt = l Fict t + Zt. Now note that 〈l̃Fictt , x̃t〉 = 〈lt x̃t〉+ 〈Zt, x̃t〉. It suffices to establish that each l̃Fictt is ε-differentially private. To argue for this, note that Gaussian mechanism[DR+14] ensures the same, since the l2 norm of l̃ Fict t is bounded by B.\nWe show that the regret of the overall algorithm A′ of the true loss vectors is, in expectation, same as that of the regret of the inner algorithm A on some perturbed version of loss vectors. Theorem 4.2 (Noisy Online Optimization). Consider a loss sequence (l1 . . . lT ) and a convex set X . Define a perturbed version of the sequence as random vectors (l̃t : t ∈ [T ]) as l̃t = lt +Zt where Zt is a random vector such that {Z1, . . . Zt} are independent and E[Zt] = 0 for all t ∈ [T ].\nLet A be a full information (or bandit) online algorithm which outputs a sequence (x̃t ∈ X : t ∈ [T ]) and takes as input l̃t (respectively 〈l̃t, x̃t〉) at time t. Let x∗ ∈ K be a fixed point in the\nconvex set. Then we have that\nE{Zt}\n[\nEA\n[\nT ∑\nt=1\n(〈lt, x̃t〉 − 〈lt, x∗〉) ]] = E{Zt} [ EA [ T ∑\nt=1\n( 〈l̃t.x̃t〉 − 〈l̃t, x∗〉 )\n]]\nWe prove the Theorem in the Appendix in Section A."
    }, {
      "heading" : "4.1 Differentially Private Bandit Linear Optimization",
      "text" : "To obtain ε-differentially private algorithms for Bandit Linear Optimization, we use the SCRiBLe algorithm from[AHR12] as the inner algorithm A. Before we state this theorem, note that maxt,l∈Y | 〈l,x̃t〉‖x̃t‖∞ | ≤ ‖Y‖1 by Holder’s inequality. Therefore, with σ 2 = ‖Y‖2 1 ε2 log 2 1 δ , choosing D = N (0, σ2IN ) ensures (ε, δ)-differential privacy. Theorem 4.3 (Bandit Linear Optimization). Let X ⊆ RN be a convex set. Fix loss vectors (l1, . . . lT ) such that maxt,x∈X |〈lt, x〉| ≤ M . We have that Algorithm 3 when run with parameters D = N (0, σ2IN ) (with σ2 = ‖Y‖ 2 1 ε2 log 2 1\nδ ) and algorithm A = SCRiBLe[AHR12] with step parameter η = √\nν log T 2N2T (M2+σ2N‖X‖2 2 ) we have the following guarantees that the regret of the\nalgorithm is bounded by\nO\n(\n√ T logT √ N2ν(M2 + σ2N‖X‖22) ) = O\n(\n√\nT logT\n√\nN2ν\n( M2 + ‖Y‖21 ε2 log2 1 δ N‖X‖22 )\n)\nBefore delving into the proof we collect some necessary lemmas/theorems. The following is a simple restatement of Theorem 5.1 in [AHR12].\nTheorem 4.4 (Theorem 5.1 in [AHR12]). Fix a loss sequence l1, . . . lT . Let X be a compact convex set and R be a ν-self concordant barrier on X . Assume maxt,x∈X |〈lt, x〉| ≤ M . Setting η ≤ 14NM then the regret of the SCRiBLe algorithm is bounded by\nRegretSCRiBLe ≤ 2η ∑ N〈lt, xt〉2 + νη−1 logT + 2M\nwhere (xt ∈ X : t ∈ [T ]) is the sequence of points played by the algorithm. We now prove Theorem 4.3.\nProof of Theorem 4.3. For the purpose of analysis we define the following pseudo loss vectors l̃t = lt + Zt, where by definition Z ∼ N (0, σ2IN ). Further note that the following (which is a direct consequence of Fact B.2 proved in the appendix) holds for Zt ∼ N (0, σ2IN )\nP(‖Zt‖22 ≥ 10σ2N logT ) ≤ 1\nT 2\nTherefore taking a union bound we get that\nP(∃t ‖Zt‖22 ≥ 10σ2N logT ) ≤ 1\nT (8)\nSince the conditions of Theorem 4.2 are satisfied, we have that\nE[Regret] = E{Zt}\n[\nEA\n[\nT ∑\nt=1\n( 〈l̃t, x̃t〉 − 〈l̃t, x∗〉 )\n]]\nTherefore we can analyze the second term as a proxy for true regret. We now wish to apply Theorem 4.4 on the term inside the expectation treating l̃t as the true losses. However,\nTheorem 4.4 does not apply to loss functions that can be arbitrarily large, therefore we condition the above expectation on the following event F = {∃t ‖Zt‖22 ≥ 10σN logT }. We have from (8) that P(F ) ≤ 1T . We now have that\nE[Regret] ≤ E[Regret|F̄ ] + P(F )E[Regret|F ]\nSince the regret is always bounded by T we get that the second term above is at most 1. Therefore we will concern ourselves with bounding the first term above. For the rest of the section we will assume the conditioning on the event F̄ . First note that since the noise vectors Zt were independent to begin with they still remain conditionally independent even when conditioned on the event F̄ . The following two statements can be seen to follow easily.\n∀t E[Zt|F ] = 0 (9)\n∀t E[‖Zt‖22|F̄ ] ≤ E[‖Zt‖22] = σ2N (10) It follows from Equation 9 that Theorem 4.2 applies even when the noise is sampled from N (0, σ2IN ) conditioned on the event F̄ . Therefore we have that\nE[Regret|F̄ ] = E{Zt} [ EA [ T ∑\nt=1\n( 〈l̃t, x̃t〉 − 〈l̃t, x∗〉 )\n]\n∣ ∣ ∣ ∣ F̄\n]\n(11)\nNow note that since due to the conditioning ‖Zt‖2 ≤ 10σ2N logT and therefore we have that\nL , maxt,x∈X |〈Zt, x〉| ≤ 4σ‖X‖2 √ N logT .\nIt can now be verified that η ≤ 14NL . Therefore we can apply Theorem 4.4 obtain that\nE[Regret|F̄ ] = E{Zt} [ EA [ T ∑\nt=1\n( 〈l̃t, x̃t〉 − 〈l̃t, x∗〉 )\n]\n∣ ∣ ∣ ∣ F̄\n]\nEquation 11\n≤ E{Zt} [ 2η ∑ N2|〈l̃t, x̃t〉|2 + νη−1 logT + 2(M + L) ∣ ∣ ∣\n∣\nF̄\n]\nTheorem 4.4\n≤ E{Zt} [ 2η ∑ N2(|〈lt, x̃t〉|2 + ‖Zt‖22‖x̃t‖22) + νη−1 logT + 2(M + L) ∣ ∣ ∣\n∣\nF̄\n]\n≤ 2ηTN2(M2 + σ2N‖X‖22) + νη−1 logT + 8σ‖X‖2 √ n logT + 2M Equation 10 ≤ O ( √ T logT √ N2ν(M2 + σ2N‖X‖22) )"
    }, {
      "heading" : "4.2 Differentially Private Multi-Armed Bandits",
      "text" : "To begin with, we note that maxt,l∈Y | 〈l,x̃t〉‖x̃t‖∞ | ≤ ‖Y‖∞ ≤ 1 as x̃t ∈ {ei : i ∈ [N ]}. Therefore, setting σ2 = 1ε2 log 1 δ is sufficient to ensure differential privacy.\nAs can be readily seen, Theorem 4.3 does not guarantee optimal regret for Multi-Armed Bandits in terms of the dependence on the dimension as we would like to get a √ N dependence on the dimension. The natural idea would be to use EXP3 as A in Algorithm 3 however the standard version of EXP3 is defined on positive losses and the noise we add can potentially make it negative. Therefore we will use the algorithm EXP2 with exploration µ proposed by ([BCBK+12] Algorithm 2). We give a description of the Algorithm 4 in the Appendix for completeness. The following theorem appears as Theorem 1 in [BCBK+12].\nTheorem 4.5 (Regret Guarantee for EXP2 with exploration µ). Let S be a finite set of N actions. For the EXP2 strategy provided that η|〈s, l̃t〉| ≤ 1, ∀s ∈ S, one has\nRegret ≤ 2γT + logN η + ηE\n[\n∑\nt\n∑\ns\npt(s)〈s, l̃t〉2 ]\nWe now state the regret guarantee for Differentially Private Multi-Armed Bandits.\nTheorem 4.6 (Differentially Private Multi-Armed Bandits). Fix loss vectors (l1 . . . lT ) such that ‖lt‖∞ ≤ 1. Algorithm 3 when run with parameters D = N (0, σ2IN) where σ2 = 1ε2 log 2 1 δ and algorithm A =Algorithm 4 with the following parameters. The action set S is {e1 . . . eN}, η = √\nlogN 2NT (1+σ2 logN) , γ = ηN\n√\n1 + σ2 logNT and the exploration distribution µ(i) = 1N .The\nregret of the algorithm is bounded by\nO ( √ NT logN(1 + σ2 logNT ) ) = O\n(√ NT logN logT\nε log\n1\nδ\n)\nProof. The proof follows similar arguments made to prove Theorem 4.3. For the purpose of analysis we define the following pseudo loss vectors\nl̃t = lt + Zt\nwhere by definition Zt ∼ N (0, σ2IN ). As before, we note that the following fact holds.\nP(‖Zt‖2∞ ≥ 10σ2 logNT ) ≤ 1\nT 2\nThe above follows from Fact B.2 proved in the Appendix. Taking a union bound, we have\nP(∃t ‖Zt‖2∞ ≥ 10σ2 logNT ) ≤ 1\nT (12)\nTo bound the norm of the loss we will use the same technique as in the proof of Theorem 4.3. Define the event F , {∃t ‖Zt‖2∞ ≥ 10σ2 logNT }. We have from (8) that P(F ) ≤ 1T . We now have that E[Regret] ≤ E[Regret|F̄ ] + P(F )E[Regret|F ] Since the regret is always bounded by T we get that the second term above is at most 1. Therefore we will concern ourselves with bounding the first term above. Once again we note that Zt remain independent even when conditioned on the event F̄ . The following statements also hold\n∀t E[Zt|F̄ ] = 0 (13) ∀t E[‖Zt‖2∞|F̄ ] ≤ E[‖Zt‖2∞] ≤ 4σ2 logN (14)\nEquation (13) follows by noting that Zt remains symmetric around the origin even after conditioning. Equation (14) follows from Fact B.1 proved in the Appendix. It is easy to see that Theorem 4.2 still applies even when the noise is sampled from N (0, σ2IN ) conditioned under the event F̄ (due to Equation 9). Therefore we have that\nE[Regret|F̄ ] = E{Zt} [ EA [ T ∑\nt=1\n( 〈l̃t, x̃t〉 − 〈l̃t, x∗〉 )\n]\n∣ ∣ ∣ ∣ F̄\n]\n(15)\nNow note that due to the conditioning ‖Zt‖2∞ ≤ 10σ2 logNT and therefore we have that\nL , maxt,x∈∆N |〈Zt, x〉| ≤ 4σ √ logNT.\nIt can be seen that the condition η|〈s, l̃t〉| ≤ 1 in Theorem 4.5 in the setting of the simplex and exploration µ(i) = 1n and under the condition F̄ is satisfied as long as we have that\nηN(1 + 10σ logNT ) ≤ γ\nwhich holds by choice of these parameters.\nE[Regret|F̄ ] = E{Zt} [ EA [ T ∑\nt=1\n( 〈l̃t, x̃t〉 − 〈l̃t, x∗〉 )\n]\n∣ ∣ ∣ ∣ F̄\n]\nEquation (15)\n≤ E{Zt} [ logN\nη + η\nT ∑\nt=1\nN‖l̃t‖2∞ + 2Tγ ∣ ∣ ∣\n∣\nF̄\n]\nTheorem 4.5\n≤ E{Zt} [ logN\nη + 2η\nT ∑\nt=1\nN(‖lt‖2∞ + ‖Zt‖2∞) + 2Tγ ∣ ∣ ∣\n∣\nF̄\n]\n≤ logN η + 2ηTN(1 + σ2 logN) + 2Tγ Equation (14) ≤ O ( √ TN logN(1 + σ2 logNT ) )"
    }, {
      "heading" : "A Noisy OCO Theorem Proof",
      "text" : "We now give the proof of Theorem 4.2\nProof. The proof of the lemma is a straightforward calculation. First note that\nE{Zt}\n[\nEA\n[\nT ∑\nt=1\n(〈lt, x̃t〉 − 〈lt, x∗〉) ]] = E{Zt} [ EA [ T ∑\nt=1\n( 〈l̃t, x̃t〉 − 〈l̃t, x∗〉 )\n]]\n+ E{Zt}\n[\nEA\n[\nT ∑\nt=1\n〈lt − l̃t, x̃t〉 ]] − E{Zt} [ EA [ T ∑\nt=1\n〈lt − l̃t, x∗〉 ]]\nNow we have that\nE{Zt}\n[\nEA\n[\nT ∑\nt=1\n〈lt − l̃t, x̃t〉 ]] − E{Zt} [ EA [ T ∑\nt=1\n〈lt − l̃t, x∗〉 ]]\n=EA\n[\nT ∑\nt=1\nE{Zt} [ 〈lT − l̃t, x̃t − x∗〉 ]\n]\n=EA\n[\nT ∑\nt=1\n〈E [Zt] ,E[x̃t − x∗]〉 ]\n=0\nThe first inequality follows because the randomness of the algorithm does not depend on Zt. The second equality follows because xt, being a function of the loss vectors of the previous round, is independent of the noise Zt and the third equality follows from the fact that E[Zt] = 0."
    }, {
      "heading" : "B Facts about Norms of Gaussian Vectors",
      "text" : "We prove the facts for the case σ2 = 1. The generals versions follow immediately.\nFact B.1. Let Z ∼ N (0, IN ), then we have that\nE[‖Z‖2∞] ≤ 3 logN\n1−N−1\nProof. The proof is a simple application of the standard Moment Generating Function trick. Note that for each i ∈ [N ], Z(i)2 is a χ2 random variable. The MGF for χ2 distribution is (1 − 2s)−1/2. Therefore we have that for all s ≤ 1/2\nesE[maxi Z(i) 2] ≤ E[esmaxi Z(i)2 ] Convexity of ex\n= E[max i\nesZ(i) 2 ]\n≤ ∑\ni\nE[esZ(i) 2 ]\n= N(1− 2s)−1/2\nTherefore we have that\nE[max i Z(i)2] ≤ logN − 1 2 log(1− 2s) s\nSetting s = 1−N −1\n2 finishes the proof.\nFact B.2. Let Z ∼ N (0, IN ), then we have that\nP(‖Z‖2∞ ≥ 10 logTN) ≤ 1\nT 2\nProof. We will show that for a fixed i ∈ [N ]\nP(Z(i)2 ≥ 10 logTN) ≤ 1 NT 2\nThe proof then follows by a simple union bound. To see the above inequality we can use the standard fact about Gaussians, i.e. for n ∼ N (0, 1), we have that\nP(|n| ≥ t) ≤ √ 2\nπ\ne −t\n2\n2\nt (Mill’s inequality)\nThe proof now follows from substitution."
    }, {
      "heading" : "C EXP2 with Exploration µ",
      "text" : "Algorithm 4 EXP2 with exploration µ\nInput: learning rate η; mixing coefficient γ; distribution µ over the action set S\n1: q1 = ( 1 ‖S‖ . . . 1 ‖S‖ ) ∈ R|S|. 2: for t = 1,2 . . . T do 3: Let pt = (1− γ)qt + γµ and play st ∼ pt 4: Estimate loss vector lt by l̃t = P + t sts T t lt, with Pt = Ept [sts T t ] 5: Update the exponential weights, for all s ∈ S,\nqt+1(s) = e−η〈s,l̃t〉qt(s) ∑\ns′∈S e −η〈s′,l̃t〉qt(s′)\n6: end for"
    } ],
    "references" : [ {
      "title" : "Competing in the dark: An efficient algorithm for bandit linear optimization",
      "author" : [ "Jacob Abernethy", "Elad Hazan", "Alexander Rakhlin" ],
      "venue" : "In COLT,",
      "citeRegEx" : "Abernethy et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Abernethy et al\\.",
      "year" : 2008
    }, {
      "title" : "Interior-point methods for full-information and bandit online learning",
      "author" : [ "Jacob D Abernethy", "Elad Hazan", "Alexander Rakhlin" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "Abernethy et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Abernethy et al\\.",
      "year" : 2012
    }, {
      "title" : "Online linear optimization via smoothing",
      "author" : [ "Jacob Abernethy", "Chansoo Lee", "Abhinav Sinha", "Ambuj Tewari" ],
      "venue" : "In COLT,",
      "citeRegEx" : "Abernethy et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Abernethy et al\\.",
      "year" : 2014
    }, {
      "title" : "Towards minimax policies for online linear optimization with bandit feedback",
      "author" : [ "Sébastien Bubeck", "Nicolo Cesa-Bianchi", "Sham M Kakade", "Shie Mannor", "Nathan Srebro", "Robert C Williamson" ],
      "venue" : "In COLT,",
      "citeRegEx" : "Bubeck et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Bubeck et al\\.",
      "year" : 2012
    }, {
      "title" : "Calibrating noise to sensitivity in private data analysis",
      "author" : [ "Cynthia Dwork", "Frank McSherry", "Kobbi Nissim", "Adam Smith" ],
      "venue" : "In Theory of Cryptography Conference,",
      "citeRegEx" : "Dwork et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Dwork et al\\.",
      "year" : 2006
    }, {
      "title" : "Adaptive online prediction by following the perturbed leader",
      "author" : [ "Marcus Hutter", "Jan Poland" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Hutter and Poland.,? \\Q2005\\E",
      "shortCiteRegEx" : "Hutter and Poland.",
      "year" : 2005
    }, {
      "title" : "Differentially private online learning",
      "author" : [ "Prateek Jain", "Pravesh Kothari", "Abhradeep Thakurta" ],
      "venue" : "In COLT,",
      "citeRegEx" : "Jain et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Jain et al\\.",
      "year" : 2012
    }, {
      "title" : "Efficient algorithms for online decision problems",
      "author" : [ "Adam Kalai", "Santosh Vempala" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "Kalai and Vempala.,? \\Q2005\\E",
      "shortCiteRegEx" : "Kalai and Vempala.",
      "year" : 2005
    }, {
      "title" : "Online learning and online convex optimization",
      "author" : [ "Shai Shalev-Shwartz" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "Shalev.Shwartz.,? \\Q2011\\E",
      "shortCiteRegEx" : "Shalev.Shwartz.",
      "year" : 2011
    }, {
      "title" : "nearly) optimal algorithms for private online learning in full-information and bandit settings",
      "author" : [ "Abhradeep Guha Thakurta", "Adam Smith" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Thakurta and Smith.,? \\Q2013\\E",
      "shortCiteRegEx" : "Thakurta and Smith.",
      "year" : 2013
    } ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "We design differentially private algorithms for the problem of online linear optimization in the full information and bandit settings with optimal Õ( √ T ) regret bounds. In the full-information setting, our results demonstrate that (ε, δ)-differential privacy may be ensured for free – in particular, the regret bounds scale as O( √ T ) + Õ ( 1 ε log 1 δ ) . For bandit linear optimization, and as a special case, for non-stochastic multi-armed bandits, the proposed algorithm achieves a regret of O (√ T log T ε log 1 δ ) , while the previously best known bound was Õ (",
    "creator" : "LaTeX with hyperref package"
  }
}