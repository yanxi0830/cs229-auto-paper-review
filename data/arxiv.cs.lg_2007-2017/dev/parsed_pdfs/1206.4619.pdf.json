{
  "name" : "1206.4619.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Inductive Kernel Low-rank Decomposition with Priors: A Generalized Nyström Method",
    "authors" : [ "Kai Zhang", "Liang Lan", "Jun Liu" ],
    "emails" : [ "kai-zhang@siemens.com", "lanliang@temple.edu", "jun-liu@siemens.com", "rauber@ifs.tuwien.ac.at", "fabian.moerchen@siemens.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Appearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\ntion. Empirical results demonstrate the efficacy and efficiency of the proposed method."
    }, {
      "heading" : "1. Introduction",
      "text" : "Low-rankness is an important structure widely exploited in machine learning. For example, the kernel matrix often has a rapidly decaying spectrum and thus small rank, and eigenvectors corresponding to large eigenvalues create useful basis function for classification (Williams & Seeger, 2000; Bach & Jordan, 2005). Therefore the low-rank constraint has been widely applied to kernel learning problems (Kulis et al., 2009; Lanckriet et al., 2004; Shalit et al., 2010; Machart et al., 2011). On the other hand, the low-rank property is very useful in reducing the memory and computational cost in large scale problems, most notably by the so called lowrank matrix decomposition. Such a decomposition produces a compact representation of large matrices, which is the key to scaling up a great variety of kernel learning algorithms, with prominent examples including (Williams & Seeger, 2001; Fowlkes et al., 2004; Drineas & Mahoney, 2005; Fine et al., 2001; Achlioptas & McSherry, 2001).\nLow-rank matrix decomposition has gained great popularity in tackling large volume of data. However, there are still some concerns with existing approaches. First, most of them are intrinsically unsupervised and only focus on numerical approximation of given ma-\ntrices. When confronted with a specific learning task, however, we believe that incorporating prior knowledge (such as partially labeled samples or grouping constraints) will lead to more effective decomposition and improved performance. Second, many decomposition methods, such as QR decomposition, incomplete Choleskey factorization (Bach & Jordan, 2002), work in a transductive manner. That means the factorization can only be computed for samples available in the training stage. It is not straightforward to generalize the decomposition to new samples, and the difficulty becomes more pronounced if (partial) label information is considered.\nTo solve the problems, in this paper we propose a novel low-rank decomposition algorithm that incorporates side information in producing desired results. We achieve this by generalizing the Nyström method in a novel way. The Nyström method is a sampling based approach and has gained great popularity in unsupervised kernel low-rank approximation, with both theoretical performance guarantees and empirical successes (Williams & Seeger, 2001; Drineas & Mahoney, 2005; Talwalkar et al., 2008). Our main novelty is to provide an interesting interpretation of the matrix completion view of the Nyström method as a bilateral extrapolation of a dictionary kernel, and generalize it to incorporate prior information in computing improved low-rank decompositions. Our approach has two important advantages. First, it has a flexible, generative structure that allows us to generalize computed lowrank factorizations to arbitrary new samples. Second, both the space and time complexities of our approach are linear in the sample size, rendering great efficiency in learning a useful low-rank kernel.\nThe rest of the paper is organized as follows. In Section 2, we introduce the Nyström method. In Section 3, we propose the generalized Nyström low-rank decomposition using priors. In Section 4 we discuss related work. In Section 5 we report empirical evaluations. The last section concludes the paper."
    }, {
      "heading" : "2. Nyström Method",
      "text" : "The Nyström method is a sampling based algorithm for approximating large kernel matrices and their eigen-systems. It originated from solving integral equations and was introduced to the machine learning community by (Williams & Seeger, 2001; Fowlkes et al., 2004; Drineas & Mahoney, 2005).\nGiven a kernel function k(·, ·) and a sample set with underlying distribution p(·), the Nyström method aims\nat solving the following integral equation∫ k(x,y)p(y)ϕi(y)dy = λiϕi(x).\nHere ϕi(x) and λi are the ith eigenfunction and eigenvalue of the operator k(·, ·) with regard to p. The idea here is to draw a set of m samples Z, called landmark points, from the underlying distribution and approximate the expectation with the empirical average as\n1\nm m∑ j=1 k(x, zj)ϕi(zj) = λiϕi(x) (1)\nBy choosing x in (1) as z1, z2, ..., zm as well, the following eigenvalue decomposition can be obtained\nWϕi = λiϕi, (2)\nwhereW ∈ Rm×m is the kernel matrix defined on landmark points, ϕi ∈ Rm×1 and λi are the ith eigenvector and eigenvalue of W .\nIn practice, given a large data set X = {xi}ni=1, the Nyström method randomly selects m landmark points Z with m ≪ n, and computes the eigenvalue decomposition of W . Then the eigenvectors of W are extrapolated to the whole sample set by (1). Interestingly, the Nyström method is shown to implicitly reconstruct the whole n× n kernel matrix K by\nK ≈ EW †E⊤. (3)\nHere W † is the pseudo-inverse, and E ∈ Rn×m is the kernel matrix defined on the sample set X and landmark points Z. The Nyström method requires O(mn) space and O(m2n) time, which are linear in the sample size. It has drawn considerable interest in applications such as clustering and manifold learning (Talwalkar et al., 2008) (Zhang & Kwok, 2010), Gaussian processes (Williams & Seeger, 2001), and kernel methods (Fine et al., 2001)."
    }, {
      "heading" : "3. Generalized Nyström Low-rank Decomposition",
      "text" : ""
    }, {
      "heading" : "3.1. Bilateral Extrapolation of Dictionary Kernel",
      "text" : "We first present an interesting interpretation of the matrix completion view of the Nyström method (3). It reconstructs ijth entry of the kernel matrix as\nKij = EiW †E⊤j , (4)\nwhere Ei ∈ R1×m is the ith row of the extrapolation matrix E. In other words, the similarity between any\ntwo samples xi and xj is constructed by first computing their respective similarities to the landmark set (Ei and Ej), and then modulated by the inverse of the similarities among the landmark points, W †. With regards to this we have the following proposition.\nProposition 1 Given m landmark points Z, use (4) to construct the similarity between any two samples, xi and xj. Let zp and zq be the closest landmark point to xi and xj, respectively. Let dp = ∥xi − zp∥, and dq = ∥xj − zq∥. Let the kernel function k(·, ·) satisfy k(x, y) − k(x, z) ≤ η∥y − z∥, and c = mmax k(·, ·). Then the reconstructed similarity Kij and the pqth entry of the W will have the following relation |Kij −Wpq| ≤ √ mη(cdp + cdq + √ mηdpdq)∥W †∥F .\nProof 1 Let Ei = [k(xi, z1) k(xi, z2) ...k(xi, zm) ] ⊤, and Wp = [k(zp, z1) k(zp, z2) ...k(zp, zm) ] ⊤, and define ∆p = Ei − Wp, ∆q = Ej − Wq. We have |∆p|2 = ∑m o=1(k(xi, zo) − k(zp, zo))2 ≤ mη2d2p, similarly, |∆q|2 ≤ mη2d2q. We also have |Ei|, |Ej | ≤ c, then we have\n|Kij −Wpq| = |(Wp +∆p)⊤W †(Wq +∆q)−Wpq| =\n∣∣(W⊤p W †Wq −Wpq)−W⊤p W †∆q −∆⊤p W †Wq +∆pW †∆q\n∣∣ ≤ c(∆p +∆q) · ∥W †∥F + |∆p| · |∆q| · ∥W †∥F = (c∆p + c∆q +∆p∆q)∥W †∥F = √ mη(cdp + cdq + √ mηdpdq)∥W †∥F ,\nHere we used the equality WpW †W⊤q = Wpq, since Wp and Wq are the pth and qth row (column) of W .\nProposition 1 gives an interesting interpretation of the kernel reconstruction mechanism of the Nyström method (4). If xi and xj happen to overlap with a pair of landmark points, zp and zq, then Kij = Wpq, i.e., the pqth entry of W will be extrapolated exactly onto (xi,xj). In case xi and xj do not overlap with any landmark point, the difference between Kij and Wpq, with zp and zq being the closest landmark points to xi and xj , will be bounded\n1 by the distances ∥xi − zp∥ and ∥xj − zq∥. The smaller the distances, the closer Kij and Wpq. In other words, the similarity matrix W on the landmark points serves as a dictionary kernel, whose entries are extrapolated bilaterally onto any pairs of samples (xi,xj) according to the proximity relation between landmark points and samples, and the reconstruction is exact on the landmark points Z which serve as the “nodes” for extrapolation.\n1A tighter bound is still open and being investigated."
    }, {
      "heading" : "3.2. Including Side Information",
      "text" : "The kernel extrapolation view of the Nyström method (Proposition 1) inspires us to generalize it to handle prior constraint in learning a low-rank kernel. Note that quality of the dictionary will have a large impact on the whole kernel matrix. In the original Nyström method (3), the dictionary kernelW is simply computed as the pairwise similarity between landmark points, which can deviate from an “ideal” one. Therefore, instead of using such an “unsupervised” dictionary, we propose to learn a new dictionary kernel that better coincides with given side information.\nSuppose we are given a set of labeled and unlabeled samples2. Let Z be a set of m pre-selected landmark points. Let E ∈ Rn×m be the extrapolation kernel matrix between samples X and landmark Z, and let El ∈ Rl×m be the rows of E corresponding to labeled samples. For simplicity, let S0 = W\n† denote the inverse of the dictionary kernel in the standard Nyström method (3). Our task is to learn (the inverse of) a new dictionary kernel, denoted by S, subject to the following considerations:\n1. unsupervised information: the reconstructed kernel ESE⊤ should preserve the structure of the original kernel matrix K, since K encodes important pairwise relation between samples.\n2. supervised information: the reconstructed kernel on the labeled samples, ElSE ⊤ l , should be\nconsistent with the given side information.\nTo achieve the first goal, note that in the standard Nyström method, EW †E⊤ provides an effective approximation of K. Therefore, we use S0 = W\n† as a prior for the (inverse) dictionary kernel S, namely, they should be close under some distance measurement. To achieve the second goal, we use the concept of kernel target alignment (Cristianini et al., 2002) and require that the reconstructed kernel, ElSE ⊤ l , is close to the ideal kernel K∗l defined on labeled samples. The ideal kernel is defined as (Kwok & Tsang, 2003)\n[K∗l ]ij = { 1 xi,xj in the same class 0 otherwise.\n(5)\nWe therefore arrive at the following problem\nmin S∈Rm×m\nλ∥S − S0∥2F + ∥ElSE⊤l −K∗l ∥2F (6)\ns.t. S ≽ 0. 2In case side information is in the form of grouping con-\nstraints, discussion is in Section 3.3.\nHere, we used the Euclidian distance to measure the closeness between two matrices. Note that in (Cristianini et al., 2002), the closeness between two kernel matrices is measured by their inner product ⟨K,K ′⟩ = ∑ i,j KijK ′ ij . Since ∥K −K ′∥2F = ⟨K,K⟩+ ⟨K ′,K ′⟩−2⟨K,K ′⟩, minimizing the Euclidian distance is related to maximizing the alignment. We choose the Euclidian distance here because we can then use the normalized kernel alignment score afterwards as an independent measure to choose the hyper-parameter λ. Details will be discussed in Section 3.8.\nWe call our method generalized Nyström low-rank decomposition, which has several desirable properties. First, as long as the inverse dictionary kernel S is psd, the resultant kernel ESE⊤ will also be psd; second, the rank of the kernel matrix can be easily controlled by the landmark size; this can be computationally much more efficient than learning a full kernel matrix subject to rank constraint; third, the extrapolation (4) is “generative” and allows us to compute the similarity between any pair of samples; this means the learned kernel matrix generalizes easily to new samples. Since the dictionary kernel is learned with prior information, the generalization to new samples naturally incorporates such information, which provides much convenience in updated environments."
    }, {
      "heading" : "3.3. Side Information as Grouping Constraints",
      "text" : "Given a set of grouping constraints (must-link and cannot-link pairs), denoted by I. Let XI be the subset of samples with such constraint. Then we define T ∈ R|XI |×|XI | such that\nTij = { 1 (xi,xj) ∈ XI 0 otherwise.\nThen our objective can be written conveniently as\nmin S∈Rm×m λ∥S − S0∥2F + ∥T⊙ (EISE⊤I )−K∗I ∥2F s.t. S ≽ 0.\nHere K∗I is defined similarly as in (5)."
    }, {
      "heading" : "3.4. Optimization",
      "text" : "The objective (6) is convex regard to S, and the psd constraint S ≽ 0 is also convex. Therefore (6) is a smooth convex problem with a global optimum.\nNote that S is a matrix with only m2 variables, where m ≪ n is a user defined value. Therefore the problem (6) involves only light optimization load. We use the gradient mapping strategy (Nemirovski, 1994) that is composed of iterative gradient descent equipped with\na projection step to find the optimal solution. Given an initial solution S(t), we update it by\nS(t+1) = S(t) + η(t)∇S(t) , (7)\nwhere ∇S is the gradient of the objective J (6) at S,\n∇S = 2λ(S − S0) + 2E⊤l (ElSE⊤l −K∗l )El.\nThe step length η(t) is determined by the ArmijoGoldstein rule (Nemirovski, 1994). In particular, we start from an initial, small scalar A, and solve the following problem\nB∗A = argmin B≽0\ntr(∇S(t)B) + A\n2 ∥B − S(t)∥2F . (8)\nThis is a standard matrix nearness problem with psd constraint, and B∗A can be computed in closed form as S(t)− 1A∇S(t) removed of negative eigenvectors/values. Then we examine J(B∗A) ≤ J(S(t)) + tr ( ∇S(t)(B∗A − S(t)) ) + A\n2 ∥B∗A − S(t)∥2F .\nIf this inequality is violated, then we increase A by a constant times and re-calculate (8) until the relation holds. Then we use η(t) = 1A as the step length for (7).\nAfter the descent step, we project the iterate S(t+1) onto the set of positive semi-definite cones as follows\nS(t+1) ← U (t+1)Λ(t+1)+ (U (t+1)) ⊤ ,\nwhere U (t+1) and Λ(t+1) are the eigenvectors and eigenvalues of S(t+1) (7), and\nΛ (t+1) + ii =\n{ Λ (t+1) ii if Λ (t+1) ii ≥ 0;\n0 otherwise. (9)\nOne can also use more advanced approaches such as the Nesterov’s method (Nemirovski, 1994) to improve the convergence rate. We do not explore details here because the size of our optimization problem is small and empirically it converges quickly due to a principled initialization (see next subsection)."
    }, {
      "heading" : "3.5. Initialization",
      "text" : "In this section, we propose a closed-form initialization which helps us quickly locate the optimal solution. The basic idea is to drop the psd constraint in (6) and compute the vanishing point of the gradient, i.e., ∂J(S) ∂S = 0, which leads to\nλS + E⊤l ElSE ⊤ l El = E ⊤ l K ∗ l El + λS0.\nThen we have\nS + PSP⊤ = Q, (10)\nwhere P = 1√ λ (E⊤l El),\nQ = S0 + 1\nλ E⊤l K ∗ l El.\nEquation (10) can be solved as follows. Suppose the diagonalization of P is P = UΛU⊤, and define S = US̃U ′, Q = UQ̃U⊤, then it can be written as\nUS̃U⊤ + UΛS̃ΛU⊤ = UQ̃U⊤ → S̃ + ΛS̃Λ⊤ = Q̃.\nSince Λ is diagonal, this becomes m2 equations\nS̃ij + ΛiiΛjjS̃ij = Qij , 1 ≤ i, j ≤ m.\nTherefore we have a closed form solution of S, as\nS = US̃U⊤, where [S̃]ij = Q̃ij\n1 + ΛiiΛjj .\nAfter computing S, we then project it onto the set of positive semi-definite cones similar to (9). Such an initial solution can be deemed as the closest psd matrix to the unconstrained version of (6). Empirically, such an initial solution alone already leads to satisfactory prediction result."
    }, {
      "heading" : "3.6. Landmark Selection",
      "text" : "Selection of the landmark points Z in the Nyström method can greatly affect its performance. Preferably, landmark points should allow faithful reconstruction of the global similarity landscape. We used the k-means based sampling scheme (Zhang & Kwok, 2010) which has shown to consistently outperform other popular landmark selection schemes such as random sampling."
    }, {
      "heading" : "3.7. Complexities",
      "text" : "The space complexity of our algorithm is O(mn), where n is sample size and m the number of landmark points. Computationally, it only requires repeated eigenvalue decomposition of m×m matrices, and a single multiplication between the n×m extrapolation E and the m × m dictionary kernel S. The over all complexity is O(m2n) + O(t log(µmax)m\n3), where t is the number of gradient mapping iterations, and µmax is the maximum eigenvalue of the Hessian. This is because A (8) is bounded by µmax and one can always find a suited step-length in log(µmax) steps. Empirically, with the initialization in Section 3.5, only a few iterations is needed. Therefore t is a small integer and our algorithm has a linear time and space complexity."
    }, {
      "heading" : "3.8. Selecting λ",
      "text" : "The hyper-parameter λ in (6) can be difficult to choose if the side information (e.g., partially labeled samples) is limited. Here we propose a heuristic to choose λ. Note that the objective (6) contains two residuals, S0 − S, and ElSE⊤l − K∗l , in terms of the Euclidian distance, which are additive and requires a tradeoff parameter λ. Here, we use a new criterion with certain invariance property to re-evaluate the goodness of fit of the solution. More specifically, we used normalized kernel alignment (NKA) (Cortes et al., 2010) between kernel matrices,\nρ[K,K ′] = ⟨KcK ′⊤c ⟩F\n∥Kc∥F ∥K ′c∥F , (11)\nwhere Kc is double-centralized K. The NKA score always has a magnitude that is smaller than 1. It is independent of the scale of the solution, and is multiplicative by nature. Let S(λ) be the optimum of (6) for a fixed λ. Then we choose the best λ as follows\nλ∗ = argmax λ∈G\nρ [S(λ), S0] · ρ [ ElS(λ)E ⊤ l ,K ∗ l ] . (12)\nHere G is the set of candidate λ’s. The criterion (12) has the following properties: (1) it is scale invariant, and does not require any extra trade-off parameter due to its multiplicative form; (2) the first term measures the closeness between S and S0, related to unsupervised structures of kernel matrix; the second term is on the closeness between ElSE ⊤ l and K ∗ l , related to side information; therefore the criterion faithfully reflects what (6) optimizes but on the other hand is numerically different; (3) a higher alignment (second term in (12)) indicates existence of a good predictor with higher probability (Cortes et al., 2010); (4) computation of the criterion does not require any extra validation set, which is suited if only limited training samples are available. Therefore, this is an informative criterion to measure the quality of solution. Empirically, it correlates nicely with the prediction accuracy on the test samples, as will be reported in Section 5."
    }, {
      "heading" : "4. Related Work",
      "text" : "This section discusses several lines of work on using side information in low-rank kernel matrices.\nOne is to rectify standard numerical low-rank decomposition procedures by injecting supervised information. An excellent example is the Choleskey with Side Information (SCI) (Bach & Jordan, 2005). The algorithm is iterative and in each step, the column of the kernel matrix that maximally reduces the hybrid of\nthe matrix approximation error and a linear prediction error is selected. One difficulty with the greedy scheme is that the approximation error takes O(n2) time to compute, and an upper bound has to be used instead, which may adversely affect the result. The algorithm takes into account the label information and can reduce the rank of factorization needed in a kernel classifier. Our approach was motivated similarly but has important differences. First, the CSI method assumes that labels of all training instances are given (extension to semi-supervised setting will require non-trivial modifications of the algorithm); in comparison, we consider the more generalized semi-supervised learning scenario. Second, the procedure is transductive and there seems to be lacking principled ways to compute factorizations for new samples; wheares our approach generalize easily to new samples by design.\nThe second line is low-rank kernel learning. Although kernel learning has drawn considerable interest, algorithms on learning low-rank kernel matrices are not very abundant (Kulis et al., 2009), in particular those in a computationally efficient way. Lanckriet et al. studied transductive kernel learning through a general, semi-definite programming (SDP) framework. The rank of the learned kernel can be controlled by choosing kernel matrix as a convex combination of a small number of base kernels. However, even special cases of it (QCQP) are still computationally expensive, with at least cubical time complexity in sample size. Kulis et al. proposed to learn a low-rank kernel by minimizing its divergence with an initial low-rank base kernel subject to distance/similarity constraints. They applied the Bregman divergence which naturally preserves the low-rankness and positive semi-definiteness of solution. The algorithm improves in efficiency, but in general it still has quadratic space and time complexities with the sample size. In (Shalit et al., 2010) an online learning algorithm is proposed on the manifold of low-rank matrices, which consists of iterative gradient step and second-order retraction. In (Machart et al., 2011), a novel low-rank kernel learning approach was proposed for regression via the use of conical combinations of base kernels and a stochastic optimization framework. Again, most of these algorithms are transductive and how to generalize the learned kernel to new samples still remains open.\nThe third line involves spectral kernel learning, which builds a kernel matrix using eigenvectors and rectified eigenvalues of the graph Laplacian. Transformation of the eigen-spectrum can be achieved analytically, such as in (Kondor & Lafferty, 2007) (Chapelle et al., 2003). In (Cristianini et al., 2002) (Cortes et al., 2010), a nonparametric transform is\ncomputed by maximizing the alignment with the target. In (Zhu et al., 2004), an extra order constraint on the weight of eigenvectors was adopted. Due to the need to compute kernel eigenvalues, spectral kernel learning requires at least quadratic space and time complexities, or even higher if advanced optimization such as QCQP is involved (Zhu et al., 2004)."
    }, {
      "heading" : "5. Experiments",
      "text" : "This section compares 7 algorithms on learning low-rank kernel: (1) Nyström: standard Nyström method; (2) CSI: Choleskey with Side Information (Bach & Jordan, 2005); (3) Cluster: cluster kernel (Chapelle et al., 2003); (4) Spectral: non-parametric spectral graph kernel (Zhu et al., 2004); (5) TSK: two stage kernel learning algorithm (Cortes et al., 2010); (6) Breg: low-rank kernel learning with Bregman divergence (Kulis et al., 2009); (7) Our method. Most algorithms can learn the n × n low-rank kernel3 matrix on labeled and unlabeled samples4 in the form of K = GG⊤, which is fed into SVM for classification. The resultant problem will be a linear SVM using G as training/testing samples (Zhang et al., 2012).\nWe use benchmark data sets from the SSL data set (Chapelle et al., 2001) and the libsvm data. For each data set, we randomly pick 100 labeled samples evenly among all classes, repeat 30 times and report the averaged classification error on unlabeled data. We used the Gaussian kernel K(x1,x2) = exp(−∥x1 − x2∥2/b). Parameter selection is difficult in semi-supervised learning, so, we choose the kernel width as the averaged pairwise squared distances between samples. Empirically, this gives reasonable performance compared with the best kernel width from some pre-defined candidates. For method (5) the base kernel are chosen from a set of RBF kernels whose widths are factors of the averaged pairwise distance as in (Cortes et al., 2010). For the regularization parameter C in linear SVM, we use the heuristic implemented in liblinear package (Fan et al., 2008). Most codes are in matlab (for method (2) we used codes in (Bach & Jordan, 2005) with core functions in C) and run on a PC with 2G memory and 2.8GHz processor.\nResults are reported in Table 1. Methods statistically better than others with a confidence level that is at least 95% (paired t-test) are highlighted. Note that method (1) does not use label information nor unla-\n3 The rank of the learned kernel is set to be 10% of sample size (or a fixed number if sample is too large).\n4Method (2) uses some heuristics to compute the kernel matrix between labeled and unlabeled samples, since only labeled samples are used in training.\nbeled data in training, therefore as a baseline method it is very efficient. Method (2) is very efficient because it only uses labeled samples for training (with C implementation). Method (3), (4), (5) require eigenvalue decomposition of the kernel matrix (or graph Lapacian), therefore they are computationally more expensive. Method (6) may require many iterations to converge. Our approach is very efficient and can be orders of magnitudes faster than some other methods.\nFrom table 1, we can see that on most data sets, algorithms using labels in kernel learning outperform the baseline algorithm (method 1), indicating the value of side information. Our approach is competitive with stat-of-the-art kernel learning algorithms. On the largest data set mnist, method (3), (4), (5) can not run on our PC due to the huge memory consumptions; in comparison, our approach is very efficient and gives the lowest error rate on this data set. We also examine the alignment score (12) used to choose the hyperparameter λ in Figure 1. As can be seen, the score correlates nicely with the classification accuracy."
    }, {
      "heading" : "6. Conclusions",
      "text" : "In this paper, we proposed an efficient kernel low-rank decomposition algorithm endowed with a flexible, non-\nparametric reconstruction mechanism, while being capable of handling side information. It shows significant performance gains in benchmark learning tasks. In the future, we will couple the dictionary learning with specific classifier such as an SVM to further improve the prediction performance. Another interesting direction is the learning of a sparse dictionary and its application in information retrieval problems."
    } ],
    "references" : [ {
      "title" : "Fast computation of low rank matrix approximations",
      "author" : [ "D. Achlioptas", "F. McSherry" ],
      "venue" : "In Proceedings of the thirty-third annual ACM symposium on Theory of computing,",
      "citeRegEx" : "Achlioptas and McSherry,? \\Q2001\\E",
      "shortCiteRegEx" : "Achlioptas and McSherry",
      "year" : 2001
    }, {
      "title" : "Kernel independent component analysis",
      "author" : [ "F.R. Bach", "M.I. Jordan" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Bach and Jordan,? \\Q2002\\E",
      "shortCiteRegEx" : "Bach and Jordan",
      "year" : 2002
    }, {
      "title" : "Predictive low-rank decomposition for kernel methods",
      "author" : [ "F.R. Bach", "M.I. Jordan" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Bach and Jordan,? \\Q2005\\E",
      "shortCiteRegEx" : "Bach and Jordan",
      "year" : 2005
    }, {
      "title" : "Twostage learning kernel algorithms",
      "author" : [ "C. Cortes", "M. Mohri", "A. Rostamizadeh" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Cortes et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Cortes et al\\.",
      "year" : 2010
    }, {
      "title" : "On kernel-target alignment",
      "author" : [ "N. Cristianini", "J. Shawe-taylor", "A. Elissee", "J. Kandola" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Cristianini et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Cristianini et al\\.",
      "year" : 2002
    }, {
      "title" : "On the Nyström method for approximating a Gram matrix for improved kernel-based learning",
      "author" : [ "P. Drineas", "M.W. Mahoney" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Drineas and Mahoney,? \\Q2005\\E",
      "shortCiteRegEx" : "Drineas and Mahoney",
      "year" : 2005
    }, {
      "title" : "Liblinear: A library for large linear classification",
      "author" : [ "Fan", "R.-E", "Chang", "K.-W", "Hsieh", "C.-J", "X.R. Wang", "Lin" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Fan et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Fan et al\\.",
      "year" : 2008
    }, {
      "title" : "Efficient svm training using low-rank kernel representations",
      "author" : [ "S. Fine", "K. Scheinberg", "N. Cristianini", "J. Shawe-taylor", "B. Williamson" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Fine et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Fine et al\\.",
      "year" : 2001
    }, {
      "title" : "Spectral grouping using the Nyström method",
      "author" : [ "C. Fowlkes", "S. Belongie", "F. Chung", "J. Malik" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "Fowlkes et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Fowlkes et al\\.",
      "year" : 2004
    }, {
      "title" : "Diffusion kernels on graphs and other discrete input spaces",
      "author" : [ "R.I. Kondor", "J. Lafferty" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Kondor and Lafferty,? \\Q2007\\E",
      "shortCiteRegEx" : "Kondor and Lafferty",
      "year" : 2007
    }, {
      "title" : "Low-rank kernel learning with bregman matrix divergences",
      "author" : [ "B. Kulis", "M.A. Sustik", "I.S. Dhillon" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Kulis et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Kulis et al\\.",
      "year" : 2009
    }, {
      "title" : "Learning with idealized kenrels",
      "author" : [ "J. Kwok", "I. Tsang" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Kwok and Tsang,? \\Q2003\\E",
      "shortCiteRegEx" : "Kwok and Tsang",
      "year" : 2003
    }, {
      "title" : "Stochastic low-rank kernel learning for regression",
      "author" : [ "P. Machart", "T. Peel", "S. Anthoine", "L. Ralaivola", "H. Glotin" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Machart et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Machart et al\\.",
      "year" : 2011
    }, {
      "title" : "Efficient methods in convex programming",
      "author" : [ "A. Nemirovski" ],
      "venue" : "Lecture Notes,",
      "citeRegEx" : "Nemirovski,? \\Q1994\\E",
      "shortCiteRegEx" : "Nemirovski",
      "year" : 1994
    }, {
      "title" : "Online learning in the manifold of low-rank matrices",
      "author" : [ "U. Shalit", "D. Weinshall", "G. Chechik" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Shalit et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Shalit et al\\.",
      "year" : 2010
    }, {
      "title" : "Stochastic low-rank kernel learning for regression",
      "author" : [ "A. Talwalkar", "S. Kumar", "H. Rowley" ],
      "venue" : "In IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Talwalkar et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Talwalkar et al\\.",
      "year" : 2008
    }, {
      "title" : "The effect of the input density distribution on kernel-based classifiers",
      "author" : [ "C. Williams", "M. Seeger" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Williams and Seeger,? \\Q2000\\E",
      "shortCiteRegEx" : "Williams and Seeger",
      "year" : 2000
    }, {
      "title" : "Using the Nyström method to speed up kernel machines",
      "author" : [ "C. Williams", "M. Seeger" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Williams and Seeger,? \\Q2001\\E",
      "shortCiteRegEx" : "Williams and Seeger",
      "year" : 2001
    }, {
      "title" : "Clustered Nyström method for large scale manifold learning and dimension reduction",
      "author" : [ "K. Zhang", "J. Kwok" ],
      "venue" : "IEEE Transactions on Neural Networks,",
      "citeRegEx" : "Zhang and Kwok,? \\Q2010\\E",
      "shortCiteRegEx" : "Zhang and Kwok",
      "year" : 2010
    }, {
      "title" : "Scaling up kernel svm on limited resources: A low-rank linearization approach",
      "author" : [ "K. Zhang", "L. Lan", "Z. Wang", "F. Moerchen" ],
      "venue" : "Journal of Machine Learning Research Workshop and Conference Proceedings,",
      "citeRegEx" : "Zhang et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2012
    }, {
      "title" : "Nonparametric transforms of graph kernels for semi-supervised learning",
      "author" : [ "X. Zhu", "J. Kandola", "Z. Ghahbramani", "J. Lafferty" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Zhu et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2004
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "Therefore the low-rank constraint has been widely applied to kernel learning problems (Kulis et al., 2009; Lanckriet et al., 2004; Shalit et al., 2010; Machart et al., 2011).",
      "startOffset" : 86,
      "endOffset" : 173
    }, {
      "referenceID" : 14,
      "context" : "Therefore the low-rank constraint has been widely applied to kernel learning problems (Kulis et al., 2009; Lanckriet et al., 2004; Shalit et al., 2010; Machart et al., 2011).",
      "startOffset" : 86,
      "endOffset" : 173
    }, {
      "referenceID" : 12,
      "context" : "Therefore the low-rank constraint has been widely applied to kernel learning problems (Kulis et al., 2009; Lanckriet et al., 2004; Shalit et al., 2010; Machart et al., 2011).",
      "startOffset" : 86,
      "endOffset" : 173
    }, {
      "referenceID" : 8,
      "context" : "Such a decomposition produces a compact representation of large matrices, which is the key to scaling up a great variety of kernel learning algorithms, with prominent examples including (Williams & Seeger, 2001; Fowlkes et al., 2004; Drineas & Mahoney, 2005; Fine et al., 2001; Achlioptas & McSherry, 2001).",
      "startOffset" : 186,
      "endOffset" : 306
    }, {
      "referenceID" : 7,
      "context" : "Such a decomposition produces a compact representation of large matrices, which is the key to scaling up a great variety of kernel learning algorithms, with prominent examples including (Williams & Seeger, 2001; Fowlkes et al., 2004; Drineas & Mahoney, 2005; Fine et al., 2001; Achlioptas & McSherry, 2001).",
      "startOffset" : 186,
      "endOffset" : 306
    }, {
      "referenceID" : 15,
      "context" : "The Nyström method is a sampling based approach and has gained great popularity in unsupervised kernel low-rank approximation, with both theoretical performance guarantees and empirical successes (Williams & Seeger, 2001; Drineas & Mahoney, 2005; Talwalkar et al., 2008).",
      "startOffset" : 196,
      "endOffset" : 270
    }, {
      "referenceID" : 8,
      "context" : "It originated from solving integral equations and was introduced to the machine learning community by (Williams & Seeger, 2001; Fowlkes et al., 2004; Drineas & Mahoney, 2005).",
      "startOffset" : 102,
      "endOffset" : 174
    }, {
      "referenceID" : 15,
      "context" : "It has drawn considerable interest in applications such as clustering and manifold learning (Talwalkar et al., 2008) (Zhang & Kwok, 2010), Gaussian processes (Williams & Seeger, 2001), and kernel methods (Fine et al.",
      "startOffset" : 92,
      "endOffset" : 116
    }, {
      "referenceID" : 7,
      "context" : ", 2008) (Zhang & Kwok, 2010), Gaussian processes (Williams & Seeger, 2001), and kernel methods (Fine et al., 2001).",
      "startOffset" : 95,
      "endOffset" : 114
    }, {
      "referenceID" : 4,
      "context" : "To achieve the second goal, we use the concept of kernel target alignment (Cristianini et al., 2002) and require that the reconstructed kernel, ElSE ⊤ l , is close to the ideal kernel K∗ l defined on labeled samples.",
      "startOffset" : 74,
      "endOffset" : 100
    }, {
      "referenceID" : 4,
      "context" : "Note that in (Cristianini et al., 2002), the closeness between two kernel matrices is measured by their inner product ⟨K,K ′⟩ = ∑ i,j KijK ′ ij .",
      "startOffset" : 13,
      "endOffset" : 39
    }, {
      "referenceID" : 13,
      "context" : "We use the gradient mapping strategy (Nemirovski, 1994) that is composed of iterative gradient descent equipped with a projection step to find the optimal solution.",
      "startOffset" : 37,
      "endOffset" : 55
    }, {
      "referenceID" : 13,
      "context" : "The step length η is determined by the ArmijoGoldstein rule (Nemirovski, 1994).",
      "startOffset" : 60,
      "endOffset" : 78
    }, {
      "referenceID" : 13,
      "context" : "One can also use more advanced approaches such as the Nesterov’s method (Nemirovski, 1994) to improve the convergence rate.",
      "startOffset" : 72,
      "endOffset" : 90
    }, {
      "referenceID" : 3,
      "context" : "More specifically, we used normalized kernel alignment (NKA) (Cortes et al., 2010) between kernel matrices,",
      "startOffset" : 61,
      "endOffset" : 82
    }, {
      "referenceID" : 3,
      "context" : "The criterion (12) has the following properties: (1) it is scale invariant, and does not require any extra trade-off parameter due to its multiplicative form; (2) the first term measures the closeness between S and S0, related to unsupervised structures of kernel matrix; the second term is on the closeness between ElSE ⊤ l and K ∗ l , related to side information; therefore the criterion faithfully reflects what (6) optimizes but on the other hand is numerically different; (3) a higher alignment (second term in (12)) indicates existence of a good predictor with higher probability (Cortes et al., 2010); (4) computation of the criterion does not require any extra validation set, which is suited if only limited training samples are available.",
      "startOffset" : 586,
      "endOffset" : 607
    }, {
      "referenceID" : 10,
      "context" : "Although kernel learning has drawn considerable interest, algorithms on learning low-rank kernel matrices are not very abundant (Kulis et al., 2009), in particular those in a computationally efficient way.",
      "startOffset" : 128,
      "endOffset" : 148
    }, {
      "referenceID" : 14,
      "context" : "In (Shalit et al., 2010) an online learning algorithm is proposed on the manifold of low-rank matrices, which consists of iterative gradient step and second-order retraction.",
      "startOffset" : 3,
      "endOffset" : 24
    }, {
      "referenceID" : 12,
      "context" : "In (Machart et al., 2011), a novel low-rank kernel learning approach was proposed for regression via the use of conical combinations of base kernels and a stochastic optimization framework.",
      "startOffset" : 3,
      "endOffset" : 25
    }, {
      "referenceID" : 4,
      "context" : "In (Cristianini et al., 2002) (Cortes et al.",
      "startOffset" : 3,
      "endOffset" : 29
    }, {
      "referenceID" : 3,
      "context" : ", 2002) (Cortes et al., 2010), a nonparametric transform is computed by maximizing the alignment with the target.",
      "startOffset" : 8,
      "endOffset" : 29
    }, {
      "referenceID" : 20,
      "context" : "In (Zhu et al., 2004), an extra order constraint on the weight of eigenvectors was adopted.",
      "startOffset" : 3,
      "endOffset" : 21
    }, {
      "referenceID" : 20,
      "context" : "Due to the need to compute kernel eigenvalues, spectral kernel learning requires at least quadratic space and time complexities, or even higher if advanced optimization such as QCQP is involved (Zhu et al., 2004).",
      "startOffset" : 194,
      "endOffset" : 212
    }, {
      "referenceID" : 20,
      "context" : ", 2003); (4) Spectral: non-parametric spectral graph kernel (Zhu et al., 2004); (5) TSK: two stage kernel learning algorithm (Cortes et al.",
      "startOffset" : 60,
      "endOffset" : 78
    }, {
      "referenceID" : 3,
      "context" : ", 2004); (5) TSK: two stage kernel learning algorithm (Cortes et al., 2010); (6) Breg: low-rank kernel learning with Bregman divergence (Kulis et al.",
      "startOffset" : 54,
      "endOffset" : 75
    }, {
      "referenceID" : 10,
      "context" : ", 2010); (6) Breg: low-rank kernel learning with Bregman divergence (Kulis et al., 2009); (7) Our method.",
      "startOffset" : 68,
      "endOffset" : 88
    }, {
      "referenceID" : 19,
      "context" : "The resultant problem will be a linear SVM using G as training/testing samples (Zhang et al., 2012).",
      "startOffset" : 79,
      "endOffset" : 99
    }, {
      "referenceID" : 3,
      "context" : "For method (5) the base kernel are chosen from a set of RBF kernels whose widths are factors of the averaged pairwise distance as in (Cortes et al., 2010).",
      "startOffset" : 133,
      "endOffset" : 154
    }, {
      "referenceID" : 6,
      "context" : "For the regularization parameter C in linear SVM, we use the heuristic implemented in liblinear package (Fan et al., 2008).",
      "startOffset" : 104,
      "endOffset" : 122
    } ],
    "year" : 2012,
    "abstractText" : "Low-rank matrix decomposition has gained great popularity recently in scaling up kernel methods to large amounts of data. However, some limitations could prevent them from working effectively in certain domains. For example, many existing approaches are intrinsically unsupervised, which does not incorporate side information (e.g., class labels) to produce task specific decompositions; also, they typically work “transductively”, i.e., the factorization does not generalize to new samples, so the complete factorization needs to be recomputed when new samples become available. To solve these problems, in this paper we propose an “inductive”-flavored method for low-rank kernel decomposition with priors. We achieve this by generalizing the Nyström method in a novel way. On the one hand, our approach employs a highly flexible, nonparametric structure that allows us to generalize the low-rank factors to arbitrarily new samples; on the other hand, it has linear time and space complexities, which can be orders of magnitudes faster than existing approaches and renders great efficiency in learning a low-rank kernel decomposiAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s). tion. Empirical results demonstrate the efficacy and efficiency of the proposed method.",
    "creator" : " TeX output 2012.05.08:1131"
  }
}