{
  "name" : "1603.02250.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Satyen Kale" ],
    "emails" : [ "dean@foster.net", "satyen@yahoo-inc.com", "howard@cc.gatech.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n60 3.\n02 25\n0v 1\n[ cs\n.L G\n] 7\nM ar\n√ T ) after T prediction rounds. We complement this result by\nshowing that no algorithm running in polynomial time per iteration can achieve regret bounded by O(T 1−δ) for any constant δ > 0 unless NP ⊆ BPP. This computational hardness result resolves an open problem presented in COLT 2014 (Kale, 2014) and also posed by Zolghadr et al. (2013). This hardness result holds even if the algorithm is allowed to access more features than the best sparse linear regressor up to a logarithmic factor in the dimension."
    }, {
      "heading" : "1 Introduction",
      "text" : "In various real-world scenarios, features for examples are constructed by running some computationally expensive algorithms. With resource constraints, it is essential to be able to make predictions with only a limited number of features computed per example. One example of this scenario, from (Cesa-Bianchi et al., 2011), is medical diagnosis of a disease, in which each feature corresponds to a medical test that the patient in question can undergo. Evidently, it is undesirable to subject a patient to a battery of medical tests, for medical as well as cost reasons. Another example from the same paper is a search engine, where a ranking of web pages must be generated for each incoming user query and the limited amount of time allowed to answer a query imposes restrictions on the number of attributes that can be evaluated in the process. In both of these problems, predictions need to be made sequentially as patients or search queries arrive online, learning a good model in the process.\nIn this paper, we model the problem of prediction with limited access to features in the most natural and basic manner as an online sparse linear regression problem. In this problem, an online learner makes real-valued predictions for the labels of examples arriving sequentially over a number of rounds. Each example has d features that can be potentially accessed by the learner. However, in each round, the learner is restricted to choosing an\narbitrary subset of features of size at most k, a budget parameter. The learner then acquires the values of the subset of features, and then makes its prediction, at which point the true label of the example is revealed to the learner. The learner suffers a loss for making an incorrect prediction (for simplicity, we use square loss in this paper). The goal of the learner is to make predictions with total loss comparable to the loss of the best sparse linear regressor with a bounded norm, where the term sparse refers to the fact that the linear regressor has nonzero weights on at most k features. To measure the performance of the online learner, we use the standard notion of regret, which is the difference between the total loss of the online learner and the total loss of the best sparse linear regressor.\nWhile regret is the primary performance metric, we are also interested in efficiency of the online learner. Ideally, we desire an online learning algorithm that minimizes regret while making predictions efficiently, i.e., in polynomial time (as a function of d and T ). In this paper, we prove that this goal is impossible unless there is a randomized polynomial-time algorithm for deciding satisfiability of 3CNF formulas, the canonical NP-hard problem. This computational hardness result resolves open problems from (Kale, 2014) and (Zolghadr et al., 2013). In fact, the computational hardness persists even if the online learner is given the additional flexibility of choosing k′ = D log(d)k features for any constant D > 0. In light of this result, in this paper we also give an inefficient algorithm for the problem which queries k′ ≥ k + 2 features in each round, that runs in O( (\nd k\n) k′) time per round, and that obtains\nregret bounded by O( d 2 (k′−k)2 √ k log(d)T )."
    }, {
      "heading" : "2 Related Work and Known Results",
      "text" : "A related setting is attribute-efficient learning (Cesa-Bianchi et al., 2011; Hazan and Koren, 2012; Kukliansky and Shamir, 2015). This is a batch learning problem in which the examples are generated i.i.d., and the goal is to simply output a linear regressor using only a limited number of features per example with bounded excess risk compared to the optimal linear regressor, when given full access to the features at test time. While the aforementioned papers give efficient, near-optimal algorithms for this problem, these algorithms do not work in the online sparse regression setting in which we are interested because here we are required to make predictions only using a limited number of features.\nIn (Kale, 2014), a simple algorithm has been suggested, which is based on running a bandit algorithm in which the actions correspond to selecting one of (\nd k\n)\nsubsets of coordinates of size k at regular intervals, and within each interval, running an online regression algorithm (such as the Online Newton-Step algorithm of Hazan et al. (2007)) over the k coordinates chosen by the bandit algorithm. This algorithm, with the right choice of interval lengths, has a regret bound of O(k2dk/3T 2/3 log(T/d)). The algorithm has exponential dependence on k both in running time and the regret. Also, Kale (2014) sketches a different algorithm with performance guarantees similar to the algorithm presented in this paper; our work builds upon that sketch and gives tighter regret bounds.\nZolghadr et al. (2013) consider a very closely related setting (called online probing) in which features and labels may be obtained by the learner at some cost (which may be dif-\nferent for different features), and this cost is factored into the loss of the learner. In the special case of their setting corresponding to the problem considered here, they given an algorithm, LQDExp3, which relies on discretizing all k-sparse weight vectors and running an exponential-weights experts algorithm on the resulting set with stochastic loss estimators, obtaining a O( √ dT ) regret bound. However the running time of their algorithm is prohibitive: O((dT )O(k)) time per iteration. In the same paper, they pose the open problem of finding a computationally efficient no-regret algorithm for the problem. The hardness result in this paper resolves this open problem.\nOn the computational hardness side, it is known that it is NP-hard to compute the optimal sparse linear regressor (Foster et al., 2015; Natarajan, 1995). The hardness result in this paper is in fact inspired by the work of Foster et al. (2015), who proved that it is computationally hard to find even an approximately optimal sparse linear regressor for an ordinary least squares regression problem given a batch of labeled data. While these results imply that it is hard to properly1 solve the offline problem, in the online setting we allow improper learning, and hence these prior results don’t yield hardness results for the online problem considered in this paper."
    }, {
      "heading" : "3 Notation and Setup",
      "text" : "We use the notation [d] = {1, 2, . . . , d} to refer to the coordinates. All vectors in this paper are in Rd, and all matrices in Rd×d. For a subset S of [d], and a vector x, we use the notation x(S) to denote the projection of x on the coordinates indexed by S. We also use the notation IS to denote the diagonal matrix which has ones in the coordinates indexed by S and zeros elsewhere: this is the identity matrix on the subspace of Rd induced by the coordinates in S, as well as the projection matrix for this subspace. We use the notation ‖ · ‖ to denote the ℓ2 norm in R\nd and ‖ · ‖0 to denote the zero “norm,” i.e., the number of nonzero coordinates. We consider a prediction problem in which the examples are vectors in Rd with ℓ2 norm bounded by 1, and labels are in the range [−1, 1]. We use square loss to measure the accuracy of a prediction: i.e., for a labeled example (x, y) ∈ Rd × [−1, 1], the loss of a prediction ŷ is (ŷ−y)2. The learner’s task is to make predictions online as examples arrive one by one based on observing only k out of d features of the learner’s choosing on any example (the learner is allowed to choose different subsets of features to observe in each round). The learner’s goal is to minimize regret relative to the best k-sparse linear regressor whose ℓ2 norm is bounded by 1.\nFormally, for t = 1, 2, . . . , T , the learner:\n1. selects a subset St ⊆ [d] of size at most k,\n2. observes xt(St), i.e., the values of the features of xt restricted to the subset St,\n3. makes a prediction ŷt ∈ [−1, 1], 1Proper learning means finding the optimal sparse linear regressor, whereas improper learning means\nfinding an arbitrary predictor with performance comparable to that of the optimal sparse linear regressor.\n4. observes the true label yt, and suffers loss (ŷt − yt)2. Define regret of the learner as\nRegret :=\nT ∑\nt=1\n(ŷt − yt)2 − min w: ‖w‖0≤k, ‖w‖≤1\nT ∑\nt=1\n(w · xt − yt)2.\nIn case ŷt is chosen using randomization, we consider expected regret instead. Given the NP-hardness of computing the optimal k-sparse linear regressor (Foster et al., 2015; Natarajan, 1995), we also consider a variant of the problem which gives the learner more flexibility than the comparator: the learner is allowed to choose k′ ≥ k coordinates to query in each round. The definition of the regret remains the same. We call this the (k, k′, d)-online sparse regression problem.\nWe are interested in the following two goals2:\n1. (No Regret) Make predictions ŷt so that regret is bounded by poly(d)T 1−δ for some\nδ > 0.\n2. (Efficiency) Make these predictions efficiently, i.e., in poly(d, T ) time per iteration.\nIn this paper, we show it is possible to get an inefficient no-regret algorithm for the online sparse regression problem. Complementing this result, we also show that an efficient noregret algorithm cannot exist, assuming the standard hardness assumption that NP 6⊆ BPP."
    }, {
      "heading" : "4 Upper bound",
      "text" : "In this section we give an inefficient algorithm for the (k, k′, d)-online sparse regression problem which obtains an expected regret of O( d 2\n(k′−k)2 √ k log(d)T ). The algorithm needs k′\nto be at least k + 2. It is inefficient because it maintains statistics for every subset of [d] of size k, of which there are (\nd k\n)\n. At a high level, the algorithm runs an experts algorithm (specifically, Hedge) treating all such subsets as experts. Each expert internally runs stochastic gradient descent only on the coordinates specified by the corresponding subset, ensuring low regret to any bounded norm parameter vector that is nonzero only on those coordinates. The Hedge algorithm ensures low regret to the best subset of coordinates, and thus the overall algorithm achieves low regret with respect to any k-sparse parameter vector. The necessity of using k′ ≥ k + 2 features in the algorithm is that the algorithm uses the additional k′−k features to generate unbiased estimators for xtx ⊤ t and ytxt in each round, which are needed to generate stochastic gradients for all the experts. These estimators have large variance unless k′ − k is large. The pseudocode is given in Algorithm 1. In the algorithm, in round t, the algorithm generates a distribution Dt over the subsets of [d] of size k; for any such subset S, we use the notation Dt(S) to denote the probability of choosing the set S in this distribution. We also define the function Π on Rd to be the projection onto the unit ball, i.e., for w ∈ Rd, Π(w) = w if ‖w‖ ≤ 1, and Π(w) = 1‖w‖w otherwise.\n2In this paper, we use the poly(·) notation to denote a polynomially-bounded function of its arguments.\nAlgorithm 1 Algorithm for Online Sparse Regression\n1: Define the parameters p = k ′−k d , q = (k ′−k)(k′−k−1) d(d−1) , ηHedge = q √ ln(d) T , and ηSGD = q √ 1 T . 2: Let D1 be the uniform distribution over all subsets of [d] of size k. 3: For every subset S of [d] of size k, let wS,1 = 0, the all-zeros vector in R\nd. 4: for t = 1, 2, . . . , T do 5: Sample a subset Ŝt of [d] of size k from Dt, and a subset Rt of [d] of size k\n′ − k drawn uniformly at random, independently of Ŝt.\n6: Acquire xt(St) for St := Ŝt ∪ Rt. 7: Make the prediction ŷt = wŜt,t · xt and obtain the true label yt. 8: Compute the matrix Xt ∈ Rd×d and the vector zt ∈ Rd defined as follows:\nXt(i, j) =\n\n \n \nxt(i)2\np if i = j and i ∈ Rt\nxt(i)xt(j) q if i 6= j and i, j ∈ Rt 0 otherwise,\nand zt(i) =\n{\nytxt(i) p if i ∈ Rt 0 otherwise,\n9: Update the distribution over the subsets: for all subsets S of [d] of size k, let\nDt+1(S) = Dt(S) exp(−ηHedge(w⊤S,tXtwS,t − 2z⊤t wS,t + y2t ))/Zt,\nwhere Zt is the normalization factor to make Dt+1 a distribution. 10: For each subset S of [d] of size k, let\nwS,t+1 = Π(wS,t − 2ηSGDIS(XtwS,t − zt)).\n11: end for\nTheorem 1. There is an algorithm for the online sparse regression problem with any given parameters (k, k′, d) such that k′ ≥ k + 2 running in O( (\nd k\n) · k′) time per iteration with O( d 2\n(k′−k)2 √ k log(d)T ) expected regret.\nProof. The algorithm is given in Algorithm 1. Since the algorithm maintains a parameter vector in Rk for each subset of [d] of size k, the running time is dominated by the time to sample from Dt and update it, and the time to update the parameter vectors. The updates can be implemented in O(k′) time, so overall each round can be implemented in O( (\nd k\n) · k′) time.\nWe now analyze the regret of the algorithm. Let Et[·] denote the expectation conditioned on all the randomness prior to round t. Then, it is easy to check, using the fact that k′−k ≥ 2, that the construction of Xt and zt in Step 8 of the algorithm has the following property:\nEt[Xt] = xtx ⊤ t and Et[zt] = ytxt. (1)\nNext, notice that in Step 9, the algorithm runs the standard Hedge-algorithm update (see, for example, Section 2.1 in (Arora et al., 2012)) on (\nd k\n)\nexperts, one for each subset of\n[d] of size k, where, in round t, the cost of the expert corresponding to subset S is defined to be3 w⊤S,tXtwS,t − 2z⊤t wS,t + y2t . (2) It is easy to check, using the facts that ‖xt‖ ≤ 1, ‖wS,t‖ ≤ 1 and p ≥ q, that the cost (2) is bounded deterministically in absolute value by O(1\nq ) = O( d\n2\n(k′−k)2 ). Let EDt [·] denote the expectation over the random choice of Ŝt from the distribution Dt conditioned on all other randomness up to and including round t. Since there are (\nd k\n)\n≤ dk experts in the Hedge algorithm here, the standard regret bound for Hedge (Arora et al., 2012, Theorem 2.3) with the specified value of ηHedge implies that for any subset S of [d] of size k, using ln ( d k )\n≤ k ln d, we have\nT ∑\nt=1\nEDt [wŜt,tXtwŜt,t−2z ⊤ t wŜt,t+y 2 t ] ≤\nT ∑\nt=1\n(w⊤S,tXtwS,t−2z⊤t wS,t+y2t )+O( d 2 (k′−k)2 √ k ln(d)T ).\n(3) Next, we note, using (1) and the fact that conditioned on the randomness prior to round t, wS,t is completely determined, that (for any S)\nEt[w ⊤ S,tXtwS,t − 2z⊤t wS,t + y2t ] = w⊤S,txtx⊤t wS,t − 2ytx⊤t wS,t + y2t = (wS,t · xt − yt)2. (4)\nTaking expectations on both sides of (3) over all the randomness in the algorithm, and using (4), we get that for any subset S of [d] of size k, we have\nT ∑\nt=1\nE[(wŜt,t · xt − yt) 2] ≤\nT ∑\nt=1\nE[(wS,t · xt − yt)2] +O( d 2 (k′−k)2 √ k log(d)T ). (5)\nThe left-hand side of (5) equals ∑T t=1 E[(ŷt − yt)2]. We now analyze the right-hand side. For any given subset S of [d] of size k, we claim that in Step 10 of the algorithm, the parameter vector wS,t is updated using stochastic gradient descent with the loss function ℓt(w) := (x ⊤ t ISw − yt)2 over the set over {w | ||w||2 ≤ 1}, only on the coordinates in S, while the coordinates not in S are fixed to 0. To prove this claim, first, we note that the premultiplication by IS in the update in Step 10 ensures that in the parameter vector wS,t+1 all coordinates that are not in S are set to 0, assuming that coordinates of wS,t not in S were 0.\nNext, at time t, consider the loss function ℓt(w) = (x ⊤ t ISw − yt)2. The gradient of this\nloss function at wS,t is\n∇ℓt(wS,t) = 2(x⊤t ISwS,t − yt)ISxt = 2IS(xtx⊤t wS,t − ytxt),\nwhere we use the fact that ISwS,t = wS,t since wS,t has zeros in coordinates not in S. Now, by (1), we have\nEt[2IS(XtwS,t − zt)] = 2IS(xtx⊤t wS,t − ytxt), 3Recall that the costs in Hedge may be chosen adaptively.\nand thus, Step 10 of the algorithm is a stochastic gradient descent update as claimed. Furthermore, a calculation similar to the one for bounding the loss of the experts in the Hedge algorithm shows that the norm of the stochastic gradient is bounded deterministically by O(1\nq ), which is O( d\n2\n(k′−k)2 ).\nUsing a standard regret bound for stochastic gradient descent (see, for example, Lemma 3.1 in (Flaxman et al., 2005)) with the specified value of ηSGD, we conclude that for any fixed vector w of ℓ2 norm at most 1, we have,\nT ∑\nt=1\nE[(x⊤t ISwS,t − yt)2] ≤ T ∑\nt=1\n(x⊤t ISw − yt)2 +O( d 2 (k′−k)2 √ T ).\nSince ISwS,t = wS,t, the left hand side of the above inequality equals ∑T t=1 E[(wS,t ·xt−yt)2]. Finally, let w be an arbitrary k-sparse vector of ℓ2 norm at most 1. Let S = {i | wi 6= 0}. Note that |S| ≤ k, and IS(w) = w. Applying the above bound for this set S, we get T ∑\nt=1\nE[(wS,t · xt − yt)2] ≤ T ∑\nt=1\n(w · xt − yt)2 +O( d 2 (k′−k)2 √ T ). (6)\nCombining the inequality (6) with inequality (5), we conclude that\nT ∑\nt=1\nE[(ŷt − yt)2] ≤ T ∑\nt=1\n(w · xt − yt)2 +O( d 2 (k′−k)2 √ k log(d)T ).\nThis gives us the required regret bound."
    }, {
      "heading" : "5 Computational lower bound",
      "text" : "In this section we show that there cannot exist an efficient no-regret algorithm for the online sparse regression problem unless NP ⊆ BPP. This hardness result follows from the hardness of approximating the Set Cover problem. We give a reduction showing that if there were an efficient no-regret algorithm AlgOSR for the online sparse regression problem, then we could distinguish, in randomized polynomial time, between two instances of the Set Cover problem: in one of which there is a small set cover, and in the other of which any set cover is large. This task is known to be NP-hard for specific parameter values. Specifically, our reduction has the following properties:\n1. If there is a small set cover, then in the induced online sparse regression problem there is a k-sparse parameter vector (of ℓ2 norm at most 1) giving 0 loss, and thus the algorithm AlgOSR must have small total loss (equal to the regret) as well.\n2. If there is no small set cover, then the prediction made by AlgOSR in any round has at least a constant loss in expectation, which implies that its total (expected) loss must be large, in fact, linear in T .\nBy measuring the total loss of the algorithm, we can distinguish between the the two instances of the Set Cover problem mentioned above with probability at least 3/4, thus yielding a BPP algorithm for an NP-hard problem.\nThe starting point for our reduction is the work of Dinur and Steurer (2014) who give a polynomial-time reduction of deciding satisfiability of 3CNF formulas to distinguishing instances of Set Cover with certain useful combinatorial properties. We denote the satisfiability problem of 3CNF formulas by SAT.\nReduction 1. For any given constant D > 0, there is a constant cD ∈ (0, 1) and a poly(nD)time algorithm that takes a 3CNF formula φ of size n as input and constructs a Set Cover instance over a ground set of size m = poly(nD) with d = poly(n) sets, with the following properties:\n1. if φ ∈ SAT, then there is a collection of k = O(dcD) sets, which covers each element exactly once, and\n2. if φ /∈ SAT, then no collection of k′ = ⌊D ln(d)k⌋ sets covers all elements; i.e., at least one element is left uncovered.\nThe Set Cover instance generated from φ can be encoded as a binary matrix Mφ ∈ {0, 1}m×d with the rows corresponding to the elements of the ground set, and the columns correspond to the sets, such that each column is the indicator vector of the corresponding set.\nUsing this reduction, we now show how an efficient, no-regret algorithm for online sparse regression can be used to give a BPP algorithm for SAT.\nTheorem 2. Let D > 0 be any given constant. Suppose there is an algorithm, AlgOSR, for the (k, k′, d)-online sparse regression problem with k = O(dcD), where cD is the constant from Reduction 1, and k′ = ⌊D ln(d)k⌋, that runs in poly(d, T ) time per iteration and has expected regret bounded by poly(d)T 1−δ for some constant δ > 0. Then NP ⊆ BPP.\nProof. Since the expected regret of AlgOSR is bounded by p(d)T 1−δ (where p(d) is a polynomial function of d), by Markov’s inequality we conclude that with probability at least 3/4, the regret of AlgOSR is bounded by p(d)·T 1−δ. Figure 2 gives a randomized algorithm, AlgSAT, for deciding satisfiability of a given 3CNF formula φ using the algorithm AlgOSR. Note that the feature vectors (i.e., the xt vectors) generated by AlgSAT are bounded in ℓ2 norm by 1, as required. It is clear that AlgSAT is a polynomial-time algorithm since T is a polynomial function of n (since m, k, d, p(d) are polynomial functions of n), and AlgOSR runs in poly(d, T ) time per iteration.\nWe now claim that this algorithm correctly decides satisfiability of φ with probability at least 3/4, and is hence a BPP algorithm for SAT.\nTo prove this, suppose φ ∈ SAT. Then, there are k sets in the Set Cover which cover all elements with each element being covered exactly once. Consider the k-sparse parameter vector w which has 1√\nk in the positions corresponding to these k sets and 0 elsewhere. Note\nthat ‖w‖ ≤ 1, as required. Note that since this collection of k sets covers each element\nAlgorithm 2 Algorithm AlgSAT for deciding satisfiability of 3CNF formulas\nRequire: A constant D > 0, and an algorithm AlgOSR for the (k, k ′, d)-online sparse re-\ngression problem with k = O(dcD), where cD is the constant from Reduction 1, and k′ = ⌊D ln(d)k⌋, that runs in poly(d, T ) time per iteration with regret bounded by p(d) · T 1−δ with probability at least 3/4.\nRequire: A 3CNF formula φ. 1: Compute the matrix Mφ and the associated parameters k, k\n′, d,m from Reduction 1. 2: Run AlgOSR with the parameters k, k\n′, d for T := ⌈max{(2p(d)mdk)1/δ, 256m2d2k2}⌉ iterations.\n3: for t = 1, 2, . . . , T do 4: Sample a row of Mφ uniformly at random; call it x̂t. 5: Sample σt ∈ {−1, 1} uniformly at random independently of x̂t. 6: Set xt =\nσt√ d x̂t and yt = σt√ dk .\n7: Obtain a set of coordinates St of size at most k ′ by running AlgOSR, and provide it the coordinates xt(St). 8: Obtain the prediction ŷt from AlgOSR, and provide it the true label yt. 9: end for\n10: if ∑T t=1(yt − ŷt)2 ≤ T2mdk then 11: Output “satisfiable”. 12: else 13: Output “unsatisfiable”. 14: end if\nexactly once, we have Mφw = 1√ k 1, where 1 is the all-1’s vector. In particular, since x̂t is a row of Mφ, we have\nw · xt = w · ( σt√ d x̂t ) = σt√ dk = yt.\nThus, (w ·xt−yt)2 = 0 for all rounds t. Since algorithm AlgOSR has regret at most p(d) ·T 1−δ with probability at least 3/4, its total loss\n∑T t=1(ŷt − yt)2 is bounded by p(d) · T 1−δ ≤ T2mdk\n(since T ≥ (2p(d)mdk)1/δ) with probability at least 3/4. Thus, in this case algorithm AlgSAT correctly outputs “satisfiable” with probability at least 3/4.\nNext, suppose φ /∈ SAT. Fix any round t and let St be the set of coordinates of size at most k′ selected by algorithm AlgOSR to query. This set St corresponds to k\n′ sets in the Set Cover instance. Since φ /∈ SAT, there is at least one element in the ground set that is not covered by any set among these k′ sets. This implies that there is at least one row of Mφ that is 0 in all the coordinates in St. Since x̂t is a uniformly random row of Mφ chosen independently of St, we have\nPr[xt(St) = 0] = Pr[x̂t(St) = 0] ≥ 1\nm .\nHere, 0 denotes the all-zeros vector of size k′.\nNow, we claim that E[ytŷt | xt(St) = 0] = 0. This is because the condition that xt(St) = 0 is equivalent to the condition that x̂t(St) = 0. Since yt is chosen from {− 1√dk , 1√ dk } uniformly at random independently of x̂t and ŷt, the claim follows. The expected loss of the online algorithm in round t can now be bounded as follows:\nE[ (ŷt − yt)2 ∣ ∣ xt(St) = 0] = E\n[\nŷ2t + 1\ndk − 2ytŷt\n∣ ∣ ∣ ∣ xt(St) = 0 ]\n= E\n[\nŷ2t + 1\ndk\n∣ ∣ ∣ ∣ xt(St) = 0 ] ≥ 1 dk ,\nand hence\nE[(yt − ŷt)2] ≥ E[(yt − ŷt)2 | xt(St) = 0] · Pr[xt(St) = 0] ≥ 1 dk · 1 m = 1 mdk .\nLet Et[·] denote expectation of a random variable conditioned on all randomness prior to round t. Since the choices of xt and yt are independent of previous choices in each round, the same argument also implies that Et[(yt − ŷt)2] ≥ 1mdk . Applying Azuma’s inequality (see Theorem 7.2.1 in (Alon and Spencer, 1992)) to the martingale difference sequence Et[(yt − ŷt)\n2] − (yt − ŷt)2 for t = 1, 2, . . . , T , since each term is bounded in absolute value by 4, we get\nPr\n[\nT ∑\nt=1\nEt[(yt − ŷt)2]− (yt − ŷt)2 ≥ 8 √ T\n]\n≤ exp ( − 64T 2·16T ) ≤ 1 4 .\nThus, with probability at least 3/4, the total loss ∑T t=1(ŷt−yt)2 is greater than ∑T t=1 Et[(yt− ŷt) 2]− 8 √ T ≥ 1 mdk T − 8 √ T ≥ T 2mdk (since T ≥ 256m2d2k2). Thus in this case the algorithm correctly outputs “unsatisfiable” with probability at least 3/4.\nParameter settings for hard instances. Theorem 2 implies that for any given constant D > 0, there is a constant cD such that the parameter settings k = O(d\ncD), and k′ = ⌊D ln(d)k⌋ yield hard instances for the online sparse regression problem. The reduction of Dinur and Steurer (2014) can be “tweaked”4 so that the cD is arbitrarily close to 1 for any constant D.\nWe can now extend the hardness results to the parameter settings k = O(dǫ) for any ǫ ∈ (0, 1) and k′ = ⌊D ln(d)k⌋ either by tweaking the reduction of Dinur and Steurer (2014) so it yields cD = ǫ if ǫ is close enough to 1, or if ǫ is small, by adding O(d\n1/ǫ) all-zeros columns to the matrix Mφ. The two combinatorial properties of Mφ in Reduction 1 are clearly still satisfied, and the proof of Theorem 2 goes through.\n4This is accomplished by simply replacing the Label Cover instance they construct with polynomially many disjoint copies of the same instance."
    }, {
      "heading" : "6 Conclusions",
      "text" : "In this paper, we prove that minimizing regret in the online sparse regression problem is computationally hard even if the learner is allowed access to many more features than the comparator, a sparse linear regressor. We complement this result by giving an inefficient no-regret algorithm.\nThe main open question remaining from this work is what extra assumptions can one make on the examples arriving online to make the problem tractable. Note that the sequence of examples constructed in the lower bound proof is i.i.d., so clearly stronger assumptions than that are necessary to obtain any efficient algorithms."
    } ],
    "references" : [ {
      "title" : "The Probabilistic Method",
      "author" : [ "Noga Alon", "Joel Spencer" ],
      "venue" : "John Wiley,",
      "citeRegEx" : "Alon and Spencer.,? \\Q1992\\E",
      "shortCiteRegEx" : "Alon and Spencer.",
      "year" : 1992
    }, {
      "title" : "The multiplicative weights update method: a meta-algorithm and applications",
      "author" : [ "Sanjeev Arora", "Elad Hazan", "Satyen Kale" ],
      "venue" : "Theory of Computing,",
      "citeRegEx" : "Arora et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Arora et al\\.",
      "year" : 2012
    }, {
      "title" : "Efficient learning with partially observed attributes",
      "author" : [ "Nicolò Cesa-Bianchi", "Shai Shalev-Shwartz", "Ohad Shamir" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Cesa.Bianchi et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Cesa.Bianchi et al\\.",
      "year" : 2011
    }, {
      "title" : "Analytical approach to parallel repetition",
      "author" : [ "Irit Dinur", "David Steurer" ],
      "venue" : "In STOC, pages 624–633,",
      "citeRegEx" : "Dinur and Steurer.,? \\Q2014\\E",
      "shortCiteRegEx" : "Dinur and Steurer.",
      "year" : 2014
    }, {
      "title" : "Online convex optimization in the bandit setting: gradient descent without a gradient",
      "author" : [ "Abraham Flaxman", "Adam Tauman Kalai", "H. Brendan McMahan" ],
      "venue" : "In SODA,",
      "citeRegEx" : "Flaxman et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Flaxman et al\\.",
      "year" : 2005
    }, {
      "title" : "Variable selection is hard",
      "author" : [ "Dean Foster", "Howard Karloff", "Justin Thaler" ],
      "venue" : "In COLT,",
      "citeRegEx" : "Foster et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Foster et al\\.",
      "year" : 2015
    }, {
      "title" : "Linear regression with limited observation",
      "author" : [ "Elad Hazan", "Tomer Koren" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Hazan and Koren.,? \\Q2012\\E",
      "shortCiteRegEx" : "Hazan and Koren.",
      "year" : 2012
    }, {
      "title" : "Logarithmic regret algorithms for online convex optimization",
      "author" : [ "Elad Hazan", "Amit Agarwal", "Satyen Kale" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Hazan et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Hazan et al\\.",
      "year" : 2007
    }, {
      "title" : "Open problem: Efficient online sparse regression",
      "author" : [ "Satyen Kale" ],
      "venue" : "In COLT, pages 1299–1301,",
      "citeRegEx" : "Kale.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kale.",
      "year" : 2014
    }, {
      "title" : "Attribute efficient linear regression with distributiondependent sampling",
      "author" : [ "Doron Kukliansky", "Ohad Shamir" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Kukliansky and Shamir.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kukliansky and Shamir.",
      "year" : 2015
    }, {
      "title" : "Sparse approximate solutions to linear systems",
      "author" : [ "B.K. Natarajan" ],
      "venue" : "SIAM J. Computing,",
      "citeRegEx" : "Natarajan.,? \\Q1995\\E",
      "shortCiteRegEx" : "Natarajan.",
      "year" : 1995
    }, {
      "title" : "Online learning with costly features and labels",
      "author" : [ "Navid Zolghadr", "Gábor Bartók", "Russell Greiner", "András György", "Csaba Szepesvári" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Zolghadr et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Zolghadr et al\\.",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "This computational hardness result resolves an open problem presented in COLT 2014 (Kale, 2014) and also posed by Zolghadr et al.",
      "startOffset" : 83,
      "endOffset" : 95
    }, {
      "referenceID" : 8,
      "context" : "This computational hardness result resolves an open problem presented in COLT 2014 (Kale, 2014) and also posed by Zolghadr et al. (2013). This hardness result holds even if the algorithm is allowed to access more features than the best sparse linear regressor up to a logarithmic factor in the dimension.",
      "startOffset" : 84,
      "endOffset" : 137
    }, {
      "referenceID" : 2,
      "context" : "One example of this scenario, from (Cesa-Bianchi et al., 2011), is medical diagnosis of a disease, in which each feature corresponds to a medical test that the patient in question can undergo.",
      "startOffset" : 35,
      "endOffset" : 62
    }, {
      "referenceID" : 8,
      "context" : "This computational hardness result resolves open problems from (Kale, 2014) and (Zolghadr et al.",
      "startOffset" : 63,
      "endOffset" : 75
    }, {
      "referenceID" : 11,
      "context" : "This computational hardness result resolves open problems from (Kale, 2014) and (Zolghadr et al., 2013).",
      "startOffset" : 80,
      "endOffset" : 103
    }, {
      "referenceID" : 2,
      "context" : "A related setting is attribute-efficient learning (Cesa-Bianchi et al., 2011; Hazan and Koren, 2012; Kukliansky and Shamir, 2015).",
      "startOffset" : 50,
      "endOffset" : 129
    }, {
      "referenceID" : 6,
      "context" : "A related setting is attribute-efficient learning (Cesa-Bianchi et al., 2011; Hazan and Koren, 2012; Kukliansky and Shamir, 2015).",
      "startOffset" : 50,
      "endOffset" : 129
    }, {
      "referenceID" : 9,
      "context" : "A related setting is attribute-efficient learning (Cesa-Bianchi et al., 2011; Hazan and Koren, 2012; Kukliansky and Shamir, 2015).",
      "startOffset" : 50,
      "endOffset" : 129
    }, {
      "referenceID" : 8,
      "context" : "In (Kale, 2014), a simple algorithm has been suggested, which is based on running a bandit algorithm in which the actions correspond to selecting one of ( d k )",
      "startOffset" : 3,
      "endOffset" : 15
    }, {
      "referenceID" : 7,
      "context" : "subsets of coordinates of size k at regular intervals, and within each interval, running an online regression algorithm (such as the Online Newton-Step algorithm of Hazan et al. (2007)) over the k coordinates chosen by the bandit algorithm.",
      "startOffset" : 165,
      "endOffset" : 185
    }, {
      "referenceID" : 7,
      "context" : "subsets of coordinates of size k at regular intervals, and within each interval, running an online regression algorithm (such as the Online Newton-Step algorithm of Hazan et al. (2007)) over the k coordinates chosen by the bandit algorithm. This algorithm, with the right choice of interval lengths, has a regret bound of O(kdT 2/3 log(T/d)). The algorithm has exponential dependence on k both in running time and the regret. Also, Kale (2014) sketches a different algorithm with performance guarantees similar to the algorithm presented in this paper; our work builds upon that sketch and gives tighter regret bounds.",
      "startOffset" : 165,
      "endOffset" : 444
    }, {
      "referenceID" : 7,
      "context" : "subsets of coordinates of size k at regular intervals, and within each interval, running an online regression algorithm (such as the Online Newton-Step algorithm of Hazan et al. (2007)) over the k coordinates chosen by the bandit algorithm. This algorithm, with the right choice of interval lengths, has a regret bound of O(kdT 2/3 log(T/d)). The algorithm has exponential dependence on k both in running time and the regret. Also, Kale (2014) sketches a different algorithm with performance guarantees similar to the algorithm presented in this paper; our work builds upon that sketch and gives tighter regret bounds. Zolghadr et al. (2013) consider a very closely related setting (called online probing) in which features and labels may be obtained by the learner at some cost (which may be dif-",
      "startOffset" : 165,
      "endOffset" : 642
    }, {
      "referenceID" : 5,
      "context" : "On the computational hardness side, it is known that it is NP-hard to compute the optimal sparse linear regressor (Foster et al., 2015; Natarajan, 1995).",
      "startOffset" : 114,
      "endOffset" : 152
    }, {
      "referenceID" : 10,
      "context" : "On the computational hardness side, it is known that it is NP-hard to compute the optimal sparse linear regressor (Foster et al., 2015; Natarajan, 1995).",
      "startOffset" : 114,
      "endOffset" : 152
    }, {
      "referenceID" : 5,
      "context" : "On the computational hardness side, it is known that it is NP-hard to compute the optimal sparse linear regressor (Foster et al., 2015; Natarajan, 1995). The hardness result in this paper is in fact inspired by the work of Foster et al. (2015), who proved that it is computationally hard to find even an approximately optimal sparse linear regressor for an ordinary least squares regression problem given a batch of labeled data.",
      "startOffset" : 115,
      "endOffset" : 244
    }, {
      "referenceID" : 5,
      "context" : "Given the NP-hardness of computing the optimal k-sparse linear regressor (Foster et al., 2015; Natarajan, 1995), we also consider a variant of the problem which gives the learner more flexibility than the comparator: the learner is allowed to choose k′ ≥ k coordinates to query in each round.",
      "startOffset" : 73,
      "endOffset" : 111
    }, {
      "referenceID" : 10,
      "context" : "Given the NP-hardness of computing the optimal k-sparse linear regressor (Foster et al., 2015; Natarajan, 1995), we also consider a variant of the problem which gives the learner more flexibility than the comparator: the learner is allowed to choose k′ ≥ k coordinates to query in each round.",
      "startOffset" : 73,
      "endOffset" : 111
    }, {
      "referenceID" : 1,
      "context" : "1 in (Arora et al., 2012)) on ( d k )",
      "startOffset" : 5,
      "endOffset" : 25
    }, {
      "referenceID" : 4,
      "context" : "1 in (Flaxman et al., 2005)) with the specified value of ηSGD, we conclude that for any fixed vector w of l2 norm at most 1, we have,",
      "startOffset" : 5,
      "endOffset" : 27
    }, {
      "referenceID" : 3,
      "context" : "The starting point for our reduction is the work of Dinur and Steurer (2014) who give a polynomial-time reduction of deciding satisfiability of 3CNF formulas to distinguishing instances of Set Cover with certain useful combinatorial properties.",
      "startOffset" : 52,
      "endOffset" : 77
    }, {
      "referenceID" : 0,
      "context" : "1 in (Alon and Spencer, 1992)) to the martingale difference sequence Et[(yt − ŷt) ] − (yt − ŷt) for t = 1, 2, .",
      "startOffset" : 5,
      "endOffset" : 29
    }, {
      "referenceID" : 3,
      "context" : "The reduction of Dinur and Steurer (2014) can be “tweaked” so that the cD is arbitrarily close to 1 for any constant D.",
      "startOffset" : 17,
      "endOffset" : 42
    }, {
      "referenceID" : 3,
      "context" : "The reduction of Dinur and Steurer (2014) can be “tweaked” so that the cD is arbitrarily close to 1 for any constant D. We can now extend the hardness results to the parameter settings k = O(d) for any ǫ ∈ (0, 1) and k′ = ⌊D ln(d)k⌋ either by tweaking the reduction of Dinur and Steurer (2014) so it yields cD = ǫ if ǫ is close enough to 1, or if ǫ is small, by adding O(d ) all-zeros columns to the matrix Mφ.",
      "startOffset" : 17,
      "endOffset" : 294
    } ],
    "year" : 2016,
    "abstractText" : "We consider the online sparse linear regression problem, which is the problem of sequentially making predictions observing only a limited number of features in each round, to minimize regret with respect to the best sparse linear regressor, where prediction accuracy is measured by square loss. We give an inefficient algorithm that obtains regret bounded by Õ( √ T ) after T prediction rounds. We complement this result by showing that no algorithm running in polynomial time per iteration can achieve regret bounded by O(T 1−δ) for any constant δ > 0 unless NP ⊆ BPP. This computational hardness result resolves an open problem presented in COLT 2014 (Kale, 2014) and also posed by Zolghadr et al. (2013). This hardness result holds even if the algorithm is allowed to access more features than the best sparse linear regressor up to a logarithmic factor in the dimension.",
    "creator" : "LaTeX with hyperref package"
  }
}