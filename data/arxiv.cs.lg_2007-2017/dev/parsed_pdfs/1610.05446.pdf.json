{
  "name" : "1610.05446.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Provably Good Early Detection of Diseases using Non-Sparse Covariance-Regularized Linear Discriminant Analysis",
    "authors" : [ "Haoyi Xiong", "Yanjie Fu", "Wenqing Hu", "Guanling Chen", "Laura E. Barnes" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The availability of Electronic Health Records (EHR) [1,2] in healthcare settings provides an unique opportunity for early detection of patients’ potential diseases using their historical health records. To predict the patients’ future disease using EHR data, existing work proposed to first extract useful features, such as diagnosis-frequencies [1–3], pairwise diagnosis transition [4,5], and graphs of diagnosis sequences [6], to represent each patient’s EHR data using the representation learning techniques. Then, a series of supervised learning techniques have been adopted to train predictive models, such as Support Vector Machine (SVM), Random Forest (RF), Bayesian Network, Linear Discriminant Analysis (LDA) [1–4, 7], using well represented EHR data with the labels of the target disease.\nAmong these methods, LDA with diagnosis-frequency vectors is frequently used as one of the common performance benchmarks [4, 7], because of LDA’s provable bayesian optimality [8]. However, recent studies demonstrate the limitation of LDA on high dimension data [9–12], such as the EHR records [13]. Because it is difficult to recover the “true” parameters, e.g., covariance matrix, from a relatively small number of samples [14]. According to the expected rate estimation for LDA classifiers [15, 16], LDA performs poorly with high misclassification rate, when the parameter\nestimation is inaccurate, under high dimension settings. For example, to predict some “less represented” dis-\neases in primary care settings, such as depression & anxiety disorders, a small number of patients having been diagnosed with target disease are given to train the LDA model. When the number of dimensions of EHR data is larger than the number of samples, the sample covariance estimation, which is frequently used in typical LDA, is singular and not invertible. Thus LDA cannot produce any valid prediction in this case. Even when the sample size is larger than the number of dimensions, the sample (inverse) covariance estimation could be quite different with the “true” (inverse) covariance matrix, as discussed in details in a recent survey [14].\nTo improve LDA learning, several regularization-based methods have been proposed to accurately estimate the (inverse) covariance matrix [10,17,18] or linear coefficients [9, 10] under high dimension and low sample size (HDLSS) settings [19]. One milestone is Covariance Regularized Discriminant Analysis proposed by Witten and Tibshirani et al. [12] based on their previous contribution to the sparse inverse covariance estimation using Graphical Lasso [20]. While existing work enhanced LDA through pursuing the sparsity of parameter estimation [9, 10, 12, 20], in this work we introduce a novel non-sparse (de-sparsified) inverse covariance matrix estimator [21] for further performance improvement.\nSpecifically, in this paper, we made following contributions:\n1) We study the problem of improving the performance of early detection of diseases using LDA models and EHR data. To the best of our knowledge, this is the first study that focuses on lowering the expected error rate of LDA models with optimized parameter estimation, for EHR-based early detection of diseases, under High Dimension and Low Sample Size settings.\n2) We proposed E2D2 based on regularized LDA models using diagnosis-frequency representation of EHR data. Different from the existing sparse LDA models, which regularize the covariance matrix [10] or linear classification coefficients [9] to leverage sparse estimation of parameters, the proposed method uses a non-sparse estimator based on graphical lasso [20] to work with LDA models. Theoretical analysis on E2D2 shows that it can bound the maximal ex-\nar X\niv :1\n61 0.\n05 44\n6v 2\n[ cs\n.L G\n] 1\n9 O\nct 2\n01 6"
    }, {
      "heading" : "2.1 LDA for Early Detection of Disease",
      "text" : ""
    }, {
      "heading" : "2 Preliminaries and Problem Formulation",
      "text" : "covariance matrix estimation converge to the population covariance matrix so Σ̄ → Σ. Further, when p >> m, the sample covariance matrix Σ̄ is singular and cannot be invert i.e., Σ̄−1 may not exist. Though some existing works suggested to use pseudo-inverse to approximate the inverse covariace matrix, the accuracy of LDA might be low. More introduction to the covariance matrix estimation under HDLSS settings can be found in survey [14]."
    }, {
      "heading" : "2.2 Performance Analysis of LDA",
      "text" : "In this section, we summarize the series of studies [15, 16,\n24, 25] in theoretical error rate estimation for LDA classifiers. Consider two p-dimension Gaussian Distributions N (µ+,Σ) and N (µ−,Σ) sharing the same covariance matrix Σ but with two different mean vectors µ+ and µ−, where Σ is assumed to be unknown. Given samples of these two distributions, we can estimate the covariance matrix Σ̂, mean vectors µ̂+ and µ̂−. The expected error rate of a linear discriminant analysis (i.e., probability of the missed classification) [15, 16] is: (2.3)\nε(µ+, µ−,Σ, µ̂+, µ̂−, Σ̂) =\nπ+ ∗ Φ  − (µ+ − (µ̂++µ̂−) 2 )\nT Σ̂−1(µ̂+ − µ̂−)√ (µ̂+ − µ̂−)T Σ̂−1ΣΣ̂−1(µ̂+ − µ̂−)\n \n+ π− ∗ Φ\n  (µ− − (µ̂++µ̂−) 2 )\nT Σ̂−1(µ̂+ − µ̂−)√ (µ̂+ − µ̂−)T Σ̂−1ΣΣ̂−1(µ̂+ − µ̂−)\n \nwhere Φ refers to the CDF function of a standard normal distribution.\nIt is obvious that the expected error rate is sensitive with the parameters µ+, µ−,Σ, µ̂+, µ̂− and Σ̂, while the true parameters µ+, µ−,Σ are assumed unknown. Even under the HDLSS settings, with a certain number of samples, it is reasonable to assume the sample estimation of mean vectors µ̄+ and µ̄− should be close to the population mean vectors, i.e., µ+ − µ̄+ → 0, µ−µ̄− → 0, and µ − µ̄ → 0. Thus, the expected error rate of the LDA model based on the sample mean vectors µ̄+ and µ̄− can be reduced to (2.4)\nε(Σ, Σ̂) = Φ  − (µ̄+ − µ̄−)\nT Σ̂−1(µ̄+ − µ̄−) 2 √ (µ̄+ − µ̄−)T Σ̂−1ΣΣ̂−1(µ̄+ − µ̄−)\n  .\nIn this way, to improve the LDA classifier with the sample mean vectors, there needs an estimator Σ̂ to minimize or lower the expected error rate ε(Σ, Σ̂)."
    }, {
      "heading" : "2.3 Problem Definition",
      "text" : "Inspired by the above preliminaries, we consider the re-\nsearch problem of improving the performance of LDA for early disease detection as minimizing the expected error rate listed in 2.3. However, the true parameter Σ are not known\nin our research settings. Thus, given the training set (x0, l0) . . . (xm−1, lm−1), the objective thereby can be further reduced to find the OPTIMAL estimation of Σ̂, µ̂+ and µ̂− that minimize the expectation of the expected error rate among all possible true parameters µ+ ∈ Rp, µ− ∈ Rp, and Σ ∈ I+p×p:\n(2.5) argmin µ̂+,µ̂−,Σ̂\n∫\nΣ∈I+p×p\nε(Σ, Σ̂)L((x0, l0) . . . |Σ)dΣ\nwhere I+p×p is the overall set of p × p positive-definite matrices, and L((x0, l0) . . . |Σ) is defined as the likelihood of the observed training samples (x0, l0) . . . (xm−1, lm−1) with the given Σ. Note that, when we have no prior knowledge about the true Σ, we can use the likelihood as the probability of the true parameters when training samples are given. A good solution to this problem should be able to minimize, and even bound the expectation of the expected error rate.\n3 E2D2: EHR-based Early Detection of Diseases using Non-Sparse Covariance-Regularized LDA\nIn this section, we introduce our proposed method for early disease detection using EHR diagnosis-frequency vectors.\nConsidering the computational complexity of the parameter search, we do not try to simply minimize the objective function in Eq. 2.5. Our proposed method consists of two phases: it first estimates a non-sparse inverse covariance matrix using with the training data set; then it incorporates the new estimation of inverse covariance matrix into a LDA classifier to predict whether the new patient will develop the targeted disease.\n3.1 Non-Sparse Inverse Covariance Matrix Estimation Using EHR Diagnosis Vectors Given the EHR diagnosis-frequency vectors x0, . . . xm−1, this phase learns a non-sparse covariance matrix, with the following three steps.\nStep 1. Estimate Sample Mean Vectors and Covariance Matrix - The proposed method first estimates the sample mean vectors µ̄+ and µ̄− using the training samples (x0, l0), (x1, l1), . . . (xm−1, lm−1). Then with the training samples, this step estimates the sample covariance matrix Σ̄ using maximized likelihood estimator addressed in Equation. 2.1.\nStep 2. Estimate Sparse Inverse Covariance Matrix - Given the sample covariance matrix Σ̂, this method estimates a sparse inverse covariance matrix Θ̂ using the Graphical Lasso estimator: (3.6)\nΘ̂ = argmin Θ∈I+p×p\n tr(Σ̄Θ)− log det(Θ) + λ ∑\nj 6=k |Θjk|\n .\nGraphical Lasso can be considered as a `1-penalized negative log-likelihood minimization estimator, which provides a sparse inverse covariance matrix Θ̂.\nStep 3. De-sparsify Inverse Covariance Matrix - Given the graphical lasso estimation Σ̂ and the sample estimation Σ̄, this step outputs a non-sparse inverse covariance matrix T̂ through de-sparsified Σ̂. The calculation is as follow:\nT̂ = 2Θ̂− Θ̂Σ̄Θ̂, where Σ̄ refers to the sample covariance matrix and Θ̂ refers to the graphical lasso estimation of inverse covariance matrix."
    }, {
      "heading" : "3.2 Integrated LDA Classification",
      "text" : "Given the estimated mean vectors µ̄+, µ̄−, inverse covariance matrix T̂ , and a vector of new patient x, E2D2 decide if the patient will develop the target disease, using a FDA model derived from Eq. 2.2 as:\n(3.7) sign ( log\nxT T̂ µ̄+ − 12 µ̄T+T̂ µ̄+ + log π+ xT T̂ µ̄− − 12 µ̄T−T̂ µ̄− + log π−\n) ,\nWhen above equation returns +1, then E2D2 classifies x as the patient who will develop the target disease. We call above LDA derivation as Non-Sparse Covariance-Regularized Discriminant Analysis, with respect to Witen and Tibishirani’s sparse Covariance-Regularized Discriminant Analysis [12], which was based on the Graphical Lasso."
    }, {
      "heading" : "4 Algorithm Analysis",
      "text" : "We report the theoretical analysis of E2D2, as follow:\n1. We first introduce a new upper bound of expected error rate ε(Σ̂,Σ), which is tightly sensitive with ||Σ̂−1||F , ||Σ̂−1 − Σ−1||F and ||µ̄+ − µ̄−||. However, Σ is assumed unknown, thus this upper bound is not deterministic.\n2. Then we introduce a key existing theory [21] on the desparsified graphical lasso estimation T̂ , which proves ||T̂ − Σ−1||∞ = Op (√ log p/m ) , under certain spar-\nsity assumptions.\n3. Through combining above two bounds, we can have a new upper bound of ε(T̂−1,Σ) which is sensitive with the known estimation T̂ , the known mean gap ||µ̄+ − µ̄−|| and known parameters p and m.\nResult 1. An upper bound of ε(Σ̂,Σ), which is sensitive to ||Σ̂−1||F , ||Σ̂−1 − Σ−1||F and ||µ̄+ − µ̄−||: (4.8)\nε(Σ̂,Σ) = Φ  − (µ̄+ − µ̄−)\nT Σ̂−1(µ̄+ − µ̄−) 2 √ (µ̄+ − µ̄−)T Σ̂−1ΣΣ̂−1(µ̄+ − µ̄−)\n \n≤ Φ ( −||µ+ − µ−||2\n2\n√ ||Σ̂−1||F + ||Σ−1 − Σ̂−1||F )\nProof of Result 1. ε(Σ̂,Σ)\n= Φ  − (µ̄+ − µ̄−) T Σ̂−1(µ̄+ − µ̄−)\n2 ( (µ̄+ − µ̄−)T Σ̂−1ΣΣ̂−1(µ̄+ − µ̄−) ) 1 2\n \n= Φ  − 〈 (µ̄+ − µ̄−), Σ̂−1(µ̄+ − µ̄−) 〉\n2||Σ̂−1(µ̄+ − µ̄−)||Σ\n \n≤ Φ ( −||µ̄+ − µ̄−||\n2\n||Σ̂−1(µ̄+ − µ̄−)|| ||Σ̂−1(µ̄+ − µ̄−)||Σ\n)\nSince Σ is a symmetric positive-definite matrix, we can consider the Cholesky Decomposition of Σ as Σ = MTM . Thus, we have ||Σ̂−1(µ̄+ − µ̄−)|| =||M−1M Σ̂−1(µ̄+ − µ̄−)||\n≤ ||M−1||F ||M Σ̂−1(µ̄+ − µ̄−)||, where ||M−1||F refers to the Frobenius norm of M−1. According definition of postive-definite-norm, we have ||Σ̂−1(µ̄+ − µ̄−)||Σ = ||M Σ̂−1(µ̄+ − µ̄−)||. Then, we have ε(Σ̂,Σ)\n≤ Φ ( −||µ̄+ − µ̄−||\n2\n||M−1||F ||M Σ̂−1(µ̄+ − µ̄−)|| ||M Σ̂−1(µ̄+ − µ̄−)||\n)\n= Φ ( −||µ̄+ − µ̄−||\n2 ||M−1||F\n)\nSince Σ−1 is a symmetric positive-definite matrix, we consider the Cholesky Decomposition of Σ−1 as Σ−1 = (MTM)−1 = (MT )−1M−1 = (M−1)TM−1. Besides, there exists ||M−1||2F ≤ ||M−1(M−1)T ||F = ||Σ−1||F . Then, we have\nε(Σ̂,Σ) ≤ Φ ( −||µ̄+ − µ̄−||\n2\n√ ||M−1(M−1)T ||F\n)\n= Φ ( −||µ̄+ − µ̄−||\n2\n√ ||Σ−1||F\n)\nWe consider the error of precision matrix estimation as ∆ = Σ−1 − Σ̂−1, Considering the triangle inequality, we have ||Σ−1|| = ||Σ̂−1 + ∆|| ≤ ||Σ̂−1|| + ||∆||. Then we can conclude:\nε(Σ̂,Σ) ≤ Φ ( −||µ̄+ − µ̄−||\n2\n√ ||Σ̂−1 + ∆||F )\n≤ Φ ( −||µ̄+ − µ̄−||\n2\n√ ||Σ̂−1||F + ||∆||F )\n= Φ ( −||µ̄+ − µ̄−||\n2\n√ ||Σ̂−1||F + ||Σ−1 − Σ̂−1||F )\nResult 2. Stochastic Bound of ||T̂ − Σ−1||F - According to [21], suppose T̂ is the de-sparsified graphical lasso esti-\nmation and Σ refers to the true population covariance matrix, under specific structural assumption [21]: (4.9) d = o( √ m/log p)\nwe have ||T̂ − Σ−1||∞ = Op (√ log p/m ) ,\nwhere p and m refer to the dimension and sample size, respectively; d refers to the maximal vector support of the population inverse covariance matrix Σ−1 i.e., d = max1≤i≤p|{j : Σ−1i,j 6= 0}|; further o(·) and Op(·) are little-o notation and big-O in probability (the notations were defined in [26]) respectively. Then, we can further conclude\n(4.10)\n||T̂ − Σ−1||F = √ ∑\n1≤i≤p\n∑\n1≤j≤p |(T̂ − Σ−1)i,j |2\n≤ √√√√√   ∑\n1≤i≤p\n∑\n1≤j≤p |(T̂ − Σ−1)i,j |\n  2\n≤ p ∗ max 1≤i≤p\n∑\n1≤j≤p |(T̂ − Σ−1)i,j |\n= p ∗ ||T̂ − Σ−1||∞ = p ∗ Op (√ log p/m ) .\nResult 3. A new stochastic upper bound of ε(T̂−1,Σ), which is sensitive to the known estimation ||T̂ ||F , the known mean gap ||µ̄+ − µ̄−|| as well as known parameters p and m: Through combining Result 1. and Result 2., we have: (4.11) ε(T̂−1,Σ)\n≤ Φ ( −||µ̄+ − µ̄−||\n2\n√ ||T̂ ||F + p ∗ Op (√ log p/m ))\nRemark. Above theoretical analysis shows that Non-Sparse Covariance-Regularized Discriminant Analysis algorithm used in E2D2 can stochastically bound the maximal expected error rate of LDA classification under two major assumptions: 1) the data (e.g., EHR diagnosis-frequency vectors) should be gaussian or subgaussian; and 2) the population inverse covariance matrix should follow the structural assumption [21] listed in Equation 4.9. In the practical usage of our method, these two assumptions might be violated. Fortunately, we can test our algorithms using the large-scale EHR data sets. The evaluation results show Non-Sparse Covariance-Regularized Discriminant Analysis (E2D2) outperformed typical LDA and other regularized LDA thus validating our theory.\nNote that [27] demonstrated that the Frobenius norm rate of convergence for graphical lasso is Op( √ (p+ d) log p/m) under a mild condition, which can also bound the maximal expected error but not as tight as\nEquation 4.11.\n5 Experimental Results In this section, we introduce the experimental design of our evaluation. Then we present the experimental results, including the performance comparison between the E2D2\nframework, existing LDA baselines and other predictive models. Later a comparison between inverse covariance matrix supports our theoretical analysis of E2D2.\n5.1 Experiment Setups Data Description - In this study, to evaluate E2D2, we\nused the de-identified EHR data from the College Health Surveillance Network (CHSN), which contains over 1 million patients and 6 million visits from 31 student health centers across the United States [28]. In the experiments, we use the EHR data from 10 participating schools. The available information includes ICD-9 diagnostic codes, CPT procedural codes, and limited demographic information. There are over 200,000 enrolled students in those 10 schools representing all geographic regions of the US. The demography of enrolled students (sex, race/ethnicity, age, undergraduate/graduate status) closely matched the demography for the population of US universities.\nData Preparation - Among all diseases recorded in CHSN, we choose mental health disorders, including anxiety disorders, mood disorders, depression disorders, and other related disorders, as the targeted disease for early detection. We represent each patient using his/her diagnosis-frequency vector based on the clustered codeset, where four clustered codes (i.e., 651, 657, 658, 662) are considered to represent the diagnoses of mental health disorders. Specifically, if a patient has any of these four codes in his/her EHR, we say that he/she has been diagnosed with mental health disorders as ground truth.\nNote that in our research, we do not predict these four types of mental disorders separately, as these four disorders are usually correlated and heavily overlapped in clinical practices [29]. Further, patients with less than two visits were excluded from the analysis. Notably, the visit data and corresponding diagnosis information within one-month (i.e., 30–90 days) of the first diagnosis of anxiety/depression in the target group is excluded for the aim of early detection at least 1 to 3-months prior to diagnosis. Until now, the diagnosisfrequency vectors used as predictors in our experiment only include the diagnosis frequency of physical health disorders and all mental health related information has been removed. In this case, our experiment is equivalent to predicting whether a patient would develop mental health disorders according to his/her past diagnoses of physical disorders.\nEvaluation Metrics - To demonstrate the effectiveness of our method, we compared our method with baseline"
    }, {
      "heading" : "5.2 Experiment Results",
      "text" : "Samples (×2) λ = 0.1 λ = 1.0 λ = 10.0 50 47.17 7.12 0.76\n100 51.42 7.21 0.76 150 53.41 7.25 0.76 200 55.90 7.32 0.76\nE2D2 using small samples is closer to the inverse covariance matrix estimated using large samples. Clearly the estimation used by CRDA is very close to the Non-sparse estimator. We compared the `1-norm error of these two estimators and the results in Table 2 show that No-sparse estimator can always outperform Graphical Lasso with less error. Note that in our experiment, we simulated a training set with a relatively large sample size (i.e., 10,000). However, for realistic predictive model training, such a large number of samples is usually not available.\n6 Related Work In this section, we first summarize previous studies related to this paper from two aspects: data mining approaches to early detection of diseases and extensions to LDA learning. Then we compare our work to the most relevant work. Further, we discuss several open issues of our study.\n6.1 Data Mining Approaches to EHR-based Early Detection of Disease\nGiven the raw EHR data, existing data mining efforts to EHR-based early detection first learn a set of features from EHR data to represent each patient. Specifically, the EHR data of each patient was represented as a vector consisting of the frequency of each diagnosis code that has been discovered in previous visits [1–3]. EHR data can also be represented using N-gram-alike [33] graphs, through counting the pairwise transitions between each pair of diagnosis codes in every visit [4, 5]. Most recently, Liu et al. proposed to rep-\nresent the EHR of a patient using the temporal graphs, in order to preserve the temporal order of diagnoses partially [6]. To reduce the dimensionality of EHR data, clustered ICD-9 codes [23] have been frequently used in practice, where each ICD-9 diagnosis code can map to one of 295 groups, compressing each raw diagnosis-frequency vector (≥ 15, 000 dimensions) to roughly 295 dimensions. Liu et al. discussed the method of dimensionality reduction for temporal EHR graphs through edge selection [6].\nGiven EHR data represented with vectors and graphs, researchers have proposed to predict the target disease through supervised learning, using downstream classifiers [4] or similarity search [1–3]. Given the EHR data with rich structures, sub-sequential pattern matching and sub-graph pattern matching are also leveraged to identify the risk of patients [5, 6]."
    }, {
      "heading" : "6.2 Extensions to LDA Models",
      "text" : "We introduce several statistical extensions to LDA in\nHDLSS settings to address the challenges presented by EHR data. As discussed above, when LDA works in HDLSS, there exists two major technical issues: 1) as shown in Equation 2.2, LDA requires the inverse covariance matrix for calculation, but the sample covariance matrix used in typical LDA is usually singular (non-invertible); and 2) the difference between sample (inverse) covariance matrix and the population (inverse) covariance matrix is extremely large, simulation studies [34] showed that the eignvectors of the two matrices can be nearly orthogonal. To handle the singular (non-invertible) covariance matrix issue, Ye et al. [30] proposed to use the Pseudo-inverse, while Direct LDA [35] leveraged the simultaneous diagonalization, to replace the matrix inverse operator. On the other hand, to obtain accurate parameter estimation for LDA under HDLSS settings, several works have proposed to sparsify the inverse covariance matrix [10–12] and linear coefficients [9].\n6.3 Comparing E2D2 to Existing Work In summary, E2D2 is distinct in three ways:\n1. First, compared to other data mining approaches for early diseases detection [1–6], E2D2 is the first work that focuses on improving the performance of LDA model for EHR data classification with diagnosisfrequency vector data representation, by addressing the expected error rates under HDLSS settings.\n2. Second, our contribution is complementary with the work in EHR data representation [4–6] and we can further improve E2D2 by incorporating advanced EHR data representation methods.\n3. Third, when compared to existing LDA extensions, E2D2 adopts a novel inverse covariance matrix estimator [21] to lower and bound the expected error rate of the\nLDA model with theoretical guarantee under HDLSS settings, while [9–12] all focus on regularizing the the parameters of LDA using the “heuristics of sparsity”.\nTo best of our knowledge, this paper is the first study that integrates [21] with LDA for Non-sparse CovarianceRegularized Discriminant Analysis and presents its theoretical properties.\n7 Conclusion In this paper, we proposed E2D2 – a novel linear discriminant analysis framework for early detection of diseases, based on electronic health record data and diagnosisfrequency vector data representation. E2D2 is designed to lower the expected error rate of LDA model using highdimensional EHR data, through regularizing the covariance matrix with a non-sparse (de-sparsified) inverse covariance estimator derived from Graphical Lasso. Our theoretical analysis showed that the proposed algorithm can stochastically bound the maximal expected error rate of LDA for high-dimensional data classification. The experimental results using real-world EHR dataset CHSN showed E2D2\noutperformed all baseline algorithms. Further the empirical studies on estimator comparison validated our theoretical analysis.\nReferences\n[1] Jimeng Sun, Fei Wang, Jianying Hu, and Shahram Edabollahi. Supervised patient similarity measure of heterogeneous patient records. ACM SIGKDD Explorations Newsletter, 14(1):16–24, 2012.\n[2] Jianying Hu Fei Wang Kenney Ng, Jimeng Sun. Personalized predictive modeling and risk factor identification using patient similarity. AMIA Summit on Clinical Research Informatics (CRI), 2015.\n[3] Fei Wang and Jimeng Sun. Psf: A unified patient similarity evaluation framework through metric learning with weak supervision. Biomedical and Health Informatics, IEEE Journal of, 19(3):1053–1060, May 2015.\n[4] Yu Huang Hao Wu Kevin Leach Laura E. Barnes Jinghe Zhang, Haoyi Xiong. MSEQ: Early detection of anxiety and depression via temporal orders of diagnoses in electronic health data. In Big Data (Workshop), 2015 International Conference on. IEEE, 2015.\n[5] Susan Jensen and UK SPSS. Mining medical data for predictive and sequential patterns: Pkdd 2001. In Proceedings of the 5th European Conference on Principles and Practice of Knowledge Discovery in Databases, 2001.\n[6] Chuanren Liu, Fei Wang, Jianying Hu, and Hui Xiong. Temporal Phenotyping from Longitudinal Electronic Health Records: A Graph Based Framework. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’15, pages 705–714, New York, NY, USA, 2015. ACM.\n[7] Fei Wang, Ping Zhang, Xiang Wang, and Jianying Hu. Clinical risk prediction by exploring high-order feature correlations. In AMIA Annual Symposium Proceedings, volume 2014, page 1170. American Medical Informatics Association, 2014.\n[8] Onur C Hamsici and Aleix M Martinez. Bayes optimality in linear discriminant analysis. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 30(4):647–657, 2008.\n[9] Line Clemmensen, Trevor Hastie, Daniela Witten, and Bjarne Ersbøll. Sparse discriminant analysis. Technometrics, 53(4), 2011.\n[10] Jun Shao, Yazhen Wang, Xinwei Deng, Sijian Wang, et al. Sparse linear discriminant analysis by thresholding for high dimensional data. The Annals of statistics, 39(2):1241–1265, 2011.\n[11] Zhihua Qiao, Lan Zhou, and Jianhua Z Huang. Effective linear discriminant analysis for high dimensional, low sample size data. In Proceeding of the World Congress on Engineering, volume 2, pages 2–4. Citeseer, 2008.\n[12] Daniela M Witten and Robert Tibshirani. Covarianceregularized regression and classification for high dimensional problems. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 71(3):615–636, 2009.\n[13] Joyce C Ho, Joydeep Ghosh, and Jimeng Sun. Marble: highthroughput phenotyping from electronic health records via sparse nonnegative tensor factorization. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 115–124. ACM, 2014.\n[14] T Tony Cai, Zhao Ren, Harrison H Zhou, et al. Estimating structured high-dimensional covariance and precision matrices: Optimal rates and adaptive estimation. Electronic Journal of Statistics, 10(1):1–59, 2016.\n[15] Amin Zollanvari and Edward R Dougherty. Random matrix theory in pattern classification: An application to error estimation. In 2013 Asilomar Conference on Signals, Systems and Computers, 2013.\n[16] Amin Zollanvari, Ulisses M Braga-Neto, and Edward R Dougherty. Analytic study of performance of error estimators for linear discriminant analysis. IEEE Transactions on Signal Processing, 59(9):4238–4255, 2011.\n[17] Roger Peck and John Van Ness. The use of shrinkage estimators in linear discriminant analysis. Pattern Analysis and Machine Intelligence, IEEE Transactions on, (5):530– 537, 1982.\n[18] Juwei Lu, Konstantinos N Plataniotis, and Anastasios N Venetsanopoulos. Regularization studies of linear discriminant analysis in small sample size scenarios with application to face recognition. Pattern Recognition Letters, 26(2):181– 191, 2005.\n[19] Peter Bühlmann and Sara Van De Geer. Statistics for high-dimensional data: methods, theory and applications. Springer Science & Business Media, 2011.\n[20] Jerome Friedman, Trevor Hastie, and Robert Tibshirani. Sparse inverse covariance estimation with the graphical lasso. Biostatistics, 9(3):432–441, 2008.\n[21] Jana Jankova, Sara van de Geer, et al. Confidence intervals for high-dimensional inverse covariance estimation. Electronic Journal of Statistics, 9(1):1205–1229, 2015.\n[22] Erik R Dubberke, Kimberly A Reske, L Clifford McDonald, and Victoria J Fraser. Icd-9 codes and surveillance for clostridium difficile–associated disease. Emerging infectious diseases, 12(10):1576, 2006.\n[23] HCUP. Appendix a - clinical classification softwarediagnoses (january 1980 through september 2014), 2014.\n[24] Peter A Lachenbruch and M Ray Mickey. Estimation of error rates in discriminant analysis. Technometrics, 10(1):1–11, 1968.\n[25] Wei Bian and Dacheng Tao. Asymptotic generalization bound of fishers linear discriminant analysis. IEEE transactions on pattern analysis and machine intelligence, 36(12):2325–2337, 2014.\n[26] Herman Chernoff. Large-sample theory: Parametric case. The Annals of Mathematical Statistics, 27(1):1–22, 1956.\n[27] Adam J Rothman, Peter J Bickel, Elizaveta Levina, Ji Zhu, et al. Sparse permutation invariant covariance estimation. Electronic Journal of Statistics, 2:494–515, 2008.\n[28] James C. Turner and Adrienne Keller. College Health Surveillance Network: Epidemiology and Health Care Utilization of College Students at U.S. 4-Year Universities. Journal of American college health: J of ACH, page 0, June 2015.\n[29] Kenneth S Kendler, John M Hettema, Frank Butera, Charles O Gardner, and Carol A Prescott. Life event dimensions of loss, humiliation, entrapment, and danger in the prediction of onsets of major depression and generalized anxiety. Archives of general psychiatry, 60(8):789–796, 2003.\n[30] Jieping Ye, Ravi Janardan, Cheong Hee Park, and Haesun Park. An optimization criterion for generalized discriminant analysis on undersampled problems. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 26(8):982–994, 2004.\n[31] Sandy H Huang, Paea LePendu, Srinivasan V Iyer, Ming TaiSeale, David Carrell, and Nigam H Shah. Toward personalizing treatment for depression: predicting diagnosis and severity. Journal of the American Medical Informatics Association, 21(6):1069–1075, 2014.\n[32] Appendix: Provably good early detection of diseases using non-sparse covariance-regularized linear discriminant analysis - https://www.dropbox.com/s/niazhtc4j8ueyic/appendixmain.pdf, 2016.\n[33] Peter F Brown, Peter V Desouza, Robert L Mercer, Vincent J Della Pietra, and Jenifer C Lai. Class-based n-gram models of natural language. Computational linguistics, 18(4):467– 479, 1992.\n[34] Iain M Johnstone. On the distribution of the largest eigenvalue in principal components analysis. Annals of statistics, pages 295–327, 2001.\n[35] Hua Yu and Jie Yang. A direct lda algorithm for highdimensional datawith application to face recognition. Pattern recognition, 34(10):2067–2070, 2001."
    }, {
      "heading" : "A Appendix",
      "text" : "In this Appendix, we include additional experiment results of E2D2 evaluation. In following tables, we present the performance comparison between E2D2 and baselines, where we introduce the results in terms of accuracy, F1-score, sensitivity, specificity, as well as the standard derivations. Specifically, we compare the performance using various experimental settings, such as the number of days in advance for early detection(e.g., 30 days, 60 days and 90 days) as well as by varying parameters for model training. E2D2 clearly outperforms other algorithms in terms of overall accuracy, F1score and sensitivity. In terms of specificity, the baseline algorithms outperform E2D2, in the most of cases. However, in terms of sensitivity and specificity trade-off, E2D2 on average gains 19.5% higher sensitivity while sacrificing 8.2% specificity, when compared to typical LDA. On opposite side of the trade-off, when compared to CRDA (based on graphical lasso), E2D2 on average gains 2.3% higher specificity while sacrificing 1.4% sensitivity. Thus, we can conclude that E2D2 overall outperforms the baseline algorithms in all experimental settings.\nAdaBoost (×10) 0.640 ± 0.048 0.582 ± 0.115 0.537 ± 0.181 0.743 ± 0.102 AdaBoost (×50) 0.637 ± 0.049 0.576 ± 0.110 0.524 ± 0.169 0.751 ± 0.089 CRDA (λ = 1.0) 0.643 ± 0.023 0.669 ± 0.042 0.732 ± 0.094 0.553 ± 0.077 CRDA (λ = 10.0) 0.658 ± 0.020 0.704 ± 0.021 0.817 ± 0.069 0.499 ± 0.084 CRDA (λ = 100.0) 0.655 ± 0.026 0.710 ± 0.014 0.848 ± 0.068 0.462 ± 0.108 LDA 0.543 ± 0.016 0.533 ± 0.033 0.525 ± 0.062 0.562 ± 0.061 Logistic Regression 0.606 ± 0.064 0.468 ± 0.217 0.424 ± 0.258 0.787 ± 0.139 E2D2 (λ = 1.0) 0.642 ± 0.024 0.669 ± 0.031 0.726 ± 0.062 0.558 ± 0.048 E2D2 (λ = 10.0) 0.664 ± 0.017 0.706 ± 0.019 0.811 ± 0.063 0.516 ± 0.076 E2D2 (λ = 100.0) 0.659 ± 0.020 0.711 ± 0.013 0.840 ± 0.060 0.478 ± 0.089 SVM 0.612 ± 0.020 0.610 ± 0.032 0.611 ± 0.064 0.614 ± 0.059 DIAG 0.561 ± 0.024 0.500 ± 0.065 0.449 ± 0.101 0.673 ± 0.071 Shrinkage (β = 0.25) 0.559 ± 0.030 0.470 ± 0.166 0.435 ± 0.169 0.682 ± 0.124 Shrinkage (β = 0.5) 0.563 ± 0.024 0.523 ± 0.051 0.486 ± 0.085 0.640 ± 0.068 Shrinkage (β = 0.75) 0.557 ± 0.027 0.468 ± 0.164 0.433 ± 0.166 0.682 ± 0.122\nDays in Advance: 60 AdaBoost (×10) 0.644 ± 0.042 0.565 ± 0.107 0.486 ± 0.140 0.802 ± 0.063 AdaBoost (×50) 0.640 ± 0.045 0.545 ± 0.119 0.458 ± 0.149 0.821 ± 0.065 CRDA (λ = 1.0) 0.680 ± 0.020 0.714 ± 0.024 0.801 ± 0.054 0.560 ± 0.044 CRDA (λ = 10.0) 0.667 ± 0.016 0.715 ± 0.016 0.835 ± 0.037 0.499 ± 0.037 CRDA (λ = 100.0) 0.661 ± 0.015 0.714 ± 0.013 0.848 ± 0.036 0.473 ± 0.046 LDA 0.553 ± 0.023 0.542 ± 0.035 0.531 ± 0.056 0.575 ± 0.056 Logistic Regression 0.612 ± 0.067 0.443 ± 0.239 0.374 ± 0.227 0.851 ± 0.095 E2D2 (λ = 1.0) 0.669 ± 0.019 0.693 ± 0.027 0.753 ± 0.062 0.584 ± 0.050 E2D2 (λ = 10.0) 0.676 ± 0.017 0.721 ± 0.017 0.838 ± 0.035 0.515 ± 0.033 E2D2 (λ = 100.0) 0.665 ± 0.015 0.716 ± 0.013 0.846 ± 0.034 0.483 ± 0.042 SVM 0.633 ± 0.030 0.628 ± 0.036 0.621 ± 0.057 0.646 ± 0.053 DIAG 0.568 ± 0.016 0.504 ± 0.047 0.443 ± 0.063 0.693 ± 0.035 Shrinkage (β = 0.25) 0.563 ± 0.034 0.419 ± 0.214 0.375 ± 0.199 0.750 ± 0.132 Shrinkage (β = 0.5) 0.574 ± 0.016 0.520 ± 0.045 0.467 ± 0.071 0.680 ± 0.055 Shrinkage (β = 0.75) 0.567 ± 0.027 0.470 ± 0.162 0.423 ± 0.153 0.711 ± 0.107\nDays in Advance: 90 AdaBoost (×10) 0.627 ± 0.045 0.562 ± 0.091 0.498 ± 0.135 0.756 ± 0.073 AdaBoost (×50) 0.627 ± 0.036 0.547 ± 0.091 0.471 ± 0.137 0.783 ± 0.072 CRDA (λ = 1.0) 0.651 ± 0.026 0.694 ± 0.026 0.793 ± 0.057 0.510 ± 0.060 CRDA (λ = 10.0) 0.656 ± 0.017 0.713 ± 0.008 0.855 ± 0.027 0.456 ± 0.055 CRDA (λ = 100.0) 0.640 ± 0.029 0.710 ± 0.010 0.878 ± 0.034 0.402 ± 0.088 LDA 0.560 ± 0.020 0.554 ± 0.032 0.548 ± 0.054 0.571 ± 0.046 Logistic Regression 0.621 ± 0.037 0.523 ± 0.100 0.440 ± 0.148 0.801 ± 0.081 E2D2 (λ = 1.0) 0.655 ± 0.021 0.685 ± 0.022 0.751 ± 0.051 0.560 ± 0.061 E2D2 (λ = 10.0) 0.664 ± 0.019 0.716 ± 0.014 0.846 ± 0.031 0.481 ± 0.049 E2D2 (λ = 100.0) 0.650 ± 0.022 0.714 ± 0.009 0.873 ± 0.030 0.428 ± 0.070 SVM 0.616 ± 0.017 0.621 ± 0.029 0.633 ± 0.065 0.599 ± 0.064 DIAG 0.573 ± 0.023 0.528 ± 0.050 0.484 ± 0.076 0.662 ± 0.060 Shrinkage (β = 0.25) 0.569 ± 0.028 0.495 ± 0.169 0.469 ± 0.169 0.670 ± 0.122 Shrinkage (β = 0.5) 0.566 ± 0.025 0.488 ± 0.166 0.459 ± 0.164 0.672 ± 0.118 Shrinkage (β = 0.75) 0.570 ± 0.016 0.540 ± 0.039 0.509 ± 0.063 0.630 ± 0.045\nAdaBoost (×10) 0.626 ± 0.032 0.518 ± 0.092 0.422 ± 0.137 0.831 ± 0.080 AdaBoost (×50) 0.636 ± 0.024 0.545 ± 0.073 0.452 ± 0.116 0.819 ± 0.072 CRDA (λ = 1.0) 0.684 ± 0.012 0.717 ± 0.013 0.802 ± 0.031 0.566 ± 0.028 CRDA (λ = 10.0) 0.677 ± 0.012 0.723 ± 0.010 0.843 ± 0.021 0.510 ± 0.026 CRDA (λ = 100.0) 0.665 ± 0.019 0.719 ± 0.009 0.857 ± 0.035 0.474 ± 0.067 LDA 0.604 ± 0.011 0.610 ± 0.011 0.620 ± 0.022 0.589 ± 0.030 Logistic Regression 0.614 ± 0.043 0.472 ± 0.141 0.376 ± 0.164 0.851 ± 0.081 E2D2 (λ = 1.0) 0.676 ± 0.013 0.699 ± 0.021 0.756 ± 0.051 0.595 ± 0.038 E2D2 (λ = 10.0) 0.682 ± 0.010 0.725 ± 0.010 0.837 ± 0.019 0.527 ± 0.019 E2D2 (λ = 100.0) 0.668 ± 0.014 0.720 ± 0.008 0.853 ± 0.030 0.484 ± 0.052 SVM 0.648 ± 0.021 0.655 ± 0.019 0.667 ± 0.026 0.630 ± 0.040 DIAG 0.613 ± 0.018 0.588 ± 0.030 0.554 ± 0.050 0.672 ± 0.040 Shrinkage (β = 0.25) 0.604 ± 0.037 0.538 ± 0.181 0.516 ± 0.176 0.691 ± 0.109 Shrinkage (β = 0.5) 0.599 ± 0.036 0.534 ± 0.179 0.512 ± 0.173 0.686 ± 0.110 Shrinkage (β = 0.75) 0.599 ± 0.036 0.537 ± 0.180 0.520 ± 0.176 0.679 ± 0.113\nDays in Advance: 60 AdaBoost (×10) 0.630 ± 0.021 0.526 ± 0.064 0.420 ± 0.097 0.839 ± 0.058 AdaBoost (×50) 0.626 ± 0.026 0.505 ± 0.074 0.393 ± 0.107 0.860 ± 0.060 CRDA (λ = 1.0) 0.676 ± 0.008 0.710 ± 0.011 0.794 ± 0.039 0.558 ± 0.043 CRDA (λ = 10.0) 0.669 ± 0.018 0.720 ± 0.013 0.851 ± 0.021 0.488 ± 0.036 CRDA (λ = 100.0) 0.661 ± 0.023 0.718 ± 0.011 0.862 ± 0.025 0.460 ± 0.064 LDA 0.591 ± 0.026 0.589 ± 0.031 0.588 ± 0.049 0.595 ± 0.045 Logistic Regression 0.585 ± 0.057 0.350 ± 0.231 0.267 ± 0.181 0.902 ± 0.071 E2D2 (λ = 1.0) 0.670 ± 0.017 0.690 ± 0.027 0.738 ± 0.058 0.601 ± 0.038 E2D2 (λ = 10.0) 0.679 ± 0.011 0.725 ± 0.009 0.845 ± 0.019 0.514 ± 0.028 E2D2 (λ = 100.0) 0.664 ± 0.021 0.719 ± 0.012 0.859 ± 0.022 0.469 ± 0.054 SVM 0.635 ± 0.019 0.643 ± 0.030 0.660 ± 0.054 0.610 ± 0.032 DIAG 0.592 ± 0.018 0.561 ± 0.045 0.527 ± 0.076 0.658 ± 0.056 Shrinkage (β = 0.25) 0.576 ± 0.043 0.458 ± 0.233 0.438 ± 0.230 0.714 ± 0.152 Shrinkage (β = 0.5) 0.587 ± 0.034 0.518 ± 0.176 0.497 ± 0.176 0.676 ± 0.120 Shrinkage (β = 0.75) 0.582 ± 0.034 0.513 ± 0.174 0.492 ± 0.172 0.672 ± 0.118\nDays in Advance: 90 AdaBoost (×10) 0.634 ± 0.026 0.541 ± 0.077 0.445 ± 0.111 0.822 ± 0.071 AdaBoost (×50) 0.628 ± 0.031 0.529 ± 0.073 0.428 ± 0.097 0.828 ± 0.052 CRDA (λ = 1.0) 0.678 ± 0.018 0.715 ± 0.019 0.809 ± 0.049 0.546 ± 0.053 CRDA (λ = 10.0) 0.664 ± 0.020 0.714 ± 0.012 0.839 ± 0.042 0.490 ± 0.071 CRDA (λ = 100.0) 0.646 ± 0.036 0.707 ± 0.015 0.853 ± 0.058 0.439 ± 0.121 LDA 0.604 ± 0.020 0.610 ± 0.024 0.621 ± 0.039 0.587 ± 0.032 Logistic Regression 0.607 ± 0.046 0.439 ± 0.166 0.339 ± 0.154 0.874 ± 0.071 E2D2 (λ = 1.0) 0.674 ± 0.020 0.702 ± 0.026 0.771 ± 0.059 0.578 ± 0.051 E2D2 (λ = 10.0) 0.675 ± 0.016 0.720 ± 0.013 0.835 ± 0.040 0.515 ± 0.053 E2D2 (λ = 100.0) 0.656 ± 0.025 0.711 ± 0.011 0.848 ± 0.052 0.464 ± 0.092 SVM 0.638 ± 0.017 0.638 ± 0.022 0.641 ± 0.039 0.635 ± 0.036 DIAG 0.602 ± 0.018 0.578 ± 0.042 0.550 ± 0.069 0.654 ± 0.038 Shrinkage (β = 0.25) 0.599 ± 0.035 0.539 ± 0.182 0.527 ± 0.182 0.671 ± 0.115 Shrinkage (β = 0.5) 0.608 ± 0.020 0.601 ± 0.029 0.593 ± 0.045 0.624 ± 0.032 Shrinkage (β = 0.75) 0.605 ± 0.025 0.600 ± 0.029 0.593 ± 0.043 0.618 ± 0.041\nAdaBoost (×10) 0.629 ± 0.034 0.529 ± 0.098 0.440 ± 0.147 0.818 ± 0.091 AdaBoost (×50) 0.636 ± 0.025 0.549 ± 0.079 0.461 ± 0.125 0.810 ± 0.084 CRDA (λ = 1.0) 0.687 ± 0.016 0.721 ± 0.015 0.810 ± 0.026 0.565 ± 0.026 CRDA (λ = 10.0) 0.673 ± 0.016 0.719 ± 0.011 0.836 ± 0.022 0.510 ± 0.039 CRDA (λ = 100.0) 0.666 ± 0.017 0.717 ± 0.010 0.844 ± 0.026 0.488 ± 0.051 LDA 0.617 ± 0.021 0.615 ± 0.024 0.612 ± 0.037 0.621 ± 0.035 Logistic Regression 0.619 ± 0.050 0.476 ± 0.185 0.396 ± 0.197 0.841 ± 0.109 E2D2 (λ = 1.0) 0.679 ± 0.020 0.701 ± 0.024 0.754 ± 0.043 0.603 ± 0.029 E2D2 (λ = 10.0) 0.686 ± 0.010 0.727 ± 0.009 0.837 ± 0.018 0.534 ± 0.021 E2D2 (λ = 100.0) 0.669 ± 0.015 0.718 ± 0.010 0.841 ± 0.026 0.498 ± 0.044 SVM 0.649 ± 0.017 0.650 ± 0.022 0.652 ± 0.040 0.646 ± 0.037 DIAG 0.611 ± 0.016 0.587 ± 0.020 0.554 ± 0.026 0.668 ± 0.025 Shrinkage (β = 0.25) 0.615 ± 0.018 0.602 ± 0.024 0.583 ± 0.037 0.647 ± 0.030 Shrinkage (β = 0.5) 0.607 ± 0.039 0.543 ± 0.182 0.523 ± 0.178 0.690 ± 0.107 Shrinkage (β = 0.75) 0.597 ± 0.052 0.489 ± 0.246 0.478 ± 0.243 0.716 ± 0.146\nDays in Advance: 60 AdaBoost (×10) 0.626 ± 0.031 0.510 ± 0.091 0.406 ± 0.122 0.846 ± 0.066 AdaBoost (×50) 0.615 ± 0.031 0.470 ± 0.100 0.356 ± 0.116 0.874 ± 0.061 CRDA (λ = 1.0) 0.680 ± 0.024 0.722 ± 0.020 0.831 ± 0.028 0.529 ± 0.032 CRDA (λ = 10.0) 0.663 ± 0.019 0.718 ± 0.014 0.856 ± 0.031 0.471 ± 0.046 CRDA (λ = 100.0) 0.656 ± 0.025 0.717 ± 0.014 0.871 ± 0.037 0.442 ± 0.073 LDA 0.621 ± 0.018 0.625 ± 0.015 0.632 ± 0.022 0.610 ± 0.038 Logistic Regression 0.590 ± 0.055 0.373 ± 0.212 0.286 ± 0.185 0.895 ± 0.079 E2D2 (λ = 1.0) 0.680 ± 0.020 0.712 ± 0.019 0.793 ± 0.036 0.566 ± 0.037 E2D2 (λ = 10.0) 0.678 ± 0.020 0.725 ± 0.016 0.850 ± 0.027 0.505 ± 0.036 E2D2 (λ = 100.0) 0.662 ± 0.019 0.719 ± 0.013 0.865 ± 0.033 0.459 ± 0.055 SVM 0.646 ± 0.010 0.654 ± 0.018 0.671 ± 0.039 0.622 ± 0.032 DIAG 0.614 ± 0.017 0.610 ± 0.021 0.604 ± 0.041 0.625 ± 0.049 Shrinkage (β = 0.25) 0.622 ± 0.014 0.624 ± 0.014 0.628 ± 0.030 0.616 ± 0.042 Shrinkage (β = 0.5) 0.601 ± 0.052 0.501 ± 0.251 0.503 ± 0.252 0.698 ± 0.153 Shrinkage (β = 0.75) 0.626 ± 0.013 0.628 ± 0.014 0.632 ± 0.028 0.620 ± 0.036\nDays in Advance: 90 AdaBoost (×10) 0.624 ± 0.036 0.519 ± 0.119 0.433 ± 0.150 0.815 ± 0.085 AdaBoost (×50) 0.625 ± 0.031 0.514 ± 0.117 0.425 ± 0.152 0.824 ± 0.093 CRDA (λ = 1.0) 0.670 ± 0.023 0.713 ± 0.016 0.821 ± 0.031 0.519 ± 0.056 CRDA (λ = 10.0) 0.659 ± 0.024 0.714 ± 0.015 0.854 ± 0.032 0.464 ± 0.061 CRDA (λ = 100.0) 0.651 ± 0.020 0.713 ± 0.012 0.867 ± 0.030 0.436 ± 0.056 LDA 0.607 ± 0.025 0.609 ± 0.025 0.614 ± 0.043 0.600 ± 0.059 Logistic Regression 0.603 ± 0.048 0.425 ± 0.187 0.336 ± 0.183 0.870 ± 0.092 E2D2 (λ = 1.0) 0.673 ± 0.014 0.705 ± 0.014 0.781 ± 0.038 0.565 ± 0.047 E2D2 (λ = 10.0) 0.669 ± 0.020 0.719 ± 0.014 0.848 ± 0.031 0.490 ± 0.050 E2D2 (λ = 100.0) 0.652 ± 0.019 0.713 ± 0.012 0.863 ± 0.031 0.441 ± 0.056 SVM 0.637 ± 0.016 0.647 ± 0.021 0.667 ± 0.045 0.607 ± 0.047 DIAG 0.600 ± 0.014 0.596 ± 0.028 0.595 ± 0.067 0.605 ± 0.071 Shrinkage (β = 0.25) 0.584 ± 0.044 0.486 ± 0.244 0.491 ± 0.251 0.676 ± 0.170 Shrinkage (β = 0.5) 0.608 ± 0.014 0.611 ± 0.020 0.616 ± 0.046 0.599 ± 0.050 Shrinkage (β = 0.75) 0.599 ± 0.037 0.549 ± 0.184 0.549 ± 0.188 0.650 ± 0.127\nAdaBoost (×10) 0.632 ± 0.035 0.537 ± 0.113 0.455 ± 0.156 0.810 ± 0.093 AdaBoost (×50) 0.631 ± 0.032 0.529 ± 0.103 0.438 ± 0.145 0.824 ± 0.090 CRDA (λ = 1.0) 0.678 ± 0.010 0.717 ± 0.012 0.816 ± 0.026 0.541 ± 0.021 CRDA (λ = 10.0) 0.673 ± 0.013 0.724 ± 0.011 0.856 ± 0.030 0.491 ± 0.040 CRDA (λ = 100.0) 0.660 ± 0.017 0.718 ± 0.009 0.866 ± 0.036 0.453 ± 0.062 LDA 0.625 ± 0.016 0.624 ± 0.016 0.624 ± 0.022 0.625 ± 0.029 Logistic Regression 0.607 ± 0.057 0.435 ± 0.210 0.360 ± 0.217 0.854 ± 0.112 E2D2 (λ = 1.0) 0.679 ± 0.018 0.705 ± 0.023 0.771 ± 0.043 0.586 ± 0.020 E2D2 (λ = 10.0) 0.681 ± 0.011 0.727 ± 0.010 0.849 ± 0.019 0.513 ± 0.025 E2D2 (λ = 100.0) 0.665 ± 0.014 0.720 ± 0.009 0.862 ± 0.036 0.467 ± 0.056 SVM 0.656 ± 0.020 0.662 ± 0.028 0.677 ± 0.045 0.635 ± 0.026 DIAG 0.626 ± 0.024 0.618 ± 0.033 0.607 ± 0.050 0.645 ± 0.033 Shrinkage (β = 0.25) 0.631 ± 0.022 0.629 ± 0.031 0.627 ± 0.050 0.635 ± 0.028 Shrinkage (β = 0.5) 0.629 ± 0.017 0.626 ± 0.024 0.623 ± 0.040 0.635 ± 0.029 Shrinkage (β = 0.75) 0.613 ± 0.043 0.560 ± 0.188 0.554 ± 0.187 0.673 ± 0.114\nDays in Advance: 60 AdaBoost (×10) 0.631 ± 0.034 0.530 ± 0.107 0.443 ± 0.155 0.819 ± 0.091 AdaBoost (×50) 0.619 ± 0.037 0.486 ± 0.114 0.385 ± 0.150 0.852 ± 0.080 CRDA (λ = 1.0) 0.690 ± 0.017 0.728 ± 0.010 0.830 ± 0.034 0.550 ± 0.058 CRDA (λ = 10.0) 0.677 ± 0.020 0.723 ± 0.014 0.845 ± 0.023 0.508 ± 0.046 CRDA (λ = 100.0) 0.665 ± 0.022 0.721 ± 0.014 0.862 ± 0.021 0.469 ± 0.053 LDA 0.633 ± 0.016 0.634 ± 0.021 0.638 ± 0.034 0.627 ± 0.023 Logistic Regression 0.592 ± 0.059 0.380 ± 0.220 0.303 ± 0.212 0.881 ± 0.098 E2D2 (λ = 1.0) 0.690 ± 0.014 0.719 ± 0.010 0.792 ± 0.033 0.588 ± 0.051 E2D2 (λ = 10.0) 0.688 ± 0.019 0.732 ± 0.013 0.849 ± 0.021 0.527 ± 0.045 E2D2 (λ = 100.0) 0.672 ± 0.021 0.723 ± 0.014 0.856 ± 0.019 0.489 ± 0.049 SVM 0.664 ± 0.017 0.675 ± 0.021 0.699 ± 0.040 0.629 ± 0.034 DIAG 0.631 ± 0.015 0.624 ± 0.023 0.614 ± 0.045 0.647 ± 0.041 Shrinkage (β = 0.25) 0.619 ± 0.042 0.563 ± 0.188 0.555 ± 0.187 0.683 ± 0.110 Shrinkage (β = 0.5) 0.634 ± 0.016 0.630 ± 0.020 0.626 ± 0.036 0.642 ± 0.037 Shrinkage (β = 0.75) 0.631 ± 0.015 0.628 ± 0.018 0.623 ± 0.029 0.639 ± 0.027\nDays in Advance: 90 AdaBoost (×10) 0.624 ± 0.032 0.510 ± 0.086 0.408 ± 0.124 0.840 ± 0.063 AdaBoost (×50) 0.627 ± 0.031 0.514 ± 0.090 0.412 ± 0.130 0.841 ± 0.069 CRDA (λ = 1.0) 0.682 ± 0.014 0.719 ± 0.016 0.815 ± 0.032 0.548 ± 0.021 CRDA (λ = 10.0) 0.673 ± 0.015 0.722 ± 0.009 0.852 ± 0.020 0.494 ± 0.037 CRDA (λ = 100.0) 0.671 ± 0.015 0.722 ± 0.009 0.855 ± 0.024 0.487 ± 0.045 LDA 0.632 ± 0.016 0.629 ± 0.019 0.626 ± 0.030 0.638 ± 0.027 Logistic Regression 0.610 ± 0.045 0.456 ± 0.145 0.358 ± 0.163 0.861 ± 0.073 E2D2 (λ = 1.0) 0.687 ± 0.016 0.716 ± 0.018 0.789 ± 0.034 0.586 ± 0.024 E2D2 (λ = 10.0) 0.680 ± 0.010 0.726 ± 0.011 0.848 ± 0.024 0.512 ± 0.021 E2D2 (λ = 100.0) 0.673 ± 0.015 0.724 ± 0.009 0.854 ± 0.021 0.492 ± 0.042 SVM 0.657 ± 0.016 0.666 ± 0.021 0.686 ± 0.040 0.627 ± 0.032 DIAG 0.627 ± 0.025 0.604 ± 0.049 0.577 ± 0.085 0.677 ± 0.045 Shrinkage (β = 0.25) 0.632 ± 0.025 0.616 ± 0.046 0.596 ± 0.077 0.668 ± 0.035 Shrinkage (β = 0.5) 0.627 ± 0.021 0.612 ± 0.040 0.594 ± 0.067 0.659 ± 0.035 Shrinkage (β = 0.75) 0.625 ± 0.021 0.612 ± 0.035 0.594 ± 0.059 0.656 ± 0.037\nAdaBoost (×10) 0.611 ± 0.017 0.462 ± 0.050 0.338 ± 0.060 0.885 ± 0.031 AdaBoost (×50) 0.615 ± 0.017 0.474 ± 0.046 0.350 ± 0.056 0.880 ± 0.030 CRDA (λ = 1.0) 0.687 ± 0.013 0.727 ± 0.013 0.831 ± 0.023 0.544 ± 0.023 CRDA (λ = 10.0) 0.672 ± 0.016 0.721 ± 0.013 0.848 ± 0.025 0.497 ± 0.031 CRDA (λ = 100.0) 0.668 ± 0.014 0.721 ± 0.012 0.858 ± 0.030 0.478 ± 0.041 LDA 0.635 ± 0.021 0.636 ± 0.020 0.638 ± 0.023 0.632 ± 0.032 Logistic Regression 0.601 ± 0.038 0.418 ± 0.148 0.307 ± 0.118 0.896 ± 0.047 E2D2 (λ = 1.0) 0.686 ± 0.014 0.714 ± 0.014 0.784 ± 0.025 0.588 ± 0.026 E2D2 (λ = 10.0) 0.683 ± 0.019 0.728 ± 0.016 0.849 ± 0.022 0.518 ± 0.028 E2D2 (λ = 100.0) 0.670 ± 0.014 0.721 ± 0.011 0.854 ± 0.028 0.486 ± 0.037 SVM 0.657 ± 0.019 0.666 ± 0.017 0.682 ± 0.025 0.633 ± 0.036 DIAG 0.628 ± 0.028 0.625 ± 0.041 0.624 ± 0.062 0.632 ± 0.030 Shrinkage (β = 0.25) 0.635 ± 0.032 0.635 ± 0.041 0.638 ± 0.057 0.631 ± 0.028 Shrinkage (β = 0.5) 0.634 ± 0.031 0.634 ± 0.039 0.635 ± 0.052 0.634 ± 0.025 Shrinkage (β = 0.75) 0.635 ± 0.027 0.634 ± 0.033 0.633 ± 0.043 0.636 ± 0.029\nDays in Advance: 60 AdaBoost (×10) 0.625 ± 0.026 0.503 ± 0.084 0.395 ± 0.125 0.855 ± 0.076 AdaBoost (×50) 0.625 ± 0.028 0.503 ± 0.088 0.397 ± 0.133 0.853 ± 0.082 CRDA (λ = 1.0) 0.687 ± 0.019 0.725 ± 0.013 0.824 ± 0.023 0.551 ± 0.045 CRDA (λ = 10.0) 0.675 ± 0.020 0.722 ± 0.011 0.845 ± 0.022 0.504 ± 0.054 CRDA (λ = 100.0) 0.670 ± 0.022 0.720 ± 0.011 0.846 ± 0.025 0.494 ± 0.061 LDA 0.636 ± 0.020 0.636 ± 0.025 0.637 ± 0.040 0.634 ± 0.029 Logistic Regression 0.621 ± 0.029 0.491 ± 0.090 0.383 ± 0.132 0.860 ± 0.078 E2D2 (λ = 1.0) 0.697 ± 0.013 0.722 ± 0.016 0.788 ± 0.041 0.606 ± 0.043 E2D2 (λ = 10.0) 0.683 ± 0.017 0.727 ± 0.011 0.845 ± 0.024 0.522 ± 0.047 E2D2 (λ = 100.0) 0.670 ± 0.021 0.720 ± 0.010 0.845 ± 0.025 0.496 ± 0.060 SVM 0.661 ± 0.022 0.669 ± 0.031 0.688 ± 0.057 0.634 ± 0.034 DIAG 0.637 ± 0.021 0.624 ± 0.026 0.605 ± 0.041 0.669 ± 0.039 Shrinkage (β = 0.25) 0.639 ± 0.018 0.634 ± 0.024 0.625 ± 0.037 0.654 ± 0.024 Shrinkage (β = 0.5) 0.619 ± 0.044 0.564 ± 0.190 0.556 ± 0.189 0.681 ± 0.109 Shrinkage (β = 0.75) 0.630 ± 0.016 0.623 ± 0.023 0.613 ± 0.037 0.647 ± 0.023\nDays in Advance: 90 AdaBoost (×10) 0.622 ± 0.018 0.492 ± 0.062 0.376 ± 0.089 0.868 ± 0.058 AdaBoost (×50) 0.618 ± 0.017 0.482 ± 0.061 0.364 ± 0.089 0.873 ± 0.060 CRDA (λ = 1.0) 0.690 ± 0.014 0.727 ± 0.012 0.828 ± 0.030 0.551 ± 0.038 CRDA (λ = 10.0) 0.678 ± 0.012 0.725 ± 0.011 0.847 ± 0.023 0.510 ± 0.023 CRDA (λ = 100.0) 0.666 ± 0.021 0.720 ± 0.013 0.861 ± 0.024 0.470 ± 0.052 LDA 0.638 ± 0.008 0.639 ± 0.016 0.641 ± 0.041 0.635 ± 0.040 Logistic Regression 0.605 ± 0.039 0.430 ± 0.155 0.324 ± 0.136 0.885 ± 0.065 E2D2 (λ = 1.0) 0.681 ± 0.011 0.710 ± 0.013 0.781 ± 0.037 0.582 ± 0.038 E2D2 (λ = 10.0) 0.685 ± 0.012 0.729 ± 0.012 0.847 ± 0.026 0.523 ± 0.021 E2D2 (λ = 100.0) 0.669 ± 0.019 0.721 ± 0.012 0.857 ± 0.025 0.481 ± 0.048 SVM 0.658 ± 0.013 0.666 ± 0.015 0.681 ± 0.036 0.636 ± 0.042 DIAG 0.629 ± 0.017 0.622 ± 0.025 0.612 ± 0.055 0.647 ± 0.062 Shrinkage (β = 0.25) 0.634 ± 0.017 0.633 ± 0.026 0.634 ± 0.053 0.634 ± 0.051 Shrinkage (β = 0.5) 0.634 ± 0.016 0.631 ± 0.024 0.629 ± 0.047 0.639 ± 0.045 Shrinkage (β = 0.75) 0.635 ± 0.012 0.632 ± 0.021 0.630 ± 0.043 0.640 ± 0.040\nAdaBoost (×10) 0.634 ± 0.024 0.559 ± 0.082 0.483 ± 0.132 0.784 ± 0.091 AdaBoost (×50) 0.629 ± 0.036 0.537 ± 0.122 0.461 ± 0.161 0.798 ± 0.096 CRDA (λ = 1.0) 0.658 ± 0.018 0.694 ± 0.028 0.779 ± 0.063 0.537 ± 0.041 CRDA (λ = 10.0) 0.656 ± 0.011 0.709 ± 0.010 0.837 ± 0.035 0.476 ± 0.043 CRDA (λ = 100.0) 0.644 ± 0.025 0.707 ± 0.007 0.858 ± 0.049 0.429 ± 0.097 LDA 0.544 ± 0.026 0.542 ± 0.029 0.541 ± 0.037 0.548 ± 0.036 Logistic Regression 0.613 ± 0.043 0.485 ± 0.142 0.400 ± 0.176 0.826 ± 0.096 E2D2 (λ = 1.0) 0.654 ± 0.025 0.677 ± 0.045 0.734 ± 0.091 0.575 ± 0.052 E2D2 (λ = 10.0) 0.666 ± 0.009 0.712 ± 0.013 0.829 ± 0.041 0.502 ± 0.039 E2D2 (λ = 100.0) 0.653 ± 0.013 0.711 ± 0.008 0.854 ± 0.043 0.452 ± 0.064 SVM 0.618 ± 0.028 0.624 ± 0.040 0.638 ± 0.060 0.599 ± 0.020 DIAG 0.563 ± 0.018 0.525 ± 0.032 0.485 ± 0.049 0.640 ± 0.041 Shrinkage (β = 0.25) 0.553 ± 0.024 0.481 ± 0.162 0.456 ± 0.155 0.650 ± 0.122 Shrinkage (β = 0.5) 0.564 ± 0.024 0.543 ± 0.031 0.520 ± 0.042 0.609 ± 0.035 Shrinkage (β = 0.75) 0.561 ± 0.024 0.542 ± 0.029 0.520 ± 0.040 0.601 ± 0.039\nDays in Advance: 60 AdaBoost (×10) 0.638 ± 0.026 0.571 ± 0.077 0.500 ± 0.128 0.776 ± 0.082 AdaBoost (×50) 0.634 ± 0.023 0.561 ± 0.070 0.481 ± 0.111 0.787 ± 0.075 CRDA (λ = 1.0) 0.660 ± 0.020 0.681 ± 0.039 0.733 ± 0.091 0.587 ± 0.067 CRDA (λ = 10.0) 0.672 ± 0.008 0.711 ± 0.011 0.811 ± 0.051 0.533 ± 0.060 CRDA (λ = 100.0) 0.664 ± 0.024 0.712 ± 0.010 0.830 ± 0.059 0.498 ± 0.100 LDA 0.551 ± 0.018 0.546 ± 0.031 0.542 ± 0.048 0.560 ± 0.035 Logistic Regression 0.607 ± 0.054 0.455 ± 0.219 0.389 ± 0.212 0.826 ± 0.110 E2D2 (λ = 1.0) 0.653 ± 0.027 0.667 ± 0.043 0.704 ± 0.087 0.602 ± 0.061 E2D2 (λ = 10.0) 0.677 ± 0.010 0.713 ± 0.017 0.805 ± 0.054 0.549 ± 0.051 E2D2 (λ = 100.0) 0.669 ± 0.015 0.714 ± 0.010 0.827 ± 0.055 0.510 ± 0.079 SVM 0.627 ± 0.026 0.626 ± 0.040 0.630 ± 0.070 0.625 ± 0.049 DIAG 0.576 ± 0.017 0.515 ± 0.037 0.453 ± 0.058 0.699 ± 0.061 Shrinkage (β = 0.25) 0.572 ± 0.030 0.484 ± 0.165 0.443 ± 0.158 0.701 ± 0.113 Shrinkage (β = 0.5) 0.579 ± 0.019 0.538 ± 0.038 0.493 ± 0.057 0.665 ± 0.050 Shrinkage (β = 0.75) 0.577 ± 0.017 0.536 ± 0.036 0.491 ± 0.053 0.663 ± 0.040\nDays in Advance: 90 AdaBoost (×10) 0.635 ± 0.024 0.565 ± 0.074 0.490 ± 0.116 0.781 ± 0.083 AdaBoost (×50) 0.624 ± 0.029 0.521 ± 0.090 0.427 ± 0.121 0.820 ± 0.070 CRDA (λ = 1.0) 0.653 ± 0.018 0.673 ± 0.032 0.720 ± 0.072 0.587 ± 0.063 CRDA (λ = 10.0) 0.663 ± 0.015 0.702 ± 0.011 0.796 ± 0.039 0.529 ± 0.059 CRDA (λ = 100.0) 0.663 ± 0.016 0.707 ± 0.010 0.814 ± 0.042 0.512 ± 0.064 LDA 0.548 ± 0.022 0.536 ± 0.037 0.525 ± 0.059 0.570 ± 0.060 Logistic Regression 0.601 ± 0.053 0.436 ± 0.206 0.358 ± 0.195 0.844 ± 0.096 E2D2 (λ = 1.0) 0.650 ± 0.012 0.659 ± 0.023 0.680 ± 0.057 0.621 ± 0.052 E2D2 (λ = 10.0) 0.665 ± 0.012 0.700 ± 0.015 0.785 ± 0.051 0.545 ± 0.059 E2D2 (λ = 100.0) 0.665 ± 0.015 0.708 ± 0.010 0.813 ± 0.039 0.516 ± 0.060 SVM 0.627 ± 0.012 0.619 ± 0.024 0.608 ± 0.047 0.646 ± 0.039 DIAG 0.564 ± 0.023 0.498 ± 0.029 0.433 ± 0.032 0.694 ± 0.033 Shrinkage (β = 0.25) 0.559 ± 0.029 0.462 ± 0.155 0.412 ± 0.140 0.706 ± 0.106 Shrinkage (β = 0.5) 0.558 ± 0.026 0.459 ± 0.154 0.409 ± 0.139 0.706 ± 0.107 Shrinkage (β = 0.75) 0.556 ± 0.034 0.412 ± 0.207 0.366 ± 0.186 0.746 ± 0.134\nAdaBoost (×10) 0.611 ± 0.033 0.475 ± 0.106 0.366 ± 0.108 0.855 ± 0.047 AdaBoost (×50) 0.612 ± 0.030 0.474 ± 0.107 0.363 ± 0.095 0.861 ± 0.035 CRDA (λ = 1.0) 0.671 ± 0.014 0.694 ± 0.028 0.754 ± 0.072 0.588 ± 0.064 CRDA (λ = 10.0) 0.675 ± 0.010 0.719 ± 0.008 0.832 ± 0.021 0.518 ± 0.030 CRDA (λ = 100.0) 0.655 ± 0.020 0.714 ± 0.006 0.861 ± 0.040 0.449 ± 0.077 LDA 0.599 ± 0.011 0.588 ± 0.018 0.573 ± 0.034 0.624 ± 0.036 Logistic Regression 0.598 ± 0.042 0.423 ± 0.165 0.321 ± 0.135 0.875 ± 0.052 E2D2 (λ = 1.0) 0.669 ± 0.015 0.686 ± 0.022 0.725 ± 0.049 0.614 ± 0.043 E2D2 (λ = 10.0) 0.681 ± 0.012 0.721 ± 0.009 0.826 ± 0.016 0.535 ± 0.027 E2D2 (λ = 100.0) 0.664 ± 0.011 0.718 ± 0.006 0.853 ± 0.031 0.476 ± 0.048 SVM 0.631 ± 0.014 0.625 ± 0.024 0.617 ± 0.043 0.644 ± 0.035 DIAG 0.600 ± 0.012 0.573 ± 0.037 0.542 ± 0.066 0.658 ± 0.053 Shrinkage (β = 0.25) 0.591 ± 0.032 0.524 ± 0.177 0.502 ± 0.175 0.681 ± 0.113 Shrinkage (β = 0.5) 0.599 ± 0.011 0.579 ± 0.029 0.554 ± 0.050 0.645 ± 0.033 Shrinkage (β = 0.75) 0.599 ± 0.012 0.578 ± 0.031 0.551 ± 0.050 0.646 ± 0.030\nDays in Advance: 60 AdaBoost (×10) 0.629 ± 0.024 0.543 ± 0.078 0.456 ± 0.122 0.802 ± 0.081 AdaBoost (×50) 0.626 ± 0.032 0.522 ± 0.110 0.431 ± 0.130 0.822 ± 0.074 CRDA (λ = 1.0) 0.670 ± 0.017 0.703 ± 0.027 0.784 ± 0.068 0.556 ± 0.053 CRDA (λ = 10.0) 0.669 ± 0.013 0.718 ± 0.009 0.844 ± 0.027 0.493 ± 0.042 CRDA (λ = 100.0) 0.657 ± 0.021 0.715 ± 0.008 0.862 ± 0.036 0.451 ± 0.074 LDA 0.595 ± 0.019 0.596 ± 0.016 0.598 ± 0.018 0.591 ± 0.032 Logistic Regression 0.608 ± 0.045 0.465 ± 0.172 0.377 ± 0.162 0.839 ± 0.081 E2D2 (λ = 1.0) 0.670 ± 0.019 0.691 ± 0.030 0.744 ± 0.070 0.595 ± 0.051 E2D2 (λ = 10.0) 0.676 ± 0.011 0.719 ± 0.011 0.831 ± 0.033 0.520 ± 0.037 E2D2 (λ = 100.0) 0.662 ± 0.016 0.717 ± 0.007 0.857 ± 0.033 0.468 ± 0.059 SVM 0.637 ± 0.021 0.635 ± 0.031 0.633 ± 0.050 0.640 ± 0.036 DIAG 0.597 ± 0.031 0.573 ± 0.037 0.542 ± 0.051 0.652 ± 0.055 Shrinkage (β = 0.25) 0.599 ± 0.028 0.583 ± 0.033 0.563 ± 0.046 0.635 ± 0.045 Shrinkage (β = 0.5) 0.597 ± 0.028 0.581 ± 0.031 0.559 ± 0.038 0.636 ± 0.039 Shrinkage (β = 0.75) 0.596 ± 0.027 0.582 ± 0.027 0.561 ± 0.032 0.631 ± 0.041\nDays in Advance: 90 AdaBoost (×10) 0.641 ± 0.032 0.554 ± 0.103 0.475 ± 0.163 0.807 ± 0.099 AdaBoost (×50) 0.644 ± 0.028 0.560 ± 0.094 0.480 ± 0.153 0.807 ± 0.098 CRDA (λ = 1.0) 0.680 ± 0.013 0.714 ± 0.018 0.800 ± 0.049 0.559 ± 0.046 CRDA (λ = 10.0) 0.670 ± 0.015 0.719 ± 0.009 0.843 ± 0.034 0.496 ± 0.054 CRDA (λ = 100.0) 0.663 ± 0.018 0.717 ± 0.007 0.854 ± 0.038 0.472 ± 0.071 LDA 0.599 ± 0.014 0.594 ± 0.030 0.590 ± 0.056 0.607 ± 0.037 Logistic Regression 0.607 ± 0.069 0.422 ± 0.258 0.371 ± 0.264 0.843 ± 0.130 E2D2 (λ = 1.0) 0.681 ± 0.013 0.703 ± 0.018 0.759 ± 0.045 0.603 ± 0.041 E2D2 (λ = 10.0) 0.681 ± 0.010 0.725 ± 0.008 0.840 ± 0.034 0.521 ± 0.046 E2D2 (λ = 100.0) 0.665 ± 0.018 0.718 ± 0.007 0.852 ± 0.036 0.479 ± 0.067 SVM 0.642 ± 0.018 0.646 ± 0.027 0.657 ± 0.053 0.627 ± 0.042 DIAG 0.606 ± 0.019 0.576 ± 0.043 0.541 ± 0.069 0.671 ± 0.036 Shrinkage (β = 0.25) 0.595 ± 0.036 0.522 ± 0.178 0.495 ± 0.176 0.694 ± 0.108 Shrinkage (β = 0.5) 0.591 ± 0.035 0.517 ± 0.176 0.488 ± 0.173 0.695 ± 0.107 Shrinkage (β = 0.75) 0.589 ± 0.036 0.519 ± 0.177 0.495 ± 0.176 0.684 ± 0.113\nAdaBoost (×10) 0.631 ± 0.022 0.529 ± 0.071 0.428 ± 0.107 0.834 ± 0.065 AdaBoost (×50) 0.633 ± 0.020 0.536 ± 0.064 0.435 ± 0.099 0.830 ± 0.060 CRDA (λ = 1.0) 0.680 ± 0.011 0.717 ± 0.008 0.811 ± 0.010 0.550 ± 0.022 CRDA (λ = 10.0) 0.669 ± 0.007 0.717 ± 0.007 0.841 ± 0.016 0.497 ± 0.019 CRDA (λ = 100.0) 0.660 ± 0.010 0.715 ± 0.008 0.852 ± 0.027 0.467 ± 0.037 LDA 0.612 ± 0.014 0.612 ± 0.020 0.612 ± 0.032 0.612 ± 0.023 Logistic Regression 0.632 ± 0.021 0.532 ± 0.070 0.431 ± 0.106 0.833 ± 0.066 E2D2 (λ = 1.0) 0.683 ± 0.012 0.708 ± 0.009 0.769 ± 0.024 0.597 ± 0.037 E2D2 (λ = 10.0) 0.678 ± 0.006 0.723 ± 0.005 0.840 ± 0.013 0.517 ± 0.018 E2D2 (λ = 100.0) 0.663 ± 0.010 0.715 ± 0.008 0.848 ± 0.024 0.478 ± 0.034 SVM 0.647 ± 0.017 0.653 ± 0.024 0.667 ± 0.042 0.626 ± 0.030 DIAG 0.613 ± 0.008 0.601 ± 0.022 0.586 ± 0.053 0.641 ± 0.056 Shrinkage (β = 0.25) 0.617 ± 0.006 0.611 ± 0.016 0.602 ± 0.038 0.632 ± 0.038 Shrinkage (β = 0.5) 0.597 ± 0.049 0.489 ± 0.245 0.479 ± 0.241 0.714 ± 0.146 Shrinkage (β = 0.75) 0.614 ± 0.011 0.605 ± 0.018 0.593 ± 0.033 0.635 ± 0.028\nDays in Advance: 60 AdaBoost (×10) 0.623 ± 0.020 0.505 ± 0.069 0.397 ± 0.102 0.848 ± 0.064 AdaBoost (×50) 0.620 ± 0.019 0.494 ± 0.064 0.380 ± 0.094 0.859 ± 0.058 CRDA (λ = 1.0) 0.679 ± 0.015 0.719 ± 0.011 0.821 ± 0.027 0.536 ± 0.041 CRDA (λ = 10.0) 0.671 ± 0.017 0.721 ± 0.010 0.851 ± 0.022 0.491 ± 0.048 CRDA (λ = 100.0) 0.659 ± 0.023 0.718 ± 0.010 0.869 ± 0.030 0.450 ± 0.071 LDA 0.615 ± 0.013 0.618 ± 0.018 0.624 ± 0.033 0.606 ± 0.032 Logistic Regression 0.618 ± 0.019 0.487 ± 0.065 0.373 ± 0.096 0.863 ± 0.059 E2D2 (λ = 1.0) 0.680 ± 0.011 0.712 ± 0.011 0.793 ± 0.032 0.567 ± 0.037 E2D2 (λ = 10.0) 0.678 ± 0.014 0.724 ± 0.010 0.844 ± 0.024 0.512 ± 0.041 E2D2 (λ = 100.0) 0.664 ± 0.020 0.720 ± 0.009 0.862 ± 0.027 0.467 ± 0.060 SVM 0.646 ± 0.017 0.653 ± 0.023 0.667 ± 0.041 0.625 ± 0.030 DIAG 0.619 ± 0.012 0.617 ± 0.018 0.615 ± 0.043 0.622 ± 0.050 Shrinkage (β = 0.25) 0.619 ± 0.018 0.620 ± 0.021 0.623 ± 0.036 0.615 ± 0.038 Shrinkage (β = 0.5) 0.620 ± 0.017 0.620 ± 0.019 0.621 ± 0.029 0.619 ± 0.035 Shrinkage (β = 0.75) 0.619 ± 0.017 0.616 ± 0.018 0.613 ± 0.027 0.624 ± 0.028\nDays in Advance: 90 AdaBoost (×10) 0.615 ± 0.033 0.488 ± 0.094 0.383 ± 0.123 0.847 ± 0.061 AdaBoost (×50) 0.614 ± 0.027 0.482 ± 0.077 0.370 ± 0.106 0.859 ± 0.055 CRDA (λ = 1.0) 0.682 ± 0.012 0.718 ± 0.013 0.812 ± 0.030 0.551 ± 0.027 CRDA (λ = 10.0) 0.677 ± 0.017 0.722 ± 0.012 0.839 ± 0.014 0.514 ± 0.033 CRDA (λ = 100.0) 0.671 ± 0.018 0.722 ± 0.011 0.856 ± 0.025 0.485 ± 0.048 LDA 0.620 ± 0.012 0.621 ± 0.022 0.626 ± 0.044 0.614 ± 0.032 Logistic Regression 0.603 ± 0.039 0.440 ± 0.134 0.335 ± 0.140 0.872 ± 0.065 E2D2 (λ = 1.0) 0.679 ± 0.011 0.704 ± 0.018 0.766 ± 0.045 0.592 ± 0.036 E2D2 (λ = 10.0) 0.681 ± 0.011 0.724 ± 0.009 0.838 ± 0.019 0.524 ± 0.025 E2D2 (λ = 100.0) 0.673 ± 0.017 0.722 ± 0.011 0.850 ± 0.019 0.496 ± 0.039 SVM 0.642 ± 0.011 0.646 ± 0.020 0.655 ± 0.040 0.628 ± 0.030 DIAG 0.619 ± 0.013 0.608 ± 0.020 0.591 ± 0.041 0.647 ± 0.045 Shrinkage (β = 0.25) 0.610 ± 0.038 0.552 ± 0.185 0.541 ± 0.182 0.679 ± 0.111 Shrinkage (β = 0.5) 0.606 ± 0.036 0.552 ± 0.185 0.547 ± 0.185 0.666 ± 0.116 Shrinkage (β = 0.75) 0.620 ± 0.004 0.615 ± 0.011 0.608 ± 0.030 0.632 ± 0.033\nAdaBoost (×10) 0.627 ± 0.030 0.522 ± 0.092 0.428 ± 0.136 0.827 ± 0.079 AdaBoost (×50) 0.627 ± 0.030 0.523 ± 0.094 0.429 ± 0.138 0.825 ± 0.080 CRDA (λ = 1.0) 0.678 ± 0.006 0.719 ± 0.005 0.825 ± 0.014 0.531 ± 0.015 CRDA (λ = 10.0) 0.669 ± 0.007 0.717 ± 0.005 0.839 ± 0.019 0.500 ± 0.027 CRDA (λ = 100.0) 0.662 ± 0.013 0.714 ± 0.004 0.845 ± 0.028 0.479 ± 0.052 LDA 0.632 ± 0.014 0.634 ± 0.020 0.640 ± 0.034 0.624 ± 0.017 Logistic Regression 0.615 ± 0.049 0.471 ± 0.184 0.389 ± 0.191 0.842 ± 0.096 E2D2 (λ = 1.0) 0.685 ± 0.008 0.716 ± 0.009 0.795 ± 0.022 0.574 ± 0.019 E2D2 (λ = 10.0) 0.677 ± 0.006 0.723 ± 0.006 0.840 ± 0.016 0.515 ± 0.019 E2D2 (λ = 100.0) 0.665 ± 0.011 0.715 ± 0.004 0.842 ± 0.025 0.488 ± 0.044 SVM 0.660 ± 0.010 0.669 ± 0.016 0.689 ± 0.035 0.631 ± 0.030 DIAG 0.633 ± 0.017 0.626 ± 0.034 0.618 ± 0.056 0.648 ± 0.028 Shrinkage (β = 0.25) 0.622 ± 0.044 0.570 ± 0.193 0.568 ± 0.197 0.677 ± 0.111 Shrinkage (β = 0.5) 0.626 ± 0.044 0.575 ± 0.193 0.574 ± 0.195 0.678 ± 0.109 Shrinkage (β = 0.75) 0.635 ± 0.013 0.632 ± 0.022 0.628 ± 0.040 0.643 ± 0.022\nDays in Advance: 60 AdaBoost (×10) 0.620 ± 0.021 0.498 ± 0.073 0.390 ± 0.114 0.851 ± 0.075 AdaBoost (×50) 0.621 ± 0.021 0.501 ± 0.073 0.394 ± 0.113 0.848 ± 0.074 CRDA (λ = 1.0) 0.677 ± 0.012 0.721 ± 0.008 0.833 ± 0.011 0.521 ± 0.026 CRDA (λ = 10.0) 0.668 ± 0.013 0.720 ± 0.010 0.851 ± 0.013 0.486 ± 0.023 CRDA (λ = 100.0) 0.657 ± 0.015 0.715 ± 0.009 0.862 ± 0.024 0.453 ± 0.041 LDA 0.632 ± 0.019 0.633 ± 0.016 0.636 ± 0.024 0.628 ± 0.039 Logistic Regression 0.605 ± 0.041 0.439 ± 0.165 0.343 ± 0.164 0.866 ± 0.088 E2D2 (λ = 1.0) 0.680 ± 0.015 0.712 ± 0.012 0.792 ± 0.019 0.568 ± 0.029 E2D2 (λ = 10.0) 0.677 ± 0.011 0.724 ± 0.009 0.850 ± 0.014 0.504 ± 0.020 E2D2 (λ = 100.0) 0.661 ± 0.014 0.717 ± 0.010 0.857 ± 0.021 0.465 ± 0.036 SVM 0.658 ± 0.014 0.672 ± 0.013 0.702 ± 0.024 0.613 ± 0.034 DIAG 0.625 ± 0.016 0.615 ± 0.035 0.603 ± 0.072 0.646 ± 0.065 Shrinkage (β = 0.25) 0.631 ± 0.018 0.626 ± 0.028 0.620 ± 0.057 0.643 ± 0.056 Shrinkage (β = 0.5) 0.632 ± 0.018 0.629 ± 0.024 0.625 ± 0.047 0.639 ± 0.049 Shrinkage (β = 0.75) 0.628 ± 0.015 0.623 ± 0.017 0.616 ± 0.035 0.640 ± 0.043\nDays in Advance: 90 AdaBoost (×10) 0.597 ± 0.032 0.414 ± 0.115 0.296 ± 0.095 0.898 ± 0.036 AdaBoost (×50) 0.612 ± 0.007 0.465 ± 0.028 0.338 ± 0.033 0.886 ± 0.025 CRDA (λ = 1.0) 0.682 ± 0.014 0.723 ± 0.009 0.828 ± 0.024 0.536 ± 0.041 CRDA (λ = 10.0) 0.674 ± 0.015 0.721 ± 0.009 0.842 ± 0.022 0.507 ± 0.044 CRDA (λ = 100.0) 0.670 ± 0.018 0.720 ± 0.009 0.847 ± 0.030 0.493 ± 0.060 LDA 0.630 ± 0.013 0.637 ± 0.013 0.650 ± 0.022 0.610 ± 0.025 Logistic Regression 0.575 ± 0.049 0.318 ± 0.210 0.230 ± 0.154 0.919 ± 0.057 E2D2 (λ = 1.0) 0.685 ± 0.011 0.716 ± 0.011 0.794 ± 0.031 0.576 ± 0.037 E2D2 (λ = 10.0) 0.679 ± 0.012 0.724 ± 0.008 0.844 ± 0.020 0.514 ± 0.035 E2D2 (λ = 100.0) 0.672 ± 0.017 0.721 ± 0.009 0.844 ± 0.026 0.501 ± 0.053 SVM 0.653 ± 0.010 0.669 ± 0.012 0.700 ± 0.030 0.606 ± 0.034 DIAG 0.616 ± 0.011 0.608 ± 0.015 0.596 ± 0.034 0.636 ± 0.038 Shrinkage (β = 0.25) 0.624 ± 0.009 0.625 ± 0.013 0.627 ± 0.030 0.622 ± 0.035 Shrinkage (β = 0.5) 0.612 ± 0.038 0.565 ± 0.189 0.570 ± 0.192 0.654 ± 0.120 Shrinkage (β = 0.75) 0.614 ± 0.039 0.566 ± 0.189 0.570 ± 0.191 0.658 ± 0.118\nAdaBoost (×10) 0.621 ± 0.038 0.494 ± 0.119 0.395 ± 0.146 0.848 ± 0.073 AdaBoost (×50) 0.618 ± 0.036 0.483 ± 0.117 0.380 ± 0.143 0.856 ± 0.075 CRDA (λ = 1.0) 0.686 ± 0.013 0.723 ± 0.012 0.821 ± 0.024 0.552 ± 0.024 CRDA (λ = 10.0) 0.675 ± 0.012 0.724 ± 0.009 0.854 ± 0.022 0.496 ± 0.035 CRDA (λ = 100.0) 0.665 ± 0.015 0.721 ± 0.008 0.864 ± 0.029 0.465 ± 0.052 LDA 0.638 ± 0.012 0.640 ± 0.015 0.644 ± 0.026 0.632 ± 0.024 Logistic Regression 0.602 ± 0.057 0.411 ± 0.220 0.330 ± 0.201 0.873 ± 0.092 E2D2 (λ = 1.0) 0.690 ± 0.011 0.716 ± 0.013 0.783 ± 0.029 0.597 ± 0.030 E2D2 (λ = 10.0) 0.684 ± 0.010 0.729 ± 0.009 0.849 ± 0.019 0.519 ± 0.022 E2D2 (λ = 100.0) 0.667 ± 0.014 0.721 ± 0.007 0.862 ± 0.028 0.472 ± 0.049 SVM 0.664 ± 0.012 0.677 ± 0.013 0.705 ± 0.029 0.623 ± 0.035 DIAG 0.639 ± 0.015 0.633 ± 0.027 0.627 ± 0.053 0.651 ± 0.042 Shrinkage (β = 0.25) 0.645 ± 0.015 0.644 ± 0.021 0.643 ± 0.037 0.648 ± 0.032 Shrinkage (β = 0.5) 0.610 ± 0.057 0.507 ± 0.254 0.501 ± 0.253 0.719 ± 0.142 Shrinkage (β = 0.75) 0.637 ± 0.012 0.633 ± 0.018 0.628 ± 0.031 0.645 ± 0.022\nDays in Advance: 60 AdaBoost (×10) 0.619 ± 0.035 0.493 ± 0.124 0.399 ± 0.156 0.839 ± 0.091 AdaBoost (×50) 0.622 ± 0.028 0.506 ± 0.095 0.409 ± 0.138 0.835 ± 0.086 CRDA (λ = 1.0) 0.682 ± 0.007 0.723 ± 0.009 0.831 ± 0.028 0.532 ± 0.030 CRDA (λ = 10.0) 0.667 ± 0.012 0.719 ± 0.008 0.852 ± 0.016 0.481 ± 0.032 CRDA (λ = 100.0) 0.658 ± 0.012 0.717 ± 0.007 0.869 ± 0.016 0.447 ± 0.032 LDA 0.637 ± 0.009 0.642 ± 0.017 0.652 ± 0.039 0.621 ± 0.036 Logistic Regression 0.610 ± 0.045 0.454 ± 0.178 0.367 ± 0.185 0.854 ± 0.101 E2D2 (λ = 1.0) 0.684 ± 0.006 0.715 ± 0.010 0.796 ± 0.031 0.572 ± 0.028 E2D2 (λ = 10.0) 0.677 ± 0.010 0.725 ± 0.008 0.849 ± 0.016 0.505 ± 0.022 E2D2 (λ = 100.0) 0.663 ± 0.012 0.719 ± 0.008 0.864 ± 0.020 0.461 ± 0.034 SVM 0.661 ± 0.012 0.675 ± 0.015 0.705 ± 0.033 0.616 ± 0.034 DIAG 0.632 ± 0.011 0.629 ± 0.029 0.628 ± 0.061 0.636 ± 0.047 Shrinkage (β = 0.25) 0.621 ± 0.042 0.573 ± 0.193 0.581 ± 0.201 0.661 ± 0.119 Shrinkage (β = 0.5) 0.637 ± 0.013 0.639 ± 0.026 0.646 ± 0.053 0.627 ± 0.038 Shrinkage (β = 0.75) 0.633 ± 0.013 0.635 ± 0.023 0.641 ± 0.044 0.625 ± 0.033\nDays in Advance: 90 AdaBoost (×10) 0.624 ± 0.024 0.508 ± 0.083 0.404 ± 0.127 0.844 ± 0.080 AdaBoost (×50) 0.622 ± 0.024 0.502 ± 0.081 0.397 ± 0.125 0.846 ± 0.079 CRDA (λ = 1.0) 0.682 ± 0.012 0.724 ± 0.007 0.833 ± 0.021 0.532 ± 0.036 CRDA (λ = 10.0) 0.669 ± 0.014 0.720 ± 0.008 0.850 ± 0.019 0.488 ± 0.041 CRDA (λ = 100.0) 0.663 ± 0.017 0.718 ± 0.007 0.859 ± 0.029 0.468 ± 0.058 LDA 0.643 ± 0.009 0.643 ± 0.015 0.644 ± 0.030 0.642 ± 0.026 Logistic Regression 0.619 ± 0.024 0.492 ± 0.084 0.386 ± 0.125 0.852 ± 0.079 E2D2 (λ = 1.0) 0.688 ± 0.007 0.719 ± 0.011 0.800 ± 0.032 0.576 ± 0.031 E2D2 (λ = 10.0) 0.680 ± 0.010 0.726 ± 0.008 0.848 ± 0.014 0.513 ± 0.024 E2D2 (λ = 100.0) 0.666 ± 0.016 0.719 ± 0.008 0.856 ± 0.026 0.475 ± 0.053 SVM 0.661 ± 0.010 0.670 ± 0.011 0.690 ± 0.024 0.631 ± 0.027 DIAG 0.634 ± 0.012 0.636 ± 0.016 0.639 ± 0.032 0.629 ± 0.032 Shrinkage (β = 0.25) 0.636 ± 0.014 0.639 ± 0.019 0.647 ± 0.033 0.625 ± 0.025 Shrinkage (β = 0.5) 0.639 ± 0.013 0.639 ± 0.019 0.641 ± 0.034 0.636 ± 0.025 Shrinkage (β = 0.75) 0.625 ± 0.043 0.574 ± 0.192 0.574 ± 0.194 0.677 ± 0.110\nAdaBoost (×10) 0.651 ± 0.031 0.617 ± 0.056 0.569 ± 0.082 0.734 ± 0.036 AdaBoost (×50) 0.635 ± 0.027 0.551 ± 0.079 0.464 ± 0.120 0.806 ± 0.070 CRDA (λ = 1.0) 0.650 ± 0.024 0.687 ± 0.019 0.768 ± 0.048 0.532 ± 0.067 CRDA (λ = 10.0) 0.658 ± 0.015 0.708 ± 0.011 0.829 ± 0.040 0.487 ± 0.057 CRDA (λ = 100.0) 0.653 ± 0.012 0.706 ± 0.015 0.837 ± 0.071 0.470 ± 0.088 LDA 0.555 ± 0.021 0.557 ± 0.024 0.561 ± 0.040 0.549 ± 0.047 Logistic Regression 0.581 ± 0.070 0.346 ± 0.266 0.296 ± 0.254 0.866 ± 0.116 E2D2 (λ = 1.0) 0.650 ± 0.024 0.677 ± 0.025 0.736 ± 0.052 0.564 ± 0.058 E2D2 (λ = 10.0) 0.661 ± 0.014 0.707 ± 0.011 0.818 ± 0.036 0.505 ± 0.052 E2D2 (λ = 100.0) 0.657 ± 0.010 0.709 ± 0.014 0.837 ± 0.062 0.477 ± 0.074 SVM 0.611 ± 0.028 0.613 ± 0.034 0.620 ± 0.060 0.601 ± 0.062 DIAG 0.569 ± 0.021 0.525 ± 0.044 0.483 ± 0.078 0.655 ± 0.083 Shrinkage (β = 0.25) 0.560 ± 0.029 0.483 ± 0.166 0.457 ± 0.171 0.663 ± 0.137 Shrinkage (β = 0.5) 0.564 ± 0.019 0.531 ± 0.043 0.500 ± 0.077 0.629 ± 0.070 Shrinkage (β = 0.75) 0.562 ± 0.018 0.528 ± 0.042 0.496 ± 0.073 0.628 ± 0.066\nDays in Advance: 60 AdaBoost (×10) 0.624 ± 0.035 0.533 ± 0.093 0.450 ± 0.136 0.799 ± 0.082 AdaBoost (×50) 0.634 ± 0.033 0.552 ± 0.093 0.472 ± 0.129 0.795 ± 0.068 CRDA (λ = 1.0) 0.651 ± 0.025 0.670 ± 0.043 0.717 ± 0.089 0.586 ± 0.064 CRDA (λ = 10.0) 0.658 ± 0.013 0.699 ± 0.018 0.799 ± 0.061 0.516 ± 0.069 CRDA (λ = 100.0) 0.648 ± 0.024 0.700 ± 0.019 0.826 ± 0.084 0.470 ± 0.120 LDA 0.559 ± 0.027 0.544 ± 0.043 0.529 ± 0.065 0.590 ± 0.051 Logistic Regression 0.613 ± 0.044 0.491 ± 0.143 0.407 ± 0.168 0.819 ± 0.091 E2D2 (λ = 1.0) 0.650 ± 0.025 0.657 ± 0.046 0.678 ± 0.087 0.622 ± 0.052 E2D2 (λ = 10.0) 0.662 ± 0.014 0.698 ± 0.024 0.785 ± 0.066 0.539 ± 0.061 E2D2 (λ = 100.0) 0.655 ± 0.015 0.704 ± 0.018 0.824 ± 0.076 0.486 ± 0.097 SVM 0.613 ± 0.029 0.601 ± 0.039 0.586 ± 0.058 0.640 ± 0.048 DIAG 0.565 ± 0.020 0.497 ± 0.060 0.437 ± 0.079 0.692 ± 0.057 Shrinkage (β = 0.25) 0.543 ± 0.037 0.322 ± 0.264 0.299 ± 0.246 0.787 ± 0.177 Shrinkage (β = 0.5) 0.565 ± 0.019 0.514 ± 0.043 0.464 ± 0.060 0.666 ± 0.050 Shrinkage (β = 0.75) 0.553 ± 0.025 0.457 ± 0.158 0.414 ± 0.151 0.691 ± 0.114\nDays in Advance: 90 AdaBoost (×10) 0.631 ± 0.029 0.537 ± 0.079 0.444 ± 0.120 0.819 ± 0.068 AdaBoost (×50) 0.633 ± 0.035 0.544 ± 0.095 0.458 ± 0.133 0.809 ± 0.067 CRDA (λ = 1.0) 0.663 ± 0.015 0.699 ± 0.018 0.784 ± 0.048 0.542 ± 0.046 CRDA (λ = 10.0) 0.662 ± 0.013 0.716 ± 0.007 0.853 ± 0.030 0.471 ± 0.051 CRDA (λ = 100.0) 0.657 ± 0.016 0.717 ± 0.006 0.867 ± 0.033 0.447 ± 0.062 LDA 0.568 ± 0.021 0.567 ± 0.027 0.568 ± 0.045 0.569 ± 0.047 Logistic Regression 0.608 ± 0.048 0.454 ± 0.176 0.362 ± 0.163 0.855 ± 0.072 E2D2 (λ = 1.0) 0.655 ± 0.020 0.680 ± 0.031 0.739 ± 0.067 0.570 ± 0.045 E2D2 (λ = 10.0) 0.670 ± 0.010 0.718 ± 0.009 0.842 ± 0.031 0.497 ± 0.040 E2D2 (λ = 100.0) 0.659 ± 0.015 0.717 ± 0.007 0.864 ± 0.033 0.453 ± 0.057 SVM 0.636 ± 0.021 0.639 ± 0.028 0.646 ± 0.048 0.626 ± 0.035 DIAG 0.571 ± 0.017 0.520 ± 0.024 0.466 ± 0.037 0.675 ± 0.046 Shrinkage (β = 0.25) 0.566 ± 0.028 0.486 ± 0.164 0.451 ± 0.155 0.682 ± 0.115 Shrinkage (β = 0.5) 0.569 ± 0.029 0.486 ± 0.163 0.447 ± 0.152 0.692 ± 0.109 Shrinkage (β = 0.75) 0.573 ± 0.019 0.541 ± 0.025 0.505 ± 0.040 0.640 ± 0.045\nAdaBoost (×10) 0.629 ± 0.033 0.553 ± 0.073 0.471 ± 0.106 0.788 ± 0.063 AdaBoost (×50) 0.632 ± 0.024 0.549 ± 0.072 0.461 ± 0.107 0.802 ± 0.060 CRDA (λ = 1.0) 0.666 ± 0.016 0.709 ± 0.011 0.816 ± 0.034 0.516 ± 0.052 CRDA (λ = 10.0) 0.658 ± 0.016 0.714 ± 0.009 0.851 ± 0.019 0.466 ± 0.043 CRDA (λ = 100.0) 0.649 ± 0.020 0.712 ± 0.007 0.868 ± 0.033 0.430 ± 0.070 LDA 0.590 ± 0.023 0.597 ± 0.027 0.607 ± 0.042 0.574 ± 0.041 Logistic Regression 0.581 ± 0.055 0.353 ± 0.211 0.273 ± 0.192 0.888 ± 0.084 E2D2 (λ = 1.0) 0.673 ± 0.012 0.707 ± 0.018 0.791 ± 0.047 0.555 ± 0.044 E2D2 (λ = 10.0) 0.667 ± 0.016 0.718 ± 0.009 0.847 ± 0.014 0.487 ± 0.040 E2D2 (λ = 100.0) 0.656 ± 0.015 0.715 ± 0.006 0.862 ± 0.027 0.449 ± 0.053 SVM 0.636 ± 0.022 0.645 ± 0.027 0.663 ± 0.043 0.610 ± 0.026 DIAG 0.595 ± 0.017 0.574 ± 0.036 0.549 ± 0.062 0.640 ± 0.046 Shrinkage (β = 0.25) 0.598 ± 0.017 0.585 ± 0.029 0.569 ± 0.051 0.628 ± 0.038 Shrinkage (β = 0.5) 0.596 ± 0.020 0.581 ± 0.035 0.563 ± 0.058 0.629 ± 0.038 Shrinkage (β = 0.75) 0.590 ± 0.022 0.575 ± 0.037 0.558 ± 0.058 0.622 ± 0.036\nDays in Advance: 60 AdaBoost (×10) 0.634 ± 0.016 0.543 ± 0.056 0.442 ± 0.086 0.826 ± 0.056 AdaBoost (×50) 0.633 ± 0.019 0.545 ± 0.057 0.447 ± 0.085 0.818 ± 0.054 CRDA (λ = 1.0) 0.674 ± 0.014 0.709 ± 0.023 0.799 ± 0.056 0.549 ± 0.039 CRDA (λ = 10.0) 0.668 ± 0.012 0.717 ± 0.009 0.842 ± 0.026 0.495 ± 0.036 CRDA (λ = 100.0) 0.659 ± 0.015 0.716 ± 0.006 0.862 ± 0.037 0.456 ± 0.064 LDA 0.595 ± 0.021 0.589 ± 0.028 0.581 ± 0.040 0.608 ± 0.024 Logistic Regression 0.618 ± 0.028 0.496 ± 0.084 0.390 ± 0.116 0.847 ± 0.068 E2D2 (λ = 1.0) 0.674 ± 0.016 0.701 ± 0.026 0.767 ± 0.060 0.581 ± 0.041 E2D2 (λ = 10.0) 0.677 ± 0.009 0.722 ± 0.009 0.839 ± 0.025 0.514 ± 0.028 E2D2 (λ = 100.0) 0.662 ± 0.012 0.718 ± 0.006 0.858 ± 0.032 0.467 ± 0.051 SVM 0.642 ± 0.015 0.644 ± 0.023 0.650 ± 0.040 0.634 ± 0.029 DIAG 0.603 ± 0.018 0.573 ± 0.045 0.540 ± 0.077 0.666 ± 0.061 Shrinkage (β = 0.25) 0.610 ± 0.015 0.596 ± 0.031 0.578 ± 0.056 0.642 ± 0.045 Shrinkage (β = 0.5) 0.612 ± 0.015 0.601 ± 0.024 0.587 ± 0.044 0.637 ± 0.042 Shrinkage (β = 0.75) 0.611 ± 0.016 0.600 ± 0.023 0.585 ± 0.040 0.638 ± 0.034\nDays in Advance: 90 AdaBoost (×10) 0.645 ± 0.034 0.577 ± 0.095 0.509 ± 0.146 0.782 ± 0.081 AdaBoost (×50) 0.648 ± 0.032 0.584 ± 0.086 0.516 ± 0.139 0.780 ± 0.078 CRDA (λ = 1.0) 0.674 ± 0.018 0.698 ± 0.033 0.760 ± 0.075 0.587 ± 0.048 CRDA (λ = 10.0) 0.679 ± 0.012 0.721 ± 0.011 0.831 ± 0.035 0.526 ± 0.044 CRDA (λ = 100.0) 0.668 ± 0.015 0.722 ± 0.006 0.860 ± 0.023 0.477 ± 0.047 LDA 0.588 ± 0.020 0.584 ± 0.026 0.581 ± 0.046 0.596 ± 0.049 Logistic Regression 0.634 ± 0.051 0.534 ± 0.167 0.471 ± 0.201 0.797 ± 0.104 E2D2 (λ = 1.0) 0.671 ± 0.019 0.689 ± 0.031 0.734 ± 0.065 0.608 ± 0.040 E2D2 (λ = 10.0) 0.680 ± 0.012 0.715 ± 0.021 0.808 ± 0.059 0.552 ± 0.050 E2D2 (λ = 100.0) 0.673 ± 0.013 0.722 ± 0.007 0.851 ± 0.021 0.494 ± 0.040 SVM 0.626 ± 0.022 0.627 ± 0.029 0.632 ± 0.047 0.620 ± 0.036 DIAG 0.603 ± 0.017 0.566 ± 0.040 0.524 ± 0.070 0.682 ± 0.046 Shrinkage (β = 0.25) 0.608 ± 0.018 0.584 ± 0.036 0.555 ± 0.063 0.661 ± 0.042 Shrinkage (β = 0.5) 0.606 ± 0.017 0.586 ± 0.033 0.560 ± 0.057 0.653 ± 0.039 Shrinkage (β = 0.75) 0.603 ± 0.018 0.585 ± 0.032 0.563 ± 0.054 0.643 ± 0.042\nAdaBoost (×10) 0.640 ± 0.022 0.565 ± 0.069 0.481 ± 0.108 0.799 ± 0.067 AdaBoost (×50) 0.639 ± 0.021 0.559 ± 0.067 0.471 ± 0.106 0.806 ± 0.068 CRDA (λ = 1.0) 0.677 ± 0.008 0.714 ± 0.018 0.807 ± 0.049 0.547 ± 0.039 CRDA (λ = 10.0) 0.669 ± 0.008 0.719 ± 0.006 0.845 ± 0.033 0.493 ± 0.044 CRDA (λ = 100.0) 0.663 ± 0.014 0.718 ± 0.003 0.858 ± 0.033 0.467 ± 0.059 LDA 0.618 ± 0.016 0.621 ± 0.027 0.629 ± 0.050 0.608 ± 0.034 Logistic Regression 0.632 ± 0.026 0.538 ± 0.088 0.449 ± 0.127 0.815 ± 0.077 E2D2 (λ = 1.0) 0.676 ± 0.013 0.702 ± 0.025 0.764 ± 0.056 0.589 ± 0.034 E2D2 (λ = 10.0) 0.678 ± 0.003 0.724 ± 0.008 0.842 ± 0.031 0.514 ± 0.031 E2D2 (λ = 100.0) 0.666 ± 0.012 0.719 ± 0.003 0.854 ± 0.031 0.478 ± 0.053 SVM 0.649 ± 0.015 0.657 ± 0.027 0.675 ± 0.054 0.622 ± 0.034 DIAG 0.610 ± 0.012 0.597 ± 0.018 0.578 ± 0.034 0.643 ± 0.033 Shrinkage (β = 0.25) 0.612 ± 0.016 0.604 ± 0.023 0.593 ± 0.040 0.631 ± 0.032 Shrinkage (β = 0.5) 0.615 ± 0.018 0.606 ± 0.028 0.594 ± 0.046 0.636 ± 0.032 Shrinkage (β = 0.75) 0.617 ± 0.020 0.610 ± 0.028 0.600 ± 0.046 0.635 ± 0.034\nDays in Advance: 60 AdaBoost (×10) 0.640 ± 0.025 0.550 ± 0.076 0.457 ± 0.122 0.823 ± 0.077 AdaBoost (×50) 0.636 ± 0.025 0.536 ± 0.082 0.439 ± 0.129 0.834 ± 0.082 CRDA (λ = 1.0) 0.683 ± 0.010 0.720 ± 0.012 0.814 ± 0.029 0.552 ± 0.022 CRDA (λ = 10.0) 0.674 ± 0.011 0.723 ± 0.007 0.848 ± 0.017 0.500 ± 0.030 CRDA (λ = 100.0) 0.664 ± 0.018 0.720 ± 0.009 0.863 ± 0.020 0.465 ± 0.052 LDA 0.614 ± 0.012 0.610 ± 0.017 0.606 ± 0.030 0.622 ± 0.028 Logistic Regression 0.589 ± 0.072 0.355 ± 0.275 0.301 ± 0.254 0.877 ± 0.114 E2D2 (λ = 1.0) 0.686 ± 0.010 0.712 ± 0.013 0.777 ± 0.029 0.594 ± 0.022 E2D2 (λ = 10.0) 0.682 ± 0.008 0.726 ± 0.007 0.842 ± 0.015 0.521 ± 0.014 E2D2 (λ = 100.0) 0.669 ± 0.016 0.722 ± 0.008 0.859 ± 0.019 0.480 ± 0.046 SVM 0.646 ± 0.013 0.646 ± 0.016 0.648 ± 0.024 0.643 ± 0.016 DIAG 0.618 ± 0.018 0.595 ± 0.037 0.564 ± 0.059 0.673 ± 0.038 Shrinkage (β = 0.25) 0.623 ± 0.017 0.606 ± 0.034 0.584 ± 0.056 0.661 ± 0.035 Shrinkage (β = 0.5) 0.620 ± 0.017 0.604 ± 0.032 0.583 ± 0.052 0.656 ± 0.032 Shrinkage (β = 0.75) 0.616 ± 0.012 0.601 ± 0.023 0.581 ± 0.039 0.651 ± 0.025\nDays in Advance: 90 AdaBoost (×10) 0.620 ± 0.024 0.497 ± 0.066 0.385 ± 0.096 0.856 ± 0.050 AdaBoost (×50) 0.622 ± 0.024 0.502 ± 0.067 0.392 ± 0.100 0.852 ± 0.054 CRDA (λ = 1.0) 0.688 ± 0.011 0.723 ± 0.010 0.812 ± 0.031 0.564 ± 0.039 CRDA (λ = 10.0) 0.673 ± 0.015 0.721 ± 0.007 0.846 ± 0.015 0.499 ± 0.042 CRDA (λ = 100.0) 0.666 ± 0.017 0.719 ± 0.006 0.855 ± 0.025 0.477 ± 0.056 LDA 0.622 ± 0.013 0.619 ± 0.022 0.616 ± 0.039 0.628 ± 0.019 Logistic Regression 0.597 ± 0.050 0.408 ± 0.186 0.314 ± 0.172 0.879 ± 0.075 E2D2 (λ = 1.0) 0.686 ± 0.012 0.709 ± 0.017 0.766 ± 0.040 0.606 ± 0.033 E2D2 (λ = 10.0) 0.683 ± 0.008 0.727 ± 0.005 0.843 ± 0.014 0.523 ± 0.023 E2D2 (λ = 100.0) 0.669 ± 0.016 0.720 ± 0.007 0.851 ± 0.019 0.486 ± 0.047 SVM 0.649 ± 0.015 0.646 ± 0.027 0.642 ± 0.050 0.656 ± 0.025 DIAG 0.620 ± 0.016 0.589 ± 0.038 0.549 ± 0.064 0.691 ± 0.035 Shrinkage (β = 0.25) 0.631 ± 0.014 0.615 ± 0.028 0.593 ± 0.049 0.668 ± 0.024 Shrinkage (β = 0.5) 0.633 ± 0.014 0.621 ± 0.025 0.602 ± 0.042 0.663 ± 0.016 Shrinkage (β = 0.75) 0.629 ± 0.012 0.616 ± 0.021 0.598 ± 0.034 0.660 ± 0.014\nAdaBoost (×10) 0.643 ± 0.028 0.567 ± 0.082 0.487 ± 0.129 0.799 ± 0.075 AdaBoost (×50) 0.638 ± 0.030 0.546 ± 0.088 0.457 ± 0.139 0.819 ± 0.082 CRDA (λ = 1.0) 0.684 ± 0.011 0.722 ± 0.012 0.823 ± 0.025 0.544 ± 0.017 CRDA (λ = 10.0) 0.674 ± 0.008 0.722 ± 0.005 0.846 ± 0.020 0.503 ± 0.031 CRDA (λ = 100.0) 0.667 ± 0.012 0.719 ± 0.004 0.853 ± 0.027 0.481 ± 0.050 LDA 0.636 ± 0.020 0.639 ± 0.023 0.643 ± 0.030 0.629 ± 0.018 Logistic Regression 0.614 ± 0.064 0.445 ± 0.238 0.377 ± 0.230 0.851 ± 0.106 E2D2 (λ = 1.0) 0.683 ± 0.012 0.712 ± 0.014 0.782 ± 0.029 0.585 ± 0.020 E2D2 (λ = 10.0) 0.682 ± 0.007 0.726 ± 0.007 0.843 ± 0.016 0.520 ± 0.016 E2D2 (λ = 100.0) 0.670 ± 0.010 0.720 ± 0.004 0.851 ± 0.025 0.488 ± 0.043 SVM 0.659 ± 0.017 0.668 ± 0.021 0.685 ± 0.033 0.634 ± 0.019 DIAG 0.625 ± 0.022 0.613 ± 0.036 0.596 ± 0.054 0.654 ± 0.018 Shrinkage (β = 0.25) 0.633 ± 0.021 0.626 ± 0.032 0.617 ± 0.048 0.649 ± 0.013 Shrinkage (β = 0.5) 0.635 ± 0.021 0.630 ± 0.030 0.622 ± 0.044 0.648 ± 0.013 Shrinkage (β = 0.75) 0.636 ± 0.021 0.630 ± 0.029 0.621 ± 0.040 0.650 ± 0.015\nDays in Advance: 60 AdaBoost (×10) 0.621 ± 0.019 0.496 ± 0.060 0.381 ± 0.092 0.862 ± 0.055 AdaBoost (×50) 0.623 ± 0.019 0.503 ± 0.061 0.390 ± 0.093 0.856 ± 0.057 CRDA (λ = 1.0) 0.685 ± 0.014 0.720 ± 0.012 0.811 ± 0.032 0.559 ± 0.044 CRDA (λ = 10.0) 0.674 ± 0.019 0.723 ± 0.010 0.848 ± 0.011 0.501 ± 0.045 CRDA (λ = 100.0) 0.666 ± 0.020 0.719 ± 0.009 0.853 ± 0.018 0.478 ± 0.055 LDA 0.631 ± 0.016 0.628 ± 0.018 0.625 ± 0.032 0.636 ± 0.037 Logistic Regression 0.608 ± 0.041 0.443 ± 0.159 0.340 ± 0.146 0.877 ± 0.068 E2D2 (λ = 1.0) 0.685 ± 0.011 0.708 ± 0.014 0.765 ± 0.039 0.605 ± 0.042 E2D2 (λ = 10.0) 0.681 ± 0.013 0.725 ± 0.008 0.839 ± 0.013 0.524 ± 0.033 E2D2 (λ = 100.0) 0.669 ± 0.019 0.720 ± 0.009 0.850 ± 0.016 0.488 ± 0.051 SVM 0.656 ± 0.013 0.663 ± 0.014 0.678 ± 0.031 0.635 ± 0.038 DIAG 0.635 ± 0.016 0.625 ± 0.022 0.610 ± 0.041 0.660 ± 0.041 Shrinkage (β = 0.25) 0.623 ± 0.043 0.569 ± 0.191 0.564 ± 0.191 0.682 ± 0.113 Shrinkage (β = 0.5) 0.625 ± 0.044 0.570 ± 0.191 0.564 ± 0.189 0.685 ± 0.113 Shrinkage (β = 0.75) 0.622 ± 0.042 0.564 ± 0.188 0.550 ± 0.184 0.694 ± 0.105\nDays in Advance: 90 AdaBoost (×10) 0.625 ± 0.029 0.515 ± 0.086 0.414 ± 0.124 0.837 ± 0.068 AdaBoost (×50) 0.627 ± 0.029 0.516 ± 0.087 0.416 ± 0.126 0.837 ± 0.070 CRDA (λ = 1.0) 0.685 ± 0.014 0.723 ± 0.010 0.823 ± 0.013 0.546 ± 0.027 CRDA (λ = 10.0) 0.676 ± 0.016 0.723 ± 0.009 0.847 ± 0.015 0.505 ± 0.040 CRDA (λ = 100.0) 0.664 ± 0.018 0.720 ± 0.008 0.863 ± 0.022 0.465 ± 0.055 LDA 0.633 ± 0.015 0.640 ± 0.017 0.654 ± 0.032 0.611 ± 0.035 Logistic Regression 0.614 ± 0.041 0.478 ± 0.126 0.380 ± 0.156 0.848 ± 0.077 E2D2 (λ = 1.0) 0.685 ± 0.014 0.713 ± 0.015 0.784 ± 0.030 0.585 ± 0.030 E2D2 (λ = 10.0) 0.684 ± 0.012 0.728 ± 0.008 0.847 ± 0.012 0.520 ± 0.024 E2D2 (λ = 100.0) 0.671 ± 0.018 0.723 ± 0.009 0.857 ± 0.020 0.485 ± 0.052 SVM 0.656 ± 0.009 0.665 ± 0.017 0.686 ± 0.047 0.625 ± 0.048 DIAG 0.628 ± 0.012 0.624 ± 0.020 0.620 ± 0.048 0.636 ± 0.051 Shrinkage (β = 0.25) 0.632 ± 0.014 0.634 ± 0.017 0.638 ± 0.038 0.625 ± 0.041 Shrinkage (β = 0.5) 0.618 ± 0.042 0.570 ± 0.191 0.575 ± 0.194 0.662 ± 0.118 Shrinkage (β = 0.75) 0.632 ± 0.015 0.637 ± 0.017 0.646 ± 0.033 0.619 ± 0.036\nAdaBoost (×10) 0.610 ± 0.017 0.454 ± 0.046 0.326 ± 0.043 0.894 ± 0.010 AdaBoost (×50) 0.614 ± 0.012 0.467 ± 0.033 0.340 ± 0.035 0.889 ± 0.014 CRDA (λ = 1.0) 0.686 ± 0.015 0.726 ± 0.012 0.830 ± 0.028 0.543 ± 0.038 CRDA (λ = 10.0) 0.673 ± 0.014 0.721 ± 0.009 0.847 ± 0.021 0.499 ± 0.038 CRDA (λ = 100.0) 0.667 ± 0.016 0.721 ± 0.008 0.857 ± 0.023 0.478 ± 0.049 LDA 0.641 ± 0.015 0.644 ± 0.016 0.648 ± 0.023 0.634 ± 0.019 Logistic Regression 0.596 ± 0.039 0.397 ± 0.148 0.284 ± 0.112 0.908 ± 0.035 E2D2 (λ = 1.0) 0.688 ± 0.009 0.719 ± 0.008 0.797 ± 0.022 0.580 ± 0.029 E2D2 (λ = 10.0) 0.682 ± 0.012 0.727 ± 0.010 0.847 ± 0.019 0.517 ± 0.027 E2D2 (λ = 100.0) 0.670 ± 0.015 0.721 ± 0.008 0.854 ± 0.022 0.485 ± 0.045 SVM 0.663 ± 0.012 0.674 ± 0.017 0.698 ± 0.034 0.629 ± 0.027 DIAG 0.635 ± 0.013 0.632 ± 0.025 0.628 ± 0.051 0.642 ± 0.043 Shrinkage (β = 0.25) 0.642 ± 0.015 0.643 ± 0.017 0.646 ± 0.029 0.637 ± 0.032 Shrinkage (β = 0.5) 0.631 ± 0.045 0.582 ± 0.194 0.585 ± 0.196 0.678 ± 0.111 Shrinkage (β = 0.75) 0.628 ± 0.044 0.576 ± 0.193 0.574 ± 0.193 0.681 ± 0.108\nDays in Advance: 60 AdaBoost (×10) 0.623 ± 0.024 0.506 ± 0.082 0.404 ± 0.126 0.842 ± 0.079 AdaBoost (×50) 0.617 ± 0.024 0.485 ± 0.073 0.372 ± 0.114 0.863 ± 0.067 CRDA (λ = 1.0) 0.685 ± 0.010 0.723 ± 0.007 0.822 ± 0.030 0.549 ± 0.043 CRDA (λ = 10.0) 0.672 ± 0.012 0.722 ± 0.009 0.851 ± 0.020 0.492 ± 0.030 CRDA (λ = 100.0) 0.663 ± 0.011 0.720 ± 0.008 0.867 ± 0.022 0.459 ± 0.032 LDA 0.644 ± 0.012 0.647 ± 0.015 0.653 ± 0.032 0.634 ± 0.033 Logistic Regression 0.595 ± 0.052 0.392 ± 0.207 0.304 ± 0.183 0.887 ± 0.083 E2D2 (λ = 1.0) 0.685 ± 0.008 0.711 ± 0.011 0.777 ± 0.034 0.592 ± 0.036 E2D2 (λ = 10.0) 0.680 ± 0.010 0.726 ± 0.007 0.848 ± 0.017 0.512 ± 0.026 E2D2 (λ = 100.0) 0.668 ± 0.009 0.721 ± 0.008 0.859 ± 0.022 0.477 ± 0.028 SVM 0.664 ± 0.012 0.674 ± 0.015 0.697 ± 0.034 0.632 ± 0.038 DIAG 0.638 ± 0.014 0.632 ± 0.029 0.625 ± 0.058 0.651 ± 0.053 Shrinkage (β = 0.25) 0.628 ± 0.045 0.574 ± 0.193 0.571 ± 0.197 0.685 ± 0.115 Shrinkage (β = 0.5) 0.600 ± 0.067 0.451 ± 0.296 0.453 ± 0.297 0.748 ± 0.168 Shrinkage (β = 0.75) 0.628 ± 0.044 0.575 ± 0.193 0.571 ± 0.195 0.685 ± 0.111\nDays in Advance: 90 AdaBoost (×10) 0.618 ± 0.011 0.481 ± 0.034 0.356 ± 0.041 0.879 ± 0.024 AdaBoost (×50) 0.621 ± 0.013 0.494 ± 0.046 0.375 ± 0.058 0.868 ± 0.037 CRDA (λ = 1.0) 0.688 ± 0.007 0.727 ± 0.009 0.832 ± 0.025 0.544 ± 0.021 CRDA (λ = 10.0) 0.681 ± 0.008 0.728 ± 0.005 0.853 ± 0.017 0.509 ± 0.027 CRDA (λ = 100.0) 0.672 ± 0.018 0.724 ± 0.007 0.860 ± 0.025 0.484 ± 0.057 LDA 0.649 ± 0.011 0.653 ± 0.017 0.664 ± 0.034 0.634 ± 0.023 Logistic Regression 0.618 ± 0.011 0.480 ± 0.034 0.354 ± 0.038 0.882 ± 0.021 E2D2 (λ = 1.0) 0.690 ± 0.010 0.720 ± 0.013 0.799 ± 0.029 0.582 ± 0.020 E2D2 (λ = 10.0) 0.686 ± 0.007 0.730 ± 0.005 0.849 ± 0.014 0.523 ± 0.018 E2D2 (λ = 100.0) 0.674 ± 0.015 0.725 ± 0.006 0.859 ± 0.026 0.489 ± 0.053 SVM 0.669 ± 0.011 0.681 ± 0.016 0.707 ± 0.031 0.631 ± 0.019 DIAG 0.642 ± 0.017 0.642 ± 0.026 0.645 ± 0.045 0.638 ± 0.020 Shrinkage (β = 0.25) 0.634 ± 0.046 0.588 ± 0.197 0.596 ± 0.202 0.672 ± 0.112 Shrinkage (β = 0.5) 0.644 ± 0.020 0.646 ± 0.027 0.651 ± 0.045 0.637 ± 0.028 Shrinkage (β = 0.75) 0.642 ± 0.018 0.643 ± 0.026 0.645 ± 0.043 0.639 ± 0.029\nAdaBoost (×10) 0.637 ± 0.028 0.571 ± 0.057 0.491 ± 0.085 0.783 ± 0.053 AdaBoost (×50) 0.640 ± 0.024 0.570 ± 0.061 0.487 ± 0.093 0.792 ± 0.053 CRDA (λ = 1.0) 0.662 ± 0.017 0.692 ± 0.028 0.762 ± 0.069 0.563 ± 0.058 CRDA (λ = 10.0) 0.670 ± 0.017 0.713 ± 0.010 0.819 ± 0.023 0.520 ± 0.047 CRDA (λ = 100.0) 0.664 ± 0.020 0.713 ± 0.008 0.834 ± 0.033 0.494 ± 0.068 LDA 0.555 ± 0.026 0.565 ± 0.033 0.579 ± 0.048 0.531 ± 0.040 Logistic Regression 0.615 ± 0.055 0.469 ± 0.206 0.395 ± 0.200 0.835 ± 0.094 E2D2 (λ = 1.0) 0.658 ± 0.019 0.677 ± 0.034 0.723 ± 0.073 0.592 ± 0.050 E2D2 (λ = 10.0) 0.672 ± 0.015 0.713 ± 0.010 0.813 ± 0.025 0.532 ± 0.042 E2D2 (λ = 100.0) 0.668 ± 0.018 0.714 ± 0.008 0.830 ± 0.026 0.506 ± 0.056 SVM 0.611 ± 0.026 0.619 ± 0.034 0.632 ± 0.050 0.590 ± 0.029 DIAG 0.568 ± 0.014 0.515 ± 0.026 0.460 ± 0.042 0.676 ± 0.046 Shrinkage (β = 0.25) 0.574 ± 0.014 0.538 ± 0.025 0.499 ± 0.041 0.649 ± 0.045 Shrinkage (β = 0.5) 0.560 ± 0.033 0.438 ± 0.220 0.413 ± 0.210 0.708 ± 0.152 Shrinkage (β = 0.75) 0.560 ± 0.025 0.480 ± 0.163 0.448 ± 0.158 0.672 ± 0.118\nDays in Advance: 60 AdaBoost (×10) 0.646 ± 0.021 0.596 ± 0.054 0.531 ± 0.095 0.762 ± 0.057 AdaBoost (×50) 0.639 ± 0.027 0.569 ± 0.083 0.491 ± 0.111 0.788 ± 0.060 CRDA (λ = 1.0) 0.654 ± 0.016 0.690 ± 0.016 0.774 ± 0.067 0.535 ± 0.088 CRDA (λ = 10.0) 0.653 ± 0.019 0.706 ± 0.010 0.833 ± 0.053 0.474 ± 0.083 CRDA (λ = 100.0) 0.643 ± 0.024 0.701 ± 0.028 0.844 ± 0.098 0.443 ± 0.124 LDA 0.556 ± 0.028 0.550 ± 0.042 0.547 ± 0.072 0.565 ± 0.065 Logistic Regression 0.631 ± 0.031 0.535 ± 0.108 0.447 ± 0.132 0.814 ± 0.073 E2D2 (λ = 1.0) 0.655 ± 0.012 0.675 ± 0.023 0.723 ± 0.070 0.587 ± 0.074 E2D2 (λ = 10.0) 0.661 ± 0.016 0.708 ± 0.009 0.823 ± 0.051 0.499 ± 0.077 E2D2 (λ = 100.0) 0.649 ± 0.021 0.705 ± 0.020 0.844 ± 0.082 0.454 ± 0.110 SVM 0.627 ± 0.019 0.625 ± 0.027 0.625 ± 0.053 0.629 ± 0.056 DIAG 0.565 ± 0.011 0.514 ± 0.046 0.468 ± 0.076 0.662 ± 0.072 Shrinkage (β = 0.25) 0.568 ± 0.012 0.530 ± 0.040 0.492 ± 0.069 0.644 ± 0.063 Shrinkage (β = 0.5) 0.567 ± 0.013 0.528 ± 0.038 0.489 ± 0.067 0.646 ± 0.059 Shrinkage (β = 0.75) 0.561 ± 0.025 0.477 ± 0.164 0.444 ± 0.163 0.677 ± 0.120\nDays in Advance: 90 AdaBoost (×10) 0.627 ± 0.034 0.572 ± 0.063 0.507 ± 0.091 0.747 ± 0.054 AdaBoost (×50) 0.632 ± 0.035 0.575 ± 0.054 0.504 ± 0.077 0.759 ± 0.058 CRDA (λ = 1.0) 0.641 ± 0.018 0.663 ± 0.041 0.716 ± 0.106 0.566 ± 0.091 CRDA (λ = 10.0) 0.651 ± 0.018 0.693 ± 0.034 0.797 ± 0.093 0.505 ± 0.096 CRDA (λ = 100.0) 0.634 ± 0.040 0.675 ± 0.101 0.808 ± 0.188 0.459 ± 0.173 LDA 0.546 ± 0.025 0.532 ± 0.038 0.518 ± 0.058 0.574 ± 0.046 Logistic Regression 0.597 ± 0.058 0.423 ± 0.217 0.351 ± 0.207 0.843 ± 0.096 E2D2 (λ = 1.0) 0.642 ± 0.022 0.663 ± 0.035 0.710 ± 0.078 0.574 ± 0.060 E2D2 (λ = 10.0) 0.658 ± 0.016 0.696 ± 0.022 0.787 ± 0.073 0.528 ± 0.084 E2D2 (λ = 100.0) 0.641 ± 0.031 0.683 ± 0.081 0.808 ± 0.164 0.475 ± 0.148 SVM 0.597 ± 0.034 0.600 ± 0.036 0.606 ± 0.047 0.587 ± 0.046 DIAG 0.568 ± 0.023 0.514 ± 0.048 0.464 ± 0.074 0.672 ± 0.066 Shrinkage (β = 0.25) 0.569 ± 0.020 0.530 ± 0.041 0.490 ± 0.065 0.648 ± 0.054 Shrinkage (β = 0.5) 0.565 ± 0.021 0.519 ± 0.041 0.473 ± 0.059 0.657 ± 0.044 Shrinkage (β = 0.75) 0.559 ± 0.019 0.511 ± 0.040 0.465 ± 0.061 0.653 ± 0.050\nAdaBoost (×10) 0.632 ± 0.029 0.541 ± 0.095 0.452 ± 0.117 0.812 ± 0.065 AdaBoost (×50) 0.631 ± 0.032 0.538 ± 0.099 0.447 ± 0.120 0.814 ± 0.062 CRDA (λ = 1.0) 0.674 ± 0.012 0.708 ± 0.019 0.792 ± 0.043 0.556 ± 0.029 CRDA (λ = 10.0) 0.675 ± 0.006 0.722 ± 0.008 0.844 ± 0.022 0.507 ± 0.017 CRDA (λ = 100.0) 0.664 ± 0.010 0.718 ± 0.004 0.858 ± 0.031 0.469 ± 0.048 LDA 0.594 ± 0.016 0.592 ± 0.019 0.591 ± 0.027 0.597 ± 0.018 Logistic Regression 0.593 ± 0.054 0.394 ± 0.200 0.305 ± 0.180 0.881 ± 0.075 E2D2 (λ = 1.0) 0.674 ± 0.018 0.700 ± 0.025 0.765 ± 0.050 0.582 ± 0.026 E2D2 (λ = 10.0) 0.681 ± 0.006 0.724 ± 0.006 0.838 ± 0.018 0.524 ± 0.020 E2D2 (λ = 100.0) 0.668 ± 0.009 0.720 ± 0.006 0.854 ± 0.028 0.481 ± 0.041 SVM 0.636 ± 0.016 0.642 ± 0.024 0.655 ± 0.044 0.618 ± 0.025 DIAG 0.594 ± 0.019 0.562 ± 0.034 0.524 ± 0.050 0.663 ± 0.033 Shrinkage (β = 0.25) 0.600 ± 0.020 0.582 ± 0.031 0.559 ± 0.045 0.641 ± 0.022 Shrinkage (β = 0.5) 0.581 ± 0.044 0.467 ± 0.235 0.449 ± 0.228 0.714 ± 0.144 Shrinkage (β = 0.75) 0.599 ± 0.014 0.582 ± 0.020 0.559 ± 0.029 0.639 ± 0.022\nDays in Advance: 60 AdaBoost (×10) 0.633 ± 0.024 0.537 ± 0.076 0.439 ± 0.110 0.827 ± 0.067 AdaBoost (×50) 0.623 ± 0.024 0.507 ± 0.065 0.396 ± 0.089 0.850 ± 0.052 CRDA (λ = 1.0) 0.676 ± 0.016 0.711 ± 0.015 0.797 ± 0.041 0.555 ± 0.052 CRDA (λ = 10.0) 0.672 ± 0.019 0.719 ± 0.015 0.837 ± 0.025 0.508 ± 0.039 CRDA (λ = 100.0) 0.668 ± 0.017 0.716 ± 0.013 0.838 ± 0.038 0.498 ± 0.054 LDA 0.603 ± 0.024 0.599 ± 0.026 0.595 ± 0.033 0.610 ± 0.032 Logistic Regression 0.613 ± 0.042 0.462 ± 0.164 0.362 ± 0.147 0.863 ± 0.069 E2D2 (λ = 1.0) 0.679 ± 0.011 0.707 ± 0.014 0.776 ± 0.041 0.582 ± 0.043 E2D2 (λ = 10.0) 0.676 ± 0.016 0.720 ± 0.012 0.834 ± 0.026 0.518 ± 0.039 E2D2 (λ = 100.0) 0.671 ± 0.017 0.718 ± 0.012 0.838 ± 0.029 0.504 ± 0.045 SVM 0.644 ± 0.016 0.645 ± 0.020 0.650 ± 0.038 0.637 ± 0.037 DIAG 0.596 ± 0.015 0.562 ± 0.033 0.522 ± 0.058 0.670 ± 0.054 Shrinkage (β = 0.25) 0.600 ± 0.016 0.580 ± 0.024 0.554 ± 0.040 0.645 ± 0.038 Shrinkage (β = 0.5) 0.596 ± 0.035 0.532 ± 0.178 0.513 ± 0.174 0.680 ± 0.113 Shrinkage (β = 0.75) 0.596 ± 0.039 0.532 ± 0.179 0.513 ± 0.175 0.678 ± 0.115\nDays in Advance: 90 AdaBoost (×10) 0.626 ± 0.022 0.519 ± 0.061 0.412 ± 0.093 0.840 ± 0.058 AdaBoost (×50) 0.631 ± 0.017 0.523 ± 0.056 0.413 ± 0.087 0.849 ± 0.053 CRDA (λ = 1.0) 0.674 ± 0.013 0.709 ± 0.020 0.796 ± 0.052 0.552 ± 0.047 CRDA (λ = 10.0) 0.674 ± 0.010 0.721 ± 0.006 0.845 ± 0.021 0.502 ± 0.034 CRDA (λ = 100.0) 0.666 ± 0.015 0.719 ± 0.006 0.856 ± 0.025 0.477 ± 0.052 LDA 0.605 ± 0.017 0.607 ± 0.026 0.612 ± 0.045 0.598 ± 0.028 Logistic Regression 0.611 ± 0.036 0.453 ± 0.130 0.345 ± 0.136 0.876 ± 0.067 E2D2 (λ = 1.0) 0.675 ± 0.013 0.700 ± 0.026 0.764 ± 0.061 0.587 ± 0.045 E2D2 (λ = 10.0) 0.682 ± 0.007 0.725 ± 0.007 0.840 ± 0.025 0.523 ± 0.030 E2D2 (λ = 100.0) 0.669 ± 0.013 0.721 ± 0.006 0.853 ± 0.023 0.486 ± 0.046 SVM 0.632 ± 0.017 0.638 ± 0.023 0.649 ± 0.039 0.616 ± 0.026 DIAG 0.597 ± 0.015 0.574 ± 0.039 0.549 ± 0.072 0.644 ± 0.063 Shrinkage (β = 0.25) 0.593 ± 0.034 0.531 ± 0.179 0.517 ± 0.182 0.668 ± 0.120 Shrinkage (β = 0.5) 0.602 ± 0.015 0.589 ± 0.028 0.575 ± 0.053 0.628 ± 0.043 Shrinkage (β = 0.75) 0.599 ± 0.015 0.586 ± 0.025 0.570 ± 0.045 0.629 ± 0.037\nAdaBoost (×10) 0.615 ± 0.010 0.484 ± 0.033 0.363 ± 0.039 0.867 ± 0.024 AdaBoost (×50) 0.615 ± 0.007 0.482 ± 0.025 0.359 ± 0.032 0.871 ± 0.023 CRDA (λ = 1.0) 0.682 ± 0.008 0.723 ± 0.008 0.829 ± 0.021 0.534 ± 0.019 CRDA (λ = 10.0) 0.671 ± 0.013 0.721 ± 0.008 0.851 ± 0.016 0.490 ± 0.035 CRDA (λ = 100.0) 0.662 ± 0.014 0.718 ± 0.007 0.861 ± 0.020 0.464 ± 0.044 LDA 0.613 ± 0.012 0.611 ± 0.018 0.610 ± 0.038 0.615 ± 0.037 Logistic Regression 0.581 ± 0.045 0.352 ± 0.189 0.255 ± 0.142 0.908 ± 0.053 E2D2 (λ = 1.0) 0.681 ± 0.009 0.712 ± 0.012 0.790 ± 0.028 0.572 ± 0.020 E2D2 (λ = 10.0) 0.681 ± 0.007 0.727 ± 0.006 0.849 ± 0.013 0.512 ± 0.019 E2D2 (λ = 100.0) 0.667 ± 0.013 0.720 ± 0.007 0.857 ± 0.020 0.478 ± 0.041 SVM 0.650 ± 0.012 0.660 ± 0.014 0.680 ± 0.024 0.620 ± 0.023 DIAG 0.619 ± 0.014 0.610 ± 0.031 0.600 ± 0.056 0.637 ± 0.037 Shrinkage (β = 0.25) 0.599 ± 0.051 0.500 ± 0.251 0.503 ± 0.256 0.696 ± 0.156 Shrinkage (β = 0.5) 0.611 ± 0.039 0.562 ± 0.189 0.566 ± 0.195 0.656 ± 0.121 Shrinkage (β = 0.75) 0.615 ± 0.009 0.611 ± 0.024 0.608 ± 0.051 0.623 ± 0.045\nDays in Advance: 60 AdaBoost (×10) 0.625 ± 0.039 0.512 ± 0.131 0.424 ± 0.156 0.826 ± 0.081 AdaBoost (×50) 0.637 ± 0.024 0.554 ± 0.072 0.466 ± 0.113 0.809 ± 0.068 CRDA (λ = 1.0) 0.677 ± 0.017 0.717 ± 0.015 0.818 ± 0.028 0.536 ± 0.032 CRDA (λ = 10.0) 0.671 ± 0.012 0.721 ± 0.008 0.848 ± 0.022 0.494 ± 0.038 CRDA (λ = 100.0) 0.662 ± 0.014 0.718 ± 0.006 0.861 ± 0.031 0.463 ± 0.055 LDA 0.623 ± 0.014 0.621 ± 0.023 0.619 ± 0.040 0.627 ± 0.023 Logistic Regression 0.600 ± 0.054 0.412 ± 0.217 0.331 ± 0.195 0.869 ± 0.090 E2D2 (λ = 1.0) 0.681 ± 0.016 0.711 ± 0.016 0.787 ± 0.033 0.574 ± 0.036 E2D2 (λ = 10.0) 0.678 ± 0.011 0.724 ± 0.009 0.843 ± 0.017 0.513 ± 0.023 E2D2 (λ = 100.0) 0.667 ± 0.014 0.720 ± 0.007 0.856 ± 0.028 0.477 ± 0.050 SVM 0.649 ± 0.017 0.654 ± 0.025 0.665 ± 0.042 0.633 ± 0.024 DIAG 0.615 ± 0.018 0.597 ± 0.032 0.574 ± 0.054 0.656 ± 0.045 Shrinkage (β = 0.25) 0.618 ± 0.018 0.605 ± 0.031 0.587 ± 0.051 0.649 ± 0.039 Shrinkage (β = 0.5) 0.608 ± 0.039 0.548 ± 0.184 0.533 ± 0.181 0.683 ± 0.110 Shrinkage (β = 0.75) 0.618 ± 0.015 0.602 ± 0.027 0.581 ± 0.045 0.655 ± 0.033\nDays in Advance: 90 AdaBoost (×10) 0.630 ± 0.023 0.531 ± 0.075 0.436 ± 0.123 0.824 ± 0.082 AdaBoost (×50) 0.630 ± 0.023 0.534 ± 0.078 0.441 ± 0.126 0.820 ± 0.083 CRDA (λ = 1.0) 0.674 ± 0.012 0.708 ± 0.017 0.794 ± 0.045 0.553 ± 0.039 CRDA (λ = 10.0) 0.671 ± 0.011 0.720 ± 0.007 0.845 ± 0.021 0.498 ± 0.035 CRDA (λ = 100.0) 0.663 ± 0.013 0.718 ± 0.004 0.857 ± 0.025 0.470 ± 0.050 LDA 0.611 ± 0.020 0.610 ± 0.025 0.608 ± 0.039 0.614 ± 0.024 Logistic Regression 0.614 ± 0.045 0.463 ± 0.174 0.374 ± 0.180 0.853 ± 0.098 E2D2 (λ = 1.0) 0.672 ± 0.018 0.693 ± 0.030 0.745 ± 0.065 0.600 ± 0.042 E2D2 (λ = 10.0) 0.678 ± 0.010 0.722 ± 0.009 0.836 ± 0.026 0.521 ± 0.033 E2D2 (λ = 100.0) 0.668 ± 0.010 0.720 ± 0.005 0.851 ± 0.022 0.485 ± 0.039 SVM 0.639 ± 0.015 0.645 ± 0.020 0.657 ± 0.035 0.622 ± 0.026 DIAG 0.610 ± 0.012 0.602 ± 0.022 0.590 ± 0.042 0.631 ± 0.031 Shrinkage (β = 0.25) 0.613 ± 0.011 0.608 ± 0.019 0.601 ± 0.036 0.626 ± 0.027 Shrinkage (β = 0.5) 0.602 ± 0.036 0.547 ± 0.183 0.540 ± 0.183 0.665 ± 0.114 Shrinkage (β = 0.75) 0.601 ± 0.036 0.545 ± 0.183 0.536 ± 0.182 0.665 ± 0.113\nAdaBoost (×10) 0.618 ± 0.026 0.485 ± 0.082 0.373 ± 0.115 0.863 ± 0.064 AdaBoost (×50) 0.618 ± 0.022 0.491 ± 0.064 0.377 ± 0.092 0.859 ± 0.052 CRDA (λ = 1.0) 0.688 ± 0.006 0.725 ± 0.007 0.824 ± 0.017 0.553 ± 0.016 CRDA (λ = 10.0) 0.680 ± 0.005 0.725 ± 0.005 0.847 ± 0.013 0.513 ± 0.013 CRDA (λ = 100.0) 0.669 ± 0.011 0.721 ± 0.003 0.855 ± 0.026 0.483 ± 0.047 LDA 0.637 ± 0.006 0.644 ± 0.010 0.655 ± 0.021 0.620 ± 0.020 Logistic Regression 0.598 ± 0.046 0.411 ± 0.175 0.313 ± 0.159 0.883 ± 0.070 E2D2 (λ = 1.0) 0.686 ± 0.007 0.717 ± 0.007 0.794 ± 0.017 0.578 ± 0.019 E2D2 (λ = 10.0) 0.684 ± 0.006 0.729 ± 0.005 0.850 ± 0.007 0.519 ± 0.010 E2D2 (λ = 100.0) 0.673 ± 0.009 0.723 ± 0.004 0.852 ± 0.024 0.494 ± 0.038 SVM 0.660 ± 0.012 0.671 ± 0.012 0.693 ± 0.014 0.626 ± 0.015 DIAG 0.623 ± 0.013 0.603 ± 0.024 0.575 ± 0.041 0.671 ± 0.029 Shrinkage (β = 0.25) 0.628 ± 0.013 0.621 ± 0.023 0.610 ± 0.039 0.646 ± 0.024 Shrinkage (β = 0.5) 0.619 ± 0.042 0.565 ± 0.190 0.560 ± 0.190 0.678 ± 0.110 Shrinkage (β = 0.75) 0.633 ± 0.012 0.629 ± 0.019 0.624 ± 0.034 0.642 ± 0.022\nDays in Advance: 60 AdaBoost (×10) 0.605 ± 0.023 0.445 ± 0.085 0.325 ± 0.074 0.885 ± 0.033 AdaBoost (×50) 0.616 ± 0.010 0.479 ± 0.038 0.356 ± 0.048 0.876 ± 0.032 CRDA (λ = 1.0) 0.684 ± 0.006 0.721 ± 0.006 0.818 ± 0.019 0.549 ± 0.023 CRDA (λ = 10.0) 0.674 ± 0.008 0.722 ± 0.006 0.844 ± 0.019 0.505 ± 0.026 CRDA (λ = 100.0) 0.673 ± 0.010 0.721 ± 0.006 0.845 ± 0.021 0.502 ± 0.035 LDA 0.626 ± 0.009 0.622 ± 0.013 0.616 ± 0.028 0.635 ± 0.031 Logistic Regression 0.589 ± 0.038 0.380 ± 0.151 0.270 ± 0.113 0.908 ± 0.038 E2D2 (λ = 1.0) 0.684 ± 0.010 0.710 ± 0.012 0.773 ± 0.027 0.595 ± 0.023 E2D2 (λ = 10.0) 0.682 ± 0.006 0.726 ± 0.007 0.844 ± 0.017 0.520 ± 0.014 E2D2 (λ = 100.0) 0.675 ± 0.008 0.722 ± 0.006 0.843 ± 0.022 0.508 ± 0.031 SVM 0.651 ± 0.006 0.659 ± 0.010 0.675 ± 0.026 0.626 ± 0.028 DIAG 0.627 ± 0.012 0.615 ± 0.023 0.597 ± 0.045 0.657 ± 0.039 Shrinkage (β = 0.25) 0.618 ± 0.041 0.562 ± 0.188 0.553 ± 0.187 0.683 ± 0.111 Shrinkage (β = 0.5) 0.620 ± 0.040 0.565 ± 0.189 0.557 ± 0.187 0.683 ± 0.110 Shrinkage (β = 0.75) 0.616 ± 0.039 0.557 ± 0.186 0.544 ± 0.183 0.688 ± 0.109\nDays in Advance: 90 AdaBoost (×10) 0.626 ± 0.033 0.507 ± 0.107 0.411 ± 0.153 0.840 ± 0.088 AdaBoost (×50) 0.632 ± 0.028 0.533 ± 0.092 0.441 ± 0.135 0.823 ± 0.080 CRDA (λ = 1.0) 0.682 ± 0.008 0.722 ± 0.008 0.825 ± 0.017 0.540 ± 0.020 CRDA (λ = 10.0) 0.664 ± 0.012 0.718 ± 0.006 0.856 ± 0.025 0.472 ± 0.044 CRDA (λ = 100.0) 0.656 ± 0.016 0.715 ± 0.005 0.865 ± 0.029 0.447 ± 0.058 LDA 0.631 ± 0.014 0.630 ± 0.018 0.631 ± 0.034 0.630 ± 0.032 Logistic Regression 0.605 ± 0.060 0.424 ± 0.232 0.353 ± 0.222 0.857 ± 0.107 E2D2 (λ = 1.0) 0.684 ± 0.010 0.714 ± 0.014 0.789 ± 0.031 0.579 ± 0.020 E2D2 (λ = 10.0) 0.676 ± 0.008 0.724 ± 0.004 0.852 ± 0.019 0.500 ± 0.030 E2D2 (λ = 100.0) 0.658 ± 0.015 0.716 ± 0.005 0.863 ± 0.029 0.452 ± 0.057 SVM 0.657 ± 0.009 0.669 ± 0.015 0.693 ± 0.031 0.621 ± 0.024 DIAG 0.625 ± 0.013 0.614 ± 0.029 0.601 ± 0.055 0.648 ± 0.045 Shrinkage (β = 0.25) 0.627 ± 0.014 0.617 ± 0.030 0.604 ± 0.056 0.651 ± 0.043 Shrinkage (β = 0.5) 0.626 ± 0.013 0.616 ± 0.027 0.603 ± 0.051 0.650 ± 0.042 Shrinkage (β = 0.75) 0.626 ± 0.014 0.617 ± 0.023 0.604 ± 0.045 0.649 ± 0.043\nAdaBoost (×10) 0.620 ± 0.037 0.484 ± 0.110 0.380 ± 0.147 0.860 ± 0.076 AdaBoost (×50) 0.625 ± 0.033 0.499 ± 0.097 0.394 ± 0.138 0.856 ± 0.074 CRDA (λ = 1.0) 0.689 ± 0.010 0.726 ± 0.009 0.824 ± 0.021 0.553 ± 0.025 CRDA (λ = 10.0) 0.677 ± 0.012 0.722 ± 0.009 0.840 ± 0.020 0.513 ± 0.029 CRDA (λ = 100.0) 0.666 ± 0.014 0.719 ± 0.007 0.853 ± 0.027 0.479 ± 0.050 LDA 0.644 ± 0.009 0.645 ± 0.012 0.648 ± 0.023 0.640 ± 0.020 Logistic Regression 0.605 ± 0.057 0.424 ± 0.204 0.339 ± 0.200 0.870 ± 0.089 E2D2 (λ = 1.0) 0.690 ± 0.007 0.719 ± 0.007 0.791 ± 0.022 0.589 ± 0.027 E2D2 (λ = 10.0) 0.684 ± 0.009 0.726 ± 0.008 0.837 ± 0.015 0.531 ± 0.012 E2D2 (λ = 100.0) 0.671 ± 0.012 0.721 ± 0.008 0.848 ± 0.026 0.494 ± 0.039 SVM 0.663 ± 0.013 0.673 ± 0.015 0.694 ± 0.024 0.632 ± 0.023 DIAG 0.633 ± 0.011 0.619 ± 0.028 0.599 ± 0.055 0.668 ± 0.046 Shrinkage (β = 0.25) 0.625 ± 0.044 0.569 ± 0.192 0.562 ± 0.193 0.689 ± 0.108 Shrinkage (β = 0.5) 0.626 ± 0.044 0.569 ± 0.192 0.561 ± 0.192 0.691 ± 0.106 Shrinkage (β = 0.75) 0.639 ± 0.011 0.633 ± 0.022 0.624 ± 0.039 0.653 ± 0.025\nDays in Advance: 60 AdaBoost (×10) 0.635 ± 0.026 0.539 ± 0.087 0.449 ± 0.141 0.820 ± 0.091 AdaBoost (×50) 0.634 ± 0.027 0.536 ± 0.089 0.445 ± 0.144 0.823 ± 0.091 CRDA (λ = 1.0) 0.692 ± 0.006 0.729 ± 0.006 0.827 ± 0.014 0.557 ± 0.015 CRDA (λ = 10.0) 0.682 ± 0.008 0.730 ± 0.004 0.860 ± 0.019 0.504 ± 0.031 CRDA (λ = 100.0) 0.674 ± 0.014 0.726 ± 0.005 0.864 ± 0.025 0.483 ± 0.051 LDA 0.642 ± 0.011 0.643 ± 0.015 0.645 ± 0.025 0.638 ± 0.017 Logistic Regression 0.623 ± 0.048 0.489 ± 0.184 0.411 ± 0.195 0.835 ± 0.105 E2D2 (λ = 1.0) 0.691 ± 0.008 0.717 ± 0.009 0.781 ± 0.019 0.601 ± 0.017 E2D2 (λ = 10.0) 0.689 ± 0.004 0.733 ± 0.004 0.854 ± 0.013 0.524 ± 0.017 E2D2 (λ = 100.0) 0.676 ± 0.013 0.727 ± 0.005 0.863 ± 0.024 0.488 ± 0.048 SVM 0.662 ± 0.008 0.668 ± 0.012 0.681 ± 0.023 0.642 ± 0.017 DIAG 0.634 ± 0.012 0.613 ± 0.026 0.582 ± 0.053 0.687 ± 0.049 Shrinkage (β = 0.25) 0.627 ± 0.044 0.565 ± 0.189 0.545 ± 0.185 0.709 ± 0.101 Shrinkage (β = 0.5) 0.642 ± 0.010 0.634 ± 0.015 0.620 ± 0.028 0.663 ± 0.028 Shrinkage (β = 0.75) 0.641 ± 0.010 0.636 ± 0.012 0.627 ± 0.021 0.655 ± 0.022\nDays in Advance: 90 AdaBoost (×10) 0.633 ± 0.027 0.536 ± 0.089 0.447 ± 0.140 0.818 ± 0.086 AdaBoost (×50) 0.631 ± 0.026 0.535 ± 0.087 0.445 ± 0.137 0.818 ± 0.085 CRDA (λ = 1.0) 0.686 ± 0.006 0.721 ± 0.009 0.813 ± 0.029 0.558 ± 0.026 CRDA (λ = 10.0) 0.675 ± 0.007 0.720 ± 0.006 0.838 ± 0.021 0.512 ± 0.028 CRDA (λ = 100.0) 0.671 ± 0.009 0.719 ± 0.004 0.844 ± 0.028 0.497 ± 0.043 LDA 0.648 ± 0.009 0.648 ± 0.018 0.651 ± 0.037 0.644 ± 0.025 Logistic Regression 0.628 ± 0.028 0.520 ± 0.095 0.427 ± 0.146 0.828 ± 0.090 E2D2 (λ = 1.0) 0.687 ± 0.009 0.713 ± 0.014 0.778 ± 0.033 0.597 ± 0.022 E2D2 (λ = 10.0) 0.683 ± 0.006 0.725 ± 0.008 0.839 ± 0.021 0.527 ± 0.018 E2D2 (λ = 100.0) 0.673 ± 0.008 0.720 ± 0.005 0.841 ± 0.024 0.505 ± 0.037 SVM 0.666 ± 0.009 0.672 ± 0.014 0.687 ± 0.030 0.644 ± 0.023 DIAG 0.635 ± 0.015 0.621 ± 0.030 0.601 ± 0.053 0.668 ± 0.032 Shrinkage (β = 0.25) 0.638 ± 0.012 0.631 ± 0.027 0.621 ± 0.051 0.656 ± 0.032 Shrinkage (β = 0.5) 0.642 ± 0.011 0.635 ± 0.026 0.626 ± 0.050 0.657 ± 0.032 Shrinkage (β = 0.75) 0.641 ± 0.010 0.635 ± 0.024 0.628 ± 0.046 0.655 ± 0.030"
    } ],
    "references" : [ {
      "title" : "Supervised patient similarity measure of heterogeneous patient",
      "author" : [ "Jimeng Sun", "Fei Wang", "Jianying Hu", "Shahram Edabollahi" ],
      "venue" : "records. ACM SIGKDD Explorations Newsletter,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2012
    }, {
      "title" : "Personalized predictive modeling and risk factor identification using patient similarity",
      "author" : [ "Jianying Hu Fei Wang Kenney Ng", "Jimeng Sun" ],
      "venue" : "AMIA Summit on Clinical Research Informatics (CRI),",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2015
    }, {
      "title" : "Psf: A unified patient similarity evaluation framework through metric learning with weak supervision",
      "author" : [ "Fei Wang", "Jimeng Sun" ],
      "venue" : "Biomedical and Health Informatics, IEEE Journal of,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2015
    }, {
      "title" : "MSEQ: Early detection of anxiety and depression via temporal orders of diagnoses in electronic health data",
      "author" : [ "Haoyi Xiong" ],
      "venue" : "In Big Data (Workshop),",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2015
    }, {
      "title" : "Mining medical data for predictive and sequential patterns: Pkdd",
      "author" : [ "Susan Jensen", "UK SPSS" ],
      "venue" : "In Proceedings of the 5th European Conference on Principles and Practice of Knowledge Discovery in Databases,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2001
    }, {
      "title" : "Temporal Phenotyping from Longitudinal Electronic Health Records: A Graph Based Framework",
      "author" : [ "Chuanren Liu", "Fei Wang", "Jianying Hu", "Hui Xiong" ],
      "venue" : "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2015
    }, {
      "title" : "Clinical risk prediction by exploring high-order feature correlations",
      "author" : [ "Fei Wang", "Ping Zhang", "Xiang Wang", "Jianying Hu" ],
      "venue" : "In AMIA Annual Symposium Proceedings,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2014
    }, {
      "title" : "Bayes optimality in linear discriminant analysis",
      "author" : [ "Onur C Hamsici", "Aleix M Martinez" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions on,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2008
    }, {
      "title" : "Sparse linear discriminant analysis by thresholding for high dimensional data",
      "author" : [ "Jun Shao", "Yazhen Wang", "Xinwei Deng", "Sijian Wang" ],
      "venue" : "The Annals of statistics,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2011
    }, {
      "title" : "Effective linear discriminant analysis for high dimensional, low sample size data",
      "author" : [ "Zhihua Qiao", "Lan Zhou", "Jianhua Z Huang" ],
      "venue" : "In Proceeding of the World Congress on Engineering,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2008
    }, {
      "title" : "Covarianceregularized regression and classification for high dimensional problems",
      "author" : [ "Daniela M Witten", "Robert Tibshirani" ],
      "venue" : "Journal of the Royal Statistical Society: Series B (Statistical Methodology),",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2009
    }, {
      "title" : "Marble: highthroughput phenotyping from electronic health records via sparse nonnegative tensor factorization",
      "author" : [ "Joyce C Ho", "Joydeep Ghosh", "Jimeng Sun" ],
      "venue" : "In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2014
    }, {
      "title" : "Estimating structured high-dimensional covariance and precision matrices: Optimal rates and adaptive estimation",
      "author" : [ "T Tony Cai", "Zhao Ren", "Harrison H Zhou" ],
      "venue" : "Electronic Journal of Statistics,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2016
    }, {
      "title" : "Random matrix theory in pattern classification: An application to error estimation",
      "author" : [ "Amin Zollanvari", "Edward R Dougherty" ],
      "venue" : "In 2013 Asilomar Conference on Signals, Systems and Computers,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2013
    }, {
      "title" : "Analytic study of performance of error estimators for linear discriminant analysis",
      "author" : [ "Amin Zollanvari", "Ulisses M Braga-Neto", "Edward R Dougherty" ],
      "venue" : "IEEE Transactions on Signal Processing,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2011
    }, {
      "title" : "The use of shrinkage estimators in linear discriminant analysis",
      "author" : [ "Roger Peck", "John Van Ness" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions on,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1982
    }, {
      "title" : "Regularization studies of linear discriminant analysis in small sample size scenarios with application to face recognition",
      "author" : [ "Juwei Lu", "Konstantinos N Plataniotis", "Anastasios N Venetsanopoulos" ],
      "venue" : "Pattern Recognition Letters,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2005
    }, {
      "title" : "Statistics for high-dimensional data: methods, theory and applications",
      "author" : [ "Peter Bühlmann", "Sara Van De Geer" ],
      "venue" : "Springer Science & Business Media,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2011
    }, {
      "title" : "Sparse inverse covariance estimation with the graphical lasso",
      "author" : [ "Jerome Friedman", "Trevor Hastie", "Robert Tibshirani" ],
      "venue" : null,
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2008
    }, {
      "title" : "Confidence intervals for high-dimensional inverse covariance estimation",
      "author" : [ "Jana Jankova", "Sara van de Geer" ],
      "venue" : "Electronic Journal of Statistics,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2015
    }, {
      "title" : "Icd-9 codes and surveillance for clostridium difficile–associated disease",
      "author" : [ "Erik R Dubberke", "Kimberly A Reske", "L Clifford McDonald", "Victoria J Fraser" ],
      "venue" : "Emerging infectious diseases,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2006
    }, {
      "title" : "Estimation of error rates in discriminant analysis",
      "author" : [ "Peter A Lachenbruch", "M Ray Mickey" ],
      "venue" : null,
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 1968
    }, {
      "title" : "Asymptotic generalization bound of fishers linear discriminant analysis",
      "author" : [ "Wei Bian", "Dacheng Tao" ],
      "venue" : "IEEE transactions on pattern analysis and machine intelligence,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2014
    }, {
      "title" : "Large-sample theory: Parametric case",
      "author" : [ "Herman Chernoff" ],
      "venue" : "The Annals of Mathematical Statistics,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 1956
    }, {
      "title" : "Sparse permutation invariant covariance estimation",
      "author" : [ "Adam J Rothman", "Peter J Bickel", "Elizaveta Levina", "Ji Zhu" ],
      "venue" : "Electronic Journal of Statistics,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2008
    }, {
      "title" : "College Health Surveillance Network: Epidemiology and Health Care Utilization of College Students at U.S. 4-Year Universities",
      "author" : [ "James C. Turner", "Adrienne Keller" ],
      "venue" : "Journal of American college health: J of ACH,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2015
    }, {
      "title" : "Life event dimensions of loss, humiliation, entrapment, and danger in the prediction of onsets of major depression and generalized anxiety",
      "author" : [ "Kenneth S Kendler", "John M Hettema", "Frank Butera", "Charles O Gardner", "Carol A Prescott" ],
      "venue" : "Archives of general psychiatry,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2003
    }, {
      "title" : "An optimization criterion for generalized discriminant analysis on undersampled problems",
      "author" : [ "Jieping Ye", "Ravi Janardan", "Cheong Hee Park", "Haesun Park" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions on,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2004
    }, {
      "title" : "Toward personalizing treatment for depression: predicting diagnosis and severity",
      "author" : [ "Sandy H Huang", "Paea LePendu", "Srinivasan V Iyer", "Ming Tai- Seale", "David Carrell", "Nigam H Shah" ],
      "venue" : "Journal of the American Medical Informatics Association,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2014
    }, {
      "title" : "Class-based n-gram models of natural language",
      "author" : [ "Peter F Brown", "Peter V Desouza", "Robert L Mercer", "Vincent J Della Pietra", "Jenifer C Lai" ],
      "venue" : "Computational linguistics,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 1992
    }, {
      "title" : "On the distribution of the largest eigenvalue in principal components analysis",
      "author" : [ "Iain M Johnstone" ],
      "venue" : "Annals of statistics,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2001
    }, {
      "title" : "A direct lda algorithm for highdimensional datawith application to face recognition",
      "author" : [ "Hua Yu", "Jie Yang" ],
      "venue" : "Pattern recognition,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2001
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "The availability of Electronic Health Records (EHR) [1,2] in healthcare settings provides an unique opportunity for early detection of patients’ potential diseases using their historical health records.",
      "startOffset" : 52,
      "endOffset" : 57
    }, {
      "referenceID" : 1,
      "context" : "The availability of Electronic Health Records (EHR) [1,2] in healthcare settings provides an unique opportunity for early detection of patients’ potential diseases using their historical health records.",
      "startOffset" : 52,
      "endOffset" : 57
    }, {
      "referenceID" : 0,
      "context" : "To predict the patients’ future disease using EHR data, existing work proposed to first extract useful features, such as diagnosis-frequencies [1–3], pairwise diagnosis transition [4,5], and graphs of diagnosis sequences [6], to represent each patient’s EHR data using the representation learning techniques.",
      "startOffset" : 143,
      "endOffset" : 148
    }, {
      "referenceID" : 1,
      "context" : "To predict the patients’ future disease using EHR data, existing work proposed to first extract useful features, such as diagnosis-frequencies [1–3], pairwise diagnosis transition [4,5], and graphs of diagnosis sequences [6], to represent each patient’s EHR data using the representation learning techniques.",
      "startOffset" : 143,
      "endOffset" : 148
    }, {
      "referenceID" : 2,
      "context" : "To predict the patients’ future disease using EHR data, existing work proposed to first extract useful features, such as diagnosis-frequencies [1–3], pairwise diagnosis transition [4,5], and graphs of diagnosis sequences [6], to represent each patient’s EHR data using the representation learning techniques.",
      "startOffset" : 143,
      "endOffset" : 148
    }, {
      "referenceID" : 3,
      "context" : "To predict the patients’ future disease using EHR data, existing work proposed to first extract useful features, such as diagnosis-frequencies [1–3], pairwise diagnosis transition [4,5], and graphs of diagnosis sequences [6], to represent each patient’s EHR data using the representation learning techniques.",
      "startOffset" : 180,
      "endOffset" : 185
    }, {
      "referenceID" : 4,
      "context" : "To predict the patients’ future disease using EHR data, existing work proposed to first extract useful features, such as diagnosis-frequencies [1–3], pairwise diagnosis transition [4,5], and graphs of diagnosis sequences [6], to represent each patient’s EHR data using the representation learning techniques.",
      "startOffset" : 180,
      "endOffset" : 185
    }, {
      "referenceID" : 5,
      "context" : "To predict the patients’ future disease using EHR data, existing work proposed to first extract useful features, such as diagnosis-frequencies [1–3], pairwise diagnosis transition [4,5], and graphs of diagnosis sequences [6], to represent each patient’s EHR data using the representation learning techniques.",
      "startOffset" : 221,
      "endOffset" : 224
    }, {
      "referenceID" : 0,
      "context" : "Then, a series of supervised learning techniques have been adopted to train predictive models, such as Support Vector Machine (SVM), Random Forest (RF), Bayesian Network, Linear Discriminant Analysis (LDA) [1–4, 7], using well represented EHR data with the labels of the target disease.",
      "startOffset" : 206,
      "endOffset" : 214
    }, {
      "referenceID" : 1,
      "context" : "Then, a series of supervised learning techniques have been adopted to train predictive models, such as Support Vector Machine (SVM), Random Forest (RF), Bayesian Network, Linear Discriminant Analysis (LDA) [1–4, 7], using well represented EHR data with the labels of the target disease.",
      "startOffset" : 206,
      "endOffset" : 214
    }, {
      "referenceID" : 2,
      "context" : "Then, a series of supervised learning techniques have been adopted to train predictive models, such as Support Vector Machine (SVM), Random Forest (RF), Bayesian Network, Linear Discriminant Analysis (LDA) [1–4, 7], using well represented EHR data with the labels of the target disease.",
      "startOffset" : 206,
      "endOffset" : 214
    }, {
      "referenceID" : 3,
      "context" : "Then, a series of supervised learning techniques have been adopted to train predictive models, such as Support Vector Machine (SVM), Random Forest (RF), Bayesian Network, Linear Discriminant Analysis (LDA) [1–4, 7], using well represented EHR data with the labels of the target disease.",
      "startOffset" : 206,
      "endOffset" : 214
    }, {
      "referenceID" : 6,
      "context" : "Then, a series of supervised learning techniques have been adopted to train predictive models, such as Support Vector Machine (SVM), Random Forest (RF), Bayesian Network, Linear Discriminant Analysis (LDA) [1–4, 7], using well represented EHR data with the labels of the target disease.",
      "startOffset" : 206,
      "endOffset" : 214
    }, {
      "referenceID" : 3,
      "context" : "Among these methods, LDA with diagnosis-frequency vectors is frequently used as one of the common performance benchmarks [4, 7], because of LDA’s provable bayesian optimality [8].",
      "startOffset" : 121,
      "endOffset" : 127
    }, {
      "referenceID" : 6,
      "context" : "Among these methods, LDA with diagnosis-frequency vectors is frequently used as one of the common performance benchmarks [4, 7], because of LDA’s provable bayesian optimality [8].",
      "startOffset" : 121,
      "endOffset" : 127
    }, {
      "referenceID" : 7,
      "context" : "Among these methods, LDA with diagnosis-frequency vectors is frequently used as one of the common performance benchmarks [4, 7], because of LDA’s provable bayesian optimality [8].",
      "startOffset" : 175,
      "endOffset" : 178
    }, {
      "referenceID" : 8,
      "context" : "However, recent studies demonstrate the limitation of LDA on high dimension data [9–12], such as the EHR records [13].",
      "startOffset" : 81,
      "endOffset" : 87
    }, {
      "referenceID" : 9,
      "context" : "However, recent studies demonstrate the limitation of LDA on high dimension data [9–12], such as the EHR records [13].",
      "startOffset" : 81,
      "endOffset" : 87
    }, {
      "referenceID" : 10,
      "context" : "However, recent studies demonstrate the limitation of LDA on high dimension data [9–12], such as the EHR records [13].",
      "startOffset" : 81,
      "endOffset" : 87
    }, {
      "referenceID" : 11,
      "context" : "However, recent studies demonstrate the limitation of LDA on high dimension data [9–12], such as the EHR records [13].",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 12,
      "context" : ", covariance matrix, from a relatively small number of samples [14].",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 13,
      "context" : "According to the expected rate estimation for LDA classifiers [15, 16], LDA performs poorly with high misclassification rate, when the parameter estimation is inaccurate, under high dimension settings.",
      "startOffset" : 62,
      "endOffset" : 70
    }, {
      "referenceID" : 14,
      "context" : "According to the expected rate estimation for LDA classifiers [15, 16], LDA performs poorly with high misclassification rate, when the parameter estimation is inaccurate, under high dimension settings.",
      "startOffset" : 62,
      "endOffset" : 70
    }, {
      "referenceID" : 12,
      "context" : "Even when the sample size is larger than the number of dimensions, the sample (inverse) covariance estimation could be quite different with the “true” (inverse) covariance matrix, as discussed in details in a recent survey [14].",
      "startOffset" : 223,
      "endOffset" : 227
    }, {
      "referenceID" : 8,
      "context" : "To improve LDA learning, several regularization-based methods have been proposed to accurately estimate the (inverse) covariance matrix [10,17,18] or linear coefficients [9, 10] under high dimension and low sample size (HDLSS) settings [19].",
      "startOffset" : 136,
      "endOffset" : 146
    }, {
      "referenceID" : 15,
      "context" : "To improve LDA learning, several regularization-based methods have been proposed to accurately estimate the (inverse) covariance matrix [10,17,18] or linear coefficients [9, 10] under high dimension and low sample size (HDLSS) settings [19].",
      "startOffset" : 136,
      "endOffset" : 146
    }, {
      "referenceID" : 16,
      "context" : "To improve LDA learning, several regularization-based methods have been proposed to accurately estimate the (inverse) covariance matrix [10,17,18] or linear coefficients [9, 10] under high dimension and low sample size (HDLSS) settings [19].",
      "startOffset" : 136,
      "endOffset" : 146
    }, {
      "referenceID" : 8,
      "context" : "To improve LDA learning, several regularization-based methods have been proposed to accurately estimate the (inverse) covariance matrix [10,17,18] or linear coefficients [9, 10] under high dimension and low sample size (HDLSS) settings [19].",
      "startOffset" : 170,
      "endOffset" : 177
    }, {
      "referenceID" : 17,
      "context" : "To improve LDA learning, several regularization-based methods have been proposed to accurately estimate the (inverse) covariance matrix [10,17,18] or linear coefficients [9, 10] under high dimension and low sample size (HDLSS) settings [19].",
      "startOffset" : 236,
      "endOffset" : 240
    }, {
      "referenceID" : 10,
      "context" : "[12] based on their previous contribution to the sparse inverse covariance estimation using Graphical Lasso [20].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 18,
      "context" : "[12] based on their previous contribution to the sparse inverse covariance estimation using Graphical Lasso [20].",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 8,
      "context" : "sparsity of parameter estimation [9, 10, 12, 20], in this work we introduce a novel non-sparse (de-sparsified) inverse covariance matrix estimator [21] for further performance im-",
      "startOffset" : 33,
      "endOffset" : 48
    }, {
      "referenceID" : 10,
      "context" : "sparsity of parameter estimation [9, 10, 12, 20], in this work we introduce a novel non-sparse (de-sparsified) inverse covariance matrix estimator [21] for further performance im-",
      "startOffset" : 33,
      "endOffset" : 48
    }, {
      "referenceID" : 18,
      "context" : "sparsity of parameter estimation [9, 10, 12, 20], in this work we introduce a novel non-sparse (de-sparsified) inverse covariance matrix estimator [21] for further performance im-",
      "startOffset" : 33,
      "endOffset" : 48
    }, {
      "referenceID" : 19,
      "context" : "sparsity of parameter estimation [9, 10, 12, 20], in this work we introduce a novel non-sparse (de-sparsified) inverse covariance matrix estimator [21] for further performance im-",
      "startOffset" : 147,
      "endOffset" : 151
    }, {
      "referenceID" : 8,
      "context" : "Different from the existing sparse LDA models, which regularize the covariance matrix [10] or linear classification coefficients [9] to leverage sparse estimation of parameters, the proposed method uses a non-sparse estimator based on graphical lasso [20] to work with LDA models.",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 18,
      "context" : "Different from the existing sparse LDA models, which regularize the covariance matrix [10] or linear classification coefficients [9] to leverage sparse estimation of parameters, the proposed method uses a non-sparse estimator based on graphical lasso [20] to work with LDA models.",
      "startOffset" : 251,
      "endOffset" : 255
    }, {
      "referenceID" : 0,
      "context" : "sentation - There are many existing approaches to represent EHR data including the use of diagnosis-frequencies [1–3], pairwise diagnosis transition [4, 5], and graph representations of diagnosis sequences [6].",
      "startOffset" : 112,
      "endOffset" : 117
    }, {
      "referenceID" : 1,
      "context" : "sentation - There are many existing approaches to represent EHR data including the use of diagnosis-frequencies [1–3], pairwise diagnosis transition [4, 5], and graph representations of diagnosis sequences [6].",
      "startOffset" : 112,
      "endOffset" : 117
    }, {
      "referenceID" : 2,
      "context" : "sentation - There are many existing approaches to represent EHR data including the use of diagnosis-frequencies [1–3], pairwise diagnosis transition [4, 5], and graph representations of diagnosis sequences [6].",
      "startOffset" : 112,
      "endOffset" : 117
    }, {
      "referenceID" : 3,
      "context" : "sentation - There are many existing approaches to represent EHR data including the use of diagnosis-frequencies [1–3], pairwise diagnosis transition [4, 5], and graph representations of diagnosis sequences [6].",
      "startOffset" : 149,
      "endOffset" : 155
    }, {
      "referenceID" : 4,
      "context" : "sentation - There are many existing approaches to represent EHR data including the use of diagnosis-frequencies [1–3], pairwise diagnosis transition [4, 5], and graph representations of diagnosis sequences [6].",
      "startOffset" : 149,
      "endOffset" : 155
    }, {
      "referenceID" : 5,
      "context" : "sentation - There are many existing approaches to represent EHR data including the use of diagnosis-frequencies [1–3], pairwise diagnosis transition [4, 5], and graph representations of diagnosis sequences [6].",
      "startOffset" : 206,
      "endOffset" : 209
    }, {
      "referenceID" : 20,
      "context" : "Given each patient’s EHR data, as shown in Figure 1, this method first retrieves the diagnosis codes [22] recorded during each visit.",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 12,
      "context" : "More introduction to the covariance matrix estimation under HDLSS settings can be found in survey [14].",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 13,
      "context" : "2 Performance Analysis of LDA In this section, we summarize the series of studies [15, 16, 24, 25] in theoretical error rate estimation for LDA classifiers.",
      "startOffset" : 82,
      "endOffset" : 98
    }, {
      "referenceID" : 14,
      "context" : "2 Performance Analysis of LDA In this section, we summarize the series of studies [15, 16, 24, 25] in theoretical error rate estimation for LDA classifiers.",
      "startOffset" : 82,
      "endOffset" : 98
    }, {
      "referenceID" : 21,
      "context" : "2 Performance Analysis of LDA In this section, we summarize the series of studies [15, 16, 24, 25] in theoretical error rate estimation for LDA classifiers.",
      "startOffset" : 82,
      "endOffset" : 98
    }, {
      "referenceID" : 22,
      "context" : "2 Performance Analysis of LDA In this section, we summarize the series of studies [15, 16, 24, 25] in theoretical error rate estimation for LDA classifiers.",
      "startOffset" : 82,
      "endOffset" : 98
    }, {
      "referenceID" : 13,
      "context" : ", probability of the missed classification) [15, 16] is: (2.",
      "startOffset" : 44,
      "endOffset" : 52
    }, {
      "referenceID" : 14,
      "context" : ", probability of the missed classification) [15, 16] is: (2.",
      "startOffset" : 44,
      "endOffset" : 52
    }, {
      "referenceID" : 10,
      "context" : "We call above LDA derivation as Non-Sparse Covariance-Regularized Discriminant Analysis, with respect to Witen and Tibishirani’s sparse Covariance-Regularized Discriminant Analysis [12], which was based on the Graphical Lasso.",
      "startOffset" : 181,
      "endOffset" : 185
    }, {
      "referenceID" : 19,
      "context" : "Then we introduce a key existing theory [21] on the desparsified graphical lasso estimation T̂ , which proves",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 19,
      "context" : "Stochastic Bound of ||T̂ − Σ−1||F - According to [21], suppose T̂ is the de-sparsified graphical lasso esti-",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 19,
      "context" : "mation and Σ refers to the true population covariance matrix, under specific structural assumption [21]:",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 23,
      "context" : "d = max1≤i≤p|{j : Σ−1 i,j 6= 0}|; further o(·) and Op(·) are little-o notation and big-O in probability (the notations were defined in [26]) respectively.",
      "startOffset" : 135,
      "endOffset" : 139
    }, {
      "referenceID" : 19,
      "context" : ", EHR diagnosis-frequency vectors) should be gaussian or subgaussian; and 2) the population inverse covariance matrix should follow the structural assumption [21] listed in Equation 4.",
      "startOffset" : 158,
      "endOffset" : 162
    }, {
      "referenceID" : 24,
      "context" : "Note that [27] demonstrated that the Frobenius norm rate of convergence for graphical lasso is Op( √ (p+ d) log p/m) under a mild condition, which can also bound the maximal expected error but not as tight as Equation 4.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 25,
      "context" : "1 Experiment Setups Data Description - In this study, to evaluate ED, we used the de-identified EHR data from the College Health Surveillance Network (CHSN), which contains over 1 million patients and 6 million visits from 31 student health centers across the United States [28].",
      "startOffset" : 274,
      "endOffset" : 278
    }, {
      "referenceID" : 26,
      "context" : "Note that in our research, we do not predict these four types of mental disorders separately, as these four disorders are usually correlated and heavily overlapped in clinical practices [29].",
      "startOffset" : 186,
      "endOffset" : 190
    }, {
      "referenceID" : 27,
      "context" : "Specifically, LDA uses the sample covariance estimation, and inverts the covariance matrix using pseudoinverse [30] when the matrix inverse is not available; Shrinkage is based on LDA, using a sparse estimation of sample covariance: Σ∗ = β ∗ Σ̄ + (1−β)∗diag(Σ̄), where diag(Σ̄) refers to the diagonal matrix of the sample estimation Σ̄.",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 10,
      "context" : "[12], which leverage the inverse covariance matrix estimated by Graphical Lasso Estimator addressed in Equation 3.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 0,
      "context" : "• Support Vector Machine (SVM) – Inspired by the previous studies [1, 2, 4], we use a linear binary SVM classifier with fine-tuned parameters.",
      "startOffset" : 66,
      "endOffset" : 75
    }, {
      "referenceID" : 1,
      "context" : "• Support Vector Machine (SVM) – Inspired by the previous studies [1, 2, 4], we use a linear binary SVM classifier with fine-tuned parameters.",
      "startOffset" : 66,
      "endOffset" : 75
    }, {
      "referenceID" : 3,
      "context" : "• Support Vector Machine (SVM) – Inspired by the previous studies [1, 2, 4], we use a linear binary SVM classifier with fine-tuned parameters.",
      "startOffset" : 66,
      "endOffset" : 75
    }, {
      "referenceID" : 28,
      "context" : ") – Inspired by the recent progress in depression prediction [31], we use a Logistic Regression classifier.",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 0,
      "context" : "Specifically, the EHR data of each patient was represented as a vector consisting of the frequency of each diagnosis code that has been discovered in previous visits [1–3].",
      "startOffset" : 166,
      "endOffset" : 171
    }, {
      "referenceID" : 1,
      "context" : "Specifically, the EHR data of each patient was represented as a vector consisting of the frequency of each diagnosis code that has been discovered in previous visits [1–3].",
      "startOffset" : 166,
      "endOffset" : 171
    }, {
      "referenceID" : 2,
      "context" : "Specifically, the EHR data of each patient was represented as a vector consisting of the frequency of each diagnosis code that has been discovered in previous visits [1–3].",
      "startOffset" : 166,
      "endOffset" : 171
    }, {
      "referenceID" : 29,
      "context" : "EHR data can also be represented using N-gram-alike [33] graphs, through counting the pairwise transitions between each pair of diagnosis codes in every visit [4, 5].",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 3,
      "context" : "EHR data can also be represented using N-gram-alike [33] graphs, through counting the pairwise transitions between each pair of diagnosis codes in every visit [4, 5].",
      "startOffset" : 159,
      "endOffset" : 165
    }, {
      "referenceID" : 4,
      "context" : "EHR data can also be represented using N-gram-alike [33] graphs, through counting the pairwise transitions between each pair of diagnosis codes in every visit [4, 5].",
      "startOffset" : 159,
      "endOffset" : 165
    }, {
      "referenceID" : 5,
      "context" : "resent the EHR of a patient using the temporal graphs, in order to preserve the temporal order of diagnoses partially [6].",
      "startOffset" : 118,
      "endOffset" : 121
    }, {
      "referenceID" : 5,
      "context" : "discussed the method of dimensionality reduction for temporal EHR graphs through edge selection [6].",
      "startOffset" : 96,
      "endOffset" : 99
    }, {
      "referenceID" : 3,
      "context" : "Given EHR data represented with vectors and graphs, researchers have proposed to predict the target disease through supervised learning, using downstream classifiers [4] or similarity search [1–3].",
      "startOffset" : 166,
      "endOffset" : 169
    }, {
      "referenceID" : 0,
      "context" : "Given EHR data represented with vectors and graphs, researchers have proposed to predict the target disease through supervised learning, using downstream classifiers [4] or similarity search [1–3].",
      "startOffset" : 191,
      "endOffset" : 196
    }, {
      "referenceID" : 1,
      "context" : "Given EHR data represented with vectors and graphs, researchers have proposed to predict the target disease through supervised learning, using downstream classifiers [4] or similarity search [1–3].",
      "startOffset" : 191,
      "endOffset" : 196
    }, {
      "referenceID" : 2,
      "context" : "Given EHR data represented with vectors and graphs, researchers have proposed to predict the target disease through supervised learning, using downstream classifiers [4] or similarity search [1–3].",
      "startOffset" : 191,
      "endOffset" : 196
    }, {
      "referenceID" : 4,
      "context" : "Given the EHR data with rich structures, sub-sequential pattern matching and sub-graph pattern matching are also leveraged to identify the risk of patients [5, 6].",
      "startOffset" : 156,
      "endOffset" : 162
    }, {
      "referenceID" : 5,
      "context" : "Given the EHR data with rich structures, sub-sequential pattern matching and sub-graph pattern matching are also leveraged to identify the risk of patients [5, 6].",
      "startOffset" : 156,
      "endOffset" : 162
    }, {
      "referenceID" : 30,
      "context" : "2, LDA requires the inverse covariance matrix for calculation, but the sample covariance matrix used in typical LDA is usually singular (non-invertible); and 2) the difference between sample (inverse) covariance matrix and the population (inverse) covariance matrix is extremely large, simulation studies [34] showed that the eignvectors of the two matrices can be nearly orthogonal.",
      "startOffset" : 305,
      "endOffset" : 309
    }, {
      "referenceID" : 27,
      "context" : "[30] proposed to use the Pseudo-inverse, while Direct LDA [35] leveraged the simultaneous diagonalization, to replace the matrix inverse operator.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 31,
      "context" : "[30] proposed to use the Pseudo-inverse, while Direct LDA [35] leveraged the simultaneous diagonalization, to replace the matrix inverse operator.",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 8,
      "context" : "On the other hand, to obtain accurate parameter estimation for LDA under HDLSS settings, several works have proposed to sparsify the inverse covariance matrix [10–12] and linear coefficients [9].",
      "startOffset" : 159,
      "endOffset" : 166
    }, {
      "referenceID" : 9,
      "context" : "On the other hand, to obtain accurate parameter estimation for LDA under HDLSS settings, several works have proposed to sparsify the inverse covariance matrix [10–12] and linear coefficients [9].",
      "startOffset" : 159,
      "endOffset" : 166
    }, {
      "referenceID" : 10,
      "context" : "On the other hand, to obtain accurate parameter estimation for LDA under HDLSS settings, several works have proposed to sparsify the inverse covariance matrix [10–12] and linear coefficients [9].",
      "startOffset" : 159,
      "endOffset" : 166
    }, {
      "referenceID" : 0,
      "context" : "First, compared to other data mining approaches for early diseases detection [1–6], ED is the first work that focuses on improving the performance of LDA model for EHR data classification with diagnosisfrequency vector data representation, by addressing the expected error rates under HDLSS settings.",
      "startOffset" : 77,
      "endOffset" : 82
    }, {
      "referenceID" : 1,
      "context" : "First, compared to other data mining approaches for early diseases detection [1–6], ED is the first work that focuses on improving the performance of LDA model for EHR data classification with diagnosisfrequency vector data representation, by addressing the expected error rates under HDLSS settings.",
      "startOffset" : 77,
      "endOffset" : 82
    }, {
      "referenceID" : 2,
      "context" : "First, compared to other data mining approaches for early diseases detection [1–6], ED is the first work that focuses on improving the performance of LDA model for EHR data classification with diagnosisfrequency vector data representation, by addressing the expected error rates under HDLSS settings.",
      "startOffset" : 77,
      "endOffset" : 82
    }, {
      "referenceID" : 3,
      "context" : "First, compared to other data mining approaches for early diseases detection [1–6], ED is the first work that focuses on improving the performance of LDA model for EHR data classification with diagnosisfrequency vector data representation, by addressing the expected error rates under HDLSS settings.",
      "startOffset" : 77,
      "endOffset" : 82
    }, {
      "referenceID" : 4,
      "context" : "First, compared to other data mining approaches for early diseases detection [1–6], ED is the first work that focuses on improving the performance of LDA model for EHR data classification with diagnosisfrequency vector data representation, by addressing the expected error rates under HDLSS settings.",
      "startOffset" : 77,
      "endOffset" : 82
    }, {
      "referenceID" : 5,
      "context" : "First, compared to other data mining approaches for early diseases detection [1–6], ED is the first work that focuses on improving the performance of LDA model for EHR data classification with diagnosisfrequency vector data representation, by addressing the expected error rates under HDLSS settings.",
      "startOffset" : 77,
      "endOffset" : 82
    }, {
      "referenceID" : 3,
      "context" : "Second, our contribution is complementary with the work in EHR data representation [4–6] and we can further improve ED by incorporating advanced EHR data representation methods.",
      "startOffset" : 83,
      "endOffset" : 88
    }, {
      "referenceID" : 4,
      "context" : "Second, our contribution is complementary with the work in EHR data representation [4–6] and we can further improve ED by incorporating advanced EHR data representation methods.",
      "startOffset" : 83,
      "endOffset" : 88
    }, {
      "referenceID" : 5,
      "context" : "Second, our contribution is complementary with the work in EHR data representation [4–6] and we can further improve ED by incorporating advanced EHR data representation methods.",
      "startOffset" : 83,
      "endOffset" : 88
    }, {
      "referenceID" : 19,
      "context" : "Third, when compared to existing LDA extensions, ED adopts a novel inverse covariance matrix estimator [21] to lower and bound the expected error rate of the LDA model with theoretical guarantee under HDLSS settings, while [9–12] all focus on regularizing the the parameters of LDA using the “heuristics of sparsity”.",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 8,
      "context" : "Third, when compared to existing LDA extensions, ED adopts a novel inverse covariance matrix estimator [21] to lower and bound the expected error rate of the LDA model with theoretical guarantee under HDLSS settings, while [9–12] all focus on regularizing the the parameters of LDA using the “heuristics of sparsity”.",
      "startOffset" : 223,
      "endOffset" : 229
    }, {
      "referenceID" : 9,
      "context" : "Third, when compared to existing LDA extensions, ED adopts a novel inverse covariance matrix estimator [21] to lower and bound the expected error rate of the LDA model with theoretical guarantee under HDLSS settings, while [9–12] all focus on regularizing the the parameters of LDA using the “heuristics of sparsity”.",
      "startOffset" : 223,
      "endOffset" : 229
    }, {
      "referenceID" : 10,
      "context" : "Third, when compared to existing LDA extensions, ED adopts a novel inverse covariance matrix estimator [21] to lower and bound the expected error rate of the LDA model with theoretical guarantee under HDLSS settings, while [9–12] all focus on regularizing the the parameters of LDA using the “heuristics of sparsity”.",
      "startOffset" : 223,
      "endOffset" : 229
    }, {
      "referenceID" : 19,
      "context" : "To best of our knowledge, this paper is the first study that integrates [21] with LDA for Non-sparse CovarianceRegularized Discriminant Analysis and presents its theoretical properties.",
      "startOffset" : 72,
      "endOffset" : 76
    } ],
    "year" : 2016,
    "abstractText" : "To improve the performance of Linear Discriminant Analysis (LDA) for early detection of diseases using Electronic Health Records (EHR) data, we propose ED – a novel framework for EHR based Early Detection of Diseases on top of Covariance-Regularized LDA models. Specifically, ED employs a non-sparse inverse covariance matrix (or namely precision matrix) estimator derived from graphical lasso and incorporates the estimator into LDA classifiers to improve classification accuracy. Theoretical analysis on ED shows that it can bound the expected error rate of LDA classification, under certain assumptions. Finally, we conducted extensive experiments using a large-scale realworld EHR dataset – CHSN. We compared our solution with other regularized LDA and downstream classifiers. The result shows ED outperforms all baselines and backups our theoretical analysis.",
    "creator" : "LaTeX with hyperref package"
  }
}