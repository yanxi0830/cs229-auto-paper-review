{
  "name" : "1606.07262.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "On the Theoretical Capacity of Evolution Strategies to Statistically Learn the Landscape Hessian",
    "authors" : [ "Ofer M. Shir", "Jonathan Roslund", "Amir Yehudayoff" ],
    "emails" : [ "ofersh@telhai.ac.il", "jroslund@lkb.upmc.fr", "amir.yehudayoff@gmail.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n60 6.\n07 26\n2v 1\n[ cs\n.N E\n] 2\n3 Ju\nKeywords: Theory of evolution strategies, statistical learning, covariance matrix adaptation, landscape Hessian, limit distributions of order statistics, extreme value distributions"
    }, {
      "heading" : "1 Introduction",
      "text" : "ESs [2], popular heuristics that excel in global optimization of continuous search landscapes, utilize a Gaussian-based update (variation) step with an evolving covariance matrix. Since the development of ESs, it has been assumed that this learned covariance matrix, which defines the variation operation, approximates the inverse Hessian of the search landscape. It was supported by the rationale that locating the global optimum by an ES can be accommodated using mutation steps that fit the actual landscape, or in other words, that the optimal covariance distribution can offer mutation steps whose equidensity probability contours match the level sets of the landscape (maximizing the progress rate at the same time) [9].\nIt has been additionally argued that reducing a general problem to an isotropic quadratic problem may be achieved by sampling search-points based upon a covariance matrix that is the inverse Hessian (in equivalence to replacing the Euclidean distance measure with the Mahalanobis metric). Since ESs operate well on such isotropically quadratic problems, a successful ES run suggests that learning the covariance was accomplished. Nevertheless, it has never been formally proven that ESs’ machinery can indeed learn the inverse of the Hessian. Rudolph [9] showed that ESs are capable of facilitating such learning and derived practical bounds on the population size toward the end of a successful learning period. That study paved the way toward accumulation of past search information by ESs by means of covariance matrices or any other forms of statistically learned algebraic structures. Especially, accumulation of selected individuals is practically utilized by derandomized ES variants [8], and it has led to the formulation of a successful family of search heuristics [1].\nThe goal of the current study is to investigate the statistical learning potential of an accumulated set of selected individuals (the so-called winners of each generation) when the ES operates in the vicinity of a landscape optimum. We argue and prove that accumulation of such winning individuals carries the potential to reveal valuable search landscape information. In particular, we consider the statistically-constructed covariance matrix over winning decision vectors and prove that it commutes with the Hessian matrix about the optimum (i.e., the two matrices share the same eigenvectors, and therefore their level sets are positioned along the same axes). This result indicates that in learning a covariance, an ES deduces the sensitive directions for an effective optimization. This carries the potential of a great benefit to an ES that learns a covariance matrix and may deduce the sensitive directions for effective optimization. Furthermore, we provide an analytic approximation of the covariance matrix, which holds for a large population size and when the Hessian is well-behaved (formal details appear below).\nThe remainder of this paper is organized as follows. The problem is formally stated in Section 2, where the assumed model is described in detail. In Section 3 we formulate the covariance matrix, derive the necessary density function, and then prove that the covariance and the Hessian commute. Section 4 provides an analytical covariance approximation for the problem of a (1, λ)-ES. A simulation study encompassing various landscape scenarios for (1, λ) selection is presented in Section 5, constituting a numerical corroboration for the theoretical outcomes in Sections 3 and 4. Finally, the results are discussed in Section 6."
    }, {
      "heading" : "2 Statistical Landscape Learning",
      "text" : "We outline the research question that we target:\nWhat is the relation between the statistically-learned covariance matrix and the landscape Hessian if a single winner is selected in each iteration assuming generated samples that follow an isotropic Gaussian (no adaptation)?\nWe focus on the a posteriori statistical construction of the covariance matrix of the decision variables upon reaching the proximity of the optimum, when subject to ES operation.\nIn what follows, we formulate the problem, assume a model and present our notation."
    }, {
      "heading" : "2.1 Problem Statement and Assumed Model",
      "text" : "Let J : Rn → R denote the objective function subject to minimization. We assume that J is minimized at the location ~x∗, which is assumed for simplicity to be the origin. The objective function may be Taylor-expanded about the optimum. We model the n-dimensional basin of attraction about ~x∗ by means of a quadratic approximation. We assume that this expansion is precise\nJ (~x− ~x∗) = J (~x) = ~xT · H · ~x, (1)\nwith H being the landscape Hessian about the optimum. The canonical non-elitist single-parent ES search process operates in the following manner: The ES generates λ search-points ~x1, . . . , ~xλ in each iteration, based upon Gaussian sampling with respect to the given search-point. We are especially concerned with the canonical variation operator of ES, which adds a normally distributed mutation ~z ∼ N ( ~0, I )\n. That is, ~x1, . . . , ~xλ are independent and\neach is N ( ~0, I ) . Following the evaluation of those λ search points with respect to J (~x), the best\n(minimal) individual is selected and recorded as\n~y = argmin{J(~xi)}. (2)\nFinally, let ω denote the winning objective function value,\nω = J(~y) = min {J1, J2, . . . , Jλ} , (3)\nwhere Ji = J(~xi). We mention the difference between the optimization phase, which aims to arrive at the optimum and is not discussed here, to the statistical learning of the basin – which lies in the focus of this study. The sampling procedure is summarized as Algorithm 1, wherein statCovariance refers to a routine for statistically constructing a covariance matrix from raw observations.\n1 t ← 0 2 S ← ∅ 3 repeat 4 for k ← 1 to λ do 5 ~x\n(t+1) k ← ~x∗ + ~zk, ~zk ∼ N\n( ~0, I )\n6 J (t+1) k ← evaluate\n(\n~x (t+1) k\n)\n7 end 8 mt+1 ← argmin ( { J (t+1) ı }λ\nı=1\n)\n9 S ← S ∪ {\n~x (t+1) mt+1\n}\n10 t ← t+ 1 11 until t ≥ Niter\noutput: Cstat =statCovariance(S) Algorithm 1: Statistical sampling by (1, λ)-selection"
    }, {
      "heading" : "2.2 Probability Functions of the ES Step",
      "text" : "The length of a mutation vector, √ ~zT~z, obeys the so-called χ-distribution with n degrees of freedom. Upon assuming a quadratic basin of attraction, ψ = J(~z) is a random variable which obeys a generalized χ2-distribution. We consider two cases:\n1. The basic, simplified case of an isotropic basin, that is, its Hessian matrix constitutes the identity: H = I. In this case, the distribution of ψ is the standard χ2-distribution, possessing the following cumulative distribution function (CDF ) accounting for the search-space dimensionality n:\nFχ2 (ψ) = 1\n2n/2Γ (n/2)\n∫ ψ\n0 t n 2 −1 exp\n(\n− t 2\n)\ndt (4)\nwith Γ(t) being the Gamma function, defined by:\nΓ(t) =\n∫ ∞\n0 xt−1 exp(−x)dx.\nThe probability density function (PDF ) is given by:\nfχ2 (ψ) = 1\n2n/2Γ (n/2) ψn/2−1 exp\n(\n−ψ 2\n)\n. (5)\n2. The generalized case of a globally minimal quadratic basin, where the Hessian matrix is positive definite with the following eigendecomposition form,\nH = UDU−1 D = diag [∆1, . . . ,∆n] , with {∆i}ni=1 being the eigenvalues. The random variable ψ = J(~z) now obeys a generalized χ2-distribution, whose exact distribution function is described as follows [5]:\nFHχ2(ψ) =\n∫ ∞\n0\n2\nπ sin tψ2 t cos\n\n−tψ + 1 2\nn ∑\nj=1\ntan−1 2∆jt\n\n\n× n ∏\nj=1\n( 1 + ∆2j t 2 )− 1 4 dt,\n(6)\nwith an unknown closed form. At the same time, this CDF is known to follow an approximation [5],\nFT χ2 (ψ) = Υη\nΓ (η)\n∫ ψ\n0 tη−1 exp (−Υt) dt, (7)\nwith Υ and η accounting for matching the first two moments of ~zTH~z (and the subscript T marks the transformed distribution):\nΥ = 1\n2\n∑n i=1 ∆i ∑n i=1∆ 2 i , η = 1 2 ( ∑n i=1∆i) 2 ∑n i=1 ∆ 2 i . (8)\nThe density function of this approximation reads:\nfT χ2 (ψ) = Υη\nΓ (η) ψη−1 exp (−Υψ) . (9)\nThe accuracy of this approximation depends upon the standard deviation of the eigenvalues [5], which is clearly related to the so-called condition number. We assume that standard deviation to be moderate, and we thus adopt this approximation herein. For the isotropic case, it can be easily verified that Eq. 9 reduces to Eq. 5.\nWe conclude this section, by summarizing the relevant notation: The random vector ~z is a normal Gaussian mutation and ψ = J(~z). The random vectors ~x1, . . . , ~xλ are λ independent copies of ~z, and Ji = J(~xi). The winner is ~y, and ω = J(~y). The matrix H is the Hessian about the optimum ~x∗, and C is the covariance matrix of ~y."
    }, {
      "heading" : "3 Covariance Matrix Formulation",
      "text" : "In the current section we formulate the covariance matrix by means of its defining density functions and then prove it commutes with the landscape Hessian.\nBy construction, the origin is set at the parent search-point, which is located at the optimum. Analytically, the covariance elements are thus reduced to the following expectation values:\nCij = ∫ xixjPDF~y (~x) d~x , (10)\nwhere PDF~y (~x) is an n-dimensional density function characterizing the winning decision variables about the optimum. In essence, the current study aims at understanding this expression in Eq. 10. To this end, revealing the nature of PDF~y is necessary for the interpretation of the covariance matrix. Importantly, the selection mechanism of the heuristic is blind to the location of the candidate solutions in the search space, and its sole criterion is the ranked function values. Specifically, in the decision-space perspective, the density function of a winning vector of decision variables ~y is related to the density of the winning function value ω via the following relation:\nPDF~y (~x) = PDFω (J (~x)) · PDF~z (~x)\nPDFψ (J (~x)) (11)\nwith PDF~z denoting the density function for generating an individual, and PDFψ denoting the density function of the objective function values (Eqs. 5 and 9). A brief justification follows. The density functions satisfy the conditional probability relation:\nPDF~y (~x) = PDFω (J (~x)) · PDF~y|ω (~x | J (~x)) . (12)\nNow consider the distribution of [~y;ω] on Rn+1. The density of ~y conditioned on the value of J (~y) is that of a normal Gaussian subject to this conditioning, since we may sample [~y;ω] by the following construction: First sample {J1, . . . , Jλ} according to PDFψ independently. Then sample {~x1, . . . , ~xλ} conditioned on the values of J1, . . . , Jλ independently. Finally, J may be set to Jℓ = ω that is minimal\nand ~y set to the respective ~xℓ. In other words, following selection, a winning value of J is chosen to be ω, and the corresponding ~x becomes the winning vector ~y. Importantly, the winning vector ~y conditioned upon the winning value ω is generated in the same manner as a normally-distributed ~z conditioned upon ψ. As a result, the conditional probability for the generation of ~y conditioned upon ω is the same as that for the creation of ~z conditioned upon ψ, i.e., PDF~y|ω = PDF~z|ψ. This density therefore reads:\nPDF~y|ω (~x | J (~x)) = PDF~z|ψ (~x | J (~x)) = PDF~z (~x)\nPDFψ (J (~x)) . (13)\nTheorem 1. The covariance matrix and the Hessian are commuting matrices when the objective function follows the quadratic approximation.\nProof. Given the density function in Eq. 11, the objective function is assumed to satisfy J (~x) = ~xT · H · ~x, and the covariance matrix reads:\nCij = ∫ xixjPDFω ( ~xT · H · ~x ) · PDF~z (~x) PDFψ (~xT · H · ~x) d~x. (14)\nConsider the orthogonal matrix U , which diagonalizes H into D and possesses a determinant of value 1:\nU−1HU = D ≡ diag [∆1,∆2, . . . ,∆n] ~ϑ = U−1~x d~ϑ = d~x .\nWe target the integral Iij = ( U−1CU ) ij and apply a change of variables into ~ϑ (after changing order of summations):\nIij = 1 √\n(2π)n\n∫ +∞\n−∞\n∫ +∞\n−∞ · · ·\n∫ +∞\n−∞ ϑiϑj exp\n(\n−1 2 ~ϑT ~ϑ\n)\n×\n× PDFω\n( ~ϑT · D · ~ϑ )\nPDFψ\n( ~ϑT · D · ~ϑ )dϑ1dϑ2 · · · dϑn.\n(15)\nIij vanishes for any i 6= j due to symmetry considerations: the overall integrand is an odd function, because all the terms are even functions, except for ϑj, ϑi when they differ. Therefore, the integration over the entire domain yields zero. Hence, I is the diagonalized form of C, with U holding the eigenvectors. C is thus diagonalized by the same eigenvectors as H, and therefore, by definition, they are commuting matrices, as claimed."
    }, {
      "heading" : "4 Analytic Approximation",
      "text" : "In this section we provide an approximation for PDFω (J (~x)) and consequently for PDF~y (~x) in order to explicitly calculate the covariance matrix using Eq. 10.\nA non-elitist multi-child selection is considered here, where in each iteration a single individual is deterministically selected out of λ generated offspring. In particular, consider a random sample from an absolutely continuous population with density PDFψ (ψ) and distribution CDFψ (ψ). In order to formulate the density of those winners, it is convenient to first characterize the distribution function of the winning event amongst λ candidates1:\nCDFω (ψ) = Pr {ω ≤ ψ} = 1− (1− CDFψ (ψ))λ . (17) 1Gupta [7] showed that when the dimension n is even, the distribution of the winners for the χ2 distribution (isotropic\nbasin case) possesses a simple form:\nCDF (n=2m) ω (ψ) = 1− exp\n(\n−λ ψ\n2\n)\n\n\nn 2 −1 ∑\n=0\nψ !\n\n\nλ\n(16)\n.\nThe density function is obtained upon differentiating Eq. 17:\nPDFω (ψ) = λ · (1− CDFψ (ψ))λ−1 · PDFψ (ψ) . (18)\nUpon substituting the explicit forms into CDFψ and PDFψ (using either Eqs. (4,5) for the standard χ 2 or Eqs. (7,9) for the generalized χ2), the desired density function PDFω (J (~x)) is obtained, however not in a closed form.\nGupta [7] derived explicit order statistic results from the Gamma distribution, to which the χ2 distribution belongs, including the distribution function as well as moments of the kth order statistic. Such results could reveal a closed form for PDFω (J (~x)), which seems cumbersome and far too complex to address when targeting Eq. 10. Next, we will seek an approximation for PDFω (J (~x)), which will enable us to realize the relation of Eq. 11 when large values of λ are assumed."
    }, {
      "heading" : "4.1 Limit Distributions of Order Statistics",
      "text" : "We treat the derived winners’ distribution for large sample sizes, i.e., when the population size λ tends to infinity. We denote the CDF in Eq. 17 with a subscript λ, Lλ (ψ) = 1− (1− CDFψ (ψ))λ, and consider the limit when λ tends to infinity:\nlim λ−→∞\nLλ (ψ) = {\n0 if CDFψ (ψ) = 0 1 if CDFψ (ψ) > 0\nAccording to the Fisher-Tippett theorem [6], also known as the extremal types theorem, the vonMises family of distributions for minima (or the minimal generalized extreme value distributions (GEVDmin)) are the only non-degenerate family of distributions satisfying this limit. They are characterized as a unified family of distributions by the following CDF:\nLκ (ψ;κ1, κ2, κ3) = 1− exp { − [ 1 + κ3 ( ψ − κ1 κ2 )]1/κ3 } . (19)\nFurthermore, since the minimum distribution moves toward the origin as λ increases, normalizing constants are needed to avoid degeneracy and to obtain:\nlim λ−→∞ Lλ (a∗λψ + b∗λ) = lim\nλ−→∞ 1− (1− CDFψ (a∗λψ + b∗λ))λ = L (ψ) ∀ψ\n(20)\nThe location parameter, κ1, and the scale parameter, κ2, are obviously interlinked to the aforementioned normalizing constants. The shape parameter, κ3, determines the identity of the characteristic CDF, namely either Weibull, Gumbell, or Frechét. This parameter is evaluated by means of the following limit, whose existence is a necessary and sufficient condition for a continuous distribution function CDFψ (ψ) to belong to the domain of attraction for minima of Lκ (ψ) :\nκ3 = lim ε−→0\n− log2 CDF\n−1 ψ (ε)− CDF−1ψ (2ε)\nCDF −1 ψ (2ε)− CDF−1ψ (4ε)\n, (21)\nwhere CDF−1ψ refers to the inverse CDF (the quantile function of CDFψ (ψ); see Theorem 9.6 in [3] [pp. 204-205]):\n• If κ3 > 0, CDFψ (ψ) belongs to the Weibull minimal domain of attraction,\n• if κ3 = 0, CDFψ (ψ) belongs to the Gumbel minimal domain of attraction, and\n• if κ3 < 0, CDFψ (ψ) belongs to the Frechét minimal domain of attraction.\nNote that Rudolph had already taken a related mathematical approach, which he termed asymptotic theory of extreme order statistics, to characterize convergence properties of ESs on a class of convex objective functions [10]. Also, GEVD is introduced to the broad perspective of Stochastic Global Optimization in [11], a book which also constitutes a proper mathematical reference for this topic, yet in a slightly different light.\nLemma 1. For the standard χ2 distribution (i.e., isotropic basin), the limit for Eq. 21 exists and reads κ3 = 2/n.\nProof. The limit needs to be evaluated about ε −→ 0. By inserting an asymptotic expansion of the Gamma function’s integrand, the overall CDF Fχ2 (Eq. 4) may be written and approximated using Stirling’s formula as\nFχ2 (ε) = 1\n2n/2Γ (n/2) ε\nn 2\n∞ ∑\nk=0\n(−1)k ( ε 2\n)k\n(\nn 2 + k\n)\nk!\n≈ 2 n · 2n/2Γ (n/2)ε n 2 ≈ (ε 4 e n\n)n 2\n,\ntaking only the zeroth-order term in the sum into consideration. The quantile (inverse) function has the form:\nF−1 χ2 (ε) ≈ 4n e · ε 2n . (22)\nTargeting the limit in Eq. 21 yields\nF−1 χ2 ((ε) − F−1 χ2 (2ε) F−1 χ2 (2ε) − F−1 χ2 (4ε) ≈\nε 2 n\n(\n2 2 n − 1\n)\nε 2 n\n(\n4 2 n − 2 2n\n) = 1\n2 2 n\n, (23)\nwhich allows to conclude with:\nκ3 = lim ε−→0\n− log2 F−1 χ2 (ε)− F−1 χ2 (2ε)\nF−1 χ2 (2ε)− F−1 χ2\n(4ε) =\n2 n . (24)\nLemma 2. For the generalized χ2 distribution (i.e., non-isotropic basin), the limit for Eq. 21 exists and also reads κ3 = 2/n.\nProof. In the limit ε −→ 0, the generalized distribution (Eq. 7) has a similar CDF, with an approximated quantile function\nF−1 T χ2 (ε) ≈ 4n e · ε 2n , (25)\nand thus Eq. 24 holds as is.\nLemma 3. For the standard and generalized χ2 distributions, the normalizing constants\na∗λ = F −1 χ2\n(\n1\nλ\n)\n, b∗λ = inf { ψ ∣ ∣Fχ2 (ψ) > 0 } = 0\nensure that the limit distribution of Eq. 20 is not degenerate.\nProof. Given the constants\na∗λ = F −1 χ2\n(\n1\nλ\n)\n≈ 4n e\n(\n1\nλ\n)2/n\n, b∗λ = 0,\nthe limit becomes (using Fχ2(ε) = FT χ2(ε) = ( ε 4 e n\n)n 2 )\nlim λ−→∞\n1− { 1− CDFψ [ ψ 4n\ne\n(\n1\nλ\n)2/n ]}λ\n=\nlim λ−→∞\n1− [ 1− r (n) ( ψn/2\nλ\n)]λ\n=\n1− exp [ −r (n) (ψ)n/2 ]\n, (26)\nwith r (n) = ( e 4n )n/2 . Hence, the limit distribution exists and is not degenerate, as claimed.\nCorollary 1. Since the shape parameter κ3 is always positive, the extreme minima of the χ 2-distributions belong to the Weibull domain of attraction. The normalized extreme minima, (ω − b∗λ)/a∗λ, may be represented by a random variable ψ̃, which then reduces Eq. 19 to the following transformed CDF (importantly, the so-called tail index reads 1/κ3 = n/2):\nCDFω (ψ) λ→∞−−−→ W\n( ψ̃ ) = 1− exp ( −ψ̃n/2 )\n(27)\nSee [3] and [4] for an overview on the family of generalized extreme value distributions and on the limit distributions of order statistics. In particular, see table 9.1 in [3][p. 200] for the relationship between the parameters of the GEVD and the Weibull distribution, which allows the reduction of Eq. 19 to Eq. 27. Also, for the exact determination of the tail index value, when assuming certain conditions on the sampling distribution, see Theorem 2.3 in [11].\nCorollary 2. Under the GEVD approximation for treating large populations, λ → ∞, upon normalizing the variable to ψ̃ = (ω − b∗λ) /a∗λ and using the tail index result 1/κ3 = n2 , the CDF and PDF forms for the single winning event read:\nCDF GEVD ω\n( ψ̃ ) = 1− exp ( −ψ̃ n2 )\nPDF GEVD ω\n( ψ̃ ) = n\n2 ψ̃\nn 2 −1 exp\n( −ψ̃ n2 ) (28)"
    }, {
      "heading" : "4.2 Covariance Derivation",
      "text" : "By setting the Weibull form as the characteristic density PDFω, we may rewrite Eq. 10 by utilizing Eq. 11 as follows with the normalized J̃(~x) ≡ (J(~x)− b∗λ) /a∗λ:\nCij = ∫ +∞\n−∞ · · ·\n∫ +∞\n−∞ xixj\nn 2 J̃(~x) n 2 −1 exp [ −J̃(~x)n2 ] ×\n× 1√ (2π)n exp ( −12~xT~x )\nΥη Γ(η)J(~x) η−1 exp (−ΥJ(~x))\ndx1dx2 · · · dxn. (29)\nJ is assumed here to satisfy J (~x) = ~xT · H · ~x, and must be normalized only for the PDFω term by means of a∗λ alone since b ∗ λ = 0:\nCij = ∫ +∞\n−∞ · · ·\n∫ +∞\n−∞ NCxixj\n( ~xTH~x ) n 2 −η ×\n× exp [ Υ~xTH~x− ( ~xTH~x a∗λ ) n 2 − 1 2 ~xT~x ] dx1dx2 · · · dxn. (30)\nwith a normalizing constant NC = nΓ(η)\n2Υη(a∗λ) n 2 −1\n√ (2π)n .\nFor the isotropic case, H = h0I, the integration is straightforward (η = n2 , Υ = 12h0 ) – the attained covariance is proportional to the inverse Hessian, multiplied by an explicit factor:\nC(H=h0I) = Γ( n 2 ) · Γ\n( 1 + 2n ) · c (n) · a∗λ 2πn/2 · H−1 (31)\nwherein\nc (n) =\n{\nπm m! n = 2m 2m+1πm\n1·3·5···(2m+1) n = 2m+ 1 (32)\nFor the general case of any positive-definite Hessian H, the integral in Eq. 30 has an unknown closed form. We were able, nevertheless, to numerically corroborate it for n = 3. We note that this form of the covariance also commutes with the Hessian, in line with the theorem discussed above."
    }, {
      "heading" : "5 Simulation Study",
      "text" : "We implemented our model into a numerical procedure in order to compare the statistical measures to the analytical calculations in practice, adhering to Algorithm 1, Cstat, statistically sampled as described therein. Numerical validation is provided here for two aspects of our theoretical work: Theorem 1 and the analytic approximation for the covariance matrix.\n5.1 Cstat versus H We generated a large set of random positive-definite matrices at various dimensions {n} with a spectrum of condition numbers. For each trial , the numerical procedure generated a random symmetric matrix A, diagonalized it into a set of orthonormal eigenvectors U, drew n random positive numbers in a diagonal matrix D, and set H = UAU−1 . We then applied Algorithm 1 by considering {H} as landscape Hessians.\nThe resultant covariance matrices, { Cstat }\n, were diagonalized and compared to the Hessian matrices and their eigendecomposition – which always matched. In practice, it was evident that the two matrices always commute (applying the commutator operator yields a zero matrix to a practical precision considering the max norm),\n∀ ‖HCstat − Cstat H‖max < 10−1, (33)\nas claimed. However, the covariance matrices were not the inverse forms of the Hessian matrices.\n5.2 Analytic Approximation for Cstat\nHere, we corroborated the analytic approximation for the covariance matrix. To this end, we considered four quadratic basins of attraction at various search-space dimensions: (H-1) n = 3, H1 = [√ 2/2 0.25 0.1; 0.25 1 0; 0.1 0 √ 2 ] (H-2) n = 10, H2 = diag [1.0, 1.5, . . . , 5.5] (H-3) n = 30, H3 = diag [ ~I10, 2 · ~I10, 3 · ~I10 ] (H-4) n = 100, H4 = 2.0 · I100×100"
    }, {
      "heading" : "5.2.1 Validating the Approximated χ2 Density fT χ2",
      "text" : "Figure 1 depicts the approximated density functions of the generalized χ2 distribution, fT χ2 (Eq. 9) for the four Hessian forms (H-1)-(H-4), which evidently constitute sound approximations.\n5.2.2 Validating the Winners’ Densities PDFω and PDF GEVD ω\nFigures 2 and 3 provide validation for the winners’ density, which was exactly described by PDFω in Eq. 18, and was later approximated by PDFGEVDω in Eq. 28 for large λ. Interestingly, PDFω, which is realized here by the approximated generalized χ2 distribution FT χ2 , exhibits decreased accuracy on\nH1, H2 and H3. Evidently, it is highly sensitive to the approximation error of FT χ2 , which is amplified by the exponent λ. At the same time, PDFGEVDω exhibits decreased accuracy on H2, H3 and H4, due to its sensitivity to the population size λ. Indeed, improvements for this approximation were evident when λ was increased (see additional settings on Figure 3)."
    }, {
      "heading" : "5.2.3 Validating the Approximated Integral",
      "text" : "Finally, we compared Cstat to the obtained analytical approximations. For the isotropic case, the result of Eq. 31 has been successfully corroborated for a range of search-space dimensions n. For instance, Cstat for the 100-dimensional case (H-4) was constructed with λ = 5000 and over 500, 000 iterations to obtain a diagonal with an expected value 0.5617 ± 0.0012. Eq. 31 obtained a value of 0.5680.\nFor the general case, we considered the 3-dimensional case (H-1). Cstat was constructed with λ = 20 and over 10, 000 iterations, to be presented side-by-side with the numerical integration of Eq. 30 in Table 1. Additionally, their explicit eigenvectors are provided therein.\nH1 (n = 3) ( λ = 20, Niter = 10 5 )\n0 0.5 1 1.5 2 2.5 0\n0.5\n1\n1.5\n2\n2.5\nRaw Sampling\n0 1 2 3 4 5 6 7 0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nNormalized Sampling\nH3 (n = 30) ( λ = 1000, Niter = 2 · 105 )\nH4 (n = 100) ( λ = 5000, Niter = 5 · 105 )\nH2 (n = 10) ( λ = 1000, Niter = 10 5 )\n0 2 4 6 8 10 12 14 16 0\n0.05\n0.1\n0.15\n0.2\n0.25\nRaw Sampling\n0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 0\n0.2\n0.4\n0.6\n0.8\n1\n1.2\n1.4\n1.6\n1.8\n2\nNormalized Sampling\nH2 (n = 10) [LEFT]: ( λ = 10000, Niter = 10 6 ) [RIGHT]: ( λ = 20000, Niter = 5 · 106 )"
    }, {
      "heading" : "6 Conclusion",
      "text" : "Our analytical work modeled passive ES learning, that is, no step-size adaptation nor covariance matrix adaptation were utilized when constructing a covariance matrix from winning decision vectors. We proved that the statistically-constructed covariance commutes with the landscape Hessian about the optimum when a quadratic basin is assumed. The implication of this result is the enhanced capacity of ESs to identify sensitive optimization directions by extracting this information from the learned covariance matrix. We then derived an analytical approximation for the covariance matrix, based on two presumptions – (i) the generalized χ2 density function was approximated, assuming moderate standard deviation of the eigenvalues, and (ii) the winners’ distribution was shown to follow the Weibull distribution with a calculated tail index when the population size λ is large, adhering to the limit distributions of order statistics. Our results were then numerically validated at multiple levels, where the accuracy of the approximations was discussed."
    } ],
    "references" : [ {
      "title" : "Contemporary Evolution Strategies",
      "author" : [ "T. Bäck", "C. Foussette", "P. Krause" ],
      "venue" : "Natural Computing Series. Springer-Verlag Berlin Heidelberg,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Evolution Strategies a Comprehensive Introduction",
      "author" : [ "H.-G. Beyer", "H.-P. Schwefel" ],
      "venue" : "Natural Computing: An International Journal, 1(1):3–52,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Extreme Value and Related Models with Applications in Engineering and Science",
      "author" : [ "E. Castillo", "A.S. Hadi", "N. Balakrishnan", "J.M. Sarabia" ],
      "venue" : "John Wiley and Sons,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Modelling Extremal Events for Insurance and Finance",
      "author" : [ "P. Embrechts", "C. Klüppelberg", "T. Mikosch" ],
      "venue" : "Springer-Verlag,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "The distribution and properties of a weighted sum of chi squares",
      "author" : [ "A.H. Feiveson", "F.C. Delaney" ],
      "venue" : "Technical report, National Aeronautics and Space Administration,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 1968
    }, {
      "title" : "Limiting forms of the frequency distribution of the largest or smallest member of a sample",
      "author" : [ "R. Fisher", "L. Tippett" ],
      "venue" : "Proc. Cambridge Philos. Soc., 24:180–190,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 1928
    }, {
      "title" : "Order Statistics from the Gamma Distribution",
      "author" : [ "S.S. Gupta" ],
      "venue" : "Technometrics, 2, May",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 1960
    }, {
      "title" : "Completely Derandomized Self-Adaptation in Evolution Strategies",
      "author" : [ "N. Hansen", "A. Ostermeier" ],
      "venue" : "Evolutionary Computation, 9(2):159–195,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "On Correlated Mutations in Evolution Strategies",
      "author" : [ "G. Rudolph" ],
      "venue" : "Parallel Problem Solving from Nature - PPSN II, pages 105–114, Amsterdam,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "Convergence rates of evolutionary algorithms for a class of convex objective functions",
      "author" : [ "G. Rudolph" ],
      "venue" : "Control and Cybernetics, 26(3),",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Stochastic Global Optimization",
      "author" : [ "A. Zhigljavsky", "A. Žilinskas" ],
      "venue" : "Springer Optimization and Its Applications. Springer US,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2007
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "1 Introduction ESs [2], popular heuristics that excel in global optimization of continuous search landscapes, utilize a Gaussian-based update (variation) step with an evolving covariance matrix.",
      "startOffset" : 19,
      "endOffset" : 22
    }, {
      "referenceID" : 8,
      "context" : "It was supported by the rationale that locating the global optimum by an ES can be accommodated using mutation steps that fit the actual landscape, or in other words, that the optimal covariance distribution can offer mutation steps whose equidensity probability contours match the level sets of the landscape (maximizing the progress rate at the same time) [9].",
      "startOffset" : 358,
      "endOffset" : 361
    }, {
      "referenceID" : 8,
      "context" : "Rudolph [9] showed that ESs are capable of facilitating such learning and derived practical bounds on the population size toward the end of a successful learning period.",
      "startOffset" : 8,
      "endOffset" : 11
    }, {
      "referenceID" : 7,
      "context" : "Especially, accumulation of selected individuals is practically utilized by derandomized ES variants [8], and it has led to the formulation of a successful family of search heuristics [1].",
      "startOffset" : 101,
      "endOffset" : 104
    }, {
      "referenceID" : 0,
      "context" : "Especially, accumulation of selected individuals is practically utilized by derandomized ES variants [8], and it has led to the formulation of a successful family of search heuristics [1].",
      "startOffset" : 184,
      "endOffset" : 187
    }, {
      "referenceID" : 4,
      "context" : "The random variable ψ = J(~z) now obeys a generalized χ2-distribution, whose exact distribution function is described as follows [5]: FHχ2(ψ) = ∫ ∞ 0 2 π sin tψ 2 t cos ",
      "startOffset" : 129,
      "endOffset" : 132
    }, {
      "referenceID" : 4,
      "context" : "At the same time, this CDF is known to follow an approximation [5], FT χ2 (ψ) = Υη Γ (η) ∫ ψ 0 t exp (−Υt) dt, (7) with Υ and η accounting for matching the first two moments of ~zTH~z (and the subscript T marks the transformed distribution): Υ = 1 2 ∑n i=1 ∆i ∑n i=1∆ 2 i , η = 1 2 ( ∑n i=1∆i) 2 ∑n i=1 ∆ 2 i .",
      "startOffset" : 63,
      "endOffset" : 66
    }, {
      "referenceID" : 4,
      "context" : "(9) The accuracy of this approximation depends upon the standard deviation of the eigenvalues [5], which is clearly related to the so-called condition number.",
      "startOffset" : 94,
      "endOffset" : 97
    }, {
      "referenceID" : 6,
      "context" : "(17) Gupta [7] showed that when the dimension n is even, the distribution of the winners for the χ distribution (isotropic basin case) possesses a simple form:",
      "startOffset" : 11,
      "endOffset" : 14
    }, {
      "referenceID" : 6,
      "context" : "Gupta [7] derived explicit order statistic results from the Gamma distribution, to which the χ2 distribution belongs, including the distribution function as well as moments of the kth order statistic.",
      "startOffset" : 6,
      "endOffset" : 9
    }, {
      "referenceID" : 5,
      "context" : "0 if CDFψ (ψ) = 0 1 if CDFψ (ψ) > 0 According to the Fisher-Tippett theorem [6], also known as the extremal types theorem, the vonMises family of distributions for minima (or the minimal generalized extreme value distributions (GEVDmin)) are the only non-degenerate family of distributions satisfying this limit.",
      "startOffset" : 76,
      "endOffset" : 79
    }, {
      "referenceID" : 2,
      "context" : "6 in [3] [pp.",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 9,
      "context" : "Note that Rudolph had already taken a related mathematical approach, which he termed asymptotic theory of extreme order statistics, to characterize convergence properties of ESs on a class of convex objective functions [10].",
      "startOffset" : 219,
      "endOffset" : 223
    }, {
      "referenceID" : 10,
      "context" : "Also, GEVD is introduced to the broad perspective of Stochastic Global Optimization in [11], a book which also constitutes a proper mathematical reference for this topic, yet in a slightly different light.",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 2,
      "context" : "(27) See [3] and [4] for an overview on the family of generalized extreme value distributions and on the limit distributions of order statistics.",
      "startOffset" : 9,
      "endOffset" : 12
    }, {
      "referenceID" : 3,
      "context" : "(27) See [3] and [4] for an overview on the family of generalized extreme value distributions and on the limit distributions of order statistics.",
      "startOffset" : 17,
      "endOffset" : 20
    }, {
      "referenceID" : 2,
      "context" : "1 in [3][p.",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 10,
      "context" : "3 in [11].",
      "startOffset" : 5,
      "endOffset" : 9
    } ],
    "year" : 2016,
    "abstractText" : "We study the theoretical capacity to statistically learn local landscape information by Evolution Strategies (ESs). Specifically, we investigate the covariance matrix when constructed by ESs operating with the selection operator alone. We model continuous generation of candidate solutions about quadratic basins of attraction, with deterministic selection of the decision vectors that minimize the objective function values. Our goal is to rigorously show that accumulation of winning individuals carries the potential to reveal valuable information about the search landscape, e.g., as already practically utilized by derandomized ES variants. We first show that the statistically-constructed covariance matrix over such winning decision vectors shares the same eigenvectors with the Hessian matrix about the optimum. We then provide an analytic approximation of this covariance matrix for a non-elitist multi-child (1, λ)-strategy, which holds for a large population size λ. Finally, we also numerically corroborate our results.",
    "creator" : "LaTeX with hyperref package"
  }
}