{
  "name" : "1106.4251.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Learning with the Weighted Trace-norm under Arbitrary Sampling Distributions",
    "authors" : [ "Rina Foygel" ],
    "emails" : [ "rina@uchicago.edu", "rsalakhu@mit.edu", "ohadsh@microsoft.com", "nati@ttic.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "We provide rigorous guarantees on learning with the weighted trace-norm under arbitrary sampling distributions. We show that the standard weighted trace-norm might fail when the sampling distribution is not a product distribution (i.e. when row and column indexes are not selected independently), present a corrected variant for which we establish strong learning guarantees, and demonstrate that it works better in practice. We provide guarantees when weighting by either the true or empirical sampling distribution, and suggest that even if the true distribution is known (or is uniform), weighting by the empirical distribution may be beneficial.\n1 Introduction\nOne of the most common approaches to collaborative filtering and matrix completion is trace-norm regularization [1, 2, 3, 4]. In this approach we attempt to complete an unknown matrix, based on a small subset of revealed entries, by finding a matrix with small trace-norm, which matches those entries as best as possible.\nThis approach has repeatedly shown good performance in practice, and is theoretically well understood for the case where revealed entries are sampled uniformly [5, 6, 7, 8, 9, 10]. Under such uniform sampling, Θ(n log(n)) entries are sufficient for good completion of an n × n matrix—i.e. a nearly constant number of entries per row. However, for arbitrary sampling distributions, the worst-case sample complexity lies between a lower bound of Ω(n4/3) [11] and an upper bound of O(n3/2) [12], i.e. requiring between n1/3 and n1/2 observations per row, and indicating it is not appropriate for matrix completion in this setting.\nMotivated by these issues, Salakhutdinov and Srebro [11] proposed to use a weighted variant of the trace-norm, which takes the distribution of the entries into account, and showed experimentally that this variant indeed leads to superior performance. However, although this recent paper established that the weighted trace-norm corrects a specific situation where the standard trace-norm fails, no general learning guarantees are provided, and it is not clear if indeed the weighted trace-norm always leads to the desired behavior. The only theoretical analysis of the weighted trace-norm that we are aware of is a recent report by Negahban and Wainwright [9] that provides reconstruction guarantees for a low-rank matrix with i.i.d. noise, but only when the sampling distribution is a product distribution, i.e. the rows index and column index of observed entries are selected independently. A product distribution assumption does not seem realistic in many cases—e.g. for the Netflix data, it would indicate that all users have the same (conditional) distribution over which movies they rate.\nar X\niv :1\n10 6.\n42 51\nv1 [\ncs .L\nG ]\n2 1\nIn this paper we rigorously study learning with a weighted trace-norm under an arbitrary sampling distribution, and show that this situation is indeed more complicated, requiring a correction to the weighting. We show that this correction is necessary, and present empirical results on the Netflix and MovieLens dataset indicating that it is also helpful in practice. We also rigorously consider weighting according to either the true sampling distribution (as in [9]) or the empirical frequencies, as is actually done in practice, and present evidence that weighting by the empirical frequencies might be advantageous. Our setting is also more general then that of [9]—we consider an arbitrary loss and do not rely in i.i.d. noise, instead presenting results in an agnostic learning framework.\nSetup and Notation. We consider an arbitrary unknown n ×m target matrix Y , where a subset of entries {Yit,jt}st=1 indexed by S = {(i1, j1), . . . , (is, js)} is revealed to us. Without loss of generality, we assume n ≥ m. Throughout most of the paper, we assume S is drawn i.i.d. according to some sampling distribution p(i, j) (with replacement). Based on this subset on entries, we would like to fill in the missing entries and obtain a prediction matrix X̂S ∈ Rn×m, with low expected loss Lp(X̂S) = Eij∼p [ `((X̂S)ij , Yij) ] , where `(x, y) is some loss function. Note that we measure the loss with respect to\nthe same distribution p(i, j) from which the training set is drawn (this is also the case in [11, 9, 12]). Given some distribution p(i, j) on [n]× [m], the weighted trace-norm of a matrix X ∈ Rn×m is given by [11]\n‖X‖tr(pr,pc) = ∥∥∥diag (pr)1/2 ·X · diag (pc)1/2∥∥∥\ntr ,\nwhere pr ∈ Rn and pc ∈ Rm denote vectors of the row- and column-marginals respectively. Note that the weighted trace-norm only depends on these marginals (but not their joint distribution) and that if pr and pc are uniform, then ‖X‖tr(pr,pc) = 1√ nm ‖X‖tr. The weighted trace-norm does not generally scale with n and m, and in particular, if X has rank r and entries bounded in [−1, 1], then ‖X‖tr(pr,pc) ≤ √ r regardless of which p (i, j) is used. This motivates us to define the class\nWr [p] = {X ∈ Rn×m : ‖X‖tr(pr,pc) ≤ √ r},\nalthough we emphasize that our results do not directly depend on the rank, andWr [p] certainly includes full-rank matrices. We analyze here estimators of the form X̂S = arg min{L̂S(X) : X ∈ Wr [p]} where L̂S(X) = 1 s ∑s t=1 `(Xit,jt , Yit,jt) is the empirical error on the observed entries.\nAlthough we focus mostly on the standard inductive setting, where the samples are drawn i.i.d. and the guarantee is on generalization for future samples drawn by the same distribution, our results can also be stated in a transductive model, where a training set and a test set are created by splitting a fixed subset of entries uniformly at random (as in [12]). The transductive setting is discussed in Section 4.2, and variants of our Theorems in this setting are found there and in Appendix B.\n2 Learning with the Standard Weighting\nIn this Section, we consider learning using the weighted trace-norm as suggested by Salakhutdinov and Srebro [11], i.e. when the weighting is according to the sampling distribution p(i, j). Following the approach of [5] and [10], we base our results on bounding the Rademacher complexity of Wr [p], as a class of functions mapping index pairs to entry values. However, we modify the analysis for the weighted trace-norm with non-uniform sampling.\nFor a class of matrices X and a sample S = {(i1, j1), . . . , (is, js)} of indexes in [n]× [m], the empirical Rademacher complexity of the class (with respect to S) is given by\nR̂S(X ) = Eσ∼{±1}s [\nsup X∈X\n1\ns s∑ t=1 σtXitjt\n] ,\nwhere σ is a vector of signs drawn uniformly at random. Intuitively, R̂S(X ) measures the extent to which the class X can “overfit” data, by finding a matrix X which correlates as strongly as possible to a sample from a matrix of random noise. For a loss `(x, y) that is Lipschitz in x, the Rademacher complexity can be used to uniformly bound the deviations |Lp(X)− L̂S(X)| for all X ∈ X , yielding a learning guarantee on the empirical risk minimizer [13].\n2.1 Guarantees for Special Sampling Distributions\nWe begin by providing guarantees for an arbitrary, possibly unbounded, Lipschitz loss `(x, y), but only under sampling distributions which are either product distributions (i.e. p(i, j) = pr(i)pc(j)) or have uniform marginals (i.e. pr and pc are uniform, but perhaps the rows and columns are not independent). In Section 2.3 below, we will see why this severe restriction on p is needed.\nTheorem 1. For an l-Lipschitz loss `, fix any matrix Y , sample size s, and distribution p, such that p is either a product distribution or has uniform marginals. Let X̂S = arg min { L̂S(X) : X ∈ Wr [p] } . Then, in expectation over the training sample S drawn\ni.i.d. from the distribution p,\nLp(X̂S) ≤ inf X∈Wr[p] Lp(X) + O\n( l · √ rn log(n)\ns\n) . (1)\nHere and elsewhere we state learning guarantees in expectation for simplicity, but all guarantees can also be obtained with high probability.\nProof. We will show how to bound the expected Rademacher complexity ES [ R̂S(Wr [p]) ] , from which\nthe desired results follows using standard arguments [13]. Following [10] by including the weights, using the duality between spectral norm ‖·‖sp and trace-norm, we compute:\nES [ R̂S(Wr [p]) ] = √ r\ns ES,σ ∥∥∥∥∥ s∑ t=1 σt eit,jt√ pr (it) pc (jt) ∥∥∥∥∥ sp  = √r s ES,σ ∥∥∥∥∥ s∑ t=1 Qt ∥∥∥∥∥ sp  , where ei,j = eie T j and Qt = σt\neit,jt√ pr(it)pc(jt) ∈ Rn×m. Since the Qt’s are i.i.d. zero-mean matrices, Theorem 6.1 of [14], combined with Remarks 6.4 and 6.5 there, establishes that\nES,σ ∥∥∥∥∥ s∑ t=1 Qt ∥∥∥∥∥ sp  = O(σ√log(n) +R log(n)) , where ‖Qt‖sp ≤ R (almost surely) and σ2 = max\n{∥∥∑E [QTt Qt]∥∥sp ,∥∥∑E [QtQTt ]∥∥sp}. Calculating these (see Appendix A ), we get R ≤ √ nm\nmini,j{npr(i)·mpc(j)} , and\nσ ≤ √√√√√smax maxi ∑\nj\np (i, j)\npr (i) pc (j) ,max j ∑ i p (i, j) pr (i) pc (j)  ≤ √\nsn\nmini,j{npr (i) ·mpc (j)} .\nIf p has uniform row- and column-marginals, then for all i, j, npr (i) = mpc (j) = 1. This yields\nES [ R̂(Wr [p]) ] ≤ O\n(√ rn log(n)\ns\n) ,\nas desired. (Here we assume s > n log(n), since otherwise we need only establish that excess error is O(l √ r), which holds trivially for any matrix in Wr [p].)\nIf p does not have uniform marginals, but instead is a product distribution, then the quantity R defined above is potentially unbounded, so we cannot apply the same simple argument. However, we can consider the “p-truncated” class of matrices\nZ = { Z(X) = ( XijI { p (i, j) ≥ log(n)\ns √ nm }) ij : X ∈ Wr [p] } .\nBy a similar calculation of the expected spectral norms, we can now bound ES [ R̂S(Z) ] ≤ O (√ rn log(n)\ns\n) .\nApplying [13], this bounds ( Lp(Z(X̂S))− L̂S(Z(X̂S)) ) (in expectation). Since Z(X̂S)ij 6= (X̂S)ij only on\nthe extremely low-probability entries, we can also bound ( Lp(X̂S)− Lp(Z(X̂S)) ) and ( L̂S(Z(X̂S))− L̂S(X̂S) ) .\nCombining these steps, we can bound ( Lp(X̂S)− L̂S(X̂S) ) . We similarly bound L̂S(X\n∗)−Lp(X∗), where X∗ = arg minX∈Wr[p] Lp(X). Since L̂S(X̂S) ≤ L̂S(X∗), this yields the desired bound on excess error. The details are given in Appendix A.\nExamining the proof of Theorem 1, we see that we can generalize the result by including distributions p with row- and column-marginals that are lower-bounded. More precisely, if p satisfies pr (i) ≥ 1Cn , pc (j) ≥ 1Cm for all i, j, then the bound (1) holds, up to a factor of C. Note that this result does not require an upper bound on the row- and column-marginals, only a lower bound, i.e. it only requires that no marginals are too low. This is important to note since the examples where the unweighted trace-norm fails under a non-uniform distribution are situations where some marginals are very high (but none are too low) [11]. This suggests that the low-probability marginals could perhaps be “smoothed” to satisfy a lower bound, without removing the advantages of the weighted trace-norm. We will exploit this in Section 3 to give a guarantee that holds more generally for arbitrary p, when smoothing is applied.\n2.2 Guarantees for bounded loss\nIn Theorem 1, we showed a strong bound on excess error, but only for a restricted class of distributions p. We now show that if the loss function ` is bounded, then we can give a non-trivial, but weaker, learning guarantee that holds uniformly over all distributions p. Since we are in any case discussing Lipschitz loss functions, requiring that the loss function be bounded essentially amounts to requiring that the entries of the matrices involved be bounded. That is, we can view this as a guarantee on learning matrices with bounded entries. In Section 2.3 below, we will show that this boundedness assumption is unavoidable if we want to give a guarantee that holds for arbitrary p.\nTheorem 2. For an l-Lipschitz loss ` bounded by b, fix any matrix Y , sample size s, and any distribution p. Let X̂S = arg min { L̂S(X) : X ∈ Wr [p] } for r ≥ 1. Then, in expectation over the training sample S\ndrawn i.i.d. from the distribution p,\nLp(X̂S) ≤ inf X∈Wr[p] Lp(X) + O\n( (l + b) · 3 √ rn log(n)\ns\n) . (2)\nThe proof is provided in Appendix A, and is again based on analyzing the expected Rademacher\ncomplexity, ES [ R̂(` ◦Wr [p]) ] ≤ O ( (l + b) · 3 √ rn log(n)\ns\n) .\n2.3 Problems with the standard weighting\nIn the previous Sections, we showed that for distributions p that are either product distributions or have uniform marginals, we can prove a square-root bound on excess error, as shown in (1). For arbitrary p, the only learning guarantee we obtain is a cube-root bound given in (2), for the special case of bounded loss. We would like to know whether the square-root bound might hold uniformly over all distributions p, and if not, whether the cube-root bound is the strongest result that we can give in this case for the bounded-loss setting, and whether any bound will hold uniformly over all p in the unbounded-loss setting.\nThe examples below demonstrate that we cannot improve the results of Theorems 1 and 2 (up to log factors), by constructing degenerate examples using non-product distributions p with non-uniform marginals. Specifically, in Example 1, we show that in the special case of bounded loss, the cube-root bound in 2 is the best possible bound (up to the log factor) that will hold for all p, by giving a construction for arbitrary n = m and arbitrary s ≤ nm, such that with 1-bounded loss, excess error is Ω ( 3 √\nn s\n) . In\nExample 2, we show that with unbounded (Lipschitz) loss, we cannot bound excess error better than a constant bound, by giving a construction for arbitrary n = m and arbitrary s ≤ nm in the unboundedloss regime, where excess error is Ω(1). For both examples we fix r = 1. We note that both examples\ncan be modified to fit the transductive setting, demonstrating that smoothing is necessary also in the transductive setting as well.\nExample 1. Let `(x, y) = min{1, |x−y|} ≤ 1, let a = (2s/n)2/3 < n, and let matrix Y and block-wise constant distribution p be given by\nY =\n( A 0\na×n2 0\n(n−a)×n2 0 (n−a)×n2\n) , (p (i, j)) =  12s · 1a×n2 0a×n2 0\n(n−a)×n2 1−an4s (n−a) n2 · 1 (n−a)×n2  , where A ∈ {±1}a×\nn 2 is any sign matrix. Clearly, ‖Y ‖tr(pr,pc) ≤ 1, and so infX∈Wr[p] Lp(X) = 0. Now\nsuppose we draw a sample S of size s from the matrix Y , according to the distribution p. We will show an ERM Ŷ such that in expectation over S, Lp(Ŷ ) ≥ 18 3 √ n s .\nConsider Y S where Y Sij = YijI {ij ∈ S}, and note that ∥∥Y S∥∥ tr(pr,pc) ≤ 1. Since L̂S(Y S) = 0, it clearly an ERM. We also have Lp(Y S) = N2s , where N is the number of ±1’s in Y which are not observed in the\nsample. Since E [N ] ≥ an4 , we see that E [ Lp(Y S) ] ≥ 12s · an 4 ≥ 1 8 3 √ n s .\nExample 2. Let `(x, y) = |x − y|. Let Y = 0n×n; trivially, Y ∈ Wr [p]. Let p (1, 1) = 1s , and p (i, 1) = p (1, j) = 0 for all i, j > 1, yielding pr (1) = pc (1) = 1s . (The other entries of p may be defined arbitrarily.) We will show an ERM Ŷ such that, in expectation over S, Lp(Ŷ ) ≥ 0.25. Let A be the matrix with X11 = s and zeros elsewhere, and note that ‖A‖tr(pr,pc) = 1. With probability ≥ 0.25, entry (1, 1) will not appear in S, in which case Ŷ = A is an ERM, with Lp(Ŷ ) = 1.\nThe following table summarizes the learning guarantees that can be established for the (standard) weighted trace-norm. As we saw, these guarantees are tight up to log-factors.\n1-Lipschitz, 1-bounded loss 1-Lipschitz, unbounded loss p = product √\nrn log(n) s\n√ rn log(n)\ns pr, pc = uniform √\nrn log(n) s\n√ rn log(n)\ns p arbitrary 3 √\nrn log(n) s 1\n3 Smoothing the weighted trace norm\nConsidering Theorem 1 and the degenerate examples in Section 2.3, it seems that in order to be able to generalize for non-product distributions, we need to enforce some sort of uniformity on the weights. The Rademacher complexity computations in the proof of Theorem 1 show that the problem lies not with large entries in the vectors pr and pc (i.e. if pr and/or pc are “spiky”), but with the small entries in these vectors. This suggests the possibility of “smoothing” any overly low row- or column-marginals, in order to improve learning guarantees.\nIn Section 3.1, we present such a smoothing, and provide guarantees for learning with a smoothed weighted trace-norm. The result suggests that there is no strong negative consequence to smoothing, but there might be a large advantage, if confronted with situations as in Examples 1 and 2. In Section 3.2 we check the smoothing correction to the weighted trace-norm on real data, and observe that indeed it can also be beneficial in practice.\n3.1 Learning guarantee for arbitrary distributions\nFix a distribution p and a constant α ∈ (0, 1), and let p̃ denote the smoothed marginals:\np̃r (i) = α · pr (i) + (1− α) · 1n , p̃ c (j) = α · pc (j) + (1− α) · 1m . (3)\nIn the theoretical results below, we use α = 12 , but up to a constant factor, the same results hold for any fixed choice of α ∈ (0, 1).\nTheorem 3. For an l-Lipschitz loss `, fix any matrix Y , sample size s, and any distribution p. Let X̂S = arg min { L̂S(X) : X ∈ Wr [p̃] } . Then, in expectation over the training sample S drawn i.i.d.\nfrom the distribution p,\nLp(X̂S) ≤ inf X∈Wr[p̃] Lp(X) + O\n( l · √ rn log(n)\ns\n) . (4)\nProof. We bound ES∼p [ R̂S(Wr [p̃]) ] ≤ O (√ rn log(n)\ns\n) , and then apply [13]. The proof of this Rademacher\nbound is essentially identical to the proof in Theorem 1, with the modified definition of Qt = σt eit,jt√ p̃r(i)p̃c(j) .\nThen ‖Qt‖sp ≤ maxij 1√ p̃r(i)p̃c(j) ≤ 2 √ nm . = R, and E [∥∥∑s t=1QtQ T t ∥∥ sp ] = s · maxi ∑ j p(i,j) p̃r(i)p̃c(j) ≤\ns ·maxi ∑ j\np(i,j) 1 2p r(i)· 12m ≤ 4sm. Similarly, E [∥∥∑s\nt=1Q T t Qt ∥∥ sp ] ≤ 4sn. Setting σ .= √ 4sn and applying [14], we obtain the result.\nMoving from Theorem 1 to Theorem 3, we are competing with a different class of matrices:\ninf X∈Wr[p] Lp(X) inf X∈Wr[p̃] Lp(X) .\nIn most applications we can think of, this change is not significant. For example, we consider the lowrank matrix reconstruction problem, where the trace-norm bound is used as a surrogate for rank. In order for the (squared) weighted trace-norm to be a lower bound on the rank, we would need to assume∥∥∥diag (pr)1/2Xdiag (pc)1/2∥∥∥2\nF ≤ 1 [10]. If we also assume that ∥∥(X∗)(i)∥∥22 ≤ m and ∥∥(X∗)(j)∥∥22 ≤ n for all rows i and columns j — i.e. the row and column magnitudes are not “spiky” — then X∗ ∈ Wr [p̃]. Note that this condition is much weaker than placing a spikiness condition on X∗ itself, e.g. requiring |X∗|∞ ≤ 1.\n3.2 Results on Netflix and MovieLens Datasets\nWe evaluated different models on two publicly-available collaborative filtering datasets: Netflix [15] and MovieLens [16]. The Netflix dataset consists of 100,480,507 ratings from 480,189 users on 17,770 movies. Netflix also provides qualification set containing 1,408,395 ratings, but due to the sampling scheme, ratings from users with few ratings are overrepresented relative to the training set. To avoid dealing with different training and test distributions, we also created our own validation and test sets, each containing 100,000 ratings set aside from the training set. The MovieLens dataset contains 10,000,054 ratings from 71,567 users and 10,681 movies. We again set aside test and validation sets of 100,000 ratings. Ratings were normalized to be zero-mean.\nWhen dealing with large datasets the most practical way to fit trace-norm regularized models is via stochastic gradient descent [17, 2, 11]. For computational reasons, however, we consider rank-truncated trace-norm minimization, by optimizing within the restricted class {X : X ∈ Wr [p] , rank (X) ≤ k} for k = 30 and k = 100, and for various values of smoothing parameters α (as in (3)). For each value of α and k, the regularization parameter was chosen by cross-validation.\nThe following table shows root mean squared error (RMSE) for the experiments. For both k=30 and k=100 the weighted trace-norm with smoothing significantly outperforms the weighted trace-norm without smoothing (α = 1), even on the differently-sampled Netflix qualification set. We also note that the proposed weighted trace-norm with smoothing outperforms max-norm regularization [18], and compares favorably with the “geometric” smoothing used by [11] as a heuristic, without theoretical or conceptual justification. A moderate value of α = 0.9 seems consistently good.\nNetflix MovieLens α k Test Qual k Test Qual k Test k Test 1 30 0.7604 0.9107 100 0.7404 0.9078 30 0.7852 100 0.7821 0.9 30 0.7589 0.9096 100 0.7391 0.9068 30 0.7831 100 0.7798 0.5 30 0.7601 0.9173 100 0.7419 0.9161 30 0.7836 100 0.7815 0.3 30 0.7712 0.9198 100 0.7528 0.9207 30 0.7864 100 0.7871 0 30 0.7887 0.9249 100 0.7659 0.9236 30 0.7997 100 0.7987\n4 The empirically-weighted trace norm\nIn practice, the sampling distribution p is not known exactly — it can only be estimated via the locations of the entries which are observed in the sample. Defining the empirical marginals\np̂r (i) = #{t : it = i}\ns , p̂c (j) = #{t : jt = j} s ,\nwe would like to give a learning guarantee when X̂S is estimated via regularization on the p̂-weighted trace-norm, rather than the p-weighted trace-norm.\nIn Section 4.1, we give bounds on excess error when learning with smoothed empirical marginals, which show that there is no theoretical disadvantage as compared to learning with the smoothed true marginals. In fact, we provide evidence that suggests there might even be an advantage to using the empirical marginals. To this end, in Section 4.2, we introduce the transductive learning setting, and give a result based on the empirical marginals which implies a sample complexity bound that is better by a factor of log 1/2(n). In Section 4.3, we show that in low-rank matrix reconstruction simulations, using empirical marginals is indeed yields better reconstructions.\n4.1 Guarantee for the standard (inductive) setting\nWe first show that when learning with the smoothed empirical marginals, defined as p̌r (i) = 12 ( p̂r (i) + 1n ) , p̌c (j) = 12 ( p̂c (j) + 1m ) ,\nwe can obtain the same guarantee as for learning with the smoothed (true) marginals, given by p̃.\nTheorem 4. For an l-Lipschitz loss `, fix any matrix Y , sample size s, and any distribution p. Let X̂S = arg min { L̂S(X) : X ∈ Wr [p̌] } . Then, in expectation over the training sample S drawn i.i.d.\nfrom the distribution p,\nLp(X̂S) ≤ inf X∈Wr[p̃] Lp(X) + O\n( l · √ rmax{n,m} log(n+m)\ns\n) . (5)\nNote that although we regularize using the (smoothed) empirically-weighted trace-norm, we still compare ourselves to the best possible matrix in the class defined by the (smoothed) true marginals.\nThe proof of the Theorem (given in Appendix A) uses Theorem 3 and involves showing that with a sample of size s = Ω(n log(n)), which is required for all Theorems so far to be meaningful, the true and empirical marginals are the same up to a constant factor. For this to be the case, such a sample size is even necessary. In fact, the log(n) factor in our analysis (e.g. in the proof of Theorem 1) arises from the bound on the expected spectral norm of a matrix, which, for a diagonal matrix, is just a bound on the deviation of empirical frequencies. Might it be possible, then, to avoid this logarithmic factor by using the empirical marginals? Although we could not establish such a result in the inductive setting, we now turn to the transductive setting, where we could indeed obtain a better guarantee.\n4.2 Guarantee for the transductive setting\nIn the transductive model, we fix a set S ⊂ [n]× [m] of size 2s, and then randomly split S into a training set S and a test set T of equal size s. The goal is to obtain a good estimator for the entries in T based on the values of the entries in S, as well as the locations (indexes) of all elements on S. We then use the (smoothed or unsmoothed) empirical marginals of S, for the weighted trace-norm.\nWe now show that, for bounded loss, there may be a benefit to weighting with the smoothed empirical marginals — the sample size requirement can be lowered to s = O ( rn log 1/2(n) ) .\nTheorem 5. For an l-Lipschitz loss ` bounded by b, fix any matrix Y and sample size s. Let S ⊂ [n]×[m] be a fixed subset of size 2s, split uniformly at random into training and test sets S and T , each of size s. Let p denote the smoothed empirical marginals of S. Let X̂S = arg min { L̂S(X) : X ∈ Wr [p] } . Then\nin expectation over the splitting of S into S and T ,\nL̂T (X̂S) ≤ inf X∈Wr[p] L̂T (X) + O\nl · √ rn log 1/2(n)\ns + b√ s\n . (6)\nThis result (proved in Appendix B) is stated in the transductive setting, with a somewhat different sampling procedure and evaluation criteria, but we believe the main difference is in the use of the empirical weights. Although it is usually straightforward to convert a transductive guarantee to an inductive one, the situation here is more complicated, since the hypothesis class depends on the weighting, and hence on the sample S. Nevertheless, we believe such a conversion might be possible, establishing a similar guarantee for learning with the (smoothed) empirically weighted trace-norm also in the inductive setting. Furthermore, by using the fact that a sample of size s = Θ(n log(n)) is sufficient for the empirical marginals to be close to the true marginals, it might be possible to obtain a learning guarantee for the true (non-empirical) weighting with a sample of size s = O(n(r log 1/2(n) + log(n))).\nTheorem 5 above can be viewed as a transductive analog to Theorem 3 (where weights are based on the combined sample S). In Appendix B we state and prove transductive analogs also to Theorem 1 (for the case where smoothing is not needed) and Theorem 2 (giving a cubic-root rate). As mentioned in Section 2.3, our lower bound examples can also be stated in the transductive setting, and thus all our guarantees and lower bounds can also be obtained in this setting.\n4.3 Simulations with empirical weights\nIn order to numerically investigate the possible advantage of empirical weighting, we performed simulations on low-rank matrix reconstruction under uniform sampling with the unweighted, and the smoothed empirically weighted, trace-norms. We choose to work with uniform sampling in order to emphasize the benefit of empirical weights, even in situations where one might not consider to use any weights at all. In all the experiments, we attempt to reconstruct a possibly noisy, random rank-2 “signal” matrix M with singular values 1√\n2 (n, n, 0, . . . , 0), ensuring ‖M‖F = n, measuring error using the squared loss1.\nSimulations were performed using Matlab, with code adapted from the SoftImpute code developed by [19]. We performed two types of simulations:\nSample complexity comparison in the noiseless setting: We define Y = M , and compute X̂S = arg min { ‖X‖ : L̂S(X) = 0 } , where ‖X‖ = ‖X‖tr or = ‖X‖tr(p̂r,p̂c), as appropriate. In Figure 1(a), we\nplot the average number of samples per row needed to get average squared error (over 100 repetitions) of at most 0.1, with both uniform weighting and empirical weighting.\nExcess error comparison in the noiseless and noisy settings: We define Y = M+νN , where noise N has i.i.d. standard normal entries. We compute X̂S = arg min { ‖X‖ : L̂S(X) ≤ ν2 } . In Figure 1(b),\nwe plot the resulting average squared error (over 100 repetitions) over a range of sample sizes s and noise levels ν, with both uniform weighting and empirical weighting.\nThe results from both experiments show a significant benefit to using the empirical marginals.\n1Although the squared loss is Lipschitz in a bounded domain, it is probably possible to improve all our results (removing the square root) in the special case of the squared loss, possibly with the additional assumption of i.i.d. noise , as in [9].\n5 Discussion\nIn this paper, we prove learning guarantees for the weighted trace-norm by analyzing expected Rademacher complexities. We show that weighting with smoothed marginals eliminates degenerate scenarios that can arise in the case of a non-product sampling distribution, and demonstrate in experiments on the Netflix and MovieLens datasets that this correction can be useful in applied settings. We also give results for empirically-weighted trace-norm regularization, and see indications that using the empirical distribution may be better than using the true distribution, even if it is available.\nReferences\n[1] N. Srebro, J. Rennie, and T. Jaakkola. Maximum-margin matrix factorization. Advances in Neural Information Processing Systems, 17, 2004.\n[2] R. Salakhutdinov and A. Mnih. Probabilistic matrix factorization. Advances in Neural Information Processing Systems, 20, 2007.\n[3] F. Bach. Consistency of trace-norm minimization. Journal of Machine Learning Research, 9:1019– 1048, 2008.\n[4] E. Candès and T. Tao. The power of convex relaxation: Near-optimal matrix completion. IEEE Trans. Inform. Theory, 56(5):2053–2080, 2009.\n[5] N. Srebro and A. Shraibman. Rank, trace-norm and max-norm. 18th Annual Conference on Learning Theory (COLT), pages 545–560, 2005.\n[6] B. Recht. A simpler approach to matrix completion. arXiv:0910.0651, 2009.\n[7] R. Keshavan, A. Montanari, and S. Oh. Matrix completion from noisy entries. Journal of Machine Learning Research, 11:2057–2078, 2010.\n[8] V. Koltchinskii, A. Tsybakov, and K. Lounici. Nuclear norm penalization and optimal rates for noisy low rank matrix completion. arXiv:1011.6256, 2010.\n[9] S. Negahban and M. Wainwright. Restricted strong convexity and weighted matrix completion: Optimal bounds with noise. arXiv:1009.2118, 2010.\n[10] R. Foygel and N. Srebro. Concentration-based guarantees for low-rank matrix reconstruction. 24th Annual Conference on Learning Theory (COLT), 2011.\n[11] R. Salakhutdinov and N. Srebro. Collaborative Filtering in a Non-Uniform World: Learning with the Weighted Trace Norm. Advances in Neural Information Processing Systems, 23, 2010.\n[12] O. Shamir and S. Shalev-Shwartz. Collaborative filtering with the trace norm: Learning, bounding, and transducing. 24th Annual Conference on Learning Theory (COLT), 2011.\n[13] P. Bartlett and S. Mendelson. Rademacher and Gaussian complexities: Risk bounds and structural results. Journal of Machine Learning Research, 3:463–482, 2002.\n[14] J.A. Tropp. User-friendly tail bounds for sums of random matrices. arXiv:1004.4389, 2010.\n[15] J. Bennett and S. Lanning. The netflix prize. In Proceedings of KDD Cup and Workshop, volume 2007, page 35. Citeseer, 2007.\n[16] MovieLens Dataset. Available at http://www.grouplens.org/node/73. 2006.\n[17] Yehuda Koren. Factorization meets the neighborhood: a multifaceted collaborative filtering model. ACM Int. Conference on Knowledge Discovery and Data Mining (KDD’08), pages 426–434, 2008.\n[18] J. Lee, B. Recht, R. Salakhutdinov, N. Srebro, and J. Tropp. Practical Large-Scale Optimization for Max-Norm Regularization. Advances in Neural Information Processing Systems, 23, 2010.\n[19] R. Mazumder, T. Hastie, and R. Tibshirani. Spectral regularization algorithms for learning large incomplete matrices. Journal of Machine Learning Research, 11:2287–2322, 2010.\n[20] Y. Seginer. The expected norm of random matrices. Combinatorics, Probability and Computing, 9 (2):149–166, 2000.\nA Proofs for the i.i.d. sampling setting\nA.1 Proof of Theorem 1\nWe first fill in the details for the Rademacher bound in the case that p has uniform row- and columnmarginals. Define\nQt = σt eit,jt√\npr (it) pc (jt) ∈ Rn×m .\nWe need to calculate R and σ2 such that ‖Qt‖sp ≤ R (almost surely) and\nσ2 = max {∥∥∥∑E [QTt Qt]∥∥∥ sp , ∥∥∥∑E [QtQTt ]∥∥∥ sp } .\nFor each t, Qt is just a matrix with a single non-zero entry of magnitude 1√\npr(i)pc(j) , for some i, j, and\nso ‖Qt‖sp ≤ maxij 1√\npr(i)pc(j)\n. = R.\nThe matrix QtQ T t ∈ Rn×n is equal to ei,i pr(i)pc(j) with probability p (i, j). Hence E [ QTt Qt ] is a diagonal\nmatrix with entries ∑ j p(i,j) pr(i)pc(j) . Similar arguments apply to Q T t Qt. Multiplying by s, and recalling the spectral norm of a diagonal matrix is simply the maximal magnitude element, we have:\nσ2 = s ·max maxi ∑ j p (i, j) pr (i) pc (j) ,max j ∑ i p (i, j) pr (i) pc (j)  . This completes the proof for the case that p has uniform row- and column- marginals.\nNext we turn to the case that p is a product distribution, p = pr × pc (with possibly non-uniform marginals). For any X ∈ Wr [p], define\nZ(X) = ( XijI { p (i, j) ≥ log(n)\ns √ nm }) ij .\nLet Z = {Z(X) : X ∈ Wr [p]}. We can then follow the proof of the bound in the uniform-marginals case, with a modified definition of Qt:\nQt = σt eit,jtI\n{ p (it, jt) ≥ log(n)s√nm } √ pr (it) pc (jt) .\nProceeding as in the proof for Theorem 1, we obtain R ≤ √ s √ nm\nlog(n) and σ 2 ≤ sn, and thus\nES∼p [ R̂S(Z) ] = O\n(√ rn log(n)\ns\n) .\nTherefore, by [13],\nE [ sup\nX∈Wr[p] Lp(Z(X))− L̂S(Z(X))\n] ≤ O ( l · √ rn log(n)\ns\n) ,\nE [ sup\nX∈Wr[p] L̂S(Z(X))− Lp(Z(X))\n] ≤ O ( l · √ rn log(n)\ns\n) .\nNext, let I = (√ p (i, j)I { p (i, j) < log(n)\ns √ nm }) ij . For any matrix M , define\n‖M‖F (pr,pc) = ∥∥∥diag (pr)1/2Mdiag (pc)1/2∥∥∥\nF .\nNow take any M with ‖M‖F (pr,pc) ≤ 1. Let M ′ = diag (pr) 1/2 Mdiag (pc) 1/2 , then ‖M ′‖F ≤ 1. We have∑\nij:p(i,j)< log(n)\ns √ nm\np (i, j)Mij = ∑ ij IijM ′ ij = 〈I,M ′〉 ≤ ‖I‖F · ‖M ′‖F\n≤ ‖I‖F = √√√√∑ ij p (i, j) I { p (i, j) < log(n) s √ nm } ≤ √ nm · log(n) s √ nm = √√ nm log(n) s .\nSince ‖M‖F ≤ ‖M‖tr for any matrix M , we then have, for any X ∈ Wr [p], ‖X‖F (pr,pc) ≤ ‖X‖tr(pr,pc) ≤ √ r, and so\n|Lp(X)− Lp(Z(X))| = ∣∣∣∣∣∣ ∑ ij 6∈I p (i, j) (`(Xij , Yij)− `(0, Yij)) ∣∣∣∣∣∣ ≤ l ·\n∑ ij 6∈I p (i, j) |Xij | ≤ √ l2r √ nm log(n) s .\nAnd, fixing some X∗ ∈ Wr [p] such that Lp(X∗) = infX∈Wr[p] Lp(X),\nE [ sup\nX∈Wr[p] L̂S(Z(X))− L̂S(X)\n] + E [ L̂S(X ∗)− L̂S(Z(X∗)) ]\n= E [ sup\nX∈Wr[p]\n1\ns s∑ t=1 I {(it, jt) 6∈ I} (`(0, Yitjt)− `(Xitjt , Yitjt))\n]\n+ E\n[ 1\ns s∑ t=1 I {(it, jt) 6∈ I} ( `(X∗itjt , Yitjt)− `(0, Yitjt)\n)]\n= E [ sup\nX∈Wr[p]\n1\ns s∑ t=1 I {(it, jt) 6∈ I} ( `(X∗itjt , Yitjt)− `(Xitjt , Yitjt)\n)]\n≤ E [ sup\nX∈Wr[p]\n1\ns s∑ t=1 I {(it, jt) 6∈ I} `(X∗itjt , Yitjt)\n] ≤ l ·E [ 1\ns s∑ t=1 I {(it, jt) 6∈ I} |X∗itjt |\n]\n= l ·E [ I {(i1, j1) 6∈ I} |X∗i1j1 | ] = l · ∑ ij 6∈I p (i, j) |X∗ij | ≤ √ l2r √ nm log(n) s\nThen writing\nLp(X̂S)− Lp(X∗) = (Lp(X̂S)− Lp(Z(X̂S))) + (Lp(Z(X̂S))− L̂S(Z(X̂S))) + (L̂S(Z(X̂S))− L̂S(X̂S))\n+(L̂S(X̂S)− L̂S(X∗)) + (L̂S(X∗)− L̂S(Z(X∗))) + (L̂S(Z(X∗))−Lp(Z(X∗))) + (Lp(Z(X∗))−Lp(X∗)) , we obtain\nE [ Lp(X̂S)− Lp(X∗) ] ≤ O\n(√ l2rn log(n)\ns\n) .\nA.2 Proof of Theorem 2\nAssume ` is l-Lipschitz and b-bounded, and r ≥ 1. We will show that (for any p)\nES∼p [ R̂S(` ◦Wr [p]) ] = O ( (l + b) · 3 √ rn log(n)\ns\n) .\nGiven a sample S, define\nT 0S = { t : pr (it) or p c (jt) < 3 √ l2r log(n)\nb2sn2\n} , T 1S = {1, . . . , s}\\T 0S .\nWe have\nR̂S(` ◦Wr [p]) = Eσ∼{±1}s [\nsup ‖X‖tr(pr,pc)≤ √ r\n1\ns s∑ t=1 σt · `(Xitjt , Yitjt)\n]\n≤ Eσ  sup ‖X‖tr(pr,pc)≤ √ r 1 s ∑ t∈T 0S σt · `(Xitjt , Yitjt) + Eσ  sup ‖X‖tr(pr,pc)≤ √ r 1 s ∑ t∈T 1S σt · `(Xitjt , Yitjt) \nBounding the first term,\nEσ  sup ‖X‖tr(pr,pc)≤ √ r 1 s ∑ t∈T 0S σt · `(Xitjt , Yitjt)  ≤ Eσ 1 s ∑ t∈T 0S |σt| · b  = b s · ∣∣T 0S∣∣ .\nIn expectation over S,\nES\n[ b s · ∣∣T 0S∣∣] = b ·Eij∼p [ I { pr (i) or pc (j) < 3 √ l2r log(n) b2sn2 }]\n= b · ∑ ij p (i, j) I\n{ pr (i) or pc (j) < 3 √ l2r log(n)\nb2sn2\n}\n≤ b · ∑ i:pr(i)< 3 √ l2r log(n)\nb2sn2\n∑ j p (i, j) + b · ∑\nj:pc(j)< 3 √ l2r log(n)\nb2sn2\n∑ i p (i, j) \n= b · ∑ i:pr(i)< 3 √ l2r log(n)\nb2sn2\npr (i) + b · ∑\nj:pc(j)< 3 √ l2r log(n)\nb2sn2\npc (j)  ≤ bn · 3 √ l2r log(n)\nb2sn2 + bm\n3\n√ l2r log(n)\nb2sn2 ≤ 2 3\n√ l2brn log(n)\ns .\nTo bound the second term, we use the fact that ‖abs(X)‖tr ≤ ‖X‖tr for any matrix X, where abs(X)\nis the matrix defined via abs(X)ij = |Xij |. We have\nEσ  sup ‖X‖tr(pr,pc)≤ √ r 1 s ∑ t∈T 1S σt · `(Xitjt , Yitjt)  ≤ Eσ\n sup ‖X‖tr(pr,pc)≤ √ r 1 s ∑ t∈T 1S σt · (`(Xitjt , Yitjt)− `(0, Yitjt) + Eσ  sup ‖X‖tr(pr,pc)≤ √ r 1 s ∑ t∈T 1S σt · `(0, Yitjt)  = Eσ\n sup ‖X‖tr(pr,pc)≤ √ r 1 s ∑ t∈T 1S σt · (`(Xitjt , Yitjt)− `(0, Yitjt)  ≤ l ·Eσ  sup ‖X‖tr(pr,pc)≤ √ r 1 s ∑ t∈T 1S σt · |Xitjt |  = l ·Eσ\n sup ‖X′‖tr≤ √ r 1 s ∑ t∈T 1S σt√ pr (it) pc (jt) · |X ′itjt |  ≤ l ·Eσ  sup ‖X′′‖tr≤ √ r 1 s ∑ t∈T 1S σt√ pr (it) pc (jt) ·X ′′itjt \n= l √ r ·Eσ  ∥∥∥∥∥∥∥∥ 1 s s∑ t=1 σt e(it,jt)I { pr (it) , p c (jt) ≥ 3 √ l2r log(n) b2sn2 } √ pr (it) pc (jt) ∥∥∥∥∥∥∥∥ sp  ,\nDefining Qt = σt e(it,jt)I\n{ pr(it),p c(jt)≥ 3 √ l2r log(n)\nb2sn2 } √ pr(it)pc(jt) , we can follow identical arguments as in the proof of the\nfirst bound of this theorem. We have\n‖Qt‖sp ≤ maxij\nI { pr (i) , pc (j) ≥ 3 √ l2r log(n) b2sn2 } √ pr (i) pc (j) ≤ 3 √ b2sn2 l2r log(n) . = R ,\nand\nσ2 . = max {∥∥∥∑E [QTt Qt]∥∥∥ sp , ∥∥∥∑E [QtQTt ]∥∥∥ sp }\n≤ s ·max maxi ∑ j p (i, j) I { pr (i) , pc (j) ≥ 3 √ l2r log(n) b2sn2 } pr (i) pc (j) ,\nmax j ∑ i\np (i, j) I { pr (i) , pc (j) ≥ 3 √ l2r log(n) b2sn2 } pr (i) pc (j)  ≤ s · 3 √ b2sn2\nl2r log(n) ·max maxi ∑ j p (i, j) pr (i) ,max j ∑ i p (i, j) pr (i)  = 3 √ b2s4n2 l2r log(n)\nThen applying [14], we get\nEσ  sup ‖X‖tr(pr,pc)≤ √ r 1 s ∑ t∈T 1S σt · `(Xitjt , Yitjt)  = l√r s ES,σ ∥∥∥∥∥ s∑ t=1 Qt ∥∥∥∥∥ sp  ≤ O ( l √ r\ns\n( σ √ log(n) +R log(n) ))\n≤ O\n( l √ r\ns\n( 6 √ b2s4n2\nl2r log(n)\n√ log(n) + 3\n√ b2sn2\nl2r log(n) log(n)\n))\n≤ O l2/3b1/3 3√rn log(n) s + l 1/3b 2/3 ( 3 √ rn log(n) s )2 . If s ≥ rn log(n), then this proves the bound. If not, then the result is trivial, since Lp(X) ≤ b for any X.\nA.3 Proof of Theorem 4\nThroughout this section, assume s ≥ 24n log(n). (If this is not the case, then we only need to prove excess error ≤ O(l √ r), which is trivial given the class Wr [p̌].) We also assume s ≤ O (nm log(nm)). (If this is not the case, then with high probability, we observe all entries of the matrix and obtain optimal recovery.) The lemmas which are cited in this proof, are proved below.\nDefine X∗ = arg min\nX∈Wr[p̃] Lp(X), r\n∗ = ‖X∗‖2tr(pr,pc) ≤ r .\nFor any sample S, define\nc(S) = max { 0, ∥∥∥∥ 1√r∗X∗ ∥∥∥∥\ntr(p̌r,p̌c)\n− 1 } .\nThen, for a fixed S,\n‖(1− c(S))X∗‖tr(p̌r,p̌c) = √ r∗(1− c(S)) ∥∥∥∥ 1√r∗X∗ ∥∥∥∥\ntr(p̃r,p̃c)\n≤ √ r ⇒ (1− c(S))X∗ ∈ Wr [p̌] .\nApplying Lemma 1 and Theorem 3,\nE [ Lp(X̂S)− L̂S(X̂S) ] ≤ E [ sup\nX∈Wr[p̌]\n( Lp(X)− L̂S(X)\n)]\n≤ E [ sup\nX∈2·Wr[p̃]\n( Lp(X)− L̂S(X) )] + 8 √ l2rnm\nn2 ≤ O\n(√ l2rn log(n)\ns\n) + 8 √ l2rnm\nn2\n≤ O\n(√ l2rn log(n)\ns\n) .\nAnd, similarly,\nE [ L̂S((1− c(S))X∗)− Lp((1− c(S))X∗) ] ≤ E [ sup\nX∈Wr[p̌]\n( L̂S(X)− Lp(X)\n)]\n≤ E [ sup\nX∈2·Wr[p̃]\n( L̂S(X)− Lp(X) )] + 8 √ l2rnm\nn2 ≤ O\n(√ l2rn log(n)\ns\n) + 8 √ l2rnm\nn2\n≤ O\n(√ l2rn log(n)\ns\n) .\nBy definition, since (1− c(S))X∗ ∈ Wr [p̌],\nE [ L̂S(X̂S)− L̂S((1− c(S))X∗) ] ≤ 0 .\nFinally, by Lemma 3,\nE [Lp((1− c(S))X∗)− Lp(X∗)] ≤ √ 2l2rn\ns .\nCombining all of the above, we get\nE [ Lp(X̂S)− Lp(X∗) ] ≤ O\n(√ l2rn log(n)\ns\n) .\nA.3.1 Lemmas for Theorem 6\nLemma 1.\nE [ sup\nX∈Wr[p̌]\n( Lp(X)− L̂S(X) )] ≤ E [ sup\nX∈2·Wr[p̃]\n( Lp(X)− L̂S(X) )] + 8 √ l2rnm\nn2 .\nE [ sup\nX∈Wr[p̌]\n( L̂S(X)− Lp(X) )] ≤ E [ sup\nX∈2·Wr[p̃]\n( L̂S(X)− Lp(X) )] + 8 √ l2rnm\nn2 .\nProof. By Lemma 2, with probability at least 1− 2n−2, for all i, j,\np̌r (i) ≥ 1 2 p̃r (i) , p̌c (j) ≥ 1 2 p̃c (j) .\nLet A be the event that these inequalities hold. If A occurs, then for any X ∈ Wr [p̌],\n‖X‖tr(pr,pc) = ∥∥∥diag (p̃r (i))1/2Xdiag (p̃c (j))1/2∥∥∥\ntr\n= ∥∥∥∥∥diag ( p̃r (i) p̌r (i) )1/2 diag (p̌r (i)) 1/2 Xdiag (p̌c (j)) 1/2 diag ( p̃c (j) p̌c (j) )1/2∥∥∥∥∥ tr\n≤ 2 ∥∥∥diag (p̌r (i))1/2Xdiag (p̌c (j))1/2∥∥∥\ntr = 2 ‖X‖tr(p̌r,p̌c) ≤ 2\n√ r .\nIn this case, Wr [p̌] ⊂ 2 · Wr [p̃], and therefore,\nsup X∈Wr[p̌]\n[ (Lp(X)− Lp(0n×m))− ( L̂S(X)− L̂S(0n×m) )] ≤ sup X∈2·Wr[p̃] [ (Lp(X)− Lp(0n×m))− ( L̂S(X)− L̂S(0n×m) )] .\nNext we consider the case that A does not occur. For any X ∈ Wr [p̌],\n|X|∞ ≤ ‖X‖F = 2 √ nm ∥∥∥∥∥diag ( 1 2n 1n )1/2 Xdiag ( 1 2m 1m )1/2∥∥∥∥∥ F\n≤ 2 √ nm ∥∥∥∥∥diag ( 1 2n 1n )1/2 Xdiag ( 1 2m 1m )1/2∥∥∥∥∥ tr ≤ 2 √ nm ‖X‖tr(p̌r,p̌c) ≤ 2 √ rnm .\nTherefore,\nsup X∈Wr[p̌]\n[ (Lp(X)− Lp(0n×m))− ( L̂S(X)− L̂S(0n×m) )]\n≤ sup X∈Wr[p̌] ∑ ij p (i, j) (`(Xij , Yij)− `(0, Yij))− 1 s ∑ t (`(Xitjt , Yitjt)− `(0, Yitjt))  ≤ l · sup\nX∈Wr[p̌] ∑ ij p (i, j) · |Xij |+ 1 s ∑ t |Xitjt |  ≤ l · sup\nX∈Wr[p̌] ∑ ij p (i, j) · 2 √ rnm+ 1 s ∑ t 2 √ rnm  ≤ 4√l2rnm And so,\nES\n[ sup\nX∈Wr[p̌]\n[ (Lp(X)− Lp(0n×m))− ( L̂S(X)− L̂S(0n×m)\n)]]\n= ES\n[ sup\nX∈Wr[p̌]\n[ (Lp(X)− Lp(0n×m))− ( L̂S(X)− L̂S(0n×m) )] · I {A}\n]\n+ ES\n[ sup\nX∈Wr[p̌]\n[ (Lp(X)− Lp(0n×m))− ( L̂S(X)− L̂S(0n×m) )] · I {Ac}\n]\n≤ ES [ sup\nX∈2·Wr[p̃]\n[ (Lp(X)− Lp(0n×m))− ( L̂S(X)− L̂S(0n×m) )] · I {A} ] + P (Ac) · 4 √ l2rnm\n≤ ES [ sup\nX∈2·Wr[p̃]\n[ (Lp(X)− Lp(0n×m))− ( L̂S(X)− L̂S(0n×m) )] · I {A} ] + 8 √ l2rnm\nn2\n≤ ES [ sup\nX∈2·Wr[p̃]\n[ (Lp(X)− Lp(0n×m))− ( L̂S(X)− L̂S(0n×m) )]] + 8 √ l2rnm\nn2 .\nwhere the last step is true because, since 0n×m ∈ 2 · Wr [p̃], for any S,\nsup X∈2·Wr[p̃]\n[ (Lp(X)− Lp(0n×m))− ( L̂S(X)− L̂S(0n×m) )] ≥ 0 .\nAnd, ES [ Lp(0n×m)− L̂S(0n×m) ] = 0, so therefore,\nES\n[ sup\nX∈Wr[p̌]\n( Lp(X)− L̂S(X) )] ≤ E [ sup\nX∈2·Wr[p̃]\n( Lp(X)− L̂S(X) )] + 8 √ l2rnm\nn2 .\nThe second claim can be proved with identical arguments.\nLemma 2. With probability at least 1− 2n−2, for all i and all j,\np̌r (i) ≥ 1 2 p̃r (i) , p̌c (j) ≥ 1 2 p̃c (j) .\nProof. Take any row i. Suppose that pr (i) ≤ 1n . Then p̃ r (i) ≤ 1n , while p̌ r (i) = 12 ( p̂r (i) + 1n ) ≥ 12n . Therefore, in this case, p̌r (i) ≥ 12 p̃ r (i) with probability 1.\nNext, suppose that pr (i) > 1n . Then, by the Chernoff inequality,\nP ( p̂r (i) < 1\n2 pr (i)\n) = P ( Bin(s, pr (i)) < spr (i) ( 1− 1\n2\n)) ≤ e− spr(i) 8\n≤ e− s8n ≤ e−3 log(n) = n−3 .\nTherefore, with probability at least 1− n−3, p̂r (i) ≥ 12p r (i), and so\np̌r (i) = 1\n2\n( p̂r (i) + 1\nn\n) ≥ 1\n2\n( 1\n2 pr (i) +\n1\nn\n) ≥ 1\n2 p̃r (i) .\nTherefore, for any row i, with probability at least 1 − n−3, p̌r (i) ≥ 12 p̃ r (i). The same reasoning applies to every column j. Therefore, with probability at least 1 − 2n−2, the statement holds for all i and all j.\nLemma 3. Fix X∗ with ‖X∗‖2tr(p̃r,p̃c) = r∗ ≤ r, and define\nc(S) = max { 0, ∥∥∥∥ 1√r∗X∗ ∥∥∥∥\ntr(p̌r,p̌c)\n− 1 } .\nThen\nE [Lp((1− c(S))X∗)− Lp(X∗)] ≤ √ 2l2rn\ns .\nProof.\nLp((1− c(S))X∗)− Lp(X∗) = ∑ ij p (i, j) ( `((1− c(S))X∗ij , Yij)− `(X∗ij , Yij) ) ≤ l ·\n∑ ij p (i, j) |(1− c(S))X∗ij −X∗ij | = l · c(S) · ∑ ij p (i, j) |X∗ij |\n= l · c(S) · ∑ ij p (i, j)√ p̃r (i) p̃c (j) · √ p̃r (i) p̃c (j) · |X∗ij |\nDefining M = ( p(i,j)√ p̃r(i)p̃c(j) ) ij ,\n= l · c(S) · 〈M, ( diag (p̃r (i)) 1/2 X∗diag (p̃c (j)) 1/2 ) ij 〉\n≤ l · c(S) · ‖M‖sp · ∥∥∥∥(diag (p̃r (i))1/2X∗diag (p̃c (j))1/2)\nij ∥∥∥∥ tr\n≤ l √ r · c(S) · ‖M‖sp .\nNow we show that ‖M‖sp ≤ 2. Take any unit vectors u ∈ Rm, v ∈ Rn. Then\nuTMv = ∑ ij p (i, j) · √ u2i p̃r (i) · √ v2j p̃c (j) ≤ 1 2 ∑ ij p (i, j) ( u2i p̃r (i) + v2j p̃c (j) )\n= 1\n2 ∑ i pr (i) · u 2 i p̃r (i) + 1 2 ∑ j pc (j) · v2j p̃c (j) ≤ 1 2 ∑ i 2u2i + 1 2 ∑ j 2v2j = 2 .\nSo, by Lemma 4,\nE [Lp((1− c(S))X∗)− Lp(X∗)] ≤ 2l √ r ·E [c(S)] ≤ 2l √ r · √ n\n2s .\nLemma 4. For any p, for any fixed X with ‖X‖tr(p̃r,p̃c) = 1,\nE [ max{0, ‖X‖tr(p̌r,p̌c) − 1} ] ≤ √ n\n2s .\nProof. By properties of the trace-norm [5], we can write diag (p̃r) 1/2 Xdiag (p̃c) 1/2 = ABT , where ‖A‖2F = ‖B‖2F = ‖X‖tr(pr,pc) = 1. Define\nD1 = diag (p̌ r) diag (p̃r) −1 , D2 = diag (p̌ c) diag (p̃c) −1 .\nThen, by properties of the trace-norm [5],\n‖X‖tr(p̌r,p̌c) = ∥∥∥diag (p̌r)1/2Xdiag (p̌c)1/2∥∥∥\ntr = ∥∥∥∥(D1/21 A)(D1/22 B)T∥∥∥∥ tr\n≤ 1 2 ∥∥∥D1/21 A∥∥∥2 F + 1 2 ∥∥∥D1/22 B∥∥∥2 F = 1\n2 ∑ i p̌r (i) p̃r (i) ‖A(i)‖22 + 1 2 ∑ j p̌c (j) p̃c (j) ‖B(j)‖22\n= 1\n4 ∑ i p̂r (i) + 1n p̃r (i) ‖A(i)‖22 + 1 4 ∑ j p̂c (j) + 1m p̃c (j) ‖B(j)‖22\n= 1\n4 ∑ i Nri + s n sp̃r (i) ‖A(i)‖22 + 1 4 ∑ j N cj + s m sp̃c (j) ‖B(j)‖22 ,\nwhere Nri is the number of samples in row i, and N c j is the number of samples in column j. Clearly,\nE 1 4 ∑ i Nri + s n sp̃r (i) ‖A(i)‖22 + 1 4 ∑ j N cj + s m sp̃c (j) ‖B(j)‖22  = 1\n4 ∑ i spr (i) + sn sp̃r (i) ‖A(i)‖22 + 1 4 ∑ j spc (j) + sm sp̃c (j) ‖B(j)‖22\n= 1\n4 ∑ i 2sp̃r (i) sp̃r (i) ‖A(i)‖22 + 1 4 ∑ j 2sp̃c (j) sp̃c (j) ‖B(j)‖22\n= 1\n2 ‖A‖2F +\n1 2 ‖B‖2F = 1 .\nAnd, we can compute\nVar(Nri ) ≤ spr (i) , Cov(Nri , Nri′) < 0, Var(N cj ) ≤ spc (j) , Cov(N cj , N cj′) < 0 .\nTherefore,\nVar ∑ i Nri sp̃r (i) ‖A(i)‖22 + ∑ j N cj sp̃c (j) ‖B(j)‖22  ≤ 2Var\n(∑ i Nri sp̃r (i) ‖A(i)‖22 ) + 2Var ∑ j N cj sp̃c (j) ‖B(j)‖22  = ∑ i 1 s2p̃r (i) 2 Var(N r i )‖A(i)‖42 + 2 ∑ i<i′ 1 s2p̃r (i) p̃r (i′) Cov(Nri , N r i′)‖A(i)‖22‖A(i′)‖22\n+ ∑ j\n1\ns2p̃c (j) Var(N cj )‖B(j)‖42 + 2 ∑ j<j′\n1\ns2p̃c (j) p̃c (j′) Cov(N cj , N c j′)‖B(j)‖22‖B(j′)‖22\n≤ ∑ i\n1\ns2p̃r (i) 2 Var(N\nr i )‖A(i)‖42 + ∑ j\n1\ns2p̃c (j) Var(N cj )‖B(j)‖42\n≤ ∑ i spr (i) s2p̃r (i) 2 ‖A(i)‖ 4 2 + ∑ j spc (j) s2p̃c (j) ‖B(j)‖42\nSince p̃r (i) ≥ 12p r (i) and p̃r (i) ≥ 12n , and similarly for the columns, we continue:\n≤ ∑ i 4n s ‖A(i)‖42 + ∑ j 4m s ‖B(j)‖42 ≤ 4n s (∑ i ‖A(i)‖22 )2 + 4m s ∑ j ‖B(j)‖22  ≤ 4n\ns ‖A‖4F +\n4m\ns ‖B‖4F ≤\n4(n+m)\ns .\nSo, we have\nE [ max{0, ‖X‖tr(p̌r,p̌c) − 1} ] ≤ E max 0, 14 ∑\ni\nNri + s n\nsp̃r (i) ‖A(i)‖22 +\n1\n4 ∑ j N cj + s m sp̃c (j) ‖B(j)‖22 − 1  \n≤ √√√√√Var 1\n4 ∑ i Nri + s n sp̃r (i) ‖A(i)‖22 + 1 4 ∑ j N cj + s m sp̃c (j) ‖B(j)‖22\n\n= √√√√√Var 1\n4 ∑ i Nri sp̃r (i) ‖A(i)‖22 + 1 4 ∑ j N cj sp̃c (j) ‖B(j)‖22  ≤√ (n+m) 4s\nB Proofs for the transductive setting\nB.1 Proof of Theorem 5\nLet S ⊂ [n]× [m] be a subset of size 2s. Let p denote the smoothed empirical marginals of S. Now choose any S ⊂ S, a training set of size s. Without loss of generality, write S = {(i1, j1), . . . , (i2s, j2s)} and S = {(i1, j1), . . . , (is, js)}.\nFirst, we bound transductive Rademacher complexity. By Lemma 12 in [5], for any sample S, R̂S(Wr [p]) = Eσ∼{±1}s [\nsup X∈Wr[p]\n1\ns s∑ t=1 σtXitjt\n]\n= Eσ∼{±1}s  sup X∈Wr[p] 1 s ∑ ij Xij  ∑ t:(it,jt)=(i,j) σt  ≤ Eσ∼{±1}n×m  sup X∈Wr[p] 1 s ∑ ij Xijσij ·#{t : (it, jt) = (i, j), 1 ≤ t ≤ s}\n = Eσ∼{±1}n×m  sup X∈Wr[p] 1 s ∑ ij Xijσij · I {(i, j) ∈ S}\n . Now define matrix Σ via\nΣij = I {(i, j) ∈ S} s √ pr (i) pc (j) .\nWe have\nES∼p [ R̂S(Wr [p]) ] ≤ ES Eσ∼{±1}n×m  sup X∈Wr[p] 1 s ∑ ij Xijσij · I {(i, j) ∈ S}  = ES Eσ∼{±1}n×m  sup X∈Wr[p] ∑ ij (√ pr (i)Xij √ pc (j) ) σijΣij\n = ES [ Eσ∼{±1}n×m [ sup\nX:‖X‖tr≤ √ r\nXijσijΣij ]] = √ r ·ES [ Eσ∼{±1}n×m [ ‖σ • Σ‖sp ]] ,\nwhere σ • Σ is the element-wise product of Σ with the random sign matrix σ = (σij). By [20], Eσ∼{±1}n×m [ ‖σ • Σ‖sp ] ≤ O ( log1/4(n+m) ) ·max { max i ∥∥Σ(i)∥∥2 ,maxj ∥∥∥Σ(j)∥∥∥2 } .\nWe now bound ∥∥Σ(i)∥∥2 and ∥∥Σ(j)∥∥2. Fix any i. Then∥∥Σ(i)∥∥22 = ∑ j Σ2ij = m∑ j=1 I {(i, j) ∈ S} (spr (i)) · (spc (j)) ≤ m∑ j=1 I { (i, j) ∈ S } (spr (i)) · ( s · 12m\n) ≤\nm∑ j=1 #{t : (it, jt) = (i, j), 1 ≤ t ≤ 2s}( 1 4# {t : it = i, 1 ≤ t ≤ 2s} ) · ( s · 12m ) ≤ #{t : it = i, 1 ≤ t ≤ 2s}( 1 4# {t : it = i, 1 ≤ t ≤ 2s} ) · ( s · 12m ) ≤ 8m s .\nSimilarly, for all j, ∥∥Σ(j)∥∥2\n2 ≤ 8ns . Therefore,\nES∼p [ R̂S(Wr [p]) ] ≤ √ r ·ES [ Eσ∼{±1}n×m [ ‖σ • Σ‖sp ]] ≤ √ r ·ES [ O ( log1/4(n) ) ·max { max i ∥∥Σ(i)∥∥2 ,maxj ∥∥∥Σ(j)∥∥∥2 }] ≤ O √rn log1/2(n) s\n . Applying Theorem 5 of [12] (using integration to obtain a bound in expectation from a bound in\nprobability),\nES [ L̂S\\S(X̂S)− inf\nX∈Wr[p] L̂S\\S(X)\n] ≤ O √ l2rn log1/2(n) + b2 s  .\nB.2 Transductive version of Theorem 1\nLet p now denote the (unsmoothed) empirical marginals of S. If pr (i) ≥ 1Cn and p c (j) ≥ 1Cm for all i, j, defining\nX̂S = arg min X∈Wr[p] L̂S(X) ,\nwe can then show that, for an l-Lipschitz loss ` bounded by b, in expectation over the split of S into training set S and test set T ,\nL̂T (X̂S) ≤ inf X∈Wr[p] L̂T (X) + O\nC1/2l · √ rn log 1/2(n) + b2\ns  . We prove this by following identical arguments as in the proof of Theorem 5, we define\nΣij = I {(i, j) ∈ S} s √ pr (i) pc (j) ,\nand obtain ∥∥Σ(i)∥∥22 ,∥∥Σ(j)∥∥22 ≤ 2Cns for all i, j, which yields\nES [ L̂S\\S(X̂S)− inf\nX∈Wr[p] L̂S\\S(X)\n] ≤ O √Cl2rn log1/2(n) + b2 s  . In fact, we can obtain the same result with a weaker requirement on p, namely\ns n max { max i ‖Σ(i)‖22,max j ‖Σ(j)‖22 } ≤ max maxi 1m m∑ j=1 1 s I { (i, j) ∈ S } pr (i) pc (j) ,max j 1 n n∑ i=1 1 s I { (i, j) ∈ S } pr (i) pc (j)  ≤ C . For instance, this quantity is likely to be bounded if S is a sample drawn from a product distribution on the matrix.\nB.3 Transductive version of Theorem 2\nLet p now denote the (unsmoothed) empirical marginals of S. We define\nX̂S = arg min X∈Wr[p] L̂S(X) ,\nwe can then show that, for an l-Lipschitz loss ` bounded by b, without any requirements on p, in expectation over the split of S into training set S and test set T ,\nL̂T (X̂S) ≤ inf X∈Wr[p] L̂T (X) + O\n( (l + b) · 3 √ rn log(n)\ns\n) .\nWe prove this by combining the proof techniques used in the proofs of Theorems 2 and 5. Define\nT 0S = { t : 1 ≤ t ≤ 2s, pr (it) or pc (jt) < 3 √ l2r log(n)\nb2sn2\n} , T 1S = {1, . . . , 2s}\\T 0S .\nWe then have R̂S(` ◦Wr [p]) = Eσ∼{±1}s [\nsup X∈Wr[p]\n1\ns s∑ t=1 σt`(Xitjt , Yitjt)\n]\n≤ Eσ∼{±1}n×m  sup X∈Wr[p] 1 s ∑ ij `(Xitjt , Yitjt)σij · I {(i, j) ∈ S}  ≤ Eσ  sup X∈Wr[p] 1 s ∑ ij `(Xitjt , Yitjt)σij · I { (i, j) ∈ S, and pr (i) , pc (j) ≥ 3 √ l2r log(n) b2sn2\n} + Eσ  sup X∈Wr[p] 1 s ∑ ij `(Xitjt , Yitjt)σij · I { (i, j) ∈ S, and pr (i) or pc (j) < 3 √ l2r log(n) b2sn2\n} . = (Term 1) + (Term 2); .\nNow define matrix Σ via\nΣij =\nI { (i, j) ∈ S, and pr (i) , pc (j) ≥ 3 √\nl2r log(n) b2sn2 } s √ pr (i) pc (j) .\nFollowing the same arguments as in the proof of Theorem 5, we obtain for all i, j,\n‖Σ(i)‖22, ‖Σ(j)‖22 ≤ 4 s · 3 √ b2sn2 l2r log(n)\nTherefore, using the same arguments as in the proof of Theorem 2,\n(Term 1) ≤ l √ rO log1/4(n) √√√√4 s · 3 √ b2sn2 l2r log(n)  = O( 3√ l2brn log(n) s ) .\nNext we have\n(Term 2) = Eσ  sup X∈Wr[p] 1 s ∑ ij `(Xitjt , Yitjt)σij · I { (i, j) ∈ S, and pr (i) or pc (j) < 3 √ l2r log(n) b2sn2 } ≤ sup X∈Wr[p] 1 s ∑ ij `(Xitjt , Yitjt) · I { (i, j) ∈ S, and pr (i) or pc (j) < 3 √ l2r log(n) b2sn2 }\n≤ 1 s ∑ ij b · I\n{ (i, j) ∈ S, and pr (i) or pc (j) < 3 √ l2r log(n)\nb2sn2\n}\n≤ 1 s ∑ i:pr(i)< 3 √ l2r log(n)\nb2sn2\n ∑ j:(i,j)∈S b + 1 s ∑ j:pc(j)< 3 √ l2r log(n)\nb2sn2\n ∑ i:(i,j)∈S b  ≤ 2n\ns\n( b · 2s · 3 √ l2r log(n)\nb2sn2\n) ≤ O ( 3 √ l2rbn log(n)\ns\n) .\nCombining the two, we get Rs(` ◦ Wr [p]) ≤ O ( 3 √ l2rbn log(n)\ns\n) , and therefore, in expectation over\nthe split of S into S and T ,\nL̂T (X̂S) ≤ inf X∈Wr[p] L̂T (X) + O\n( (l + b) · 3 √ rn log(n)\ns\n) ."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2011,
    "abstractText" : "We provide rigorous guarantees on learning with the weighted trace-norm under arbitrary<lb>sampling distributions. We show that the standard weighted trace-norm might fail when the<lb>sampling distribution is not a product distribution (i.e. when row and column indexes are<lb>not selected independently), present a corrected variant for which we establish strong learning<lb>guarantees, and demonstrate that it works better in practice. We provide guarantees when<lb>weighting by either the true or empirical sampling distribution, and suggest that even if the<lb>true distribution is known (or is uniform), weighting by the empirical distribution may be<lb>beneficial.",
    "creator" : "LaTeX with hyperref package"
  }
}