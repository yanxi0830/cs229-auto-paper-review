{
  "name" : "1608.06408.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Online Learning to Rank with Top-k Feedback",
    "authors" : [ "Sougata Chaudhuri", "Ambuj Tewari" ],
    "emails" : [ "sougata@umich.edu", "tewria@umich.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Keywords: Learning to Rank, Online Learning, Partial Monitoring, Online Bandits, Learning Theory"
    }, {
      "heading" : "1. Introduction",
      "text" : "Learning to rank (Liu, 2011) is a supervised machine learning problem, where the output space consists of rankings of objects. Most learning to rank methods are based on supervised batch learning, i.e., rankers are trained on batch data in an offline setting. The accuracy of a ranked list, in comparison to the actual relevance of the documents, is measured by various ranking measures, such as Discounted Cumulative Gain (DCG) (Järvelin and Kekäläinen, 2000), Average Precision (AP) (Baeza-Yates and Ribeiro-Neto, 1999) and others.\nCollecting reliable training data can be expensive and time consuming. In certain applications, such as deploying a new web app or developing a custom search engine, collecting large amount of high quality labeled data might be infeasible (Sanderson, 2010). Moreover, a ranker trained from batch data might not be able to satisfy rapidly changing user needs and preferences. Thus, a promising direction of research is development of online ranking\nar X\niv :1\n60 8.\n06 40\n8v 1\n[ cs\n.L G\n] 2\n3 A\nsystems, where a ranker is updated on the fly. One type of online ranking models learn from implicit feedback inferred from user clicks on ranked lists (Hofmann et al., 2013). However, there are some potential drawbacks in learning from user clicks. It is possible that the system is designed for explicit ratings but not clicks. Moreover, a clicked item might not actually be relevant to the user and there is also the problem of bias towards top ranked items in inferring feedback from user clicks (Joachims, 2002).\nWe develop models for online learning of ranking systems, from explicit but highly restricted feedback. At a high level, we consider a ranking system which interacts with users over a time horizon, in a sequential manner. At each round, the system presents a ranked list of m items to the user, with the quality of the ranked list judged by the relevance of the items to the user. The relevance of the items, reflecting varying user preferences, is encoded as relevance vectors. The system’s objective is to learn from the feedback it receives and update its ranker over time, to satisfy as many users as possible. However, the feedback that the system receives at end of each round is not the full relevance vector, but relevance of only the top k ranked items, where k m (typically, k = 1 or 2). We consider two problem settings under the general framework: non-contextual and contextual. In the first setting, we assume that the set of items to be ranked are fixed (i.e., there are no context on items), with the relevance vectors varying according to users’ preferences. In the second setting, we assume that set of items vary, as traditional query-document lists. We highlight two motivating examples for such feedback model, encompassing economic and user-burden constraints and privacy concerns.\nPrivacy Concerns: Assume that a medical company wants to build an app to suggest activities (take a walk, meditate, watch relaxing videos, etc.) that can lead to reduction of stress in a certain highly stressed segment of the population. The activities do not have contextual representation and are fixed over time. Not all activities are likely to be equally suitable for everyone under all conditions since the effects of the activities vary depending on the user attributes like age & gender and on the context such as time of day & day of week. The user has liberty to browse through all the suggested activities, and the company would like the user to rate every activity (may be on an 1−5 scale), reflecting the relevances, so that it can keep refining its ranking strategy. However, in practice, though the user may scan through all suggested activities and have a rough idea about how relevant each one is to her; she is unlikely to give feedback on the usefulness (relevance) of every activity due to privacy concerns and cognitive burden. Hence, in exchange of the user using the app, the company only asks for careful rating of the top 1 or 2 ranked activities. The apps performance would still be based on the full ranked list, compared to the implicit relevance vector that the user generates, but it gets feedback on the relevances of only top 1 or 2 ranked activities.\nEconomic Constraints: Assume that a small retail company wants to build an app that produces a ranked list of suggestions to a user query, for categories of different products. The app develops features representing the categories, and thus, the interaction with the users happen in a traditional query-documents lists framework (user query and retrieved activities are jointly represented through a feature matrix). Different categories are likely to have varying relevance to different users, depending on user characteristics such as age, gender, etc. Like in the first example, the user has liberty to browse through all the suggestions but she will likely feel too burdened to give carefully considered rating on\neach suggestion, unless the company provides some economic incentives to do so. Though the company needs high quality feedback on each suggestion to keep refining the ranking strategy, it cannot afford to give incentives due to budget constraints. Similar to the first example, the company only asks, and possibly pays, for rating on the top 1 or 2 ranked suggestions, in lieu of using the app, but its performance is judged on the full ranked list and implicit relevance vector.\nWe cast the online learning to rank problem as an online game between a learner and an adversary, played over time horizon T . That is, we do not make any stochastic assumption on the relevance vector generation process or the context (features) generation process (in the second problem setting). The adversary is considered to be oblivious to the learner’s strategies. We separately discuss the two problem settings, and our contributions in each, in greater details.\nNon-contextual setting: Existing work loosely related to ranking of a fixed set of items to satisfy diverse user preferences (Radlinski et al., 2008, 2009; Agrawal et al., 2009; Wen et al., 2014) has focused on learning an optimal ranking of a subset of items, to be presented to an user, with performance judged by a simple 0-1 loss. The loss in a round is 0 if among the top k (out of m) items presented to a user, the user finds at least one relevant item. All of the work falls under the framework of online bandit learning. In contrast, our model focuses on optimal ranking of the entire list of items, where the performance of the system is judged by practical ranking measures like DCG and AP. The challenge is to decide when and how efficient learning is possible with the highly restricted feedback model. Theoretically, the top k feedback model is neither full-feedback nor bandit-feedback since not even the loss (quantified by some ranking measure) at each round is revealed to the learner. The appropriate framework to study the problem is that of partial monitoring (Cesa-Bianchi, 2006). A very recent paper shows another practical application of partial monitoring in the stochastic setting (Lin et al., 2014). Recent advances in the classification of partial monitoring games tell us that the minimax regret, in an adversarial setting, is governed by a property of the loss and feedback functions called observability (Bartok et al., 2014; Foster and Rakhlin, 2012), where observability is of two kinds: local and global.\nOur contributions: We instantiate these general observability notions for our problem with top 1 (k = 1) feedback. We prove that, for some ranking measures, namely PairwiseLoss (Duchi et al., 2010), DCG and Precision@n (Liu et al., 2007), global observability holds. This immediately shows that the upper bound on regret scales as O(T 2/3). Specifically for PairwiseLoss and DCG, we further prove that local observability fails, when restricted to the top 1 feedback case, illustrating that their minimax regret scales as Θ(T 2/3). However, the generic algorithm that enjoys O(T 2/3) regret for globally observable games necessarily maintains explicit weights on each action in learner’s action set. It is impractical in our case to do so, since the learner’s action set is the exponentially large set of m! rankings over m objects. We propose a generic algorithm for learning with top k feedback, which uses blocking and a black-box full information algorithm. Specifically, we instantiate the black box algorithm with Follow The Perturbed Leader (FTPL) strategy, which leads to an efficient algorithm achieving O(T 2/3) regret bound for PairwiseLoss, DCG and Precision@n, with O(m logm) time spent per step. Moreover, the regret of our efficient algorithm has a logarithmic dependence on number of learner’s actions (i.e., polynomial\ndependence on m), whereas the generic algorithm has a linear dependence on number of actions (i.e., exponential dependence on m).\nFor several measures, their normalized versions are also considered. For example, the normalized versions of PairwiseLoss, DCG and Precision@n are called AUC (Cortes and Mohri, 2004), NDCG (Järvelin and Kekäläinen, 2002) and AP respectively. We show an unexpected result for the normalized versions: they do not admit sub-linear regret algorithms with top 1 feedback. This is despite the fact that the opposite is true for their unnormalized counterparts! Intuitively, the normalization makes it hard to construct an unbiased estimators of the (unobserved) relevance vectors. Surprisingly, we are able to translate this intuitive hurdle into a provable impossibility.\nWe also present some preliminary experiments on simulated datasets to explore the performance of our efficient algorithm and compare its regret to its full information counterpart.\nContextual Setting: The requirement of having a fixed set of items to rank, in the first part of our work, somewhat limits practical applicability. In fact, in the classic multiarmed bandit problem, while non-contextual bandits have received a lot of attention, the authors Langford and Zhang (2008) mention that “settings with no context information are rare in practice”. The second part of our work introduces context, by combining querylevel ranking with the explicit but restricted feedback model. At each round, the adversary generates a document list of length m, pertaining to a query. The learner sees the list and produces a real valued score vector to rank the documents. We assume that the ranking is generated by sorting the score vector in descending order of its entries. The adversary then generates a relevance vector but, like in the non-contextual setting, the learner gets to see the relevance of only the top k items of the ranked list. The learner’s loss in each round, based on the learner’s score vector and the full relevance vector, is measured by some continuous ranking surrogates. We focus on continuous surrogates, e.g., the cross entropy surrogate in ListNet (Cao et al., 2007) and hinge surrogate in RankSVM (Joachims, 2002), instead of discontinuous ranking measures like DCG, or AP, because the latter lead to intractable optimization problems in the query-documents setting. Just like in the non-contextual setting, we note that the top k feedback model is neither full feedback nor bandit feedback models. The problem is an instance of partial monitoring, extended to a setting with side information (documents list) and an infinite set of learner’s moves (all real valued score vectors). For such an extension of partial monitoring there exists no generic theoretical or algorithmic framework to the best of our knowledge.\nOur contributions: In this setting, first, we propose a general, efficient algorithm for online learning to rank with top k feedback and show that it works in conjunction with a number of ranking surrogates. We characterize the minimum feedback required, i.e., the value of k, for the algorithm to work with a particular surrogate by formally relating the feedback mechanism with the structure of the surrogates. We then apply our general techniques to three convex ranking surrogates and one non-convex surrogate. The convex surrogates considered are from three major learning to ranking methods: squared loss from a pointwise method (Cossock and Zhang, 2008), hinge loss used in the pairwise RankSVM (Joachims, 2002) method, and (modified) cross-entropy surrogate used in the listwise ListNet (Cao et al., 2007) method. The non-convex surrogate considered is the\nSmoothDCG surrogate (Chapelle and Wu, 2010). For the three convex surrogates, we establish an O(T 2/3) regret bound.\nThe convex surrogates we mentioned above are widely used but are known to fail to be calibrated with respect to NDCG (Ravikumar et al., 2011). Our second contribution is to show that for the entire class of NDCG calibrated surrogates, no online algorithm can have sub-linear (in T ) regret with top 1 feedback, i.e., the minimax regret of an online game for any NDCG calibrated surrogate is Ω(T ). The proof for this result relies on exploiting a connection between the construction of optimal adversary strategies for hopeless finite action partial monitoring games (Piccolboni and Schindelhauer, 2001b) and the structure of NDCG calibrated surrogates. We only focus on NDCG calibrated surrogates for the impossibility results since no (convex) surrogate can be calibrated for AP and ERR (Calauzenes et al., 2012). This impossibility result is the first of its kind for a natural partial monitoring problem with side information when the learner’s action space is infinite. Note, however, that there does exist work on partial monitoring problems with continuous learner actions, but without side information (Kleinberg and Leighton, 2003; Cesa-Bianchi, 2006), and vice versa (Bartók and Szepesvári, 2012; Gentile and Orabona, 2014).\nWe apply our algorithms on benchmark ranking datasets, demonstrating the ability to efficiently learn a ranking function in an online fashion, from highly restricted feedback.\nThe rest of the paper is divided into the following sections. Section 2 and its subsections detail the notations, definitions and technicalities associated with online ranking with restricted feedback in the non-contextual setting. Section 3 and its subsections detail the notations, definitions and technicalities associated with online ranking with restricted feedback in the contextual setting. Section 4 demonstrates the performance of our algorithms on simulated and commercial datasets. Section 5 discusses open questions and future directions of research."
    }, {
      "heading" : "2. Online Ranking with Restricted Feedback- Non Contextual Setting",
      "text" : "All proofs not in main text are in Appendix A."
    }, {
      "heading" : "2.1 Notation and Preliminaries",
      "text" : "The fixed m items to be ranked are numbered {1, 2, . . . ,m}. A permutation σ gives a mapping from ranks to items and its inverse σ−1 gives a mapping from items to ranks. Thus, σ−1(i) = j means item i is placed at position j while σ(i) = j means item j is placed at position i. The supervision is in form of a relevance vector R = {0, 1, . . . , n}m, representing relevance of each document to the query. If n = 1, the relevance vector is binary graded. For n > 1, relevance vector is multi-graded. R(i) denotes ith component of R. The subscript t is exclusively used to denote time t. We denote {1, . . . , n} by [n]. The learner can choose from m! actions (permutations) whereas nature/adversary can choose from 2m outcomes (when relevance levels are restricted to binary) or from nm outcomes (when there are n relevance levels, n > 2). We sometimes refer to the learner’s ith action (in some fixed ordering of m! available actions) as σi (resp. adversary’s ith action as Ri). Note that σ−1i simply means that we are viewing permutation σi as mapping from items to ranks. Also, a vector can be row or column vector depending on context.\nAt round t, the learner outputs a permutation (ranking) σt of the objects (possibly using some internal randomization, based on feedback history so far), and simultaneously, adversary generates relevance vector Rt. The quality of σt is judged against Rt by some ranking measure RL. Crucially, only the relevance of the top ranked object, i.e., Rt(σt(1)), is revealed to the learner at end of round t. Thus, the learner gets to know neither Rt (full information problem) nor RL(σt, Rt) (bandit problem). The objective of the learner is to minimize the expected regret with respect to best permutation in hindsight:\nEσ1,...,σT [ T∑ t=1 RL(σt, Rt) ] −min σ T∑ t=1 RL(σ,Rt). (1)\nWhen RL is a gain, not loss, we need to negate the quantity above. The worst-case regret of a learner strategy is its maximal regret over all possible choices of R1, . . . , RT . The minimax regret is the minimal worst-case regret over all learner strategies."
    }, {
      "heading" : "2.2 Ranking Measures",
      "text" : "We consider ranking measures which can be expressed in the form f(σ) · R, where the function f : Rm → Rm is composed of m copies of a univariate, monotonic, scalar valued function. Thus, f(σ) = [fs(σ−1(1)), fs(σ−1(2)), . . . , f s(σ−1(m))], where fs : R → R. Monotonic (increasing) means fs(σ−1(i)) ≥ fs(σ−1(j)), whenever σ−1(i) > σ−1(j). Monotonic (decreasing) is defined similarly. The following popular ranking measures can be expressed in the form f(σ) · r.\nPairwiseLoss & SumLoss: PairwiseLoss is restricted to binary relevance vectors and defined as:\nPL(σ,R) = m∑ i=1 m∑ j=1 1(σ−1(i) < σ−1(j))1(R(i) < R(j))\nPairwiseLoss cannot be directly expressed in the form of f(σ) · R. Instead, we consider SumLoss, defined as:\nSumLoss(σ,R) = m∑ i=1 σ−1(i) R(i)\nSumLoss has the form f(σ) ·R, where f(σ) = σ−1. It has been shown by Ailon (2014) that regret under the two measures are equal:\nT∑ t=1 PL(σt, Rt)− T∑ t=1 PL(σ,Rt) = T∑ t=1 SumLoss(σt, Rt)− T∑ t=1 SumLoss(σ,Rt). (2)\nDiscounted Cumulative Gain: DCG is a gain function which admits non-binary relevance vectors and is defined as:\nDCG(σ,R) = m∑ i=1 2R(i) − 1 log2(1 + σ −1(i))\nand becomes ∑m\ni=1 R(i) log2(1+σ −1(i)) for R(i) ∈ {0, 1}. Thus, for binary relevance, DCG(σ,R)\nhas the form f(σ) ·R, where f(σ) = [ 1 log2(1+σ −1(1)) , 1 log2(1+σ −1(2)) , . . . , 1 log2(1+σ −1(m)) ].\nPrecision@n Gain: Precision@n is a gain function restricted to binary relevance and is defined as\nPrecision@n(σ,R) = m∑ i=1 1(σ−1(i) ≤ n) R(i)\nPrecision@n can be written as f(σ) ·R where f(σ) = [1(σ−1(1) < n), . . . ,1(σ−1(m) < n)]. It should be noted that for n = k (i.e., when feedback is on top n items), feedback is actually the same as full information feedback, for which efficient algorithms already exist.\nNormalized measures are not of the form f(σ) ·R: PairwiseLoss, DCG and Precision@n are unnormalized versions of popular ranking measures, namely, Area Under Curve (AUC), Normalized Discounted Cumulative Gain (NDCG) and Average Precision (AP) respectively. None of these can be expressed in the form f(σ) ·R.\nNDCG: NDCG is a gain function, admits non-binary relevance and is defined as:\nNDCG(σ,R) = 1\nZ(R) m∑ i=1 2R(i) − 1 log2(1 + σ −1(i))\nand becomes 1Z(R) ∑m i=1 R(i) log2(1+σ −1(i)) forR(i) ∈ {0, 1}. Here Z(R) = maxσ ∑m i=1 2R(i)−1 log2(1+σ −1(i)) is the normalizing factor (Z(R) = max σ ∑m i=1 R(i) log2(1+σ −1(i)) for binary relevance). It can be clearly seen that NDCG(σ,R) = f(σ)·g(R), where f(σ) is same as in DCG but g(R) = RZ(R) is non-linear in R.\nAP: AP is a gain function, restricted to binary relevance and is defined as:\nAP (σ,R) = 1\n‖R‖1 m∑ i=1\n∑ j≤i 1(R(σ(j)) = 1)\ni 1(R(σ(i) = 1)\nIt can be clearly seen that AP cannot be expressed in the form f(σ) ·R. AUC: AUC is a loss function, restricted to binary relevance and is defined as:\nAUC(σ,R) = 1\nN(R) m∑ i=1 m∑ j=1 1(σ−1(i) < σ−1(j))1(R(i) < R(j))\nwhere N(R) = ( ∑m i=1 1(R(i) = 1)) · (m − ∑m\ni=1 1(R(i) = 1)). It can be clearly seen that AUC cannot be expressed in the form f(σ) ·R.\nNote: We will develop our subsequent theory and algorithms for binary valued relevance vectors, and show how they can be extended to multi-graded vectors when ranking measure is DCG/NDCG."
    }, {
      "heading" : "2.3 Relevant Definitions from Partial Monitoring",
      "text" : "We develop all results in context of SumLoss, with binary relevance vector. We then extend the results to other ranking measures. Our main results on regret bounds build on some of the theory for abstract partial monitoring games developed by Bartok et al. (2014) and Foster and Rakhlin (2012). For ease of understanding, we reproduce the relevant notations\nand definitions in context of SumLoss. We will specifically mention when we derive results for top k feedback, with general k, and when we restrict to top 1 feedback.\nLoss and Feedback Matrices: The online learning game with the SumLoss measure and top 1 feedback can be expressed in form of a pair of loss matrix and feedback matrix. The loss matrix L is an m! × 2m dimensional matrix, with rows indicating the learner’s actions (permutations) and columns representing adversary’s actions (relevance vectors). The entry in cell (i, j) of L indicates loss suffered when learner plays action i (i.e., σi) and adversary plays action j (i.e., Rj), that is, Li,j = σ −1 i · Rj = ∑m k=1 σ −1 i (k)Rj(k). The feedback matrix H has same dimension as loss matrix, with (i, j) entry being the relevance of top ranked object, i.e., Hi,j = Rj(σi(1)). When the learner plays action σi and adversary plays action Rj , the true loss is Li,j , while the feedback received is Hi,j .\nTable 1 and 2 illustrate the matrices, with number of objects m = 3. In both the tables, the permutations indicate rank of each object and relevance vector indicates relevance of each object. For example, σ5 = 312 means object 1 is ranked 3, object 2 is ranked 1 and object 3 is ranked 2. R5 = 100 means object 1 has relevance level 1 and other two objects have relevance level 0. Also, L3,4 = σ3 · R4 = ∑3 i=1 σ −1 3 (i)R4(i) = 2 · 0 + 1 · 1 + 3 · 1 = 4; H3,4 = R4(σ3(1)) = R4(2) = 1. Other entries are computed similarly.\nLet `i ∈ R2 m denote row i of L. Let ∆ be the probability simplex in R2m , i.e., ∆ = {p ∈ R2m : ∀ 1 ≤ i ≤ 2m, pi ≥ 0, ∑ pi = 1}. The following definitions, given for abstract problems by Bartok et al. (2014), has been refined to fit our problem context.\nDefinition 1: Learner action i is called optimal under distribution p ∈ ∆, if `i ·p ≤ `j ·p, for all other learner actions 1 ≤ j ≤ m!, j 6= i. For every action i ∈ [m!], probability cell of i is defined as Ci = {p ∈ ∆ : action i is optimal under p}. If a non-empty cell Ci is 2m − 1 dimensional (i.e, elements in Ci are defined by only 1 equality constraint), then associated action i is called Pareto-optimal.\nNote that since entries in H are relevance levels of objects, there can be maximum of 2 distinct elements in each row of H, i.e., 0 or 1 (assuming binary relevance).\nDefinition 2: The signal matrix Si, associated with learner’s action σi, is a matrix with 2 rows and 2m columns, with each entry 0 or 1, i.e., Si ∈ {0, 1}2×2 m . The entries of `th column of Si are respectively: (Si)1,` = 1(Hi,` = 0) and (Si)2,` = 1(Hi,` = 1).\nNote that by definitions of signal and feedback matrices, the 2nd row of Si (2nd column of S>i )) is precisely the ith row of H. The 1st row of Si (1st column of S > i )) is the (boolean) complement of ith row of H."
    }, {
      "heading" : "2.4 Minimax Regret for SumLoss",
      "text" : "The minimax regret for SumLoss, restricted to top 1 feedback, will be established by showing that: a) SumLoss satisfies global observability, and b) it does not satisfy local observability."
    }, {
      "heading" : "2.4.1 Global Observability",
      "text" : "Definition 3: The condition of global observability holds, w.r.t. loss matrix L and feedback matrixH, if for every pair of learner’s actions {σi, σj}, it is true that `i−`j ∈ ⊕k∈[m!]Col(S>k ) (where Col refers to column space).\nThe global observability condition states that the (vector) loss difference between any pair of learner’s actions has to belong to the vector space spanned by columns of (transposed) signal matrices corresponding to all possible learner’s actions. We derive the following theorem on global observability for SumLoss.\nTheorem 1. The global observability condition, as per Definition 3, holds w.r.t. loss matrix L and feedback matrix H defined for SumLoss, for any m ≥ 1.\nProof. For any σa (learner’s action) and Rb (adversary’s action), we have\nLa,b = σ −1 a ·Rb = m∑ i=1 σ−1a (i)Rb(i) 1 = m∑ j=1 j Rb(σa(j)) 2 = m∑ j=1 j Rb(σ̃j(a)(1)) 3 = m∑ j=1 j (S>σ̃j(a))Rb,2.\nThus, we have\n`a = [La,1, La,2, . . . , La,2m ] = [ m∑ j=1 j (S>σ̃j(a))R1,2, m∑ j=1 j (S>σ̃j(a))R2,2, .., m∑ j=1 j (S>σ̃j(a))R2m ,2] 4 = m∑ j=1 j (S>σ̃j(a)):,2.\nEquality 4 shows that `a is in the column span of m of the m! possible (transposed) signal matrices, specifically in the span of the 2nd columns of those (transposed) m matrices. Hence, for all actions σa, it is holds that `a ∈ ⊕k∈[m!]Col(S>k ). This implies that\n`a − `b ∈ ⊕k∈[m!]Col(S>k ), ∀ σa, σb.\n1. Equality 1 holds because σ−1a (i) = j ⇒ i = σa(j). 2. Equality 2 holds because of the following reason. For any permutation σa and for every j ∈ [m], ∃ a permutation σ̃j(a), s.t. the object which is assigned rank j by σa is the same object assigned rank 1 by σ̃j(a), i.e., σa(j) = σ̃j(a)(1).\n3. In Equality 3, (S> σ̃−1 j(a) )Rb,2 indicates the Rbth row and 2nd column of (transposed) signal matrix Sσ̃j(a) , corresponding to learner action σ̃j(a). Equality 3 holds becauseRb(σ̃j(a)(1)) is the entry in the row corresponding to action σ̃j(a) and column corresponding to action Rb of H (see Definition 2).\n4. Equality 4 holds from the observation that for a particular j, [(S>σ̃j(a))R1,2, (S > σ̃j(a) )R2,2, . . . ,\n(S>σ̃j(a))R2m ,2] forms the 2nd column of (S > σ̃j(a) ), i.e., (S>σ̃j(a)):,2."
    }, {
      "heading" : "2.4.2 Local Observability",
      "text" : "Definition 4: Two Pareto-optimal (learner’s) actions i and j are called neighboring actions if Ci ∩ Cj is a (2m − 2) dimensional polytope (where Ci is probability cell of action σi). The neighborhood action set of two neighboring (learner’s) actions i and j is defined as N+i,j = {k ∈ [m!] : Ci ∩ Cj ⊆ Ck}.\nDefinition 5: A pair of neighboring (learner’s) actions i and j is said to be locally observable if `i − `j ∈ ⊕k∈N+i,jCol(S > k ). The condition of local observability holds if every pair of neighboring (learner’s) actions is locally observable. We now show that local observability condition fails for L,H under SumLoss. First, we present the following two lemmas characterizing Pareto-optimal actions and neighboring actions for SumLoss.\nLemma 2. For SumLoss, each of learner’s action σi is Pareto-optimal, where Paretooptimality has been defined in Definition 1.\nProof. For any p ∈ ∆, we have `i ·p = ∑2m j=1 pj (σ −1 i ·Rj) = σ −1 i · ( ∑2m j=1 pjRj) = σ −1 i ·E[R], where the expectation is taken w.r.t. p. By dot product rule between 2 vectors, li · p is minimized when ranking of objects according to σi and expected relevance of objects are in opposite order. That is, the object with highest expected relevance is ranked 1 and so on. Formally, li · p is minimized when E[R(σi(1))] ≥ E[R(σi(2))] ≥ . . . ≥ E[R(σi(m))].\nThus, for action σi, probability cell is defined as Ci = {p ∈ ∆ : ∑2m\nj=1 pj = 1, E[R(σi(1))] ≥ E[R(σi(2))] ≥ . . . ≥ E[R(σi(m))]}. Note that, p ∈ Ci iff action i is optimal w.r.t. p. Since Ci is obviously non-empty and it has only 1 equality constraint (hence 2\nm−1 dimensional), action i is Pareto optimal.\nThe above holds true for all learner’s actions σi.\nLemma 3. A pair of learner’s actions {σi, σj} is a neighboring actions pair, if there is exactly one pair of objects, numbered {a, b}, whose positions differ in σi and σj. Moreover, a needs to be placed just before b in σi and b needs to placed just before a in σj.\nProof. From Lemma 2, we know that every one of learner’s actions is Pareto-optimal and Ci, associated with action σi, has structure Ci = {p ∈ ∆ : ∑2m j=1 pj = 1, E[R(σi(1))] ≥ E[R(σi(2))] ≥ . . . ≥ E[R(σi(m))]}.\nLet σi(k) = a, σi(k + 1) = b. Let it also be true that σj(k) = b, σj(k + 1) = a and σi(n) = σj(n), ∀n 6= {k, k + 1}. Thus, objects in {σi, σj} are same in all places except in a pair of consecutive places where the objects are interchanged.\nThen, Ci ∩ Cj = {p ∈ ∆ : ∑2m\nj=1 pj = 1, E[R(σi(1)] ≥ . . . ≥ E[R(σi(k)] = E[R(σi(k + 1)] ≥ . . . ≥ E[R(σi(m)]}. Hence, there are two equalities in the non-empty set Ci ∩ Cj and it is an (2m − 2) dimensional polytope. Hence condition of Definition 4 holds true and {σi, σj} are neighboring actions pair.\nLemma 2 and 3 lead to following result.\nTheorem 4. The local observability condition, as per Definition 5, fails w.r.t. loss matrix L and feedback matrix H defined for SumLoss, already at m = 3."
    }, {
      "heading" : "2.5 Minimax Regret Bound",
      "text" : "We establish the minimax regret for SumLoss by combining results on global and local observability. First, we get a lower bound by combining our Theorem 4 with Theorem 4 of Bartok et al. (2014).\nCorollary 5. Consider the online game for SumLoss with top-1 feedback and m = 3. Then, for every learner’s algorithm, there is an adversary strategy generating relevance vectors, such that the expected regret of the learner is Ω(T 2/3).\nThe fact that the game is globally observable ( Theorem 1), combined with Theorem 3.1 in Cesa-Bianchi (2006), gives an algorithm (inspired by the algorithm originally given in Piccolboni and Schindelhauer (2001a)) obtaining O(T 2/3) regret.\nCorollary 6. The algorithm in Figure 1 of Cesa-Bianchi (2006) achieves O(T 2/3) regret bound for SumLoss.\nHowever, the algorithm in Cesa-Bianchi (2006) is intractable in our setting since the algorithm necessarily enumerates all the actions of the learner in each round, which is exponential in m in our case (m! to be exact). Moreover, the regret bound of the algorithm also has a linear dependence on the number of actions, which renders the bound useless.\nDiscussion: The results above establish that the minimax regret for SumLoss, restricted to top-1 feedback, is Θ(T 2/3). Theorem 4 of Bartok et al. (2014) says the following: A partial monitoring game which is both globally and locally observable has minimax regret Θ(T 1/2), while a game which is globally observable but not locally observable has minimax regret Θ(T 2/3). In Theorem 1, we proved global observability, when feedback is restricted to relevance of top ranked item. The global observability result automatically extends to feedback on top k items, for k > 1. This is because for top k feedback, with k > 1, the learner receives strictly greater information at end of each round than top 1 feedback (for example, the learner can just throw away relevance feedback on items ranked 2nd onwards). So, with top k feedback, for general k, the game will remain at least globally observable. In fact, our algorithm in the next section will achieve O(T 2/3) regret bound for SumLoss with top k feedback, k ≥ 1. However, the same is not true for failure of local observability.\nFeedback on more than top ranked item can make the game strictly easier for the learner and may make local observability condition hold, for some k > 1. In fact, for k = m (full feedback), the game will be a simple bandit game (disregarding computational complexity), and hence locally observable."
    }, {
      "heading" : "2.6 Algorithm for Obtaining Minimax Regret under SumLoss with Top k Feedback",
      "text" : "We first provide a general algorithmic framework for getting an O(T 2/3) regret bound for SumLoss, with feedback on top k ranked items per round, for k ≥ 1. We then instantiate a specific algorithm, which spends O(m logm) time per round (thus, highly efficient) and obtains a regret of rate O(poly(m) T 2/3)."
    }, {
      "heading" : "2.6.1 General Algorithmic Framework",
      "text" : "Our algorithm combines blocking with a randomized full information algorithm. We first divide time horizon T into blocks (referred to as blocking). Within each block, we allot a small number of rounds for pure exploration, which allows us to estimate the average of the full relevance vectors generated by the adversary in that block. The estimated average vector is cumulated over blocks and then fed to a full information algorithm for the next block. The randomized full information algorithm exploits the information received at the beginning of the block to maintain distribution over permutations (learner’s actions). In each round in the new block, actions are chosen according to the distribution and presented to the user.\nThe key property of the randomized full information algorithm is this: for any online game in an adversarial setting played over T rounds, if the loss of each action is known at end of each round (full information), the algorithm should have an expected regret rate of O(C √ T ), where the regret is the difference between cumulative loss of the algorithm and cumulative loss of best action in hindsight, and C is a parameter specific to the full information algorithm.\nOur algorithm is motivated by the reduction from bandit-feedback to full feedback scheme given in Blum and Mansour (2007). However, the reduction cannot be directly applied to our problem, because we are not in the bandit setting and hence do not know loss of any action. Further, the algorithm of Blum and Mansour (2007) necessarily spends N rounds per block to try out each of the N available actions — this is impractical in our setting since N = m!.\nAlgorithm 1 describes our approach. A key aspect is the formation of estimate of average relevance vector of a block (line 16), for which we have the following lemma:\nLemma 7. Let the average of (full) relevance vectors over the time period {1, 2, . . . , t} be denoted as Ravg1:t , that is, R avg 1:t = ∑t n=1\nRn t ∈ Rm. Let {i1, i2, . . . , idm/ke} be dm/ke arbitrary\ntime points, chosen uniformly at random, without replacement, from {1, . . . , t}. At time point ij, only k distinct components of relevance vector Rij , i.e., {Rij (k · (j−1) + 1), Rij (k · (j − 1) + 2), . . . , Rij (k · j)}, becomes known, ∀j ∈ {1, . . . , dm/ke} (for j = dm/ke, there might be less than k components available). Then the vector formed from the m revealed\ncomponents, i.e. R̂t = [Rij (k · (j − 1) + 1), Rij (k · (j − 1) + 2), . . . , Rij (k · j)]{j=1,2,...,dm/ke} is an unbiased estimator of Ravg1:t .\nProof. We can write R̂t = ∑dm/ke\nj=1 ∑k `=1Rij (k · (j − 1) + `)ek·(j−1)+`, where ei is the m\ndimensional standard basis vector along coordinate j. Then, taking expectation over the randomly chosen time points, we have: Ei1,...,idm/ke(R̂t) = ∑dm/ke j=1 Eij [ ∑k `=1Rij (k · (j− 1) +\n`)ek·(j−1)+`] = ∑m\nj=1 ∑k `=1 ∑t n=1 Rn(k · (j − 1) + `)ek·(j−1)+` t = Ravg1:t .\nSuppose we have a full information algorithm whose regret in t rounds is upper bounded by C √ T for some constant C and let CI be the maximum loss that the learner can suffer in a round. Note that CI depends on the loss used and on the range of the relevance scores. We have the following regret bound, obtained from application of Algorithm 1 on SumLoss with top k feedback.\nTheorem 8. Let C,CI be the constants defined above. The expected regret under SumLoss, obtained by applying Algorithm 1, with relevance feedback on top k ranked items per round (k ≥ 1), and the expectation being taken over randomized learner’s actions σt, is\nE [ T∑ t=1 SumLoss(σt, Rt) ] −min σ T∑ t=1 SumLoss(σt, Rt) ≤ CIdm/keK + C T√ K . (3)\nOptimizing over block size K, the final regret bound is:\nE [ T∑ t=1 SumLoss(σt, Rt) ] −min σ T∑ t=1 SumLoss(σt, Rt) ≤ 2(CI)1/3C2/3dm/ke1/3T 2/3. (4)"
    }, {
      "heading" : "2.6.2 Computationally Efficient Algorithm with FTPL",
      "text" : "We instantiate our general algorithm with Follow The Perturbed Leader (FTPL) full information algorithm (Kalai and Vempala, 2005). The following modifications are needed in Algorithm 1 to implement FTPL as the full information algorithm:\nInitialization of parameters: In line 3 of the algorithm, the parameter specific to FTPL is randomization parameter ∈ R.\nExploitation round: σt, during exploitation, is sampled by FTPL as follows: sample pt ∈ [0, 1/ ]m from the product of uniform distribution in each dimension. Output permutation σt = M(ŝi−1 + pt) where M(y) = argmin\nσ σ−1 · y.\nDiscussion: The key reason for using FTPL as the full information algorithm is that the structure of our problem allows the permutation σt to be chosen during exploitation round via a simple sorting operation on m objects. This leads to an easily implementable algorithm which spends only O(m logm) time per round (sorting is in fact the most expensive step in the algorithm). The reason that the simple sorting operation does the trick is the following: FTPL only implicitly maintains a distribution over m! actions (permutations) at beginning\n1For e.g., assume m = 7 and k = 2. Then place items (1, 2) in cell 1, items (3, 4) in cell 2, items (5, 6) in cell 3 and item 7 in cell 4.\nAlgorithm 1 RankingwithTop-kFeedback(RTop-kF)- Non Contextual\n1: T = Time horizon, K = No. of (equal sized) blocks, FI= randomized full information algorithm. 2: Time horizon divided into equal sized blocks {B1, . . . , BK}, where Bi = {(i− 1)(T/K) + 1, . . . , i(T/K)}. 3: Initialize ŝ0 = 0 ∈ Rm. Initialize any other parameter specific to FI. 4: For i = 1, . . . ,K 5: Select dm/ke time points {i1, . . . , idm/ke} from block Bi, uniformly at random, without replacement. 6: Divide the m items into dm/ke cells, with k distinct items in each cell. 1 7: For t ∈ Bi 8: If t = ij ∈ {i1, . . . , idm/ke} 9: Exploration round: 10: Output any permutation σt which places items of jth cell in top k positions (in any order). 11: Receive feedback as relevance of top k items of σt (i.e., items of jth cell). 12: Else 13: Exploitation round: 14: Feed ŝi−1 to the randomized full information algorithm FI and output σt according to FI. 15: end for 16: Set R̂i ∈ Rm as vector of relevances of the m items collected during exploration rounds. 17: Update ŝi = ŝi−1 + R̂i. 18: end for\nof each round. Instead of having an explicit probability distribution over each action and sampling from it, FTPL mimics sampling from a distribution over actions by randomly perturbing the information vector received so far (say ŝi−1 in block Bi) and then sorting the items by perturbed score. The random perturbation puts an implicit weight on each of the m! actions and sorting is basically sampling according to the weights. This is an advantage over general full information algorithms based on exponential weights, which maintain explicit weight on actions and samples from it.\nWe have the following corollary:\nCorollary 9. The expected regret of SumLoss, obtained by applying Algorithm 1, with FTPL full information algorithm and feedback on top k ranked items at end of each round (k ≥ 1),\nand K = O\n( m1/3T 2/3\ndm/ke2/3\n) , = O( 1√\nmK ), is:\nE [ T∑ t=1 SumLoss(σt, Rt) ] −min σ T∑ t=1 SumLoss(σt, Rt) ≤ O(m7/3dm/ke1/3T 2/3). (5)\nwhere O(·) hides some numeric constants.\nAssuming that dm/ke ∼ m/k, the regret rate in Corollary 9 is O\n( m8/3T 2/3\nk1/3\n)"
    }, {
      "heading" : "2.7 Regret Bounds for PairwiseLoss, DCG and Precision@n",
      "text" : "PairwiseLoss: As we saw in Eq. 2, the regret of SumLoss is same as regret of PairwiseLoss. Thus, SumLoss in Corollary 9 can be replaced by PairwiseLoss to get exactly same result.\nDCG: All the results of SumLoss can be extended to DCG (see Appendix A). Moreover, the results can be extended even for multi-graded relevance vectors. Thus, the minimax regret under DCG, restricted to feedback on top ranked item, even when the adversary can play multi-graded relevance vectors, is Θ(T 2/3).\nThe main differences between SumLoss and DCG are the following. The former is a loss function; the latter is a gain function. Also, for DCG, f(σ) 6= σ−1 (see definition in Sec.2.2 ) and when relevance is multi-graded, DCG cannot be expressed as f(σ) · R, as clear from definition. Nevertheless, DCG can be expressed as f(σ) · g(R), , where g(R) = [gs(R(1)), gs(R(2)), . . . , gs(R(m))], gs(i) = 2i−1 is constructed from univariate, monotonic, scalar valued functions (g(R) = R for binary graded relevance vectors). Thus, Algorithm 1 can be applied (with slight variation), with FTPL full information algorithm and top k feedback, to achieve regret of O(T 2/3). The slight variation is that during exploration rounds, when relevance feedback is collected to form the estimator at end of the block, the relevances should be transformed by function gs(·). The estimate is then constructed in the transformed space and fed to the full information algorithm. In the exploitation round, the selection of σt remains exactly same as in SumLoss, i.e., σt = M(ŝi−1 + pt) where M(y) = argmin\nσ σ−1 · y. This is because argmax σ f(σ) · y = argmin σ σ−1 · y, by definition of\nf(σ) in DCG. Let relevance vectors chosen by adversary have n+ 1 grades, i.e., R ∈ {0, 1, . . . , n}m. In practice, n is almost always less than 5. We have the following corollary:\nCorollary 10. The expected regret of DCG, obtained by applying Algorithm 1, with FTPL full information algorithm and feedback on top k ranked items at end of each round (k ≥ 1),\nand K = O\n( m1/3T 2/3\ndm/ke2/3\n) , = O( 1\n(2n−1)2 √ mK ), is:\nmax σ T∑ t=1 DCG(σt, Rt)− E [ T∑ t=1 DCG(σt, Rt) ] ≤ O((2n − 1)m4/3dm/ke1/3T 2/3). (6)\nAssuming that dm/ke ∼ m/k, the regret rate in Corollary 10 is O\n( (2n − 1)m5/3T 2/3\nk1/3\n) .\nPrecision@n: Since Precision@n = f(σ) ·R, the global observability property of SumLoss can be easily extended to it and Algorithm 1 can be applied, with FTPL full information algorithm and top k feedback, to achieve regret of O(T 2/3). In the exploitation round, the selection of σt remains exactly same as in SumLoss, i.e., σt = M(ŝi−1 + pt) where M(y) = argmin\nσ σ−1 · y.\nHowever, the local observability property of SumLoss does not extend to Precision@n. The reason is that while f(·) of SumLoss is strictly monotonic, f(·) of Precision@n is monotonic but not strict. Precision@n depends only on the objects in the top n positions of the ranked list, irrespective of the order. A careful review shows that Lemma 3 fails to extend to the case of Precision@n, due to lack of strict monotonicity. Thus, we cannot define the neighboring action set of the Pareto optimal action pairs, and hence cannot prove or disprove local observability.\nWe have the following corollary:\nCorollary 11. The expected regret of Precision@n, obtained by applying Algorithm 1, with FTPL full information algorithm and feedback on top k ranked items at end of each round\n(k ≥ 1), and K = O\n( m1/3T 2/3\ndm/ke2/3\n) , = O( 1√\nmK ), is:\nmax σ T∑ t=1 Precision@n(σt, Rt)− E [ T∑ t=1 Precision@n(σt, Rt) ] ≤ O(n m1/3dm/ke1/3T 2/3).\n(7)\nAssuming that dm/ke ∼ m/k, the regret rate in Corollary 11 is O\n( n m2/3T 2/3\nk1/3\n) ."
    }, {
      "heading" : "2.8 Non-Existence of Sublinear Regret Bounds for NDCG, AP and AUC",
      "text" : "As stated in Sec. 2.2, NDCG, AP and AUC are normalized versions of measures DCG, Precision@n and PairwiseLoss. We have the following lemma for all these normalized ranking measures.\nLemma 12. The global observability condition, as per Definition 1, fails for NDCG, AP and AUC, when feedback is restricted to top ranked item.\nCombining the above lemma with Theorem 2 of Bartok et al. (2014), we conclude that there cannot exist any algorithm which has sub-linear regret for any of the following measures: NDCG, AP or AUC, when restricted to top 1 feedback.\nTheorem 13. There exists an online game, for NDCG with top-1 feedback, such that for every learner’s algorithm, there is an adversary strategy generating relevance vectors, such that the expected regret of the learner is Ω(T ). Furthermore, the same lower bound holds if NDCG is replaced by AP or AUC."
    }, {
      "heading" : "3. Online Ranking with Restricted Feedback- Contextual Setting",
      "text" : "All proofs not in the main text are in Appendix B."
    }, {
      "heading" : "3.1 Problem Setting and Learning to Rank Algorithm",
      "text" : "First, we introduce some additional notations to Section 2.1. In the contextual setting, each query and associated items (documents) are represented jointly as a feature matrix. Each feature matrix, X ∈ Rm×d, consists of a list of m documents, each represented as a feature vector in Rd. The feature matrices are considered side-information (context) and represents varying items, as opposed to the fixed set of items in the first part of our work. Xi: denotes ith row of X. We assume feature vectors representing documents are bounded by RD in `2 norm. The relevance vectors are same as before.\nAs per traditional learning to rank setting with query-document matrices, documents are ranked by a ranking function. The prevalent technique is to represent a ranking function as a scoring function and get ranking by sorting scores in descending order. A linear scoring function produces score vector as fw(X) = Xw = s w ∈ Rm, with w ∈ Rd. Here, sw(i)\nrepresents score of ith document (sw points to score s being generated by using parameter w). We assume that ranking parameter space is bounded in `2 norm, i.e, ‖w‖2 ≤ U , ∀ w. πs = argsort(s) is the permutation induced by sorting score vector s in descending order. As a reminder, a permutation π gives a mapping from ranks to documents and π−1 gives a mapping from documents to ranks.\nPerformance of ranking functions are judged, based on the rankings obtained from score vectors, by ranking measures like DCG, AP and others. However, the measures themselves are discontinuous in the score vector produced by the ranking function, leading to intractable optimization problems. Thus, most learning to rank methods are based on minimizing surrogate losses, which can be optimized efficiently. A surrogate φ takes in a score vector s and relevance vector R and produces a real number, i.e., φ : Rm × {0, 1, . . . , n}m 7→ R. φ(·, ·) is said to be convex if it is convex in its first argument, for any value of the second argument. Ranking surrogates are designed in such a way that the ranking function learnt by optimizing the surrogates has good performance with respect to ranking measures.\nFormal problem setting: We formalize the problem as a game being played between a learner and an oblivious adversary over T rounds (i.e., an adversary who generates moves without knowledge of the learner’s algorithm). The learner’s action set is the uncountably infinite set of score vectors in Rm and the adversary’s action set is all possible relevance vectors, i.e., (n+1)m possible vectors. At round t, the adversary generates a list of documents, represented by a matrix Xt ∈ Rm×d, pertaining to a query (the document list is considered as side information). The learner receives Xt, produces a score vector s̃t ∈ Rm and ranks the documents by sorting according to score vector. The adversary then generates a relevance vector Rt but only reveals the relevances of top k ranked documents to the learner. The learner uses the feedback to choose its action for the next round (updates an internal scoring function). The learner suffers a loss as measured in terms of a surrogate φ, i.e, φ(s̃t, Rt). As is standard in online learning setting, the learner’s performance is measured in terms of its expected regret:\nE [ T∑ t=1 φ(s̃t, Rt) ] − min ‖w‖2≤U T∑ t=1 φ(Xtw,Rt),\nwhere the expectation is taken w.r.t. to randomization of learner’s strategy and Xtw = s w t is the score produced by the linear function parameterized by w.\nRelation between feedback and structure of surrogates: Algorithm 2 is our general algorithm for learning a ranking function, online, from partial feedback. The key step in Algorithm 2 is the construction of the unbiased estimator z̃t of the surrogate gradient ∇w=wtφ(Xtw,Rt). The information present for the construction process, at end of round t, is the random score vector s̃t (and associated permutation σ̃t) and relevance of top-k items of σ̃t, i.e., {Rt(σ̃t(1)), . . . , Rt(σ̃t(k)}. Let Et [·] be the expectation operator w.r.t. to randomization at round t, conditioned on (w1, . . . , wt). Then z̃t being an unbiased estimator of gradient of surrogate, w.r.t wt, means the following: Et [z̃t] = ∇w=wtφ(Xtw,Rt). We note that conditioned on the past, the score vector swtt = Xtwt is deterministic. We start with a general result relating feedback to the construction of unbiased estimator of a vector valued function. Let P denote a probability distribution on Sm, i.e, ∑ σ∈Sm P(σ) = 1. For a distinct set of indices (j1, j2, . . . , jk) ⊆ [m], we denote p(ji, j2, . . . , jk) as the the sum of probability\nAlgorithm 2 Ranking with Top-k Feedback (RTop-kF)- Contextual 1: Exploration parameter γ ∈ (0, 12), learning parameter η > 0, ranking parameter w1 = 0 ∈ R d 2: For t = 1 to T 3: Receive Xt (document list pertaining to query qt) 4: Construct score vector swtt = Xtwt and get permutation σt = argsort(s wt t ) 5: Qt(s) = (1− γ)δ(s− swtt ) + γUniform([0, 1]m) (δ is the Dirac Delta function). 6: Sample s̃t ∼ Qt and output the ranked list σ̃t = argsort(s̃t) (Effectively, it means σ̃t is drawn from Pt(σ) = (1− γ)1(σ = σt) + γm!) 7: Receive relevance feedback on top-k items, i.e., (Rt(σ̃t(1)), . . . , Rt(σ̃t(k))) 8: Suffer loss φ(s̃t, Rt) (Neither loss nor Rt revealed to learner) 9: Construct z̃t, an unbiased estimator of gradient ∇w=wtφ(Xtw,Rt), from top-k feedback. 10: Update w = wt − ηz̃t 11: wt+1 = min{1, U‖w‖2 }w (Projection onto Euclidean ball of radius U). 12: End For\nof permutations whose first k objects match objects (j1, . . . , jk), in order. Formally,\np(j1, . . . , jk) = ∑ π∈Sm P(π)1(π(1) = j1, . . . , π(k) = jk). (8)\nWe have the following lemma relating feedback and structure of surrogates:\nLemma 14. Let F : Rm 7→ Ra be a vector valued function, where m ≥ 1, a ≥ 1. For a fixed x ∈ Rm, let k entries of x be observed at random. That is, for a fixed probability distribution P and some random σ ∼ P(Sm), observed tuple is {σ, xσ(1), . . . , xσ(k)}. A necessary condition for existence of an unbiased estimator of F (x), that can be constructed from {σ, xσ(1), . . . , xσ(k)}, is that it should be possible to decompose F (x) over k (or less) coordinates of x at a time. That is, F (x) should have the structure:\nF (x) = ∑\n(i1,i2,...,i`)∈ mP`\nhi1,i2,...,i`(xi1 , xi2 , . . . , xi`) (9)\nwhere ` ≤ k, mP` is ` permutations of m and h : R` 7→ Ra (the subscripts in h are used to denote possibly different functions in the decomposition structure). Moreover, when F (x) can be written in form of Eq 9 , with ` = k, an unbiased estimator of F (x), based on {σ, xσ(1), . . . , xσ(k)}, is,\ng(σ,xσ(1), . . . , xσ(k)) =∑ (j1,j2,...,jk)∈Sk\nhσ(j1),...,σ(jk)(xσ(j1), . . . , xσ(jk))∑ (j1,...,jk)∈Sk p(σ(j1), . . . , σ(jk))\n(10)\nwhere Sk is the set of k! permutations of [k] and p(σ(1), . . . , σ(k)) is as in Eq 8 .\nIllustrative Examples: We provide simple examples to concretely illustrate the abstract functions in Lemma 14. Let F (·) be the identity function, and x ∈ Rm. Thus, F (x) = x and the function decomposes over k = 1 coordinate of x as follows: F (x) = ∑m i=1 xiei, where ei ∈ Rm is the standard basis vector along coordinate i. Hence, hi(xi) = xiei. Based on top-1 feedback, following is an unbiased estimator of F (x): g(σ, xσ(1)) = xσ(1)eσ(1)\np(σ(1)) , where p(σ(1)) = ∑\nπ∈Sm P(π)1(π(1) = σ(1)). In another example, let F : R3 7→ R2 and\nx ∈ R3. Let F (x) = [x1 + x2;x2 + x3]>. Then the function decomposes over k = 1 coordinate of x as F (x) = x1e1 + x2(e1 + e2) + x3e2, where ei ∈ R2. Hence, h1(x1) = x1e1, h2(x2) = x2(e1 + e2) and h3(x3) = x3e2. An unbiased estimator based on top-1 feedback is: g(σ, xσ(1)) = hσ(1)(xσ(1))\np(σ(1)) ."
    }, {
      "heading" : "3.2 Unbiased Estimators of Gradients of Surrogates",
      "text" : "Algorithm 2 can be implemented for any ranking surrogate as long as an unbiased estimator of the gradient can be constructed from the random feedback. We will use techniques from online convex optimization to obtain formal regret guarantees. We will thus construct the unbiased estimator of four major ranking surrogates. Three of them are popular convex surrogates, one each from the three major learning to rank methods, i.e., pointwise, pairwise and listwise methods. The fourth one is a popular non-convex surrogate.\nShorthand notations: We note that by chain rule,∇w=wtφ(Xtw,Rt)=X>t ∇swtt φ(s wt t , Rt),\nwhere swtt = Xtwt. Since Xt is deterministic in our setting, we focus on unbiased estimators of ∇swtt φ(s wt t , Rt) and take a matrix-vector product with Xt. To reduce notational clutter in our derivations, we drop w from sw and the subscript t throughout. Thus, in our derivations, z̃ = z̃t, X = Xt, s = s wt t (and not s̃t), σ = σ̃t (and not σt), R = Rt, ei is standard basis vector in Rm along coordinate i and p(·) as in Eq. 8 with P = Pt where Pt is the distribution in round t in Algorithm 2."
    }, {
      "heading" : "3.2.1 Convex Surrogates",
      "text" : "Pointwise Method: We will construct the unbiased estimator of the gradient of squared loss (Cossock and Zhang, 2006): φsq(s,R) = ‖s − R‖22. The gradient ∇sφsq(s,R) is 2(s − R) ∈ Rm. As we have already demonstrated in the example following Lemma 14, we can construct unbiased estimator of R from top-1 feedback ({σ,R(σ(1))}). Concretely, the unbiased estimator is:\nz̃ = X> ( 2 ( s− R(σ(1))eσ(1)\np(σ(1))\n)) .\nPairwise Method: We will construct the unbiased estimator of the gradient of hingelike surrogate in RankSVM (Joachims, 2002): φsvm(s,R) = ∑ i 6=j=1 1(R(i) > R(j)) max(0, 1+\ns(j)− s(i)). The gradient is given by ∇sφsvm(s,R) = ∑m\ni 6=j=1 1(R(i) > R(j))1(1 + s(j) > s(i))(ej − ei) ∈ Rm. Since s is a known quantity, from Lemma 14, we can construct F (R) as follows: F (R) = Fs(R) = ∑m i 6=j=1 hs,i,j(R(i), R(j)), where hs,i,j(R(i), R(j)) = 1(R(i) > R(j))1(1 + s(j) > s(i))(ej − ei). Since Fs(R) is decomposable over 2 coordinates of R at a time, we can construct an unbiased estimator from top-2 feedback ({σ,R(σ(1)), R(σ(2))}).\nThe unbiased estimator is: z̃ = X> ( hs,σ(1),σ(2)(R(σ(1)), R(σ(2))) + hs,σ(2),σ(1)(R(σ(2)), R(σ(1)))\np(σ(1), σ(2)) + p(σ(2), σ(1))\n) .\nWe note that the unbiased estimator was constructed from top-2 feedback. The following lemma, in conjunction with the necessary condition of Lemma 14 shows that it is the minimum information required to construct the unbiased estimator.\nLemma 15. The gradient of RankSVM surrogate, i.e., φsvm(s,R) cannot be decomposed over 1 coordinate of R at a time.\nListwise Method: Convex surrogates developed for listwise methods of learning to rank are defined over the entire score vector and relevance vector. Gradients of such surrogates cannot usually be decomposed over coordinates of the relevance vector. We will focus on the cross-entropy surrogate used in the highly cited ListNet (Cao et al., 2007) ranking algorithm and show how a very natural modification to the surrogate makes its gradient estimable in our partial feedback setting.\nThe authors of the ListNet method use a cross-entropy surrogate on two probability distributions on permutations, induced by score and relevance vector respectively. More formally, the surrogate is defined as follows2. Define m maps from Rm to R as: Pj(v) = exp(v(j))/ ∑m j=1 exp(v(j)) for j ∈ [m]. Then, for score vector s and relevance vector R,\nφLN(s,R) = − ∑m i=1 Pi(R) logPi(s) and∇sφLN(s,R) = ∑m i=1 ( − exp(R(i))∑m j=1 exp(R(j)) + exp(s(i))∑m j=1 exp(s(j)) ) ei. We have the following lemma about the gradient of φLN .\nLemma 16. The gradient of ListNet surrogate φLN (s,R) cannot be decomposed over k, for k = 1, 2, coordinates of R at a time.\nIn fact, an examination of the proof of the above lemma reveals that decomposability at any k < m does not hold for the gradient of LisNet surrogate, though we only prove it for k = 1, 2 (since feedback for top k items with k > 2 does not seem practical). Due to Lemma 14, this means that if we want to run Alg. 2 under top-k feedback, a modification of ListNet is needed. We now make such a modification.\nWe first note that the cross-entropy surrogate of ListNet can be easily obtained from a standard divergence, viz. Kullback-Liebler divergence. Let p, q ∈ Rm be 2 probability distributions ( ∑m i=1 pi = ∑m i=1 qi = 1). Then KL(p, q) = ∑m i=1 pi log(pi)− ∑m i=1 pi log(qi)−∑m\ni=1 pi + ∑m\ni=1 qi. Taking pi = Pi(R) and qi = Pi(s), ∀ i ∈ [m] (where Pi(v) is as defined in φLN) and noting that φLN(s,R) needs to be minimized w.r.t. s (thus we can ignore the∑m\ni=1 pi log(pi) term in KL(p, q)), we get the cross entropy surrogate from KL. Our natural modification now easily follows by considering KL divergence for un-normalized\nvectors (it should be noted that KL divergence is an instance of a Bregman divergence). Define m maps from Rm to R as: P ′j(v) = exp(v(j)) for j ∈ [m]. Now define pi = P ′i (R) and qi = P ′ i (s). Then, the modified surrogate φKL(s,R) is:\nm∑ i=1 eR(i) log(eR(i))− m∑ i=1 eR(i) log(es(i))− m∑ i=1 eR(i) + m∑ i=1 es(i),\n2The ListNet paper actually defines a family of losses based on probability models for top r documents, with r ≤ m. We use r = 1 in our definition since that is the version implemented in their experimental results.\nand m∑ i=1 (exp(s(i))− exp(R(i))) ei is its gradient w.r.t. s. Note that φKL(s,R) is nonnegative and convex in s. Equating gradient to 0 ∈ Rm, at the minimum point, s(i) = R(i), ∀ i ∈ [m]. Thus, the sorted order of optimal score vector agrees with sorted order of relevance vector and it is a valid ranking surrogate.\nNow, from Lemma 14, we can construct F (R) as follows: F (R) = Fs(R) = ∑m\ni=1 hs,i(R(i)), where hs,i(R(i)) = (exp(s(i))− exp(R(i))) ei. Since Fs(R) is decomposable over 1 coordinate of R at a time, we can construct an unbiased estimator from top-1 feedback ({σ,R(σ(1))}). The unbiased estimator is:\nz̃ = X> (\n(exp(s(σ(1)))− exp(R(σ(1))))eσ(1) p(σ(1)) ) Other Listwise Methods: As we mentioned before, most listwise convex surrogates will not be suitable for Algorithm 2 with top-k feedback. For example, the class of popular listwise surrogates that are developed from structured prediction perspective (Chapelle et al., 2007; Yue et al., 2007) cannot have unbiased estimator of gradients from top-k feedback since they are based on maps from full relevance vectors to full rankings and thus cannot be decomposed over k = 1 or 2 coordinates of R. It does not appear they have any natural modification to make them amenable to our approach."
    }, {
      "heading" : "3.2.2 Non-convex Surrogate",
      "text" : "We provide an example of a non-convex surrogate for which Alg. 2 is applicable (however it will not have any regret guarantees due to non-convexity). We choose the SmoothDCG surrogate given in (Chapelle and Wu, 2010), which has been shown to have very competitive empirical performance. SmoothDCG, like ListNet, defines a family of surrogates, based on the cut-off point of DCG (see original paper (Chapelle and Wu, 2010) for details). We consider SmoothDCG@1, which is the smooth version of DCG@1 (i.e., DCG which focuses just on the top-ranked document). The surrogate is defined as: φSD(s,R) =\n1∑m j=1 exp(s(j)/ ) ∑m i=1G(R(i)) exp(s(i)/ ), where is a (known) smoothing parameter and G(a) = 2a − 1. The gradient of the surrogate is:\n[∇sφSD(s,R)] = m∑ i=1 hs,i(R(i)),\nhs,i(R(i)) = G(Ri)  m∑ j=1 1 [ exp(s(i)/ )∑ j′ exp(s(j ′)/ ) 1(i=j) − exp((s(i) + s(j))/ ) ( ∑ j′ exp(s(j ′)/ ))2 ]ej  Using Lemma 14, we can write F (R) = Fs(R) = ∑m i=1 hs,i(R(i)) where hs,i(R(i)) is defined above. Since Fs(R) is decomposable over 1 coordinate of R at a time, we can construct an unbiased estimator from top-1 feedback ({σ,R(σ(1))}), with unbiased estimator being:\ns(σ(1))z̃ = X> G(R(σ(1))) p(σ(1)) m∑ j=1 1 [ exp(s(σ(1))/ )∑ j′ exp(s(j ′)/ ) 1(σ(1)=j) − exp((s(σ(1)) + s(j))/ ) ( ∑ j′ exp(s(j ′)/ ))2 ]ej "
    }, {
      "heading" : "3.3 Computational Complexity of Algorithm 2",
      "text" : "Three of the four key steps governing the complexity of Algorithm 2, i.e., construction of s̃t, σ̃t and sorting can all be done in O(m log(m)) time.The only bottleneck could have been calculations of p(σ̃t(1)) in squared loss, (modified) ListNet loss and SmoothDCG loss, and p(σ̃t(1), σ̃t(2)) in RankSVM loss, since they involve sum over permutations. However, they have a compact representation, i.e., p(σ̃t(1)) = (1 − γ + γm)1(σ̃t(1) = σt(1)) + γ m1(σ̃t(1) 6= σt(1)) and p(σ̃t(1), σ̃t(2)) = (1 − γ + γm(m−1))1(σ̃t(1) = σt(1), σ̃t(2) = σt(2)) + γ\nm(m−1) [∼ 1(σ̃t(1) = σt(1), σ̃t(2) = σt(2))]. The calculations follow easily due to the nature of Pt (step-6 in algorithm) which put equal weights on all permutations other than σt."
    }, {
      "heading" : "3.4 Regret Bounds",
      "text" : "The underlying deterministic part of our algorithm is online gradient descent (OGD) (Zinkevich, 2003). The regret of OGD, run with unbiased estimator of gradient of a convex function, as given in Theorem 3.1 of (Flaxman et al., 2005), in our problem setting is:\nE [ T∑ t=1 φ(Xtwt, Rt) ] ≤ min w:‖w‖2≤U T∑ t=1 φ(Xtw,Rt) + U2 2η + η 2 E [ T∑ t=1 ‖z̃t‖22 ] (11)\nwhere z̃t is unbiased estimator of ∇w=wtφ(Xtw,Rt), conditioned on past events, η is the learning rate and the expectation is taken over all randomness in the algorithm.\nHowever, from the perspective of the loss φ(s̃t, Rt) incurred by Algorithm 2, at each round t, the RHS above is not a valid upper bound. The algorithms plays the score vector suggested by OGD (s̃t = Xtwt) with probability 1− γ (exploitation) and plays a randomly selected score vector (i.e., a draw from the uniform distribution on [0, 1]m), with probability γ (exploration). Thus, the expected number of rounds in which the algorithm does not follow the score suggested by OGD is γT , leading to an extra regret of order γT . Thus, we have 3\nE [ T∑ t=1 φ(s̃t, Rt) ] ≤ E [ T∑ t=1 φ(Xtwt, Rt) ] +O (γT ) (12)\nWe first control Et‖z̃t‖22, for all convex surrogates considered in our problem (we remind that z̃t is the estimator of a gradient of a surrogate, calculated at time t. In Sec 3.2.1 , we omitted showing w in sw and index t). To get bound on Et‖z̃t‖22, we used the following norm relation that holds for any matrix X (Bhaskara and Vijayaraghavan, 2011): ‖X‖p→q = sup v 6=0 ‖Xv‖q ‖v‖p , where q is the dual exponent of p (i.e., 1 q+ 1 p = 1), and the following lemma derived from it:\nLemma 17. For any 1 ≤ p ≤ ∞, ‖X>‖1→p = ‖X‖q→∞ = maxmj=1 ‖Xj:‖p, where Xj: denotes jth row of X and m is the number of rows of matrix.\nWe have the following result:\n3The instantaneous loss suffered at each of the exploration round can be maximum of O(1), as long as φ(s,R) is bounded, ∀ s and ∀ R. This is true because the score space is `2 norm bounded, maximum relevance grade is finite in practice and we consider Lipschitz, convex surrogates.\nLemma 18. For parameter γ in Algorithm 2 , RD being the bound on `2 norm of the feature vectors (rows of document matrix X), m being the upper bound on number of documents per query, U being the radius of the Euclidean ball denoting the space of ranking parameters and Rmax being the maximum possible relevance value (in practice always ≤ 5), let Cφ ∈ {Csq, Csvm, CKL} be polynomial functions of RD,m,U,Rmax, where the degrees of the polynomials depend on the surrogate (φsq, φsvm, φKL), with no degree ever greater than four. Then we have,\nEt [ ‖z̃t‖22 ] ≤ C φ\nγ (13)\nPlugging Eq. 13 and Eq. 12 in Eq. 11, and optimizing over η and γ, (which gives η = O(T−2/3) and γ = O(T−1/3)), we get the final regret bound:\nTheorem 19. For any sequence of instances and labels (Xt, Rt){t∈[T ]}, applying Algorithm 2 with top-1 feedback for φsq and φKL and top-2 feedback for φsvm, will produce the following bound:\nE [ T∑ t=1 φ(s̃t, Rt) ] − min w:‖w‖2≤U T∑ t=1 φ(Xtw,Rt) ≤ CφO ( T 2/3 ) (14)\nwhere Cφ is a surrogate dependent function, as described in Lemma 18 , and expectation is taken over underlying randomness of the algorithm, over T rounds.\nDiscussion: It is known that online bandit games are special instances of partial monitoring games. For bandit online convex optimization problems with Lipschitz, convex surrogates, the best regret rate known so far, that can be achieved by an efficient algorithm, is O(T 3/4) (however, see the work of Bubeck and Eldan (2015) for a non-constructive O(log4(T ) √ T ) bound). Surprisingly, Alg. 2, when applied in a partial monitoring setting to the Lipschitz, convex surrogates that we have listed, achieves a better regret rate than what is known in the bandit setting. Moreover, as we show subsequently, for an entire class of Lipschitz convex surrogates (subclass of NDCG calibrated surrogates), sub-linear (in T ) regret is not even achievable. Thus, our work indicates that even within the class of Lipschitz, convex surrogates, regret rate achievable is dependent on the structure of surrogates; something that does not arise in bandit convex optimization."
    }, {
      "heading" : "3.5 Impossibility of Sublinear Regret for NDCG Calibrated Surrogates",
      "text" : "Learning to rank methods optimize surrogates to learn a ranking function, even though performance is measured by target measures like NDCG. This is done because direct optimization of the measures lead to NP-hard optimization problems. One of the most desirable properties of any surrogate is calibration, i.e., the surrogate should be calibrated w.r.t the target (Bartlett et al., 2006). Intuitively, it means that a function with small expected surrogate loss on unseen data should have small expect target loss on unseen data. We focus on NDCG calibrated surrogates (both convex and non-convex) that have been characterized by Ravikumar et al. (2011). We first state the necessary and sufficient condition for a surrogate to be calibrated w.r.t NDCG. For any score vector s and distribution η on relevance space Y, let φ̄(s, η) = ER∼ηφ(s,R). Moreover, we define G(R) = (G(R1), . . . , G(Rm))>. Z(R) is defined in Sec 2.2.\nTheorem 20. (Ravikumar et al., 2011, Thm. 6) A surrogate φ is NDCG calibrated iff for any distribution η on relevance space Y, there exists an invertible, order preserving map g : Rm 7→ Rm s.t. the unique minimizer s∗φ(η) can be written as\ns∗φ(η) = g ( ER∼η [ G(R)\nZ(R)\n]) . (15)\nInformally, Eq. 15 states that argsort(s∗φ(η)) ⊆ argsort(ER∼η [ G(R) Z(R) ] ) Ravikumar et al. (2011) give concrete examples of NDCG calibrated surrogates, including how some of the popular surrogates can be converted into NDCG calibrated ones: e.g., the NDCG calibrated version of squared loss is ‖s− G(R)Z(R) ‖ 2 2.\nWe now state the impossibility result for the class of NDCG calibrated surrogates when feedback is restricted to top ranked item.\nTheorem 21. Fix the online learning to rank game with top 1 feedback and any NDCG calibrated surrogate. Then, for every learner’s algorithm, there exists an adversary strategy such that the learner’s expected regret is Ω(T ).\nWe note that the proof of Theorem. 3 of Piccolboni and Schindelhauer (2001b) cannot be directly extended to prove the impossibility result because it relies on constructing a connected graph on vertices defined by neighboring actions of learner. In our case, due to the continuous nature of learner’s actions, the graph will be an empty graph and proof will break down."
    }, {
      "heading" : "4. Experiments",
      "text" : "We conducted experiments on simulated and commercial datasets to demonstrate the performance of our algorithms."
    }, {
      "heading" : "4.1 Non Contextual Setting",
      "text" : "Objectives: We had the following objectives while conducting experiments in the noncontextual, online ranking with partial feedback setting:\n• Investigate how performance of Algorithm 1 is affected by size of blocks during blocking.\n• Investigate how performance of the algorithm is affected by amount of feedback received (i.e., generalizing k in top k feedback).\n• Demonstrate the difference between regret rate of our algorithm, which operates in partial feedback setting, with regret rate of a full information algorithm which receives full relevance vector feedback at end of each round.\nWe applied Algorithm 1 in conjunction with Follow-The-Perturbed-Leader (FTPL) full information algorithm, as described in Sec. 2.6. We note that since our work is first of its kind in the literature, we had no comparable baselines. The generic partial monitoring algorithms that do exist cannot be applied due to computational inefficiency (Sec. 2.5).\nExperimental Setting: All our experiments were conducted with respect to the DCG measure, which is quite popular in practice, and binary graded relevance vectors. Our experiments were conducted on the following simulated dataset. We fixed number of items to 20( m = 20). We then fixed a “true” relevance vector which had 5 items with relevance level 1 and 15 items with relevance level 0. We then created a total of T=10000 relevance vectors by corrupting the true relevance vector. The corrupted copies were created by independently flipping each relevance level (0 to 1 and vice-versa) with a small probability. The reason for creating adversarial relevance vectors in such a way was to reflect diversity of preferences in practice. In reality, it is likely that most users will have certain similarity in preferences, with small deviations on certain items and certain users. That is, some items are likely to be relevance in general, with most items not relevant to majority of users, with slight deviation from user to user. Moreover, the comparator term in our regret bound (i.e., cumulative loss/gain of the true best ranking in hindsight) only makes sense if there is a ranking which satisfies most users.\nResults: We plotted average regret over time under DCG. Average regret over time means cumulative regret up to time t, divided by t, for 1 ≤ t ≤ T . Figure 1 demonstrates the effect of block size on the regret rate under DCG. We fixed feedback to relevance of top ranked item (k = 1). As can be seen in Corollary 10, the optimal regret rate is achieved by optimizing over number of blocks (hence block size), which requires prior knowledge of time horizon T . We wanted to demonstrate how the regret rate (and hence the performance of the algorithm) differs with different block sizes. The optimal number of blocks in our setting is K ∼ 200, with corresponding block size being dT/Ke = 50. As can be clearly seen, with optimal block size, the regret drops fastest and becomes steady after a point. K = 10 means that block size is 1000. This means over the time horizon, number of exploitation rounds greatly dominates number of exploration rounds, leading to regret dropping at a slower rate initially than than the case with optimal block size. However, the regret drops of pretty sharply later on. This is because the relevance vectors are slightly corrupted copies of a “true” relevance vector and the algorithm gets a good estimate of the true relevance vector quickly and then more exploitation helps. When K = 400 (i.e, block size is 25), most of the time, the algorithm is exploring, leading to a substantially worse regret and poor performance.\nFigure 2 demonstrates the effect of amount of feedback on the regret rate under DCG. We fixed K = 200, and varied feedback as relevance of top k ranked items per round, where k = 1, 5, 10. Validating our regret bound, we see that as k increases, the regret decreases.\nFigure 3 compares regret of our algorithm, working with top 1 feedback and FTPL full information algorithm, working with full relevance vector feedback at end of each round. We fixed K = 200 and the comparison was done from 1000 iterations onwards, i.e., roughly after the initial learning phase. FTPL full information algorithm has regret rate of O(T 1/2) (ignoring other parameters). So, as expected, FTPL with full information feedback outperforms our algorithm with highly restricted feedback; yet, we have demonstrated, both theoretically and empirically, that it is possible to have a good ranking strategy with highly restricted feedback."
    }, {
      "heading" : "4.2 Contextual Setting",
      "text" : "Objective: Since our contextual, online learning to rank with restricted feedback setting involves query-document matrices, we could conduct experiments on commercial, publicly available ranking datasets.Our objective was to demonstrate that it is possible to learn a good ranking function, even with highly restricted feedback, when standard online learning\nto rank algorithms would require full feedback at end of each round. As stated before, though our algorithm is designed to minimize surrogate based regret, the surrogate loss is only of interest. The users only care about the ranking presented to them, and indeed the algorithm interacts with users by presenting ranked lists and getting feedback on top\nranked item(s). We tested the quality of the ranked lists, and hence the performance of the evolving ranking functions, against the full relevance vectors, via ranking measure NDCG, cutoff at the 10th item. NDCG, cutoff at a point n, is defined as follows:\nNDCGn(σ,R) = 1\nZn(R) n∑ i=1 2R(σ(i)) − 1 log2(1 + i)\nwhere Zn(R) = max σ∈Sm\n∑n i=1 2R(σ(i))−1 log2(1+i) . We want to emphasize that the algorithm was in no\nway affected by the fact that we were measuring its performance with respect to NDCG cutoff at 10. In fact, the cutoff point can be varied, but usually, researchers report performance under NDCG cutoff at 5 or 10.\nBaselines: We applied Algorithm 2, with top 1 feedback, on Squared, KL (un-normalized ListNet) and SmoothDCG surrogates, and with top 2 feedback, on the RankSVM surrogate. Based on the objective of our work, we selected two different ranking algorithms as baselines. The first one is the online version of ListNet ranking algorithm (which is essentially OGD on cross-entropy function), with full relevance vector revealed at end of every round. ListNet is not only one of the most cited ranking algorithms (over 700 citations according to Google Scholar), but also one of the most validated algorithms (Tax et al., 2015). We emphasize that some of the ranking algorithms in literature, which have shown better empirical performance than ListNet, are based on non-convex surrogates with complex, non-linear ranking functions. These algorithms cannot usually be converted into online algorithms which learn from streaming data. Our second algorithm is a simple, fully random algorithm , which outputs completely random ranking of documents at each round.\nThis algorithm, in effect, receives no feedback at end of each round. Thus, we are comparing Algorithm 2, which learns from highly restricted feedback, with an algorithm which learns from full feedback and an algorithm which receives no feedback and learns nothing.\nDatasets: We compared the various ranking functions on two large scale commercial datasets. They were Yahoo’s Learning to Rank Challenge dataset (Chapelle and Chang, 2011) and a dataset published by Russian search engine Yandex (IM-2009). The Yahoo dataset has 19944 unique queries with 5 distinct relevance levels, while Yandex has 9126 unique queries with 5 distinct relevance levels.\nExperimental Setting: We selected time horizon T = 200, 000 (Yahoo) and T = 100, 000 (Yandex) iterations for our experiments (thus, each algorithm went over each dataset multiple times). The reason for choosing different time horizons is that there are roughly double the number of queries in Yahoo dataset as compared to Yandex dataset. All the online algorithms, other than the fully random one, involve learning rate η and exploration parameter γ (full information ListNet does not involve γ and SmoothDCG has an additional smoothing parameter ). While obtaining our regret guarantees, we had established that η = O(T−2/3) and γ = O(T−1/3). In our experiments, for each instance of Algorithm 2, we selected a time varying η = 0.01\nt2/3 and γ = 0.1 t1/3 , for round t . We fixed = 0.01. For ListNet,\nwe selected η = 0.01 t1/2 , since regret guarantee in OGD is established with η = O(T−1/2). We plotted average NDCG10 against time, where average NDCG10 at time t is the cumulative NDCG10 up to time t, divided by t. We made an important observation while comparing the performance plots of the algorithms. As we have shown, construction of the unbiased estimators involve division by a probability value (Eq 10). The particular probability value can be γm , which is very small since γ goes to 0, when the top ranked item of the randomly drawn permutation does not match the top ranked item of the permutation given by the deterministic score vector (Sec 3.3). The mismatch happens with very low probability (since the random permutation is actually the deterministic permutation with high probability). While theoretically useful, in practice, dividing by such small value negatively affected the gradient estimation and hence the performance of our algorithm. So, when the mismatch happened, we scaled up γ on the mismatch round by a constant, to remove the negative effect.\nResults: Figure 4 and Figure 5 show that ListNet, with full information feedback at end of each round, has highest average NDCG value throughout, as expected. However, Algorithm 2, with the convex surrogates, produce competitive performance. In fact, in the Yahoo dataset, our algorithm, with RankSVM and KL, are very close to the performance of ListNet. RanSVM based algorithm does better than the others, since the estimator of RankSVM gradient is constructed from top 2 feedback, leading to lower variance of the estimator. KL based algorithm does much better than Squared loss based algorithm on Yahoo and equally as well on Yandex dataset. Crucially, our algorithm, based on all three convex surrogates, perform significantly better than the purely random algorithm, and are much closer to ListNet in performance, despite being much closer to the purely random algorithm in terms of feedback. Our algorithm, with SmoothDCG, on the other hand, produce poor performance. We believe the reason is the non-convexity of the surrogate, which leads\nto the optimization procedure possibly getting stuck at a local minima. In batch setting, such problem is avoided by an annealing technique that successively reduces . We are not aware of an analogue in an online setting. Possible algorithms optimizing non-convex surrogates in an online manner, which require gradient of the surrogate, may be adapted to this partial feedback setting. The main purpose for including SmoothDCG in our work was to show that unbiased estimation of gradient, from restricted feedback, is possible even for non-convex surrogates."
    }, {
      "heading" : "5. Conclusion and Future Directions",
      "text" : "We studied the problem of online learning to rank with a novel, restricted feedback model. The work is divided into two parts: in the first part, the set of items to be ranked is fixed, with varying user preferences, and in the second part, the items vary, as traditional query-documents matrices. The parts are tied by the feedback model; where the user gives feedback only on top k ranked items at end of each round, though the performance of the learner’s ranking strategy is judged against full, implicit relevance vectors. In the first part, we gave comprehensive results on learnability with respect to a number of practically important ranking measures. We also gave a generic algorithm, along with an efficient instantiation, which achieves sub-linear regret rate for certain ranking measures. In the second part, we gave an efficient algorithm, which works on a number of popular ranking surrogates, to achieve sub-linear regret rate. We also gave an impossibility result for an entire class of ranking surrogates. Finally, we conducted experiments on simulated and commercial ranking datasets to demonstrate the performance of our algorithms.\nWe highlight some of the open questions of interest:\n• What are the minimax regret rates with top k feedback model, for k > 1, for the ranking measures DCG, PairwiseLoss, Precision@n and their normalized versions NDCG, AUC and AP? Specifically, NDCG and AP are very popular in the learning to rank community. We showed that with top 1 feedback model, no algorithm can achieve sublinear regret for NDCG and AP. Is it possible to get sub-linear regret with 1 < k < m?\n• We used FTPL as the sub-routine in Algorithm 1 to get an efficient algorithm. It might be possible to use other full information algorithms as sub-routine, retaining the efficiency, but getting tighter rates in terms of parameters (other than T ) and better empirical performance.\n• We applied Algorithm 2 on three convex surrogates and one non-convex surrogates. It would be interesting to investigate what other surrogates the algorithm can be applied on, guided by Lemma 14, and test its empirical performance. Since the algorithm learns a ranking function in the traditional query-documents setting, the question is more of practical interest.\n• We saw that Algorithm 2, when applied to SmoothDCG, does not produce competitive empirical performance. It has been shown that a ranking function, learnt by optimizing SmoothDCG in the batch setting, has extremely competitive empirical performance (Qin and Liu, 2006). In the batch setting, simulated annealing is used to prevent the optimization procedure getting stuck in local minima. Any algorithm that\noptimizes non-convex surrogates in an online manner, by accessing its gradient, can replace the online gradient descent part in our algorithm and tested on SmoothDCG for empirical performance.\n• We proved an impossibility result for NDCG calibrated surrogates with top 1 feedback. What is the minimax regret for NDCG calibrated surrogates, with top k feedback, for k > 1?"
    }, {
      "heading" : "Acknowledgments",
      "text" : "The authors acknowledge the support of NSF under grants IIS-1319810 and CAREER IIS1452099."
    }, {
      "heading" : "Appendix A.",
      "text" : "We provide technical details of results of Online Ranking with Restricted Feedback- Non Contextual Setting.\nProof of Theorem 4:\nProof. We will explicitly show that local observability condition fails by considering the case when number of objects is m = 3. Specifically, action pair {σ1, σ2}, in Table 1 are neighboring actions, using Lemma 3 . Now every other action {σ3, σ4, σ5, σ6} either places object 2 at top or object 3 at top. It is obvious that the set of probabilities for which E[R(1)] ≥ E[R(2)] = E[R(3)] cannot be a subset of any C3, C4, C5, C6. From Def. 4, the neighborhood action set of actions {σ1, σ2} is precisely σ1 and σ2 and contains no other actions. By definition of signal matrices Sσ1 , Sσ2 and entries `1, `2 in Table 1 and 2, we have,\nSσ1 = Sσ2 = [ 1 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 ] `1 − `2 = [ 0 1 −1 0 0 1 −1 0 ] .\n(16)\nIt is clear that `1 − `2 /∈ Col(S>σ1). Hence, Definition 5 fails to hold.\nProof of Theorem 8:\nProof. Full information feedback: Instead of top k feedback, assume that at end of each round, after learner reveals its action, the full relevance vector R is revealed to the learner. Since the knowledge of full relevance vector allows the learner to calculate the loss for every action (SumLoss(σ,R), ∀ σ), the game is in full information setting, and the learner, using the full information algorithm, will have an O(C √ T ) expected regret for SumLoss (ignoring computational complexity). Here, C denotes parameter specific to the full information algorithm used.\nBlocking with full information feedback: We consider a blocked variant of the full information algorithm. We still assume that full relevance vector is revealed at end of each round. Let the time horizon T be divided into K blocks, i.e., {B1, . . . , BK}, of equal size. Here, Bi = {(i−1)(T/K)+1, (i−1)(T/K)+2, (i−1)T/K+3, . . . , i(T/K)}. While operating in a block, the relevance vectors revealed at end of each round are accumulated, but not used to generate learner’s actions like in the “without blocking” variant. Assume at the start of block Bi, there was some vector si−1 ∈ Rm. Then, at each round in the block, the randomized full information algorithm exploits si−1 and outputs a permutation (basically maintains a distribution over actions, using si−1, and samples from the distribution). At the end of a block, the average of the accumulated relevance vectors (Ravgi ) for the block is used to update, as si−1 + R avg i , to get si for the next block. The process is repeated for each block.\nFormally, the full information algorithm creates distribution ρi over the actions, at beginning of block Bi, exploiting information si−1. Thus, ρi ∈ ∆, where ∆ is the probability simplex over m! actions. Note that ρi is a deterministic function of {Ravg1 , . . . , R avg i−1}.\nSince action σt, for t ∈ Bi, is generated according to distribution ρi (we will denote this as σt ∼ ρi), and in block i, distribution ρi is fixed, we have\nEσt∼ρi [ ∑ t∈[Bi] SumLoss(σt, Rt)] = ∑ t∈Bi ρi · [SumLoss(σ1, Rt), . . . , SumLoss(σm!, Rt)].\n(dot product between 2 vectors of length m!). Thus, the total expected loss of this variant of the full information problem is: E T∑ t=1 [SumLoss(σt, Rt)] = K∑ i=1 Eσt∼ρi [ ∑ t∈Bi SumLoss(σt, Rt)]\n= K∑ i=1 ∑ t∈Bi ρi · [SumLoss(σ1, Rt), . . . , SumLoss(σm!, Rt)]\n= K∑ i=1 ∑ t∈Bi ρi · [σ−11 ·Rt, . . . , σ −1 m! ·Rt)] By defn. of SumLoss\n= T\nK K∑ i=1 ρi · [σ−11 ·R avg i , . . . , σ −1 m! ·R avg i ]\n= T\nK K∑ i=1 Eσi∼ρi [SumLoss(σi, R avg i )]\n= T\nK Eσ1∼ρ1,...,σK∼ρK K∑ i=1 SumLoss(σi, R avg i ) (17)\nwhere Ravgi = ∑\nt∈Bi Rt T/K . Note that, at end of every block i ∈ [K], ρi is updated to ρi+1.\nBy the regret bound of the full information algorithm, for K rounds of full information problem, we have:\nEσ1∼ρ1,...,σK∼ρK K∑ i=1 SumLoss(σi, R avg i ) ≤ minσ K∑ i=1 SumLoss(σ,Ravgi ) + C √ K\n= min σ K∑ i=1 σ−1 ·Ravgi + C √ K\n= min σ T∑ t=1 σ−1 · Rt T/K + C √ K\n(18)\nNow, since\nmin σ T∑ t=1 σ−1 · Rt T/K = min σ 1 T/K T∑ t=1 SumLoss(σ,Rt),\ncombining Eq. 17 and Eq. 18, we get:\nT∑ t=1 Eσt∈ρi [SumLoss(σt, Rt)] ≤ minσ T∑ t=1 SumLoss(σ,Rt) + C T√ K . (19)\nBlocking with top k feedback: However, in our top k feedback model, the learner does not get to see the full relevance vector at each end of round. Thus, we form the unbiased estimator R̂i of R avg i , using Lemma 14. That is, at start of each block, we choose dm/ke time points uniformly at random, and at those time points, we output a random permutation which places k distinct objects on top (refer to Algorithm 1). At the end of the block, we form the vector R̂i which is the unbiased estimator of R avg i . Note that using random vector R̂i instead of true R avg i introduces randomness in the distribution ρi itself . But significantly, ρi is dependent only on information received up to the beginning of block i and is independent of the information collected in the block. We show the exclusive dependence as ρi(R̂1, R̂2, .., R̂i−1). Thus, for block i, we have:\nEσt∼ρi(R̂1,R̂2,..,R̂i−1) ∑ t∈[Bi] SumLoss(σt, Rt)\n= T\nK Eσi∼ρi(R̂1,R̂2,..,R̂i−1)SumLoss(σi, R avg i )\n(From Eq. 17)\n= T\nK Eσi∼ρi(R̂1,R̂2,..,R̂i−1)ER̂iSumLoss(σi, R̂i)\n(∵ SumLoss is linear in both arguments and R̂i is unbiased)\n= T\nK ER̂iEσi∼ρi(R̂1,R̂2,..,R̂i−1)SumLoss(σi, R̂i).\nIn the last step above, we crucially used the fact that, since random distribution ρi is independent of R̂i, the order of expectations is interchangeable. Taking expectation w.r.t. R̂1, R̂2, .., R̂i−1, we get,\nER̂1,...,R̂i−1Eσt∼ρi(R̂1,R̂2,..,R̂i−1) ∑ t∈[Bi] SumLoss(σt, Rt)\n= T\nK ER̂1,...,R̂i−1,R̂iEσi∼ρi(R̂1,R̂2,..,R̂i−1)SumLoss(σi, R̂i).\n(20)\nThus,\nE T∑ t=1 SumLoss(σt, Rt) = E K∑ i=1 ∑ t∈[Bi] SumLoss(σt, Rt)\n= K∑ i=1 ER̂1,...,R̂i−1Eσt∼ρi(R̂1,R̂2,..,R̂i−1) ∑ t∈[Bi] SumLoss(σt, Rt)\n= T\nK K∑ i=1 ER̂1,...,R̂i−1,R̂iEσi∼ρi(R̂1,R̂2,..,R̂i−1)SumLoss(σi, R̂i)\n(From Eq. 20)\n= T\nK ER̂1,...,R̂KEσi∼ρi(R̂1,R̂2,..,R̂i−1) K∑ i=1 SumLoss(σi, R̂i)\nNow using Eq. 18, we can upper bound the last term above as\n≤ T K {ER̂1,...,R̂K [minσ K∑ i=1 σ−1 · R̂i] + C √ K}\n≤ T K {min σ K∑ i=1 σ−1 ·Ravgi + C √ K}\n(Jensen’s Inequality)\n≤ min σ T∑ t=1 σ−1 ·Rt + C T√ K\n= min σ T∑ t=1 SumLoss(σ,Rt) + C T√ K .\nEffect of exploration: Since in each block Bi, dm/ke rounds are reserved for exploration, where we do not draw σt from distribution ρi, we need to account for it in our regret bound. Exploration leads to an extra regret of CIdm/keK, where CI is a constant depending on the loss under consideration and specific full information algorithm used. The extra regret is because loss in each of the exploration rounds is at most CI and there are a total of dm/keK exploration rounds over all K blocks. Thus, overall regret :\nE [ T∑ t=1 SumLoss(σt, Rt) ] −min σ T∑ t=1 SumLoss(σ,Rt) ≤ CIdm/keK + C T√ K . (21)\nNow we optimize over K, to get:\nE [ T∑ t=1 SumLoss(σt, Rt) ] ≤ min σ T∑ t=1 SumLoss(σ,Rt) + 2(C I)1/3C2/3dm/ke1/3T 2/3 (22)\nProof of Corollary 9:\nProof. We only need to instantiate the constants C and CI from Theorem 8, with respect to SumLoss and FTPL. FTPL has the following parameters in its regret bound, for any online full information linear optimization problem: D is the `1 diameter of learner’s action set, R is upper bound on difference between losses of 2 actions on same information vector and A is the `1 diameter of the set of information vectors (adversary’s action set).\nFor SumLoss, it can be easily calculated that R = ∑m\ni=1 σ −1(i)R(i) = O(m2), D =∑m\ni=1 σ −1(i) = O(m2), and A = ∑m i=1R(i) = O(m) .\nFTPL gets O(C √ T ) regret over T rounds when =\n√ D\nRAT . Here, C = 2 √ DRA and\nCI = R. Substituting the values of D,R,A, we conclude.\nExtension of results from SumLoss to DCG and Precision@n:\nDCG: Due to structural differences, there are minor differences in definitions and proofs of theorems for SumLoss and DCG. We give pointers in the in proving that local observability condition fails to hold for DCG, when restricted to top 1 feedback. We can skip the explicit proof of global observability, since the application of Algorithm 1 already establishes that O(T 2/3) regret can be achieved.\nWith slight abuse of notations, the loss matrix L implicitly means gain matrix, where entry in cell {i, j} of L is f(σi) · g(Rj). The columns of feedback matrix H are expanded to account for greater number of moves available to adversary (due to multi-graded relevance vectors). In Definition 1, learner action i is optimal if `i · p ≥ `j · p, ∀j 6= i.\nIn Definition 2, the maximum number of distinct elements that can be in a row of H is n+ 1. The signal matrix now becomes Si ∈ {0, 1}(n+1)×2 m , where (Si)j,` = 1(Hi,` = j − 1).\nLocal Observability Fails: Since we are trying to establish a lower bound, it is sufficient to show it for binary relevance vectors, since the adversary can only be more powerful otherwise.\nIn Lemma 2, proved for SumLoss, `i · p equates to f(σ) ·E[R]. From definition of DCG, and from the structure and properties of f(·), it is clear that `i · p is maximized under the same condition, i.e, E[R(σi(1)] ≥ E[R(σi(2)] ≥ . . . ≥ E[R(σi(m)]. Thus, all actions are Pareto-optimal.\nCareful observation of Lemma 3 shows that it is directly applicable to DCG, in light of extension of Lemma 2 to DCG.\nFinally, just like in SumLoss, simple calculations with m = 3 and n = 1, in light of Lemma 2 and 3, show that local observability condition fails to hold.\nWe show the calculations:\nSσ1 = Sσ2 = [ 1 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 ]\n`σ1 =[0, 1/2, 1/ log2 3, 1/2 + 1/ log2 3, 1, 3/2,\n1 + 1/ log2 3, 3/2 + 1/ log2 3]\n`σ2 =[0, 1/ log2 3, 1/2, 1/2 + 1/ log2 3, 1, 1 + 1/ log2 3,\n3/2, 3/2 + 1/ log2 3]\nIt is clear that `1 − `2 /∈ Col(S>σ1). Hence, Definition 5 fails to hold. Proof of Corrolary 10\nFor DCG, the parameters of FTPL are: R = ∑m\ni=1 f s(σ−1(i))gs(R(i)) = O(m(2n − 1)), D = ∑m\ni=1 f s(σ−1(i)) = O(m), A = ∑m i=1 g\ns(R(i)) = O(m(2n − 1)). Again, C = 2 √ DRA and CI = R.\nPrecision@n: Proof of Corrolary 11\nFor Precision@n, the parameters of FTPL are: D = ∑m\ni=1 f s(σ−1(i)) = O(n), R =∑m\ni=1 f s(σ−1(i))gs(R(i)) = O(n), A = ∑m i=1 g s(R(i)) = O(m). Again, C = 2 √ DRA and\nCI = R.\nNon-existence of Sublinear Regret Bounds for NDCG, AP and AUC\nWe show via simple calculations that for the case m = 3, global observability condition fails to hold for NDCG, when feedback is restricted to top ranked item, and relevance vectors are restricted to take binary values. It should be noted that allowing for multigraded relevance vectors only makes the adversary more powerful; hence proving for binary relevance vectors is enough.\nThe intuition behind failure to satisfy global observability condition is that theNDCG(σ,R) = f(σ) · g(R), where where g(r) = R/Z(R) (see Sec.2.2 ). Thus, g(·) cannot be represented by univariate, scalar valued functions. This makes it impossible to write the difference between two rows of the loss matrix as linear combination of columns of (transposed) signal matrices.\nSimilar intuitions hold for AP and AUC. Proof of Lemma 12\nProof. We will first consider NDCG and then, AP and AUC.\nNDCG: The first and last row of Table 1, when calculated for NDCG, are:\n`σ1 = [1, 1/2, 1/ log2 3, (1 + log2 3/2))/(1 + log2 3), 1, 3/(2(1 + 1/ log2 3)), 1, 1]\n`σ6 = [1, 1, log2 2/ log2 3, 1, 1/2, 3/(2(1 + 1/ log2 3)), (1 + (log2 3)/2))/(1 + log2 3), 1]\nWe remind once again that NDCG is a gain function, as opposed to SumLoss. The difference between the two vectors is:\n`σ1 − `σ6 = [0,−1/2, 0,− log2 3/(2(1 + log2 3)), 1/2, 0, log2 3/(2(1 + log2 3)), 0].\nThe signal matrices are same as SumLoss:\nSσ1 = Sσ2 = [ 1 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 ]\nSσ3 = Sσ5 = [ 1 1 0 0 1 1 0 0 0 0 1 1 0 0 1 1 ]\nSσ4 = Sσ6 = [ 1 0 1 0 1 0 1 0 0 1 0 1 0 1 0 1 ] It can now be easily checked that `σ1 − `σ6 does not lie in the (combined) column span\nof the (transposed) signal matrices.\nWe show similar calculations for AP and AUC. AP:\nWe once again take m = 3. The first and last row of Table 1, when calculated for AP, is:\n`σ1 = [1, 1/3, 1/2, 7/12, 1, 5/6, 1, 1] `σ6 = [1, 1, 1/2, 1, 1/3, 5/6, 7/12, 1]\nLike NDCG, AP is also a gain function.\nThe difference between the two vectors is:\n`σ1 − `σ6 = [0,−2/3, 0,−5/12, 2/3, 0, 5/12, 0].\nThe signal matrices are same as in the SumLoss case:\nSσ1 = Sσ2 = [ 1 1 1 1 0 0 0 0 0 0 0 0 1 1 1 1 ]\nSσ3 = Sσ5 = [ 1 1 0 0 1 1 0 0 0 0 1 1 0 0 1 1 ]\nSσ4 = Sσ6 = [ 1 0 1 0 1 0 1 0 0 1 0 1 0 1 0 1 ] It can now be easily checked that `σ1 − `σ6 does not lie in the (combined) column span\nof the (transposed) signal matrices.\nAUC: For AUC, we will show the calculations for m = 4. This is because global observability does hold with m = 3, as the normalizing factors for all relevance vectors with non-trivial mixture of 0 and 1 are same (i.e, when relevance vector has 1 irrelevant and 2 relevant objects, and 1 relevant and 2 irrelevant objects, the normalizing factors are same). The normalizing factor changes from m = 4 onwards; hence global observability fails.\nTable 1 will be extended since m = 4. Instead of illustrating the full table, we point out the important facts about the loss matrix table with m = 4 for AUC.\nThe 24 relevance vectors heading the columns are: R1 = 0000, R2 = 0001, R3 = 0010, R4 = 0100, R5 = 1000, R6 = 0011, R7 = 0101, R8 = 1001, R9 = 0110, R10 = 1010, R11 = 1100, R12 = 0111, R13 = 1011, R14 = 1101, R15 = 1110, R16 = 1111.\nWe will calculate the losses of 1st and last (24th) action, where σ1 = 1234 and σ24 = 4321.\n`σ1 = [0, 1, 2/3, 1/3, 0, 1, 3/4, 1/2, 1/2, 1/4, 0, 1, 2/3, 1/3, 0, 0] `σ24 = [0, 0, 1/3, 2/3, 1, 0, 1/4, 1/2, 1/2, 3/4, 1, 0, 1/3, 2/3, 1, 0]\nAUC, like SumLoss, is a loss function. The difference between the two vectors is:\n`σ1 − `σ24 = [0, 1, 1/3,−1/3,−1, 1, 1/2, 0, 0,−1/2,−1, 1, 1/3,−1/3,−1, 0].\nThe signal matrices for AUC with m = 4 will be slightly different. This is because there are 24 signal matrices, corresponding to 24 actions. However, groups of 6 actions will share the same signal matrix. For example, all 6 permutations that place object 1 first will have same signal matrix, all 6 permutations that place object 2 first will have same signal matrix, and so on. For simplicity, we denote the signal matrices as S1, S2, S3, S4, where Si corresponds to signal matrix where object i is placed at top. We have:\nS1 = [ 1 1 1 1 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 1 1 ]\nS2 = [ 1 1 1 0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 1 1 1 ]\nS3 = [ 1 1 0 1 1 0 1 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 ]\nS4 = [ 1 0 1 1 1 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 0 1 1 1 0 0 0 1 1 1 0 1 ] It can now be easily checked that `σ1 − `σ24 does not lie in the (combined) column span\nof transposes of S1, S2, S3, S4."
    }, {
      "heading" : "Appendix B.",
      "text" : "We provide technical details of results of Online Ranking with Restricted Feedback- Contextual Setting.\nProof of Lemma 14: We restate the lemma before giving the proof, for ease of reading:\nLemma 14: Let F : Rm 7→ Ra be a vector valued function, where m ≥ 1, a ≥ 1. For a fixed x ∈ Rm, let k entries of x be observed at random. That is, for a fixed probability distribution P and some random σ ∼ P(Sm), observed tuple is {σ, xσ(1), . . . , xσ(k)}. The necessary condition for existence of an unbiased estimator of F (x), that can be constructed from {σ, xσ(1), . . . , xσ(k)}, is that it should be possible to decompose F (x) over k (or less) coordinates of x at a time. That is, F (x) should have the following structure:\nF (x) = ∑\n(i1,i2,...,i`)∈ mP`\nhi1,i2,...,i`(xi1 , xi2 , . . . , xi`)\nwhere ` ≤ k, mP` is ` permutations of m and h : R` 7→ Ra. Moreover, when F (x) can be written in form of Eq 9 , with ` = k, an unbiased estimator of F (x), based on {σ, xσ(1), . . . , xσ(k)}, is,\ng(σ, xσ(1), . . . , xσ(k)) =\n∑ (j1,j2,...,jk)∈Sk\nhσ(j1),...,σ(jk)(xσ(j1), . . . , xσ(jk))∑ (j1,...,jk)∈Sk p(σ(j1), . . . , σ(jk))\nwhere Sk is the set of k! permutations of [k] and p(σ(1), . . . , σ(k)) is as in Eq 8 .\nProof. For a fixed x ∈ Rm and probability distribution P, let the random permutation be σ ∼ P(Sm) and the observed tuple be {σ, xσ(1), . . . , xσ(k)}. Let Ĝ = G(σ, xσ(1), . . . , xσ(k)) be an unbiased estimator of F (x) based on the random observed tuple. Taking expectation,\nwe get: F (x) =Eσ∼P [ Ĝ ] = ∑ π∈Sm P(π)G(π, xπ(1), . . . , xπ(k))\n= ∑\n(i1,i2,...,ik)∈ mPk ∑ π∈Sm P(π)1(π(1) = i1, π(2) = i2, . . . , π(k) = ik)G(π, xi1 , xi2 , . . . , xik)\nWe note that P(π) ∈ [0, 1] is independent of x for all π ∈ Sm. Then we can use the following construction of function h(·):\nhi1,i2,...,ik(xi1 , . . . , xik) = ∑ π∈Sm P(π)1(π(1) = i1, π(2) = i2, . . . , π(k) = ik)G(π, xi1 , xi2 , . . . , xik)\nand thus,\nF (x) = ∑\n(i1,i2,...,ik)∈ mPk\nhi1,i2,...,ik(xi1 , xi2 , . . . , xi)\nHence, we conclude that for existence of an unbiased estimator based on the random observed tuple, it should be possible to decompose F (x) over k (or less) coordinates of x at a time. The “less than k” coordinates arguement follows simply by noting that if F (x) can be decomposed over ` coordinates at a time (` < k) and observation tuple is {σ, xσ(1), . . . , xσ(k))}, then any k− ` observations can be thrown away and the rest used for construction of the unbiased estimator.\nThe construction of the unbiased estimator proceeds as follows: Let F (x) = ∑m i=1 hi(xi) and feedback is for top-1 item (k = 1). The unbiased estimator\naccording to Lemma. 14 is:\ng(σ, xσ(1)) = hσ(1)(xσ(1))\np(σ(1)) = hσ(1)(xσ(1))∑ π P(π)1(π(1) = σ(1))\nTaking expectation w.r.t. σ, we get:\nEσ[g(σ, xσ(1))] = m∑ i=1 hi(xi)( ∑ π P(π)1(π(1) = i))∑ π P(π)1(π(1) = i) = m∑ i=1 hi(xi) = F (x)\nNow, let F (x) = m∑\ni 6=j=1 hi,j(xi, xj) and the feedback is for top-2 item (k = 2). The\nunbiased estimator according to Lemma. 14 is:\ng(σ, xσ(1), xσ(2)) = hσ(1),σ(2)(xσ(1), xσ(2)) + hσ(2),σ(1)(xσ(2), xσ(1))\np(σ(1), σ(2)) + p(σ(2), σ(1))\nWe will use the fact that for any 2 permutations σ1, σ2, which places the same 2 objects in top-2 positions but in opposite order, estimators based on σ1 (i.e, g(σ1, xσ1(1), xσ1(2))) and σ2 (i.e, g(σ2, xσ2(1), xσ2(2))) have same numerator and denominator. For eg., let σ1(1) = i, σ1(2) = j. Numerator and denominator for g(σ1, xσ1(1), xσ1(2)) are hi,j(xi, xj)+hj,i(xj , xi) and p(i, j) + p(j, i) respectively. Now let σ2(1) = j, σ2(2) = i. Then numerator and denominator for g(σ2, xσ2(1), xσ2(2)) are hj,i(xj , xi) + hi,j(xi, xj) and p(j, i) + p(i, j) respectively.\nThen, taking expectation w.r.t. σ, we get:\nEσg(σ, xσ(1), xσ(2)) = m∑\ni 6=j=1\n(hi,j(xi, xj) + hj,i(xj , xi))p(i, j)\np(i, j) + p(j, i)\n= m∑\ni>j=1\n(hi,j(xi, xj) + hj,i(xj , xi))(p(i, j) + p(j, i))\np(i, j) + p(j, i)\n= m∑\ni>j=1\n(hi,j(xi, xj) + hj,i(xj , xi)) = m∑\ni 6=j=1 hi,j(xi, xj) = F (x)\nThis chain of logic can be extended for any k ≥ 3. Explicitly, for general k ≤ m, let S(i1, i2, . . . , ik) denote all permutations of the set {i1, . . . , ik}. Then, taking expectation of the unbiased estimator will give:\nEσg(σ, xσ(1), . . . , xσ(k))\n= ∑\n(i1,i2,...,ik)∈ mPk\n( ∑ (j1,...,jk)∈S(i1,...,ik) hj1,...,jk(xj1 , . . . , xjk) ) p(i1, . . . , ik)∑\n(j1,...,jk)∈S(i1,...,ik) p(j1, . . . , jk)\n= m∑ i1>i2>...>ik=1\n( ∑ (j1,...,jk)∈S(i1,...,ik) hj1,...,jk(xj1 , . . . , xjk) )( ∑ (j1,...,jk)∈S(i1,...,ik) p(j1, . . . , jk) ) ∑\n(j1,...,jk)∈S(i1,...,ik) p(j1, . . . , jk)\n= m∑\ni1>i2>...>ik=1  ∑ (j1,...,jk)∈S(i1,...,ik) hj1,...,jk(xj1 , . . . , xjk)  =\n∑ (i1,i2,...,ik)∈ mPk hi1,i2,...,ik(xi1 , xi2 , . . . , xik) = F (x)\nNote: For k = m, i.e., when the full feedback is received, the unbiased estimator is:\ng(σ, xσ(1), . . . , xσ(m)) =\n∑ (j1,j2,...,jm)∈Sm\nhσ(j1),...,σ(jm)(xσ(j1), . . . , xσ(jm))∑ (j1,...,jm)∈Sm p(σ(j1), . . . , σ(jm))\n=\n∑ (i1,i2,...,im)∈ mPm hi1,...,im(xi1 , . . . , xim)\n1 = F (x)\nHence, with full information, the unbiased estimator of F (x) is actually F (x) itself, which is consistent with the theory of unbiased estimator.\nProof of Lemma 18 :\nProof. All our unbiased estimators are of the form X>f(s,R, σ). We will actually get a bound on f(s,R, σ) by using Lemma 17 and p→ q norm relation, to equate out X:\n‖z̃‖2 = ‖X>f(s,R, σ)‖2 ≤ ‖X>‖1→2‖f(s,R, σ)‖1 ≤ RD‖f(s,R, σ)‖1\nsince RD ≥ maxmj=1 ‖Xj:‖2. Squared Loss: The unbiased estimator of gradient of squared loss, as given in the main text, is:\nz̃ = X>(2(s− R(σ(1))eσ(1)\np(σ(1)) )) where p(σ(1)) = ∑\nπ∈Sm P(π)1(π(1) = σ(1)) (P = Pt is the distribution at round t as in Algorithm 2 )\nNow we have:\n‖s− R(σ(1))eσ(1)\np(σ(1)) ‖1 ≤ mRDU + Rmax p(σ(1)) ≤ mRDURmax p(σ(1)\nThus, taking expectation w.r.t σ, we get:\nEσ‖z̃‖22 ≤ m2R4DU2R2maxEσ 1\np(σ(1))2 = m2R4DU 2R2max m∑ i=1 p(i) p2(i)\nNow, since p(i) ≥ γ m , ∀ i, we get: Eσ‖z̃‖22 ≤ Csq γ , where Csq = m4R4DU 2R2max.\nRankSVM Surrogate: The unbiased estimator of gradient of the RankSVM surrogate, as given in the main text, is:\nz̃ = X> ( hs,σ(1),σ(2)(R(σ(1)), R(σ(2))) + hs,σ(2),σ(1)(R(σ(2)), R(σ(1)))\np(σ(1), σ(2)) + p(σ(2), σ(1)) ) where hs,i,j(R(i), R(j)) = 1(R(i) > R(j))1(1 + s(j) > s(i))(ej − ei) and p(σ(1), σ(2)) =∑ π∈Sm P(π)1(π(1) = σ(1), π(2) = σ(2)) (P = Pt as in Algorithm 2)).\nNow we have:\n‖ hs,σ(1),σ(2)(Rσ(1), Rσ(2)) + hs,σ(2),σ(1)(Rσ(2), Rσ(1))\np(σ(1), σ(2)) + p(σ(2), σ(1)) ‖1 ≤\n2\np(σ(1), σ(2)) + p(σ(2), σ(1))\nThus, taking expectation w.r.t σ, we get:\nEσ‖z̃‖22 ≤ 4R2DEσ 1\n(p(σ(1), σ(2)) + p(σ(2), σ(1)))2 ≤ 4R2D m∑ i>j p(i, j) + p(j, i) (p(i, j) + p(j, i))2\nNow, since p(i, j) ≥ γ m2 , ∀ i, j, we get: Eσ‖z̃‖22 ≤ Csvm γ , where Csvm = O(m4R2D).\nKL based Surrogate: The unbiased estimator of gradient of the KL based surrogate, as given in the main text, is:\nz̃ = X> (\n(exp(s(σ(1)))− exp(R(σ(1))))eσ(1) p(σ(1))\n)\nwhere p(σ(1)) = ∑\nπ∈Sm P(π)1(π(1) = σ(1)) (P = Pt as in Alg. 2) ). Now we have:\n‖ (exp(s(σ(1)))− exp(R(σ(1))))eσ(1)\np(σ(1)) ‖1 ≤\nexp(RDU)\np(σ(1))\nThus, taking expectation w.r.t σ, we get:\nEσ‖z̃‖22 ≤ R2D exp(2RDU)Eσ 1\np(σ(1))2\nFollowing the same arguement as in squared loss, we get: Eσ‖z̃‖22 ≤ CKL\nγ , where CKL =\nm2R2D exp(2RDU).\nProof of Lemma 15 :\nProof. Let m = 3. Collection of all terms which are functions of 1st coordinate of R, i.e, R(1), in the gradient of RankSVM is: 1(R(1) > R(2))1(1 + s(2) > s(1))(e2 − e1) + 1(R(2) > R(1))1(1 + s(1) > s(2))(e1 − e2) + 1(R(1) > R(3))1(1 + s(3) > s(1))(e3 − e1) + 1(R(3) > R(1))1(1 + s(1) > s(3))(e1 − e3). Now let s(1) = 1, s(2) = 0, s(3) = 0. Then the collection becomes: 1(R(2) > R(1))(e1 − e2) + 1(R(3) > R(1))(e1 − e3) = (1(R(2) > R(1))+1(R(3) > R(1)))e1−1(R(2) > R(1))e2−1(R(3) > R(1))e3. Now, if the gradient can be decomposed over each coordinate of R, then the collection of terms associated with R(1) should only and only be a function of R(1). Specifically, (1(R(2) > R(1))+1(R(3) > R(1))) (the non-zero coefficient of e1) should be a function of only R(1) (similarly for e2 and e3).\nNow assume that the (1(R(2) > R(1))+1(R(3) > R(1))) can be expressed as a function of R(1) only. Then the difference between the coefficient’s values, for the following two cases: R(1) = 0, R(2) = 0, R(3) = 0 and R(1) = 1, R(2) = 0, R(3) = 0, would be same as the difference between the coefficient’s values, for the following two cases: R(1) = 0, R(2) = 1, R(3) = 1 and R(1) = 1, R(2) = 1, R(3) = 1 (Since the difference would be affected only by change in R(1) value). It can be clearly seen that the change in value between the first two cases is: 0−0 = 0, while the change in value between the second two cases is: 2−0 = 2. Thus, we reach a contradiction.\nProof of Lemma 16 :\nProof. The term associated with the 1st coordinate of R, i.e, R(1), in the gradient of ListNet is = ∑m\ni=1 ( − exp(R(i))∑m j=1 exp(R(j)) + exp(s(i))∑m j=1 exp(s(j)) ) ei (in fact, the same term is associated\nwith every coordinate of R).\nSpecifically, f(R) = ( − exp(R(1))∑m j=1 exp(R(j)) + exp(s1)∑m j=1 exp(s(j)) ) is the non-zero coefficient of e1, associated with R(1). Now, if f(R) would have only been a function of R(1), then ∂2f(R) ∂R(1)∂R(j) , ∀ j 6= 1 would have been zero. It can be clearly seen this is not the case.\nNow, the term associated jointly with R(1) and R(2), in the gradient of ListNet is same as before, i.e, ∑m\ni=1 ( − exp(R(i))∑m j=1 exp(R(j)) + exp(s(i))∑m j=1 exp(s(j)) ) ei (since R(1) and R(2)\nare present in all the summation terms of the gradient).\nSpecifically, f(R) = ( − exp(R(i))∑m j=1 exp(R(j)) + exp(s(i))∑m j=1 exp(s(j)) ) is the non-zero coefficient of\ne1. Now, if f(R) would have only been a function of R(1) and R(2), then ∂3f(R)\n∂R(1)∂R(2)∂R(j) ,\n∀j 6= 1, j 6= 2 would have been zero. It can be clearly seen this is not the case. The same argument can be extended for any k < m.\nProof of Theorem. 21:\nProof. (Sketch) The proof builds on the proof of hopeless finite action partial monitoring games given by Piccolboni and Schindelhauer (2001b). An examination of their proof of Theorem. 3 indicates that for hopeless games, there have to exist two probability distributions (over adversary’s actions), which are indistinguishable in terms of feedback but the optimal learner’s actions for the distributions are different. We first provide a mathematical explanation as to why such existence lead to hopeless games. Then, we provide a characterization of indistinguishable probability distributions in our problem setting, and then exploit the characterization of optimal actions for NDCG calibrated surrogates (Theorem 20) to explicitly construct two such probability distributions. This proves the result. Full proof is given below.\nProof. We will first fix the setting of the online game. We consider m = 3 and fixed the document matrix X ∈ R3×3 to be the identity. At each round of the game, the adversary generates the fixed X and the learner chooses a score vector s ∈ R3. Making the matrix X identity makes the distinction between weight vectors w and scores s irrelevant since s = Xw = w. We note that allowing the adversary to vary X over the rounds only makes him more powerful, which can only increase the regret. We also restrict the adversary to choose binary relevance vectors. Once again, allowing adversary to choose multi-graded relevance vectors only makes it more powerful. Thus, in this setting, the adversary can now choose among 23 = 8 possible relevance vectors. The learner’s action set is infinite, i.e., the learner can choose any score vector s = Xw = Rm. The loss function φ(s,R) is any NDCG calibrated surrogate and feedback is the relevance of top-ranked item at each round, where ranking is induced by sorted order (descending) of score vector. We will use p to denote randomized adversary one-short strategies, i.e. distributions over the 8 possible relevance score vectors. Let s∗p = argmins ER∼pφ(s,R). We note that in the definition of NDCG calibrated surrogates, Ravikumar et al. (2011) assume that the optimal score vector for each distribution over relevance vectors is unique and we subscribe to that assumption. The assumption was taken to avoid some boundary conditions.\nIt remains to specify the choice of U , a bound on the Euclidean norm of the weight vectors (same as score vectors for us right now) that is used to define the best loss in\nhindsight. It never makes sense for the learner to play anything outside the set ∪ps∗p so that we can set U = max{‖s‖2 : s ∈ ∪ps∗p}.\nThe paragraph following Lemma 6 of Thm. 3 in Piccolboni and Schindelhauer (2001b) gives the main intuition behind the argument the authors developed to prove hopelessness of finite action partial monitoring games. To make our proof self contained, we will explain the intuition in a rigorous way.\nKey insight: Two adversary strategies p, p̃ are said to be indistinguishable from the learner’s feedback perspective, if for every action of the learner, the probability distribution over the feedbacks received by learner is the same for p and p̃. Now assume that adversary always selects actions according to one of the two such indistinguishable strategies. Thus, the learner will always play one of s∗p and s ∗ p̃. By uniqueness, s ∗ p 6= s∗p̃. Then, the learner incurs a constant (non-zero) regret on any round where adversary plays according to p and learner plays s∗p̃, or if the adversary plays according to p̃ and learner plays s ∗ p. We show that in such a setting, adversary can simply play according to (p+ p̃)/2 and the learner suffers an expected regret of Ω(T ).\nAssume that the adversary selects {R1, . . . , RT } from product distribution ⊗p. Let the number of times the learner plays s∗p and s ∗ p̃ be denoted by random variables N p 1 and Np2 respectively, where N p shows the exclusive dependence on p. It is always true that Np1 + N p 2 = T . Moreover, let the expected per round regret be p when learner plays s ∗ p̃ , where the expectation is taken over the randomization of adversary. Now, assume that adversary selects {R1, . . . , RT } from product distribution ⊗p̃. The corresponding notations become N p̃1 and N p̃ 2 and p̃. Then,\nE(R1,...,RT )∼⊗pE(s1,...,sT )[Regret((s1, . . . , sT ), (R1, . . . , RT ))] = 0 · E[N p 1 ] + p · E[N p 2 ]\nand\nE(R1,...,RT )∼⊗p̃E(s1,...,sT )[Regret((s1, . . . , sT ), (R1, . . . , RT ))] = p̃ · E[N p̃ 1 ] + 0 · E[N p̃ 2 ]\nSince p and p̃ are indistinguishable from perspective of learner, E[Np1 ] = E[N p̃ 1 ] = E[N1] and E[Np2 ] = E[N p̃ 2 ] = E[N2]. That is, the random variable denoting number of times s∗p is played by learner does not depend on adversary distribution (same for s∗p̃.). Using this fact and averaging the two expectations, we get:\nE(R1,...,RT )∼⊗p+⊗p̃2 E(s1,...,sT )[Regret((s1, . . . , sT ), (R1, . . . , RT ))] = p̃ 2 · E[N1] + p 2 · E[N2]\n≥ min( p 2 , p̃ 2 ) · E[N1 +N2] = · T\nSince\nsup R1,...,RT\nE[Regret((s1, . . . , sT ), (R1, . . . , RT ))] ≥\nE(R1,...,RT )∼⊗p+⊗p̃2 E(s1,...,sT )[Regret((s1, . . . , sT ), (R1, . . . , RT ))]\nwe conclude that for every learner algorithm, adversary has a strategy, s.t. learner suffers an expected regret of Ω(T ).\nNow, the thing left to be shown is the existence of two indistinguishable distributions p and p̃, s.t. s∗p 6= s∗p̃.\nCharacterization of indistinguishable strategies in our problem setting: Two adversary’s strategies p and p̃ will be indistinguishable, in our problem setting, if for every score vector s, the relevances of the top-ranked item, according to s, are same for relevance vector drawn from p and p̃. Since relevance vectors are restricted to be binary, mathematically, it means that ∀s, PR∼p(R(πs(1)) = 1) = PR∼p̃(R(πs(1)) = 1) (actually, we also need ∀s, PR∼p(R(πs(1)) = 0) = PR∼p̃(R(πs(1)) = 0), but due to the binary nature, PR∼p(R(πs(1)) = 1) = PR∼p̃(R(πs(1)) = 1) =⇒ PR∼p(R(πs(1)) = 0) = PR∼p̃(R(πs(1)) = 0)). Since the equality has to hold ∀s, this implies ∀j ∈ [m], PR∼p(R(j) = 1) = PR∼p̃(R(j) = 1) (as every item will be ranked at top by some score vector). Hence, ∀j ∈ [m], ER∼p[R(j)] = ER∼p̃[R(j)] =⇒ ER∼p[R] = ER∼p̃[R]. It can be seen clearly that the chain of implications can be reversed. Hence, ∀s, PR∼p(R(πs(1)) = 1) = PR∼p̃(R(πs(1)) = 1) ⇐⇒ ER∼p[R] = ER∼p̃[R].\nExplicit adversary strategies: Following from the discussion so far and Theorem 20, if we can show existence of two strategies p and p̃ s.t. ER∼p[R] = ER∼p̃[R], but argsort ( ER∼p [ G(R) Z(R) ]) 6=\nargsort ( ER∼p̃ [ G(R) Z(R) ]) , we are done.\nThe 8 possible relevance vectors (adversary’s actions) are (R1, R2, R3, R4, R5, R6, R7, R8) = (000, 110, 101, 011, 100, 010, 001, 111). Let the two probability vectors be:\np = (0.0, 0.1, 0.15, 0.05, 0.2, 0.3, 0.2, 0.0)\np̃ = (0.0, 0.3, 0.0, 0.0, 0.15, 0.15, 0.4, 0.0).\nThe data is provided in table format in Table. 3. Under the two distributions, it can be checked that ER∼p[R] = ER∼p̃[R] = (0.45, 0.45, 0.4)>. However, ER∼p [ G(R) Z(R) ] = (0.3533, 0.3920, 0.3226)>, but ER∼p̃ [ G(R) Z(R) ] = (0.3339, 0.3339, 0.4000)>.\nHence, argsort ( ER∼p [ G(R) Z(R) ]) = [2, 1, 3]> but argsort ( ER∼p̃ [ G(R) Z(R) ]) ∈ {[3, 1, 2]>, [3, 2, 1]>}."
    } ],
    "references" : [ {
      "title" : "Diversifying search results",
      "author" : [ "Rakesh Agrawal", "Sreenivas Gollapudi", "Alan Halverson", "Samuel Ieong" ],
      "venue" : "In Proceedings of the Second ACM International Conference on Web Search and Data Mining,",
      "citeRegEx" : "Agrawal et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Agrawal et al\\.",
      "year" : 2009
    }, {
      "title" : "Improved bounds for online learning over the permutahedron and other ranking polytopes",
      "author" : [ "Nir Ailon" ],
      "venue" : "In Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "Ailon.,? \\Q2014\\E",
      "shortCiteRegEx" : "Ailon.",
      "year" : 2014
    }, {
      "title" : "Modern information retrieval, volume 463",
      "author" : [ "Ricardo Baeza-Yates", "Berthier Ribeiro-Neto" ],
      "venue" : "ACM Press,",
      "citeRegEx" : "Baeza.Yates and Ribeiro.Neto.,? \\Q1999\\E",
      "shortCiteRegEx" : "Baeza.Yates and Ribeiro.Neto.",
      "year" : 1999
    }, {
      "title" : "Convexity, classification, and risk bounds",
      "author" : [ "Peter L Bartlett", "Michael I Jordan", "Jon D McAuliffe" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "Bartlett et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Bartlett et al\\.",
      "year" : 2006
    }, {
      "title" : "Partial monitoring with side information",
      "author" : [ "Gábor Bartók", "Csaba Szepesvári" ],
      "venue" : "In Algorithmic Learning Theory,",
      "citeRegEx" : "Bartók and Szepesvári.,? \\Q2012\\E",
      "shortCiteRegEx" : "Bartók and Szepesvári.",
      "year" : 2012
    }, {
      "title" : "Partial monitoringclassification, regret bounds, and algorithms",
      "author" : [ "Gabor Bartok", "Dean P. Foster", "David Pal", "Alexander Rakhlin", "Csaba Szepesvari" ],
      "venue" : "Mathematics of Operations Research,",
      "citeRegEx" : "Bartok et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bartok et al\\.",
      "year" : 2014
    }, {
      "title" : "Approximating matrix p-norms",
      "author" : [ "Aditya Bhaskara", "Aravindan Vijayaraghavan" ],
      "venue" : "In Proceedings of the twenty-second annual ACM-SIAM symposium on Discrete Algorithms,",
      "citeRegEx" : "Bhaskara and Vijayaraghavan.,? \\Q2011\\E",
      "shortCiteRegEx" : "Bhaskara and Vijayaraghavan.",
      "year" : 2011
    }, {
      "title" : "Learning, regret minimization, and equilibria",
      "author" : [ "A. Blum", "Y. Mansour" ],
      "venue" : null,
      "citeRegEx" : "Blum and Mansour.,? \\Q2007\\E",
      "shortCiteRegEx" : "Blum and Mansour.",
      "year" : 2007
    }, {
      "title" : "Multi-scale exploration of convex functions and bandit convex optimization",
      "author" : [ "Sébastien Bubeck", "Ronen Eldan" ],
      "venue" : "arXiv preprint arXiv:1507.06580,",
      "citeRegEx" : "Bubeck and Eldan.,? \\Q2015\\E",
      "shortCiteRegEx" : "Bubeck and Eldan.",
      "year" : 2015
    }, {
      "title" : "On the (non-) existence of convex, calibrated surrogate losses for ranking",
      "author" : [ "Clément Calauzenes", "Nicolas Usunier", "Patrick Gallinari" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Calauzenes et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Calauzenes et al\\.",
      "year" : 2012
    }, {
      "title" : "Learning to rank: from pairwise approach to listwise approach",
      "author" : [ "Zhe Cao", "Tao Qin", "Tie-Yan Liu", "Ming-Feng Tsai", "Hang Li" ],
      "venue" : "In Proceedings of the 24th International conference on Machine learning,",
      "citeRegEx" : "Cao et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Cao et al\\.",
      "year" : 2007
    }, {
      "title" : "Prediction, learning, and games",
      "author" : [ "Nicolo Cesa-Bianchi" ],
      "venue" : null,
      "citeRegEx" : "Cesa.Bianchi.,? \\Q2006\\E",
      "shortCiteRegEx" : "Cesa.Bianchi.",
      "year" : 2006
    }, {
      "title" : "Yahoo! learning to rank challenge overview",
      "author" : [ "Olivier Chapelle", "Yi Chang" ],
      "venue" : "Journal of Machine Learning Research-Proceedings Track,",
      "citeRegEx" : "Chapelle and Chang.,? \\Q2011\\E",
      "shortCiteRegEx" : "Chapelle and Chang.",
      "year" : 2011
    }, {
      "title" : "Gradient descent optimization of smoothed information retrieval metrics",
      "author" : [ "Olivier Chapelle", "Mingrui Wu" ],
      "venue" : "Information retrieval,",
      "citeRegEx" : "Chapelle and Wu.,? \\Q2010\\E",
      "shortCiteRegEx" : "Chapelle and Wu.",
      "year" : 2010
    }, {
      "title" : "Large margin optimization of ranking measures",
      "author" : [ "Olivier Chapelle", "Quoc Le", "Alex Smola" ],
      "venue" : "In NIPS Workshop: Machine Learning for Web Search,",
      "citeRegEx" : "Chapelle et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Chapelle et al\\.",
      "year" : 2007
    }, {
      "title" : "Auc optimization vs. error rate minimization",
      "author" : [ "Corinna Cortes", "Mehryar Mohri" ],
      "venue" : "Advances in neural information processing systems,",
      "citeRegEx" : "Cortes and Mohri.,? \\Q2004\\E",
      "shortCiteRegEx" : "Cortes and Mohri.",
      "year" : 2004
    }, {
      "title" : "Subset ranking using regression",
      "author" : [ "David Cossock", "Tong Zhang" ],
      "venue" : "In Conference on Learning theory,",
      "citeRegEx" : "Cossock and Zhang.,? \\Q2006\\E",
      "shortCiteRegEx" : "Cossock and Zhang.",
      "year" : 2006
    }, {
      "title" : "Statistical analysis of bayes optimal subset ranking",
      "author" : [ "David Cossock", "Tong Zhang" ],
      "venue" : "Information Theory, IEEE Transactions on,",
      "citeRegEx" : "Cossock and Zhang.,? \\Q2008\\E",
      "shortCiteRegEx" : "Cossock and Zhang.",
      "year" : 2008
    }, {
      "title" : "On the consistency of ranking algorithms",
      "author" : [ "John C. Duchi", "Lester W. Mackey", "Michael I. Jordan" ],
      "venue" : "In Proceedings of the 27th International Conference on Machine Learning,",
      "citeRegEx" : "Duchi et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Duchi et al\\.",
      "year" : 2010
    }, {
      "title" : "Online convex optimization in the bandit setting",
      "author" : [ "Abraham D Flaxman", "Adam Tauman Kalai", "H Brendan McMahan" ],
      "venue" : "In Proceedings of the sixteenth annual ACM-SIAM symposium on Discrete algorithms,",
      "citeRegEx" : "Flaxman et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Flaxman et al\\.",
      "year" : 2005
    }, {
      "title" : "No internal regret via neighborhood watch",
      "author" : [ "Dean P. Foster", "Alexander Rakhlin" ],
      "venue" : "In International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "Foster and Rakhlin.,? \\Q2012\\E",
      "shortCiteRegEx" : "Foster and Rakhlin.",
      "year" : 2012
    }, {
      "title" : "On multilabel classification and ranking with bandit feedback",
      "author" : [ "Claudio Gentile", "Francesco Orabona" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Gentile and Orabona.,? \\Q2014\\E",
      "shortCiteRegEx" : "Gentile and Orabona.",
      "year" : 2014
    }, {
      "title" : "Balancing exploration and exploitation in listwise and pairwise online learning to rank",
      "author" : [ "Katja Hofmann", "Shimon Whiteson", "Maarten de Rijke" ],
      "venue" : "Information Retrieval,",
      "citeRegEx" : "Hofmann et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Hofmann et al\\.",
      "year" : 2013
    }, {
      "title" : "IR evaluation methods for retrieving highly relevant documents",
      "author" : [ "Kalervo Järvelin", "Jaana Kekäläinen" ],
      "venue" : "In Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval,",
      "citeRegEx" : "Järvelin and Kekäläinen.,? \\Q2000\\E",
      "shortCiteRegEx" : "Järvelin and Kekäläinen.",
      "year" : 2000
    }, {
      "title" : "Cumulated gain-based evaluation of IR techniques",
      "author" : [ "Kalervo Järvelin", "Jaana Kekäläinen" ],
      "venue" : "ACM Transactions on Information Systems,",
      "citeRegEx" : "Järvelin and Kekäläinen.,? \\Q2002\\E",
      "shortCiteRegEx" : "Järvelin and Kekäläinen.",
      "year" : 2002
    }, {
      "title" : "Optimizing search engines using clickthrough data",
      "author" : [ "Thorsten Joachims" ],
      "venue" : "In Proceedings of the 8th ACM SIGKDD,",
      "citeRegEx" : "Joachims.,? \\Q2002\\E",
      "shortCiteRegEx" : "Joachims.",
      "year" : 2002
    }, {
      "title" : "Efficient algorithms for online decision problems",
      "author" : [ "Adam Kalai", "Santosh Vempala" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "Kalai and Vempala.,? \\Q2005\\E",
      "shortCiteRegEx" : "Kalai and Vempala.",
      "year" : 2005
    }, {
      "title" : "The value of knowing a demand curve: Bounds on regret for online posted-price auctions",
      "author" : [ "Robert Kleinberg", "Tom Leighton" ],
      "venue" : "In Foundations of Computer Science,",
      "citeRegEx" : "Kleinberg and Leighton.,? \\Q2003\\E",
      "shortCiteRegEx" : "Kleinberg and Leighton.",
      "year" : 2003
    }, {
      "title" : "The epoch-greedy algorithm for multi-armed bandits with side information",
      "author" : [ "John Langford", "Tong Zhang" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Langford and Zhang.,? \\Q2008\\E",
      "shortCiteRegEx" : "Langford and Zhang.",
      "year" : 2008
    }, {
      "title" : "Combinatorial partial monitoring game with linear feedback and its applications",
      "author" : [ "Tian Lin", "Bruno Abrahao", "Robert Kleinberg", "John Lui" ],
      "venue" : "In Proceedings of the 31th International Conference on Machine Learning,",
      "citeRegEx" : "Lin et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2014
    }, {
      "title" : "Learning to rank for information retrieval",
      "author" : [ "Tie-Yan Liu" ],
      "venue" : "Springer Science & Business Media,",
      "citeRegEx" : "Liu.,? \\Q2011\\E",
      "shortCiteRegEx" : "Liu.",
      "year" : 2011
    }, {
      "title" : "Letor: Benchmark dataset for research on learning to rank for information retrieval",
      "author" : [ "Tie-Yan Liu", "Jun Xu", "Tao Qin", "Wenying Xiong", "Hang Li" ],
      "venue" : "In Proceedings of SIGIR 2007 workshop on learning to rank for information retrieval,",
      "citeRegEx" : "Liu et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2007
    }, {
      "title" : "Discrete prediction games with arbitrary feedback and loss",
      "author" : [ "Antonio Piccolboni", "Christian Schindelhauer" ],
      "venue" : "In Computational Learning Theory,",
      "citeRegEx" : "Piccolboni and Schindelhauer.,? \\Q2001\\E",
      "shortCiteRegEx" : "Piccolboni and Schindelhauer.",
      "year" : 2001
    }, {
      "title" : "Discrete prediction games with arbitrary feedback and loss",
      "author" : [ "Antonio Piccolboni", "Christian Schindelhauer" ],
      "venue" : "In COLT,",
      "citeRegEx" : "Piccolboni and Schindelhauer.,? \\Q2001\\E",
      "shortCiteRegEx" : "Piccolboni and Schindelhauer.",
      "year" : 2001
    }, {
      "title" : "Learning diverse rankings with multi-armed bandits",
      "author" : [ "Filip Radlinski", "Robert Kleinberg", "Thorsten Joachims" ],
      "venue" : "In Proceedings of the 25th International conference on Machine learning,",
      "citeRegEx" : "Radlinski et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Radlinski et al\\.",
      "year" : 2008
    }, {
      "title" : "Redundancy, diversity and interdependent document relevance",
      "author" : [ "Filip Radlinski", "Paul N Bennett", "Ben Carterette", "Thorsten Joachims" ],
      "venue" : "In ACM SIGIR,",
      "citeRegEx" : "Radlinski et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Radlinski et al\\.",
      "year" : 2009
    }, {
      "title" : "On NDCG consistency of listwise ranking methods",
      "author" : [ "Pradeep Ravikumar", "Ambuj Tewari", "Eunho Yang" ],
      "venue" : "In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "Ravikumar et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Ravikumar et al\\.",
      "year" : 2011
    }, {
      "title" : "Test collection based evaluation of information retrieval systems, volume 13",
      "author" : [ "Mark Sanderson" ],
      "venue" : "Now Publishers Inc,",
      "citeRegEx" : "Sanderson.,? \\Q2010\\E",
      "shortCiteRegEx" : "Sanderson.",
      "year" : 2010
    }, {
      "title" : "A cross-benchmark comparison of 87 learning to rank methods",
      "author" : [ "Niek Tax", "Sander Bockting", "Djoerd Hiemstra" ],
      "venue" : "Information Processing and Management,",
      "citeRegEx" : "Tax et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Tax et al\\.",
      "year" : 2015
    }, {
      "title" : "Efficient learning in large-scale combinatorial semi-bandits",
      "author" : [ "Zheng Wen", "Branislav Kveton", "Azin Ashkan" ],
      "venue" : "In Proceedings of the 32nd International Conference on Machine Learning,",
      "citeRegEx" : "Wen et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Wen et al\\.",
      "year" : 2014
    }, {
      "title" : "A support vector method for optimizing average precision",
      "author" : [ "Yisong Yue", "Thomas Finley", "Filip Radlinski", "Thorsten Joachims" ],
      "venue" : "In Proceedings of ACM SIGIR,",
      "citeRegEx" : "Yue et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Yue et al\\.",
      "year" : 2007
    }, {
      "title" : "Online convex programming and generalized infinitesimal gradient ascent",
      "author" : [ "Martin Zinkevich" ],
      "venue" : "In Proceedings of the 20th International Conference on Machine Learning",
      "citeRegEx" : "Zinkevich.,? \\Q2003\\E",
      "shortCiteRegEx" : "Zinkevich.",
      "year" : 2003
    } ],
    "referenceMentions" : [ {
      "referenceID" : 30,
      "context" : "Learning to rank (Liu, 2011) is a supervised machine learning problem, where the output space consists of rankings of objects.",
      "startOffset" : 17,
      "endOffset" : 28
    }, {
      "referenceID" : 23,
      "context" : "The accuracy of a ranked list, in comparison to the actual relevance of the documents, is measured by various ranking measures, such as Discounted Cumulative Gain (DCG) (Järvelin and Kekäläinen, 2000), Average Precision (AP) (Baeza-Yates and Ribeiro-Neto, 1999) and others.",
      "startOffset" : 169,
      "endOffset" : 200
    }, {
      "referenceID" : 2,
      "context" : "The accuracy of a ranked list, in comparison to the actual relevance of the documents, is measured by various ranking measures, such as Discounted Cumulative Gain (DCG) (Järvelin and Kekäläinen, 2000), Average Precision (AP) (Baeza-Yates and Ribeiro-Neto, 1999) and others.",
      "startOffset" : 225,
      "endOffset" : 261
    }, {
      "referenceID" : 37,
      "context" : "In certain applications, such as deploying a new web app or developing a custom search engine, collecting large amount of high quality labeled data might be infeasible (Sanderson, 2010).",
      "startOffset" : 168,
      "endOffset" : 185
    }, {
      "referenceID" : 22,
      "context" : "One type of online ranking models learn from implicit feedback inferred from user clicks on ranked lists (Hofmann et al., 2013).",
      "startOffset" : 105,
      "endOffset" : 127
    }, {
      "referenceID" : 25,
      "context" : "Moreover, a clicked item might not actually be relevant to the user and there is also the problem of bias towards top ranked items in inferring feedback from user clicks (Joachims, 2002).",
      "startOffset" : 170,
      "endOffset" : 186
    }, {
      "referenceID" : 0,
      "context" : "Non-contextual setting: Existing work loosely related to ranking of a fixed set of items to satisfy diverse user preferences (Radlinski et al., 2008, 2009; Agrawal et al., 2009; Wen et al., 2014) has focused on learning an optimal ranking of a subset of items, to be presented to an user, with performance judged by a simple 0-1 loss.",
      "startOffset" : 125,
      "endOffset" : 195
    }, {
      "referenceID" : 39,
      "context" : "Non-contextual setting: Existing work loosely related to ranking of a fixed set of items to satisfy diverse user preferences (Radlinski et al., 2008, 2009; Agrawal et al., 2009; Wen et al., 2014) has focused on learning an optimal ranking of a subset of items, to be presented to an user, with performance judged by a simple 0-1 loss.",
      "startOffset" : 125,
      "endOffset" : 195
    }, {
      "referenceID" : 11,
      "context" : "The appropriate framework to study the problem is that of partial monitoring (Cesa-Bianchi, 2006).",
      "startOffset" : 77,
      "endOffset" : 97
    }, {
      "referenceID" : 29,
      "context" : "A very recent paper shows another practical application of partial monitoring in the stochastic setting (Lin et al., 2014).",
      "startOffset" : 104,
      "endOffset" : 122
    }, {
      "referenceID" : 5,
      "context" : "Recent advances in the classification of partial monitoring games tell us that the minimax regret, in an adversarial setting, is governed by a property of the loss and feedback functions called observability (Bartok et al., 2014; Foster and Rakhlin, 2012), where observability is of two kinds: local and global.",
      "startOffset" : 208,
      "endOffset" : 255
    }, {
      "referenceID" : 20,
      "context" : "Recent advances in the classification of partial monitoring games tell us that the minimax regret, in an adversarial setting, is governed by a property of the loss and feedback functions called observability (Bartok et al., 2014; Foster and Rakhlin, 2012), where observability is of two kinds: local and global.",
      "startOffset" : 208,
      "endOffset" : 255
    }, {
      "referenceID" : 18,
      "context" : "We prove that, for some ranking measures, namely PairwiseLoss (Duchi et al., 2010), DCG and Precision@n (Liu et al.",
      "startOffset" : 62,
      "endOffset" : 82
    }, {
      "referenceID" : 31,
      "context" : ", 2010), DCG and Precision@n (Liu et al., 2007), global observability holds.",
      "startOffset" : 29,
      "endOffset" : 47
    }, {
      "referenceID" : 15,
      "context" : "For example, the normalized versions of PairwiseLoss, DCG and Precision@n are called AUC (Cortes and Mohri, 2004), NDCG (Järvelin and Kekäläinen, 2002) and AP respectively.",
      "startOffset" : 89,
      "endOffset" : 113
    }, {
      "referenceID" : 24,
      "context" : "For example, the normalized versions of PairwiseLoss, DCG and Precision@n are called AUC (Cortes and Mohri, 2004), NDCG (Järvelin and Kekäläinen, 2002) and AP respectively.",
      "startOffset" : 120,
      "endOffset" : 151
    }, {
      "referenceID" : 10,
      "context" : ", the cross entropy surrogate in ListNet (Cao et al., 2007) and hinge surrogate in RankSVM (Joachims, 2002), instead of discontinuous ranking measures like DCG, or AP, because the latter lead to intractable optimization problems in the query-documents setting.",
      "startOffset" : 41,
      "endOffset" : 59
    }, {
      "referenceID" : 25,
      "context" : ", 2007) and hinge surrogate in RankSVM (Joachims, 2002), instead of discontinuous ranking measures like DCG, or AP, because the latter lead to intractable optimization problems in the query-documents setting.",
      "startOffset" : 39,
      "endOffset" : 55
    }, {
      "referenceID" : 17,
      "context" : "The convex surrogates considered are from three major learning to ranking methods: squared loss from a pointwise method (Cossock and Zhang, 2008), hinge loss used in the pairwise RankSVM (Joachims, 2002) method, and (modified) cross-entropy surrogate used in the listwise ListNet (Cao et al.",
      "startOffset" : 120,
      "endOffset" : 145
    }, {
      "referenceID" : 25,
      "context" : "The convex surrogates considered are from three major learning to ranking methods: squared loss from a pointwise method (Cossock and Zhang, 2008), hinge loss used in the pairwise RankSVM (Joachims, 2002) method, and (modified) cross-entropy surrogate used in the listwise ListNet (Cao et al.",
      "startOffset" : 187,
      "endOffset" : 203
    }, {
      "referenceID" : 10,
      "context" : "The convex surrogates considered are from three major learning to ranking methods: squared loss from a pointwise method (Cossock and Zhang, 2008), hinge loss used in the pairwise RankSVM (Joachims, 2002) method, and (modified) cross-entropy surrogate used in the listwise ListNet (Cao et al., 2007) method.",
      "startOffset" : 280,
      "endOffset" : 298
    }, {
      "referenceID" : 14,
      "context" : "For example, the normalized versions of PairwiseLoss, DCG and Precision@n are called AUC (Cortes and Mohri, 2004), NDCG (Järvelin and Kekäläinen, 2002) and AP respectively. We show an unexpected result for the normalized versions: they do not admit sub-linear regret algorithms with top 1 feedback. This is despite the fact that the opposite is true for their unnormalized counterparts! Intuitively, the normalization makes it hard to construct an unbiased estimators of the (unobserved) relevance vectors. Surprisingly, we are able to translate this intuitive hurdle into a provable impossibility. We also present some preliminary experiments on simulated datasets to explore the performance of our efficient algorithm and compare its regret to its full information counterpart. Contextual Setting: The requirement of having a fixed set of items to rank, in the first part of our work, somewhat limits practical applicability. In fact, in the classic multiarmed bandit problem, while non-contextual bandits have received a lot of attention, the authors Langford and Zhang (2008) mention that “settings with no context information are rare in practice”.",
      "startOffset" : 90,
      "endOffset" : 1080
    }, {
      "referenceID" : 13,
      "context" : "SmoothDCG surrogate (Chapelle and Wu, 2010).",
      "startOffset" : 20,
      "endOffset" : 43
    }, {
      "referenceID" : 36,
      "context" : "The convex surrogates we mentioned above are widely used but are known to fail to be calibrated with respect to NDCG (Ravikumar et al., 2011).",
      "startOffset" : 117,
      "endOffset" : 141
    }, {
      "referenceID" : 9,
      "context" : "We only focus on NDCG calibrated surrogates for the impossibility results since no (convex) surrogate can be calibrated for AP and ERR (Calauzenes et al., 2012).",
      "startOffset" : 135,
      "endOffset" : 160
    }, {
      "referenceID" : 27,
      "context" : "Note, however, that there does exist work on partial monitoring problems with continuous learner actions, but without side information (Kleinberg and Leighton, 2003; Cesa-Bianchi, 2006), and vice versa (Bartók and Szepesvári, 2012; Gentile and Orabona, 2014).",
      "startOffset" : 135,
      "endOffset" : 185
    }, {
      "referenceID" : 11,
      "context" : "Note, however, that there does exist work on partial monitoring problems with continuous learner actions, but without side information (Kleinberg and Leighton, 2003; Cesa-Bianchi, 2006), and vice versa (Bartók and Szepesvári, 2012; Gentile and Orabona, 2014).",
      "startOffset" : 135,
      "endOffset" : 185
    }, {
      "referenceID" : 4,
      "context" : "Note, however, that there does exist work on partial monitoring problems with continuous learner actions, but without side information (Kleinberg and Leighton, 2003; Cesa-Bianchi, 2006), and vice versa (Bartók and Szepesvári, 2012; Gentile and Orabona, 2014).",
      "startOffset" : 202,
      "endOffset" : 258
    }, {
      "referenceID" : 21,
      "context" : "Note, however, that there does exist work on partial monitoring problems with continuous learner actions, but without side information (Kleinberg and Leighton, 2003; Cesa-Bianchi, 2006), and vice versa (Bartók and Szepesvári, 2012; Gentile and Orabona, 2014).",
      "startOffset" : 202,
      "endOffset" : 258
    }, {
      "referenceID" : 1,
      "context" : "It has been shown by Ailon (2014) that regret under the two measures are equal:",
      "startOffset" : 21,
      "endOffset" : 34
    }, {
      "referenceID" : 5,
      "context" : "Our main results on regret bounds build on some of the theory for abstract partial monitoring games developed by Bartok et al. (2014) and Foster and Rakhlin (2012).",
      "startOffset" : 113,
      "endOffset" : 134
    }, {
      "referenceID" : 5,
      "context" : "Our main results on regret bounds build on some of the theory for abstract partial monitoring games developed by Bartok et al. (2014) and Foster and Rakhlin (2012). For ease of understanding, we reproduce the relevant notations",
      "startOffset" : 113,
      "endOffset" : 164
    }, {
      "referenceID" : 5,
      "context" : "The following definitions, given for abstract problems by Bartok et al. (2014), has been refined to fit our problem context.",
      "startOffset" : 58,
      "endOffset" : 79
    }, {
      "referenceID" : 5,
      "context" : "First, we get a lower bound by combining our Theorem 4 with Theorem 4 of Bartok et al. (2014).",
      "startOffset" : 73,
      "endOffset" : 94
    }, {
      "referenceID" : 11,
      "context" : "1 in Cesa-Bianchi (2006), gives an algorithm (inspired by the algorithm originally given in Piccolboni and Schindelhauer (2001a)) obtaining O(T 2/3) regret.",
      "startOffset" : 5,
      "endOffset" : 25
    }, {
      "referenceID" : 11,
      "context" : "1 in Cesa-Bianchi (2006), gives an algorithm (inspired by the algorithm originally given in Piccolboni and Schindelhauer (2001a)) obtaining O(T 2/3) regret.",
      "startOffset" : 5,
      "endOffset" : 129
    }, {
      "referenceID" : 11,
      "context" : "1 in Cesa-Bianchi (2006), gives an algorithm (inspired by the algorithm originally given in Piccolboni and Schindelhauer (2001a)) obtaining O(T 2/3) regret. Corollary 6. The algorithm in Figure 1 of Cesa-Bianchi (2006) achieves O(T 2/3) regret bound for SumLoss.",
      "startOffset" : 5,
      "endOffset" : 219
    }, {
      "referenceID" : 11,
      "context" : "However, the algorithm in Cesa-Bianchi (2006) is intractable in our setting since the algorithm necessarily enumerates all the actions of the learner in each round, which is exponential in m in our case (m! to be exact).",
      "startOffset" : 26,
      "endOffset" : 46
    }, {
      "referenceID" : 5,
      "context" : "Theorem 4 of Bartok et al. (2014) says the following: A partial monitoring game which is both globally and locally observable has minimax regret Θ(T 1/2), while a game which is globally observable but not locally observable has minimax regret Θ(T 2/3).",
      "startOffset" : 13,
      "endOffset" : 34
    }, {
      "referenceID" : 7,
      "context" : "Our algorithm is motivated by the reduction from bandit-feedback to full feedback scheme given in Blum and Mansour (2007). However, the reduction cannot be directly applied to our problem, because we are not in the bandit setting and hence do not know loss of any action.",
      "startOffset" : 98,
      "endOffset" : 122
    }, {
      "referenceID" : 7,
      "context" : "Our algorithm is motivated by the reduction from bandit-feedback to full feedback scheme given in Blum and Mansour (2007). However, the reduction cannot be directly applied to our problem, because we are not in the bandit setting and hence do not know loss of any action. Further, the algorithm of Blum and Mansour (2007) necessarily spends N rounds per block to try out each of the N available actions — this is impractical in our setting since N = m!.",
      "startOffset" : 98,
      "endOffset" : 322
    }, {
      "referenceID" : 26,
      "context" : "2 Computationally Efficient Algorithm with FTPL We instantiate our general algorithm with Follow The Perturbed Leader (FTPL) full information algorithm (Kalai and Vempala, 2005).",
      "startOffset" : 152,
      "endOffset" : 177
    }, {
      "referenceID" : 5,
      "context" : "Combining the above lemma with Theorem 2 of Bartok et al. (2014), we conclude that there cannot exist any algorithm which has sub-linear regret for any of the following measures: NDCG, AP or AUC, when restricted to top 1 feedback.",
      "startOffset" : 44,
      "endOffset" : 65
    }, {
      "referenceID" : 16,
      "context" : "1 Convex Surrogates Pointwise Method: We will construct the unbiased estimator of the gradient of squared loss (Cossock and Zhang, 2006): φsq(s,R) = ‖s − R‖2.",
      "startOffset" : 111,
      "endOffset" : 136
    }, {
      "referenceID" : 25,
      "context" : "Pairwise Method: We will construct the unbiased estimator of the gradient of hingelike surrogate in RankSVM (Joachims, 2002): φsvm(s,R) = ∑ i 6=j=1 1(R(i) > R(j)) max(0, 1+ s(j)− s(i)).",
      "startOffset" : 108,
      "endOffset" : 124
    }, {
      "referenceID" : 10,
      "context" : "We will focus on the cross-entropy surrogate used in the highly cited ListNet (Cao et al., 2007) ranking algorithm and show how a very natural modification to the surrogate makes its gradient estimable in our partial feedback setting.",
      "startOffset" : 78,
      "endOffset" : 96
    }, {
      "referenceID" : 14,
      "context" : "For example, the class of popular listwise surrogates that are developed from structured prediction perspective (Chapelle et al., 2007; Yue et al., 2007) cannot have unbiased estimator of gradients from top-k feedback since they are based on maps from full relevance vectors to full rankings and thus cannot be decomposed over k = 1 or 2 coordinates of R.",
      "startOffset" : 112,
      "endOffset" : 153
    }, {
      "referenceID" : 40,
      "context" : "For example, the class of popular listwise surrogates that are developed from structured prediction perspective (Chapelle et al., 2007; Yue et al., 2007) cannot have unbiased estimator of gradients from top-k feedback since they are based on maps from full relevance vectors to full rankings and thus cannot be decomposed over k = 1 or 2 coordinates of R.",
      "startOffset" : 112,
      "endOffset" : 153
    }, {
      "referenceID" : 13,
      "context" : "We choose the SmoothDCG surrogate given in (Chapelle and Wu, 2010), which has been shown to have very competitive empirical performance.",
      "startOffset" : 43,
      "endOffset" : 66
    }, {
      "referenceID" : 13,
      "context" : "SmoothDCG, like ListNet, defines a family of surrogates, based on the cut-off point of DCG (see original paper (Chapelle and Wu, 2010) for details).",
      "startOffset" : 111,
      "endOffset" : 134
    }, {
      "referenceID" : 41,
      "context" : "4 Regret Bounds The underlying deterministic part of our algorithm is online gradient descent (OGD) (Zinkevich, 2003).",
      "startOffset" : 100,
      "endOffset" : 117
    }, {
      "referenceID" : 19,
      "context" : "1 of (Flaxman et al., 2005), in our problem setting is:",
      "startOffset" : 5,
      "endOffset" : 27
    }, {
      "referenceID" : 6,
      "context" : "To get bound on Et‖z̃t‖2, we used the following norm relation that holds for any matrix X (Bhaskara and Vijayaraghavan, 2011): ‖X‖p→q = sup v 6=0 ‖Xv‖q ‖v‖p , where q is the dual exponent of p (i.",
      "startOffset" : 90,
      "endOffset" : 125
    }, {
      "referenceID" : 8,
      "context" : "For bandit online convex optimization problems with Lipschitz, convex surrogates, the best regret rate known so far, that can be achieved by an efficient algorithm, is O(T 3/4) (however, see the work of Bubeck and Eldan (2015) for a non-constructive O(log(T ) √ T ) bound).",
      "startOffset" : 203,
      "endOffset" : 227
    }, {
      "referenceID" : 3,
      "context" : "t the target (Bartlett et al., 2006).",
      "startOffset" : 13,
      "endOffset" : 36
    }, {
      "referenceID" : 3,
      "context" : "t the target (Bartlett et al., 2006). Intuitively, it means that a function with small expected surrogate loss on unseen data should have small expect target loss on unseen data. We focus on NDCG calibrated surrogates (both convex and non-convex) that have been characterized by Ravikumar et al. (2011). We first state the necessary and sufficient condition for a surrogate to be calibrated w.",
      "startOffset" : 14,
      "endOffset" : 303
    }, {
      "referenceID" : 36,
      "context" : "15 states that argsort(sφ(η)) ⊆ argsort(ER∼η [ G(R) Z(R) ] ) Ravikumar et al. (2011) give concrete examples of NDCG calibrated surrogates, including how some of the popular surrogates can be converted into NDCG calibrated ones: e.",
      "startOffset" : 61,
      "endOffset" : 85
    }, {
      "referenceID" : 32,
      "context" : "3 of Piccolboni and Schindelhauer (2001b) cannot be directly extended to prove the impossibility result because it relies on constructing a connected graph on vertices defined by neighboring actions of learner.",
      "startOffset" : 5,
      "endOffset" : 42
    }, {
      "referenceID" : 38,
      "context" : "ListNet is not only one of the most cited ranking algorithms (over 700 citations according to Google Scholar), but also one of the most validated algorithms (Tax et al., 2015).",
      "startOffset" : 157,
      "endOffset" : 175
    }, {
      "referenceID" : 12,
      "context" : "They were Yahoo’s Learning to Rank Challenge dataset (Chapelle and Chang, 2011) and a dataset published by Russian search engine Yandex (IM-2009).",
      "startOffset" : 53,
      "endOffset" : 79
    } ],
    "year" : 2016,
    "abstractText" : "We consider two settings of online learning to rank where feedback is restricted to top ranked items. The problem is cast as an online game between a learner and sequence of users, over T rounds. In both settings, the learners objective is to present ranked list of items to the users. The learner’s performance is judged on the entire ranked list and true relevances of the items. However, the learner receives highly restricted feedback at end of each round, in form of relevances of only the top k ranked items, where k m. The first setting is non-contextual, where the list of items to be ranked is fixed. The second setting is contextual, where lists of items vary, in form of traditional query-document lists. No stochastic assumption is made on the generation process of relevances of items and contexts. We provide efficient ranking strategies for both the settings. The strategies achieve O(T ) regret, where regret is based on popular ranking measures in first setting and ranking surrogates in second setting. We also provide impossibility results for certain ranking measures and a certain class of surrogates, when feedback is restricted to the top ranked item, i.e. k = 1. We empirically demonstrate the performance of our algorithms on simulated and real world datasets.",
    "creator" : "LaTeX with hyperref package"
  }
}