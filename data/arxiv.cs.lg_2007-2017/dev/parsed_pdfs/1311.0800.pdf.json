{
  "name" : "1311.0800.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "eshcar@yahoo-inc.com", "zkarnin@yahoo-inc.com", "tomerk@technion.ac.il", "rlempel@yahoo-inc.com", "orens@yahoo-inc.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n31 1.\n08 00\n√ k times faster than a single player. That is, distributing learning to\nk players gives rise to a factor √ k parallel speed-up. We complement this result with a lower bound showing this is in general the best possible. On the other extreme, we present an algorithm that achieves the ideal factor k speed-up in learning performance, with communication only logarithmic in 1/ε."
    }, {
      "heading" : "1 Introduction",
      "text" : "Over the past years, multi-armed bandit (MAB) algorithms have been employed in an increasing amount of large-scale applications. MAB algorithms rank results of search engines (Radlinski et al., 2008; Yue and Joachims, 2009), choose between stories or ads to\n∗Most of this work was done while the author was at Yahoo Labs, Haifa.\nshowcase on web sites (Agarwal et al., 2008; Chakrabarti et al., 2008), accelerate model selection and stochastic optimization tasks (Maron and Moore, 1994; Mnih et al., 2008), and more. In many of these applications, the workload is simply too high to be handled by a single processor. In the web context, for example, the sheer volume of user requests and the high rate at which they arrive, require websites to use many front-end machines that run in multiple data centers. In the case of model selection tasks, a single evaluation of a certain model or configuration might require considerable computation time, so that distributing the exploration process across several nodes may result with a significant gain in performance. In this paper, we study such large-scale MAB problems in a distributed environment where learning is performed by several independent nodes that may take actions and observe rewards in parallel.\nFollowing recent MAB literature (Even-Dar et al., 2006; Audibert et al., 2010; Gabillon et al., 2011; Karnin et al., 2013), we focus on the problem of identifying a “good” bandit arm with high confidence. In this problem, we may repeatedly choose one arm (corresponding to an action) and observe a reward drawn from a probability distribution associated with that arm. Our goal is to find an arm with an (almost) optimal expected reward, with as few arm pulls as possible (that is, minimize the simple regret (Bubeck et al., 2009)). Our objective is thus explorative in nature, and in particular we do not mind the incurred costs or the involved regret. This is indeed the natural goal in many applications, such as in the case of model selection problems mentioned above. In our setup, a distributed strategy is evaluated by the number of arm pulls per node required for the task, which correlates with the parallel speed-up obtained by distributing the learning process.\nWe abstract a distributed MAB system as follows. In our model, there are k players that correspond to k independent machines in a cluster. The players are presented with a set of arms, with a common goal of identifying a good arm. Each player receives a stream of queries upon each it chooses an arm to pull. This stream is usually regulated by some load balancer ensuring the load is roughly divided evenly across players. To collaborate, the players may communicate with each other. We assume that the bandwidth of the underlying network is limited, so that players cannot simply share every piece of information. Also, communicating over the network might incur substantial latencies, so players should refrain from doing so as much as possible. When measuring communication of a certain multi-player protocol we consider the number of communication rounds it requires, where in a round of communication each player broadcasts a single message (of arbitrary size) to all other players. Round-based models are natural in distributed learning scenarios, where frameworks such as MapReduce (Dean and Ghemawat, 2008) are ubiquitous.\nWhat is the tradeoff between the learning performance of the players, and the communication between them? At one extreme, if all players broadcast to each other each and every arm reward as it is observed, they can simply simulate the decisions of a serial, optimal algorithm. However, the communication load of this strategy is of course prohibitive. At the other extreme, if the players never communicate, each will suffer the learning curve of a single player, thereby avoiding any possible speed-up the distributed system may provide. Our goal in this work is to better understand this tradeoff between inter-player communication\nand learning performance. Considering the high cost of communication, perhaps the simplest and most important question that arises is how well can the players learn while keeping communication to the very minimum. More specifically, is there a non-trivial strategy by which the players can identify a “good” arm while communicating only once, at the end of the process? As we discuss later on, this is a non-trivial question. On the positive side, we present a k-player algorithm that attains an asymptotic parallel speed-up of √ k factor, as compared to the conventional, serial setting. In fact, our approach demonstrates how to convert virtually any serial exploration strategy to a distributed algorithm enjoying such speed-up. Ideally, one could hope for a factor k speed-up in learning performance; however, we show a lower bound on the required number of pulls in this case, implying that our √ k speed-up is essentially optimal. At the other end of the trade-off, we investigate how much communication is necessary for obtaining the ideal factor k parallel speed-up. We present a k-player strategy achieving such speed-up, with communication only logarithmic in 1/ε. As a corollary, we derive an algorithm that demonstrates an explicit trade-off between the number of arm pulls and the amount of inter-player communication."
    }, {
      "heading" : "1.1 Related Work",
      "text" : "Recently there has been an increasing interest in distributed and collaborative learning problems. In the MAB literature, several recent works consider multi-player MAB scenarios in which players actually compete with each other, either on arm-pulls resources (Gabillon et al., 2011) or on the rewards received (Liu and Zhao, 2010). In contrast, we study a collaborative multi-player problem and investigate how sharing observations helps players achieve their common goal. The related work of Kanade et al. (2012) in the context of non-stochastic (i.e. adversarial) experts also deals with a collaborative problem in a similar distributed setup, and examine the trade-off between communication and the cumulative regret.\nAnother line of recent work was focused on distributed stochastic optimization (Duchi et al., 2010; Agarwal and Duchi, 2011; Dekel et al., 2012) and distributed PACmodels (Balcan et al., 2012; Daumé III et al., 2012b,a), investigating the involved communication trade-offs. The techniques developed there, however, are inherently “batch” learning methods and thus are not directly applicable to our MAB problem which is online in nature. Questions involving network topology (Duchi et al., 2010; Dekel et al., 2012) and delays (Agarwal and Duchi, 2011) are relevant to our setup as well; however, our present work focuses on establishing the first non-trivial guarantees in a distributed collaborative MAB setting."
    }, {
      "heading" : "2 Problem Setup and Statement of Results",
      "text" : "In our model of the Distributed Multi-Armed Bandit problem, there are k ≥ 1 individual players. The players are given n arms, enumerated by [n] := {1, 2, . . . , n}. Each arm i ∈ [n] is associated with a reward, which is a [0, 1]-valued random variable with expectation pi.\nFor convenience, we assume that the arms are ordered by their expected rewards, that is p1 ≥ p2 ≥ · · · ≥ pn. At every time step t = 1, 2, . . . , T , each player pulls one arm of his choice and observes an independent sample of its reward. Each player may choose any of the arms, regardless of the other players and their actions. At the end of the game, each player must commit to a single arm. In a communication round, that may take place at any predefined time step, each player may broadcast a message to all other players. While we do not restrict the size of each message, in a reasonable implementation a message should not be larger than Õ(n) bits.\nIn the best-arm identification version of the problem, the goal of a multi-player algorithm given some target confidence level δ > 0, is that with probability at least 1 − δ all players correctly identify the best arm (i.e. the arm having the maximal expected reward). For simplicity, we assume in this setting that the best arm is unique. Similarly, in the (ε, δ)-PAC variant the goal is that each player finds an ε-optimal (or “ε-best”) arm, that is an arm i with pi ≥ p1−ε, with high probability. In this paper we focus on the more general (ε, δ)-PAC setup, which also includes best-arm identification for ε = 0.\nWe use the notation ∆i := p1 − pi to denote the suboptimality gap of arm i, and occasionally use ∆⋆ := ∆2 for denoting the minimal gap. In the best-arm version of the problem, where we assume that the best arm is unique, we have ∆i > 0 for all i > 1. When dealing with the (ε, δ)-PAC setup, we also consider the truncated gaps ∆εi := max{∆i, ε}. In the context of MAB problems, we are interested in deriving distribution-dependent bounds, namely, bounds that are stated as a function of ε, δ and also the distribution-specific values ∆ := (∆2, . . . ,∆n). The Õ notation in our bounds hides polylogarithmic factors in n, k, ε, δ, and also in ∆2, . . . ,∆n. In the case of serial exploration algorithms (i.e., when there is only one player), the lower bounds of Mannor and Tsitsiklis (2004) and Audibert et al. (2010) show that in general Ω̃(Hε) pulls are necessary for identifying an ε-arm, where\nHε :=\nn ∑\ni=2\n1\n(∆εi ) 2 . (1)\nIntuitively, the hardness of the task is therefore captured by the quantity Hε, which is roughly the number of arm pulls needed to find an ε-best arm with a reasonable probability; see also (Audibert et al., 2010) for a discussion. Our goal in this work is therefore to establish bounds in the distributed model that are expressed as a function of Hε, in the same vein of the bounds known in the classic MAB setup1."
    }, {
      "heading" : "2.1 Baseline approaches",
      "text" : "We now discuss several baseline approaches for the problem, starting with our main focus— the single round setting. The first obvious approach, already mentioned earlier, is the nocommunication strategy: just let each player explore the arms in isolation of the other\n1If one is interested in distribution-free bounds, then the problem at hand is trivial as the (distributed) uniform sampling strategy is optimal in this setting, up to polylogarithmic factors; see also (Mannor and Tsitsiklis, 2004) for a relevant discussion.\nplayers, following an independent instance of some serial strategy; at the end of the executions, all players hold an ε-best arm. Clearly, this approach performs poorly in terms of learning performance, needing Ω̃(Hε) pulls per player in the worst case and not leading to any parallel speed-up.\nAnother straightforward approach is to employ a majority vote among the players: let each player independently identify an arm, and choose the arm having most of the votes (alternatively, at least half of the votes). However, this approach does not lead to any improvement in performance: for this vote to work, each player has to solve the problem correctly with reasonable probability, which already require Ω̃(Hε) pulls of each. Even if we somehow split the arms between players and let each player explore a share of them, a majority vote would still fail since those players getting the “good” arms might have to pull arms Ω̃(Hε) times—a small MAB instance might be as hard as the full-sized problem (in terms of the complexity measure Hε).\nWhen considering algorithms employing multiple communication rounds, we use an ideal simulated serial algorithm (i.e., a full-communication approach) as our baseline. This approach is of course prohibited in our context, but is able to achieve the optimal parallel speed-up, linear in the number of players k."
    }, {
      "heading" : "2.2 Our results",
      "text" : "We now discuss our approach and overview our algorithmic results. These are summarized in Table 1 below, that compares the different algorithms in terms of parallel speed-up and communication.\nOur approach for the one-round case is based on the idea of majority vote. For the bestarm identification task, our observation is that by letting each player explore a smaller set of n/ √ k arms chosen at random and choose one of them as “best”, about √ k of the players would come up with the global best arm. This (partial) consensus on a single arm is a key aspect in our approach, since it allows the players to identify the correct best arm among the votes of all k players, after sharing information only once. Our approach leads to a factor √ k parallel speed-up which, as we demonstrate in our lower bound, is the optimal factor in this setting. Although our goal here is pure exploration, in our algorithms each player follows an explore-exploit strategy. The idea is that a player should sample his recommended arm as much as his budget permits, even if it was easy to identify in his small-sized problem. This way we can guarantee that the top arms are sampled to a sufficient precision by the time each of the players has to choose a single best arm.\nThe algorithm for the (ε, δ)-PAC setup is similar, but its analysis is more challenging. As mentioned above, an agreement on a single arm is essential for a vote to work. Here, however, there might be several ε-best arms, so arriving at a consensus on a single one is more difficult. Nonetheless, by examining two different regimes, namely when there are “many” ε-best arms and when there are “few” of them, our analysis shows that a vote can still work and achieve the √ k multiplicative speed-up.\nIn the case of multiple communication rounds, we present a distributed elimination-based algorithm that discards arms right after each communication round. Between rounds, we\nshare the work load between players uniformly. We show that the number of such rounds can be reduced to as low as O(log(1/ε)), by eliminating all 2−r-suboptimal arms in the r’th round. A similar idea was employed in (Auer and Ortner, 2010) for improving the regret bound of UCB with respect to the parameters ∆i. We also use this technique to develop an algorithm that performs only R communication rounds, for any given parameter R ≥ 1, that achieves a slightly worse multiplicative ε2/Rk speed-up."
    }, {
      "heading" : "3 One Communication Round",
      "text" : "This section considers the most basic variant of the multi-player MAB problem, where each player is only allowed a single transmission, when finishing her queries. For the clarity of exposition, we first consider the best-arm identification setting in Section 3.1. Section 3.2 deals with the (ε, δ)-PAC setup. We demonstrate the tightness of our result in Section 3.3 with a lower bound for the required budget of arm pulls in this setting.\nOur algorithms in this section assume the availability of a serial algorithm A(A, ε), that given a set of arms A and target accuracy ε, identifies an ε-best arm in A with probability at least 2/3 using no more than\ncA ∑\ni∈A\n1\n(∆εi ) 2 log |A| ∆εi\n(2)\narm pulls, for some constant cA > 1. For example, the Successive Elimination algorithm (Even-Dar et al., 2006) and the Exp-Gap Elimination algorithm (Karnin et al., 2013) provide a guarantee of this form. Essentially, any exploration strategy whose guarantee is expressed as a function of Hε can be used as the procedure A, with technical modifications in our analysis."
    }, {
      "heading" : "3.1 Best-arm Identification Algorithm",
      "text" : "We now describe our one-round best-arm identification algorithm. For simplicity, we present a version matching δ = 1/3, meaning that the algorithm produces the correct arm with probability at least 2/3; we later explain how to extend it to deal with arbitrary values of δ.\nOur algorithm is akin to a majority vote among the multiple players, in which each player pulls arms in two stages. In the first Explore stage, each player independently solves a “smaller” MAB instance on a random subset of the arms using the exploration strategy A. In the second Exploit stage, each player exploits the arm identified as “best” in the first stage, and communicates that arm and its observed average reward. See Algorithm 1 below for a precise description. An appealing feature of our algorithm is that it requires each player to transmit a single message of constant size (up to logarithmic factors).\nAlgorithm 1 One-round Best-arm\ninput time horizon T output an arm 1: for player j = 1 to k do 2: choose a subset Aj of 6n/ √ k arms uniformly at random\n3: Explore: execute ij ← A(Aj , 0) using at most 12T pulls (and halting the algorithm early if necessary); if the algorithm fails to identify any arm or does not terminate gracefully, let ij be an arbitrary arm 4: Exploit: pull arm ij for 1 2 T times and let q̂j be its average reward 5: communicate the numbers ij , q̂j 6: end for\n7: let ki be the number of players j with ij = i, and define A = {i : ki > √ k} 8: let p̂i = (1/ki) ∑\n{j : ij=i} q̂j for all i 9: return argmaxi∈A p̂i; if the set A is empty, output an arbitrary arm.\nIn Theorem 3.1 we prove that Algorithm 1 indeed achieves the promised upper bound.\nTheorem 3.1. Algorithm 1 identifies the best arm correctly with probability at least 2/3 using no more than\nO\n(\n1√ k ·\nn ∑\ni=2\n1\n∆2i log\nn\n∆i\n)\narm pulls per player, provided that 6 ≤ √ k ≤ n. The algorithm uses a single communication round, in which each player communicates Õ(1) bits.\nBy repeating the algorithm O(log(1/δ)) times and taking the majority vote of the independent runs, we can amplify the success probability to 1− δ for any given δ > 0. Note that\nwe can still do that with one communication round (at the end of all executions), but each player now has to communicate O(log(1/δ)) values2.\nTheorem 3.2. There exists a k-player algorithm that given\nO\n(\n1√ k ·\nn ∑\ni=2\n1\n∆2i log\nn\n∆i log\n1\nδ\n)\narm pulls, identifies the best arm correctly with probability at least 1− δ. The algorithm uses a single communication round, in which each player communicates O(log(1/δ)) numerical values.\nWe now prove Theorem 3.1. We show that a budget T of samples (arm pulls) per player, where\nT ≥ 24cA√ k\n· n ∑\ni=2\n1\n∆2i ln\nn\n∆i , (3)\nsuffices for the players to jointly identify the best arm i⋆ with the desired probability. Clearly, this would imply the bound stated in Theorem 3.1. We note that we did not try to optimize the constants in the above expression.\nWe begin by analyzing the Explore phase of the algorithm. Our first lemma shows that each player chooses the global best arm and identifies it as the local best arm with sufficiently large probability.\nLemma 3.3. When (3) holds, each player identifies the (global) best arm correctly after the Explore phase with probability at least 2/ √ k.\nProof. Let\nH = ∑\ni 6=i⋆\n1\n∆2i ln\nn\n∆i\nand for all j,\nHj = ∑\ni⋆ 6=i∈Aj\n1\n∆2i ln\nn\n∆i .\nThen E[Hj | i⋆ ∈ Aj ] ≤ 6H/ √ k by the linearity of expectation, and Markov’s inequality\nthus gives that Pr[Hj ≤ 12H/ √ k | i⋆ ∈ Aj ] ≥ 1/2. Clearly, we also have Pr[i⋆ ∈ Aj ] = 6/ √ k which implies that\nPr\n[\ni⋆ ∈ Aj and Hj ≤ 12√ k H\n]\n≥ 3√ k . (4)\n2In fact, by letting each player pick a slightly larger subset of O( √ log(1/δ) ·n/ √ k) arms, we can amplify the success probability to 1 − δ without needing to communicate more than 2 values per player. However, this approach only works when k = Ω(log(1/δ)).\nNow consider the “local” MAB problem facing player j, over the subset of arms Aj . If the (global) best arm i⋆ is amongst the arms in Aj, then by eq. (2) the instance of the procedure A player j executes needs no more than\nTj := cA ∑\ni⋆ 6=i∈Aj\n1\n∆2i ln\nn\n∆i = cAHj\npulls in order to identify i⋆ successfully with probability 2/3. In case that Hj ≤ 12H/ √ k,\nwe have Tj ≤ 12cAH/ √ k ≤ T/2 , which means that the pulls budget of player j suffices for\nidentifying the best arm. Together with (4), we conclude that with probability at least 2/ √ k player j identifies the best arm correctly.\nWe next address the Exploit phase. The next simple lemma shows that the popular arms (i.e. those selected by many players) are estimated to a sufficient precision.\nLemma 3.4. Provided that (3) holds, we have |p̂i − pi| ≤ 12∆⋆ for all arms i ∈ A with probability at least 5/6.\nProof. Consider some arm i ∈ A. The estimate p̂i is the average reward of\n1 2 kiT ≥ 12\n√ kT ≥ 2\n∆2⋆ ln(12n)\narm pulls (of the Exploit phase). Hoeffding’s inequality now gives that\nPr[|p̂i − pi| > 12∆⋆] ≤ 2 exp(−12∆ 2 ⋆ · 12kiT ) ≤\n1\n6n ,\nand the lemma follows via a union bound.\nWe can now prove Theorem 3.1.\nProof (of Theorem 3.1). Let us first show that with probability at least 5/6, the best arm i is contained in the set A. To this end, notice that ki⋆ is the sum of k i.i.d. Bernoulli random variables {Ij}j where Ij is the indicator of whether player j chooses arm i⋆ after the Explore phase. By Lemma 3.3 we have that E[Ij ] ≥ 2/ √ k for all j, hence by Hoeffding’s inequality,\nPr[ki⋆ ≤ √ k] ≤ Pr[ki⋆ −E[ki⋆ ] ≤ − √ k] ≤ exp(−2k/k) ≤ 1\n6\nwhich implies that i⋆ ∈ A with probability at least 5/6. Next, note that with probability at least 5/6 the arm i ∈ A having the highest empirical reward p̂i is the one with the highest expected reward pi. Indeed, this follows directly from Lemma 3.4 that shows that with probability at least 5/6, for all arms i ∈ A the estimate p̂i is within 1\n2 ∆ of the true bias pi. Hence, via a union bound we conclude that with probability\nat least 2/3, the best arm is in A and has the highest empirical reward. In other words, with probability at least 2/3 the algorithm outputs the best arm i⋆."
    }, {
      "heading" : "3.2 (ε, δ)-PAC Algorithm",
      "text" : "We now present an algorithm whose purpose is to recover an ε-optimal arm. Here, there might be more than one ε-best arm, so each “successful” player might come up with a different ε-best arm. Nevertheless, our analysis below shows that with high probability, a subset of the players can still agree on a single ε-best arm, which makes it possible to identify it among the votes of all players. Our algorithm is described in Algorithm 2, and the following theorem states its guarantees.\nAlgorithm 2 One-round ε-arm\ninput time horizon T , accuracy ε output an arm 1: for player j = 1 to k do 2: choose a subset Aj of 12n/ √ k arms uniformly at random\n3: Explore: execute ij ← A(Aj, ε) using at most 12T pulls (and halting the algorithm early if necessary); if the algorithm fails to identify any arm or does not terminate gracefully, let ij be an arbitrary arm 4: Exploit: pull arm ij for 1 2 T times, and let q̂j be the average reward 5: communicate the numbers ij , q̂j 6: end for 7: let ki be the number of players j with ij = i 8: let ti = 1 2 kiT and p̂i = (1/ki) ∑ {j : ij=i} q̂j for all i\n9: define A = {i ∈ [n] : ti ≥ (1/ε2) ln(12n)} 10: return argmaxi∈A p̂i; if the set A is empty, output an arbitrary arm.\nTheorem 3.5. Algorithm 2 identifies a 2ε-best arm with probability at least 2/3 using no more than\nO\n(\n1√ k ·\nn ∑\ni=2\n1\n(∆εi ) 2 log\nn\n∆εi\n)\narm pulls per player, provided that 24 ≤ √ k ≤ n. The algorithm uses a single communication round, in which each player communicates Õ(1) bits.\nBefore proving the theorem, we first state several key lemmas. In the following, let nε and n2ε denote the number of ε-best and 2ε-best arms respectively. Our analysis considers two different regimes: n2ε ≤ 150 √ k and n2ε > 1 50 √ k, and shows that in any case,\nT ≥ 400cA√ k\nn ∑\ni=2\n1\n(∆εi ) 2 ln\n24n\n∆εi (5)\nsuffices for identifying a 2ε-best arm with the desired probability. Clearly, this implies the bound stated in Theorem 3.5.\nThe first lemma shows that at least one of the players is able to find an ε-best arm. As we later show, this is sufficient for the success of the algorithm in case there are many 2ε-best arms.\nLemma 3.6. When (5) holds, at least one player successfully identifies an ε-best arm in the Explore phase, with probability at least 5/6.\nThe next lemma is more refined and states that in case there are few 2ε-best arms, the probability of each player to successfully identify an ε-best arm grows linearly with nε. Lemma 3.7. Assume that n2ε ≤ 150 √ k. When (5) holds, each player identifies an ε-best\narm in the Explore phase, with probability at least 2nε/ √ k.\nThe last lemma we need analyzes the accuracy of the estimated rewards of arms in the set A.\nLemma 3.8. With probability at least 5/6, we have |p̂i − pi| ≤ ε/2 for all arms i ∈ A. Before proving the lemmas, let us first show how they imply Theorem 3.5.\nProof (of Theorem 3.5). We shall prove that with probability 5/6 the set A contains at least one ε-best arm. This would complete the proof, since Lemma 3.8 assures that with probability 5/6, the estimates p̂i of all arms i ∈ A are at most ε/2-away from the true reward pi, and in turn implies (via a union bound) that with probability 2/3 the arm i ∈ A having the maximal empirical reward p̂i must be a 2ε-best arm.\nFirst, consider the case n2ε > 1 50\n√ k. Lemma 3.6 shows that with probability 5/6 there\nexists a player j that identifies an ε-best arm ij . Since for at least n2ε arms ∆i ≤ 2ε, we have\ntij ≥ 12T ≥ 400 2 √ k · n2ε − 1 (2ε)2 ln 24n 2ε ≥ 1 ε2 ln(12n) ,\nthat is, ij ∈ A. Next, consider the case n2ε ≤ 150 √ k. Let N denote the number of players that identified some ε-best arm. The random variable N is a sum of Bernoulli random variables {Ij}j where Ij indicates whether player j identified some ε-best arm. By Lemma 3.7, E[Ij ] ≥ 2nε/ √ k and thus by Hoeffding’s inequality,\nPr[N < nε √ k] = Pr[N −E[N ] ≤ −nε √ k] ≤ exp(−2n2ε) ≤ 1\n6 .\nThat is, with probability 5/6, at least nε √ k players found an ε-best arm. A pigeon-hole\nargument now shows that in this case there exists an ε-best arm i⋆ selected by at least √ k players. Hence, with probability 5/6 the number of samples of this arm collected in the Exploit phase is at least\nti⋆ ≥ 12 √ kT > 1\nε2 ln(12n),\nwhich means that i⋆ ∈ A."
    }, {
      "heading" : "3.2.1 Proofs of Lemmas",
      "text" : "For the proofs in this section, we need some additional notation. For any player j, let i⋆j denote the best arm in Aj , with ties broken arbitrarily. Let ∆\n⋆ j := ∆i⋆j = mini∈Aj ∆i and\n∆i,j := max{∆i −∆⋆j , ε} for all i ∈ Aj . Finally, define\nH := ∑\ni⋆ 6=i∈[n]\n1\n(∆2εi ) 2 ln\n2n\n∆2εi\nand\nH localj := ∑\ni⋆ j 6=i∈Aj\n1\n∆2i,j ln\nn\n∆i,j , Hglobalj :=\n∑\ni⋆ j 6=i∈Aj\n1\n(∆2εi ) 2 ln\n2n\n∆2εi\nfor all players j.\nProof (of Lemma 3.6). The proof is analogical to that of Lemma 3.3. Let i⋆ be the best arm and let j be a player that chose it. The funds required by player j in order to succeed choosing an ε-best arm with probability at least 2/3 is at most\nTj = cA ∑\ni⋆ 6=i∈Aj\n1\n(∆εi ) 2 ln\nn\n∆εi .\nGiven Eq. (5), E[Tj ] ≤ T/4, meaning that by Markov Pr[Tj ≤ T/2] ≥ 1/2. Since the probability of choosing the best arm is 12/ √ k it follows that for any fixed player j, with probability at least 2 3 · 1 2 · 12/ √ k = 4/ √ k the player identified an ε-best arm. Thus, the probability that all of the players fail to identify an ε-best arm is bounded by\n(\n1− 4√ k\n)k ≤ e−4 √ k < 1\n6 ,\nand the lemma follows. Proof (of Lemma 3.7). For convenience, let α = 12/ √ k and note that by our assumptions α ≤ 1 2 . Also, since we assume √ k ≤ n we have n2ε ≤ 12n.\nFix some player j and let Xε and X2ε denote the number of ε-best and 2ε-best arms chosen by this player, respectively. Consider the event B = {Xε = X2ε = 1} in which the player chooses exactly one ε-optimal arm but no other 2ε-optimal arm. The probability that\nthis event occurs is\nPr[B] = nε ( n−n2ε αn−1 ) (\nn αn\n) = αnε\n( n−n2ε αn−1 ) (\nn−1 αn−1 )\n≥ αnε ( n− n2ε − αn n− αn )αn = αnε ( 1− n2ε n− αn )αn ≥ αnε (\n1− 2n2ε n\n)αn\n(since α ≤ 1 2 )\n≥ αnε (1− 2αn2ε) ((1− x)a ≥ 1− ax for x ≤ 1)\n≥ 1 2 αnε = 6nε√ k\n(since αn2ε ≤ 14)\nOn the other hand, given the event B, for player j we have ∆⋆j ≤ ε and ∆i ≥ 2ε for all i 6= i⋆j . Consequently, ∆i,j ≥ 12∆i = 12∆2εi for all i ∈ Aj . Hence,\nH localj = ∑\ni⋆ j 6=i∈Aj\n1\n∆2i,j ln\nn\n∆i,j\n≤ 4 ∑\ni⋆ j 6=i∈Aj\n1\n(∆2εi ) 2 ln\n2n\n∆2εi\n= 4Hglobalj\nDenoting ∆>2ε := {i : ∆i > 2ε}, we now have\nE[H localj | B] ≤ 4E[Hglobalj | B]\n≤ 412n√ k · 1|∆>2ε| ∑\ni∈∆>2ε\n1\n∆2i ln\n2n\n∆i\n< 50n√ k · 1 n ∑\ni 6=i⋆\n1\n(∆2εi ) 2 ln\n2n\n∆2εi\n= 50√ k H\nMarkov’s inequality now gives Pr[H localj ≤ 100H/ √ k | B] ≥ 1 2 , and together with Pr[B] ≥\n6nε/ √ k we obtain that\nPr\n[\nB and H localj ≤ 100√ k H\n]\n≥ 3nε√ k .\nContinuing as in the proof of Lemma 3.3, we get that when (5) holds, with probability at least 2nε/ √ k (i) Aj contains an ε-best arm which is the only 2ε-best arm in Aj , and (ii) player j successfully identifies an arm which is ε-best with respect to the best arm in Aj . This\nimplies that with probability at least 2nε/ √ k, the arm ij selected by player j is ε-best.\nProof (of Lemma 3.8). Since each estimate p̂i is the empirical average of at least ti samples of arm i, Hoeffding’s inequality gives\nPr[|p̂i − pi| > 12ε] ≤ 2 exp(−12ε 2ti) ≤\n1\n6n ,\nand a union bound concludes the proof."
    }, {
      "heading" : "3.3 Lower Bound",
      "text" : "The following theorem suggests that in general, for identifying the best arm k players achieve a multiplicative speed-up of at most Õ( √ k) when allowing one transmission per player (at the end of the game). Clearly, this also implies that a similar lower bound holds in the PAC setup, and proves that our algorithmic results for the one-round case are essentially tight.\nTheorem 3.9. There exist rewards p1, . . . , pn ∈ [0, 1] and integer T such that for any kplayer strategy that uses a single round of communication at the end of the game,\n• each individual player must use at least T/ √ k arm pulls for them to collectively identify\nthe best arm with probability at least 2/3;\n• there exist a single-player algorithm that needs at most Õ(T ) pulls for identifying the best arm with probability at least 2/3.\nOur proof of Theorem 3.9 is based on a simple lower bound for the MAB problem, that follows directly from Lemma 5.1 of (Anthony and Bartlett, 1999).\nLemma 3.10. Consider a MAB problem with two arms and rewards 1 2 + ε, 1 2 − ε. There exists a constant c such that any algorithm that with probability at least 2/3 identifies the best arm, needs at least c/ε2 pulls in expectation. Proof (of Theorem 3.9). Let c1 = √ c/2, where c is the constant of Lemma 3.10. Consider a MAB instance over n arms, with the rewards being a random permutation σ of 1 2 +∆, 1 2 −\n∆, 0, . . . , 0, where ∆ := 1/ √ n. A serial algorithm of choice (say, the Successive Elimination algorithm) is able to identify the best arm in this setting with probability 2/3 using at most Õ(n + 1/∆2) = Õ(n) arm pulls (see e.g., Even-Dar et al. (2006)).\nAssume that the players follow some algorithm, each using no more than c1n/ √ k pulls. Without loss of generality, we may assume that the sequence of rewards, as well as the internal random bits of the algorithm (if it is randomized), were drawn before the execution has started. We shall denote this sequence of random variables by h.\nFirst, fix some arbitrary sequence h. Let Ti,j denote the number of times arm i (in decreasing order of rewards) was pulled by player j, and Ti denote the total number of pulls of arm i. Since the budget of each player is c1n/ √ k, we have Prσ[T1,j > 0] ≤ c1/ √ k and consequently\nEσ[T1,j ] ≤ Eσ[T1,j | T1,j > 0] · Pr[T1,j > 0] ≤ c21n\nk\nfor any player j. Similarly, Eσ[T2,j] ≤ c21n/k and we get that\nEσ[T1 + T2] ≤ 2c21n = 2c21 ∆2 < c ∆2 .\nSince the above holds for any given sequence h, this implies that Eσ[Eh[T1 + T2]] < c/∆ 2, which means that there exists a permutation σ0 under which Eh[T1 + T2] < c/∆ 2. For the permutation σ0 and its corresponding reward setting, the algorithm does not sample the top two arms enough (in expectation), and by Lemma 3.10 it cannot succeed with probability greater than 2/3."
    }, {
      "heading" : "4 Multiple Communication Rounds",
      "text" : "In this section we establish an explicit tradeoff between the performance of a multi-player algorithm and the number of communication rounds it uses, in terms of the accuracy ε. Our observation is that by allowing O(log(1/ε)) rounds of communication, it is possible to achieve the optimal speedup of factor k. That is, we do not gain any improvement in learning performance by allowing more than O(log(1/ε)) rounds.\nAlgorithm 3 Multi-Round ε-Arm\ninput (ε, δ) output an arm 1: initialize S0 ← [n], r ← 0, t0 ← 0 2: repeat\n3: set r ← r + 1 4: let εr ← 2−r, tr ← (2/kε2r) ln(4nr2/δ) 5: for player j = 1 to k do 6: sample each arm i ∈ Sr−1 for tr − tr−1 times 7: let p̂rj,i be the average reward of arm i (in all rounds so far of player j) 8: communicate the numbers p̂rj,1, . . . , p̂ r j,n 9: end for\n10: let p̂ri = (1/k) ∑k j=1 p̂ r j,i for all i ∈ Sr−1, and let p̂r⋆ = maxi∈Sr−1 p̂ri 11: set Sr ← Sr−1 \\ {i ∈ Sr−1 : p̂ri < p̂r⋆ − εr} 12: until εr ≤ ε/2 or |Sr| = 1 13: return an arm from Sr\nOur algorithm is given in Algorithm 3. The idea is to eliminate in each round r (i.e., right after the rth communication round) all 2−r-suboptimal arms. We accomplish this by letting each player sample uniformly all remaining arms and communicate the results to other players. Then, players are able to eliminate suboptimal arms with high confidence. If each such round is successful, after log2(1/ε) rounds only ε-best arms survive. Theorem 4.1 below bounds the number of arm pulls used by this algorithm.\nTheorem 4.1. With probability at least 1− δ, Algorithm 3\n• identifies the optimal arm using\nO\n(\n1 k ·\nn ∑\ni=2\n1\n(∆εi ) 2 log\n(\nn δ log 1\n∆εi\n)\n)\narm pulls per player;\n• terminates after no more than 1 + ⌈log2(1/ε)⌉ rounds of communication (or after 1 + ⌈log2(1/∆⋆)⌉ rounds for ε = 0).\nProof. Without loss of generality, we may assume that the rewards of all arms are drawn before the algorithm is executed, so that the empirical averages p̂ri are defined for all arms at all rounds (even for arms that were eliminated prior to some round). Since p̂ri is the empirical average of ktr samples of arm i (aggregated from all players), for any round r and arm i we have by Hoeffding’s inequality,\nPr[|p̂ri − pi| ≥ 12εr] ≤ 2 exp(−12ε 2 rktr) =\nδ\n2nr2 .\nHence, a union bound gives that |p̂ri − pi| < εr/2 for all i and r with probability at least\n1− ∞ ∑\nr=1\nn ∑\ni=1\nδ\n2nr2 = 1−\n∞ ∑\nr=1\nδ\n2r2 ≥ 1− δ .\nThat is, with probability at least 1 − δ, an ε-optimal arm i is never eliminated by the algorithm, as the event p̂ri < p̂ r ⋆ − εr implies that either p̂ri < pi − εr/2 or p̂rj > pj + εr/2 for some arm j. In addition, any suboptimal arm i does not survive round ri = ⌈log2(1/∆εi )⌉+1, since ∆εi ≥ 2εri and so for r = ri,\np̂ri < pi + εr/2 = p1 + εr/2−∆i ≤ p̂r1 + εr −∆i ≤ p̂r1 − εr ≤ p̂r⋆ − εr .\nThat is, with probability at least 1 − δ, after ⌈log2(1/ε)⌉ + 1 rounds (when the algorithm terminates) all remaining arms are ε-optimal. When ε = 0, the algorithm terminates once only a single arm survives, and with high probability this occurs after at most ⌈log2(1/∆⋆)⌉+1 rounds.\nWe conclude by computing the total number of arms pulls required for the algorithm. Let Ti be the total number of times arm i 6= 1 is pulled by one of the players. Since ri ≤\nlog2(4/∆ ε i ), we have\nTi ≤ 2 k · (2ri)2 ln 4nr\n2 i\nδ\n≤ 2 k\n(\n4\n∆εi\n)2\nln\n(\n4n\nδ log22\n4\n∆εi\n)\n= O\n(\n1 k · 1 (∆εi ) 2 log\n(\nn δ log 1\n∆εi\n))\n.\nConsequently, the total number of arm pulls per player is T2 + ∑n\ni=2 Ti, which gives the theorem.\nBy properly tuning the elimination thresholds εr of Algorithm 3 in accordance with the target accuracy ε, we can establish an explicit trade-off between the number of communication rounds and the number of arm pulls each player needs. In particular, we can design a multi-player algorithm that terminates after at most R communication rounds, for any given parameter R > 0. This, however, comes at the cost of a compromise in learning performance as quantified in the following corollary.\nCorollary 4.2. Given a parameter R > 0, set εr ← εr/R for all r ≥ 1 in Algorithm 3. With probability at least 1− δ, the modified algorithm\n• identifies an ε-best arm using\nÕ\n(\nε−2/R\nk ·\nn ∑\ni=2\n1\n(∆εi ) 2\n)\narm pulls per player;\n• terminates after at most R rounds of communication.\nProof. For all arms i ∈ [n], let\nri =\n⌈\nR 1 + log2(1/∆\nε i )\nlog2(1/ε)\n⌉\n.\nSince ∆i ≥ 2εri, if the algorithm is successful any arm i which is not εri-optimal is eliminated after at most ri rounds. Clearly, after R rounds only ε-optimal arms survive. This happens with probability at least 1− δ.\nIt remains to bound the number of arm pulls the algorithm uses. It is easy to verify that 2εri ≥ ε1/R ·∆εi , thus the number of times arm i was pulled by each of the players is\nTi = 1 k · 4 ε2ri ln 2nr2i δ = O\n( ε−2/R\nk · 1 (∆εi ) 2 log nR δ\n)\n,\nand the theorem follows."
    }, {
      "heading" : "5 Conclusions and Further Research",
      "text" : "We have considered a collaborative MAB exploration problem, in which several independent players explore a set of arms with a common goal, and obtained the first non-trivial results in such setting. Our main results apply for the specifically interesting regime where each of the players is allowed a single transmission; this setting fits naturally to common distributed frameworks such as MapReduce. An interesting open question in this context is whether one can obtain a strictly better speed-up result (which, in particular, is independent of ε) by allowing more than a single round. Even when allowing merely two communication rounds, it is unclear whether the √ k speed-up can be improved. Intuitively, the difficulty here is that in the second phase of a reasonable strategy each player should focus on the arms that excelled in the first phase; this makes the sub-problems being faced in the second phase as hard as the entire MAB instance, in terms of the quantity Hε. Nevertheless, we expect our oneround approach to serve as a building-block in the design of future distributed exploration algorithms, that are applicable in more complex communication models.\nAn additional interesting problem for future research is how to translate our results to the regret minimization setting. In particular, it would be nice to see a conversion of algorithms like UCB (Auer et al., 2002) to a distributed setting. In this respect, perhaps a more natural distributed model is a one resembling that of Kanade et al. (2012), that have established a regret vs. communication trade-off in the non-stochastic setting."
    } ],
    "references" : [ {
      "title" : "Distributed delayed stochastic optimization",
      "author" : [ "A. Agarwal", "J.C. Duchi" ],
      "venue" : "In NIPS, pages 873–881,",
      "citeRegEx" : "Agarwal and Duchi.,? \\Q2011\\E",
      "shortCiteRegEx" : "Agarwal and Duchi.",
      "year" : 2011
    }, {
      "title" : "Online models for content optimization",
      "author" : [ "D. Agarwal", "B.-C. Chen", "P. Elango", "N. Motgi", "S.-T. Park", "R. Ramakrishnan", "S. Roy", "J. Zachariah" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Agarwal et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Agarwal et al\\.",
      "year" : 2008
    }, {
      "title" : "Neural network learning: Theoretical foundations",
      "author" : [ "M. Anthony", "P. Bartlett" ],
      "venue" : null,
      "citeRegEx" : "Anthony and Bartlett.,? \\Q1999\\E",
      "shortCiteRegEx" : "Anthony and Bartlett.",
      "year" : 1999
    }, {
      "title" : "Best arm identification in multi-armed bandits",
      "author" : [ "J.-Y. Audibert", "S. Bubeck", "R. Munos" ],
      "venue" : "In COLT, pages",
      "citeRegEx" : "Audibert et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Audibert et al\\.",
      "year" : 2010
    }, {
      "title" : "UCB revisited: Improved regret bounds for the stochastic multiarmed bandit problem",
      "author" : [ "P. Auer", "R. Ortner" ],
      "venue" : "Periodica Mathematica Hungarica,",
      "citeRegEx" : "Auer and Ortner.,? \\Q2010\\E",
      "shortCiteRegEx" : "Auer and Ortner.",
      "year" : 2010
    }, {
      "title" : "Finite-time analysis of the multiarmed bandit problem",
      "author" : [ "P. Auer", "N. Cesa-Bianchi", "P. Fischer" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "Auer et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Auer et al\\.",
      "year" : 2002
    }, {
      "title" : "Distributed learning, communication complexity and privacy",
      "author" : [ "M. Balcan", "A. Blum", "S. Fine", "Y. Mansour" ],
      "venue" : "Arxiv preprint arXiv:1204.3514,",
      "citeRegEx" : "Balcan et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Balcan et al\\.",
      "year" : 2012
    }, {
      "title" : "Pure exploration in multi-armed bandits problems",
      "author" : [ "S. Bubeck", "R. Munos", "G. Stoltz" ],
      "venue" : "In Algorithmic Learning Theory,",
      "citeRegEx" : "Bubeck et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Bubeck et al\\.",
      "year" : 2009
    }, {
      "title" : "Mortal multi-armed bandits",
      "author" : [ "D. Chakrabarti", "R. Kumar", "F. Radlinski", "E. Upfal" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Chakrabarti et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Chakrabarti et al\\.",
      "year" : 2008
    }, {
      "title" : "Efficient protocols for distributed classification and optimization",
      "author" : [ "H. Daumé III", "J.M. Phillips", "A. Saha", "S. Venkatasubramanian" ],
      "venue" : "In ALT,",
      "citeRegEx" : "III et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "III et al\\.",
      "year" : 2012
    }, {
      "title" : "Protocols for learning classifiers on distributed data",
      "author" : [ "H. Daumé III", "J.M. Phillips", "A. Saha", "S. Venkatasubramanian" ],
      "venue" : "AISTAT,",
      "citeRegEx" : "III et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "III et al\\.",
      "year" : 2012
    }, {
      "title" : "MapReduce: simplified data processing on large clusters",
      "author" : [ "J. Dean", "S. Ghemawat" ],
      "venue" : "Commun. ACM,",
      "citeRegEx" : "Dean and Ghemawat.,? \\Q2008\\E",
      "shortCiteRegEx" : "Dean and Ghemawat.",
      "year" : 2008
    }, {
      "title" : "Optimal distributed online prediction using mini-batches",
      "author" : [ "O. Dekel", "R. Gilad-Bachrach", "O. Shamir", "L. Xiao" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Dekel et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Dekel et al\\.",
      "year" : 2012
    }, {
      "title" : "Distributed dual averaging",
      "author" : [ "J. Duchi", "A. Agarwal", "M.J. Wainwright" ],
      "venue" : "in networks. NIPS,",
      "citeRegEx" : "Duchi et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Duchi et al\\.",
      "year" : 2010
    }, {
      "title" : "Action elimination and stopping conditions for the multi-armed bandit and reinforcement learning problems",
      "author" : [ "E. Even-Dar", "S. Mannor", "Y. Mansour" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Even.Dar et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Even.Dar et al\\.",
      "year" : 2006
    }, {
      "title" : "Multi-bandit best arm identification",
      "author" : [ "V. Gabillon", "M. Ghavamzadeh", "A. Lazaric", "S. Bubeck" ],
      "venue" : null,
      "citeRegEx" : "Gabillon et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Gabillon et al\\.",
      "year" : 2011
    }, {
      "title" : "Distributed non-stochastic experts",
      "author" : [ "V. Kanade", "Z. Liu", "B. Radunovic" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Kanade et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Kanade et al\\.",
      "year" : 2012
    }, {
      "title" : "Almost optimal exploration in multi-armed bandits",
      "author" : [ "Z. Karnin", "T. Koren", "O. Somekh" ],
      "venue" : "In Proceedings of the 30th International Conference on Machine Learning,",
      "citeRegEx" : "Karnin et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Karnin et al\\.",
      "year" : 2013
    }, {
      "title" : "Distributed learning in multi-armed bandit with multiple players",
      "author" : [ "K. Liu", "Q. Zhao" ],
      "venue" : "IEEE Transactions on Signal Processing,",
      "citeRegEx" : "Liu and Zhao.,? \\Q2010\\E",
      "shortCiteRegEx" : "Liu and Zhao.",
      "year" : 2010
    }, {
      "title" : "The sample complexity of exploration in the multi-armed bandit problem",
      "author" : [ "S. Mannor", "J. Tsitsiklis" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Mannor and Tsitsiklis.,? \\Q2004\\E",
      "shortCiteRegEx" : "Mannor and Tsitsiklis.",
      "year" : 2004
    }, {
      "title" : "Hoeffding races: Accelerating model selection search for classification and function approximation",
      "author" : [ "O. Maron", "A.W. Moore" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Maron and Moore.,? \\Q1994\\E",
      "shortCiteRegEx" : "Maron and Moore.",
      "year" : 1994
    }, {
      "title" : "Empirical bernstein stopping",
      "author" : [ "V. Mnih", "C. Szepesvári", "J.-Y. Audibert" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Mnih et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2008
    }, {
      "title" : "How does clickthrough data reflect retrieval quality",
      "author" : [ "F. Radlinski", "M. Kurup", "T. Joachims" ],
      "venue" : "In CIKM,",
      "citeRegEx" : "Radlinski et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Radlinski et al\\.",
      "year" : 2008
    }, {
      "title" : "Interactively optimizing information retrieval systems as a dueling bandits problem",
      "author" : [ "Y. Yue", "T. Joachims" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Yue and Joachims.,? \\Q2009\\E",
      "shortCiteRegEx" : "Yue and Joachims.",
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 22,
      "context" : "MAB algorithms rank results of search engines (Radlinski et al., 2008; Yue and Joachims, 2009), choose between stories or ads to",
      "startOffset" : 46,
      "endOffset" : 94
    }, {
      "referenceID" : 23,
      "context" : "MAB algorithms rank results of search engines (Radlinski et al., 2008; Yue and Joachims, 2009), choose between stories or ads to",
      "startOffset" : 46,
      "endOffset" : 94
    }, {
      "referenceID" : 1,
      "context" : "showcase on web sites (Agarwal et al., 2008; Chakrabarti et al., 2008), accelerate model selection and stochastic optimization tasks (Maron and Moore, 1994; Mnih et al.",
      "startOffset" : 22,
      "endOffset" : 70
    }, {
      "referenceID" : 8,
      "context" : "showcase on web sites (Agarwal et al., 2008; Chakrabarti et al., 2008), accelerate model selection and stochastic optimization tasks (Maron and Moore, 1994; Mnih et al.",
      "startOffset" : 22,
      "endOffset" : 70
    }, {
      "referenceID" : 20,
      "context" : ", 2008), accelerate model selection and stochastic optimization tasks (Maron and Moore, 1994; Mnih et al., 2008), and more.",
      "startOffset" : 70,
      "endOffset" : 112
    }, {
      "referenceID" : 21,
      "context" : ", 2008), accelerate model selection and stochastic optimization tasks (Maron and Moore, 1994; Mnih et al., 2008), and more.",
      "startOffset" : 70,
      "endOffset" : 112
    }, {
      "referenceID" : 14,
      "context" : "Following recent MAB literature (Even-Dar et al., 2006; Audibert et al., 2010; Gabillon et al., 2011; Karnin et al., 2013), we focus on the problem of identifying a “good” bandit arm with high confidence.",
      "startOffset" : 32,
      "endOffset" : 122
    }, {
      "referenceID" : 3,
      "context" : "Following recent MAB literature (Even-Dar et al., 2006; Audibert et al., 2010; Gabillon et al., 2011; Karnin et al., 2013), we focus on the problem of identifying a “good” bandit arm with high confidence.",
      "startOffset" : 32,
      "endOffset" : 122
    }, {
      "referenceID" : 15,
      "context" : "Following recent MAB literature (Even-Dar et al., 2006; Audibert et al., 2010; Gabillon et al., 2011; Karnin et al., 2013), we focus on the problem of identifying a “good” bandit arm with high confidence.",
      "startOffset" : 32,
      "endOffset" : 122
    }, {
      "referenceID" : 17,
      "context" : "Following recent MAB literature (Even-Dar et al., 2006; Audibert et al., 2010; Gabillon et al., 2011; Karnin et al., 2013), we focus on the problem of identifying a “good” bandit arm with high confidence.",
      "startOffset" : 32,
      "endOffset" : 122
    }, {
      "referenceID" : 7,
      "context" : "Our goal is to find an arm with an (almost) optimal expected reward, with as few arm pulls as possible (that is, minimize the simple regret (Bubeck et al., 2009)).",
      "startOffset" : 140,
      "endOffset" : 161
    }, {
      "referenceID" : 11,
      "context" : "Round-based models are natural in distributed learning scenarios, where frameworks such as MapReduce (Dean and Ghemawat, 2008) are ubiquitous.",
      "startOffset" : 101,
      "endOffset" : 126
    }, {
      "referenceID" : 15,
      "context" : "In the MAB literature, several recent works consider multi-player MAB scenarios in which players actually compete with each other, either on arm-pulls resources (Gabillon et al., 2011) or on the rewards received (Liu and Zhao, 2010).",
      "startOffset" : 161,
      "endOffset" : 184
    }, {
      "referenceID" : 18,
      "context" : ", 2011) or on the rewards received (Liu and Zhao, 2010).",
      "startOffset" : 35,
      "endOffset" : 55
    }, {
      "referenceID" : 13,
      "context" : "Another line of recent work was focused on distributed stochastic optimization (Duchi et al., 2010; Agarwal and Duchi, 2011; Dekel et al., 2012) and distributed PACmodels (Balcan et al.",
      "startOffset" : 79,
      "endOffset" : 144
    }, {
      "referenceID" : 0,
      "context" : "Another line of recent work was focused on distributed stochastic optimization (Duchi et al., 2010; Agarwal and Duchi, 2011; Dekel et al., 2012) and distributed PACmodels (Balcan et al.",
      "startOffset" : 79,
      "endOffset" : 144
    }, {
      "referenceID" : 12,
      "context" : "Another line of recent work was focused on distributed stochastic optimization (Duchi et al., 2010; Agarwal and Duchi, 2011; Dekel et al., 2012) and distributed PACmodels (Balcan et al.",
      "startOffset" : 79,
      "endOffset" : 144
    }, {
      "referenceID" : 13,
      "context" : "Questions involving network topology (Duchi et al., 2010; Dekel et al., 2012) and delays (Agarwal and Duchi, 2011) are relevant to our setup as well; however, our present work focuses on establishing the first non-trivial guarantees in a distributed collaborative MAB setting.",
      "startOffset" : 37,
      "endOffset" : 77
    }, {
      "referenceID" : 12,
      "context" : "Questions involving network topology (Duchi et al., 2010; Dekel et al., 2012) and delays (Agarwal and Duchi, 2011) are relevant to our setup as well; however, our present work focuses on establishing the first non-trivial guarantees in a distributed collaborative MAB setting.",
      "startOffset" : 37,
      "endOffset" : 77
    }, {
      "referenceID" : 0,
      "context" : ", 2012) and delays (Agarwal and Duchi, 2011) are relevant to our setup as well; however, our present work focuses on establishing the first non-trivial guarantees in a distributed collaborative MAB setting.",
      "startOffset" : 19,
      "endOffset" : 44
    }, {
      "referenceID" : 9,
      "context" : "In the MAB literature, several recent works consider multi-player MAB scenarios in which players actually compete with each other, either on arm-pulls resources (Gabillon et al., 2011) or on the rewards received (Liu and Zhao, 2010). In contrast, we study a collaborative multi-player problem and investigate how sharing observations helps players achieve their common goal. The related work of Kanade et al. (2012) in the context of non-stochastic (i.",
      "startOffset" : 162,
      "endOffset" : 416
    }, {
      "referenceID" : 18,
      "context" : ", when there is only one player), the lower bounds of Mannor and Tsitsiklis (2004) and Audibert et al.",
      "startOffset" : 54,
      "endOffset" : 83
    }, {
      "referenceID" : 3,
      "context" : ", when there is only one player), the lower bounds of Mannor and Tsitsiklis (2004) and Audibert et al. (2010) show that in general Ω̃(Hε) pulls are necessary for identifying an ε-arm, where",
      "startOffset" : 87,
      "endOffset" : 110
    }, {
      "referenceID" : 3,
      "context" : "Intuitively, the hardness of the task is therefore captured by the quantity Hε, which is roughly the number of arm pulls needed to find an ε-best arm with a reasonable probability; see also (Audibert et al., 2010) for a discussion.",
      "startOffset" : 190,
      "endOffset" : 213
    }, {
      "referenceID" : 19,
      "context" : "The first obvious approach, already mentioned earlier, is the nocommunication strategy: just let each player explore the arms in isolation of the other If one is interested in distribution-free bounds, then the problem at hand is trivial as the (distributed) uniform sampling strategy is optimal in this setting, up to polylogarithmic factors; see also (Mannor and Tsitsiklis, 2004) for a relevant discussion.",
      "startOffset" : 353,
      "endOffset" : 382
    }, {
      "referenceID" : 4,
      "context" : "A similar idea was employed in (Auer and Ortner, 2010) for improving the regret bound of UCB with respect to the parameters ∆i.",
      "startOffset" : 31,
      "endOffset" : 54
    }, {
      "referenceID" : 14,
      "context" : "For example, the Successive Elimination algorithm (Even-Dar et al., 2006) and the Exp-Gap Elimination algorithm (Karnin et al.",
      "startOffset" : 50,
      "endOffset" : 73
    }, {
      "referenceID" : 17,
      "context" : ", 2006) and the Exp-Gap Elimination algorithm (Karnin et al., 2013) provide a guarantee of this form.",
      "startOffset" : 46,
      "endOffset" : 67
    }, {
      "referenceID" : 2,
      "context" : "1 of (Anthony and Bartlett, 1999).",
      "startOffset" : 5,
      "endOffset" : 33
    }, {
      "referenceID" : 2,
      "context" : "1 of (Anthony and Bartlett, 1999). Lemma 3.10. Consider a MAB problem with two arms and rewards 1 2 + ε, 1 2 − ε. There exists a constant c such that any algorithm that with probability at least 2/3 identifies the best arm, needs at least c/ε pulls in expectation. Proof (of Theorem 3.9). Let c1 = √ c/2, where c is the constant of Lemma 3.10. Consider a MAB instance over n arms, with the rewards being a random permutation σ of 1 2 +∆, 1 2 − ∆, 0, . . . , 0, where ∆ := 1/ √ n. A serial algorithm of choice (say, the Successive Elimination algorithm) is able to identify the best arm in this setting with probability 2/3 using at most Õ(n + 1/∆) = Õ(n) arm pulls (see e.g., Even-Dar et al. (2006)).",
      "startOffset" : 6,
      "endOffset" : 699
    }, {
      "referenceID" : 5,
      "context" : "In particular, it would be nice to see a conversion of algorithms like UCB (Auer et al., 2002) to a distributed setting.",
      "startOffset" : 75,
      "endOffset" : 94
    }, {
      "referenceID" : 5,
      "context" : "In particular, it would be nice to see a conversion of algorithms like UCB (Auer et al., 2002) to a distributed setting. In this respect, perhaps a more natural distributed model is a one resembling that of Kanade et al. (2012), that have established a regret vs.",
      "startOffset" : 76,
      "endOffset" : 228
    } ],
    "year" : 2013,
    "abstractText" : "We study exploration in Multi-Armed Bandits in a setting where k players collaborate in order to identify an ε-optimal arm. Our motivation comes from recent employment of bandit algorithms in computationally intensive, large-scale applications. Our results demonstrate a non-trivial tradeoff between the number of arm pulls required by each of the players, and the amount of communication between them. In particular, our main result shows that by allowing the k players to communicate only once, they are able to learn √ k times faster than a single player. That is, distributing learning to k players gives rise to a factor √ k parallel speed-up. We complement this result with a lower bound showing this is in general the best possible. On the other extreme, we present an algorithm that achieves the ideal factor k speed-up in learning performance, with communication only logarithmic in 1/ε.",
    "creator" : "LaTeX with hyperref package"
  }
}