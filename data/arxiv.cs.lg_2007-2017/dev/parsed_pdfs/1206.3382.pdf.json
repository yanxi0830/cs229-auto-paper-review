{
  "name" : "1206.3382.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Online Planning in MDPs: Rationality and Optimization",
    "authors" : [ "Zohar Feldman" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "In the past couple of decades, Markov decision processes (MDPs) have become a standard framework for planning under uncertainty. An MDP 〈S,A, Tr,R〉 is defined by a set of states S, a set of actions A, a stochastic transition function Tr : S × A × S → [0, 1], and a reward function\nar X\niv :1\n20 6.\n33 82\nv1 [\ncs .A\nR : S × A× S → R. The current state of the agent is fully observable, and the objective of the agent is to act so to maximize its accumulated reward. In the finite horizon setting that will be used for most of the paper, the reward is accumulated along H steps.\nThe desire to attack problems of increasing complexity has led researches to consider online planning in MDPs. In online planning, the decision process of the agent is focused on the next action to perform, rather than on computing a quality policy for the entire MDP. The agent is given a generative model which allows for simulated execution of all possible sequences of actions, from any state of the MDP. The decision process consists of a simulation-based planning, terminated either according to a predefined schedule or due to an external interrupt, and followed by a recommendation of action to perform at the current state. Once that action is applied in the real environment, it modifies the environment and the decision process is repeated from the new state to select the next action and so on.\nA prominent approach to online planning in MDPs, successfully used also in non-deterministic and imperfect information games, is Monte-Carlo (MC) planning. Numerous MC planning algorithms have been proposed in the literature. The sparse sampling algorithm by Kearns, Mansour, and Ng [10] offered near-optimal action selection in time exponential in H but independent of the state space size. Several other works, closer to our focus here, introduced interruptable algorithms that are designed to provide convergence to the best action if enough time is given, and small performance loss if the algorithm is stopped prematurely [18, 13, 12, 6, 5, 15, 19]. The quality of the action a recommended for state s with H steps-to-go is assessed in terms of the probability that a is sub-optimal, and in terms of the (closely related) simple regret ∆H [s, a]. The latter is the performance loss resulting from taking a and then following an optimal policy π∗ for the remaining H − 1 steps, instead of following π∗ from the beginning. That is,\n∆H [s, a] = QH(s, π ∗(s,H))−QH(s, a),\nwhere QH(s, a) = Es′ [ R(s, a, s′) +QH−1(s ′, π∗(s′, H − 1)) ] .\nWhile empirical attractiveness of alternative MC planning algorithms depends on specifics of the problem in hand, only some of these algorithms provide formal guarantees on their expected performance improvement over time. In particular, none of the MC planning algorithms in use these days breaks the barrier of the worst-case polynomial reduction of error probability, and thus they at best guarantee polynomial simple regret over time.\nThis is precisely the contribution of this paper: We introduce an MC planning algorithm for MDPs, BRUE, that achieves over time exponential reduction of both the error probability and simple regret. The key in this simple to implement and computationally efficient algorithm is decoupling between certain contradicting objectives that should be addressed by the sampling process for online MDP planning. Our preliminary empirical evaluation on a standard benchmark for comparison between MC planning algorithms shows that BRUE not only provides superior performance guarantees, but is also very effective in practice."
    }, {
      "heading" : "2 MONTE-CARLO PLANNING",
      "text" : "A general scheme for Monte-Carlo planning, MCT, that gives rise to various specific algorithms for online MDP planning, is depicted in Figure 1. Starting with the current state s0, MCT performs an iterative construction of a tree rooted at s0. At each iteration, MCT issues a probe from s0, expands the tree based on the outcome of the probe, and updates information stored at the nodes of the tree. Once the simulation phase is over, MCT issues a recommendation of action to perform in s0, and this is based on the information collected at the nodes of the tree. For compatibility of the notation with prior literature, in what follows we refer to the tree nodes via the states associated with these nodes. Note that, due to the Markovian nature of MDPs, it is unreasonable to distinguish between nodes associated with the same state at the same depth. Hence, the actual graph constructed by most instances of MCT forms a DAG over nodes (s, h) ∈ S × {0, 1, . . . ,H}. By A(s) ⊆ A in what follows we refer to the subset of actions applicable in state s.\nNumerous concrete instances of MCT have been proposed, with UCT [12]\nprobably being the most popular such algorithm these days [8, 17, 3, 2, 7, 11]. To give a concrete sense of MCT’s components, as well as to ground some intuitions discussed later on, below we describe the specific setting of MCT corresponding to the core UCT algorithm:\n• probe-S: The probes ρ = 〈s0, a1, s1, . . . , ak, sk〉 are all issued from the root node s0. The probe ends either when reached a sink state, that is, A(sk) = ∅, or when k = H. Each node/action pair (s, a) is associated with a counter n(s, a) and a value accumulator Q̂(s, a); both n(s, a) and Q̂(s, a) are initialized to 0, and then updated by the update-statistics procedure. Given si, the next-on-the-probe action ai+1 is determined by the deterministic UCB1 policy [1], originally proposed for optimal cumulative regret minimization in stochastic multiarmed bandit (MAB) problems [14]: If n(si, a) > 0 for all a ∈ A(si), then\nai+1 = argmax a\n[ Q̂(si, a) + c √ log n(si)\nn(si, a)\n] , (1)\nwhere n(s) = ∑\na n(s, a). Otherwise, ai+1 is selected uniformly at random from actions a ∈ A(si) with n(si, a) = 0. In both cases, si+1 is then sampled according to the conditional probability P(S|si, ai+1), induced by the transition function Tr.\n• expand-tree: Each probe ρ = 〈s0, a1, s1, . . . , ak, sk〉 induces a state trace 〈s0, s1, . . . , si〉 inside T , as well as a state trace 〈si+1, . . . , sk〉 outside of T . In principle, T can be expanded with any prefix of 〈si+1, . . . , sk〉; a practically popular choice appears to be expanding T with only the upper-most node si+1. (If T is constructed as a DAG, it is expanded with the first node along ρ that leaves T .)\n• update-statistics: For each node si along ρ that is now part of the expanded tree T , the counter n(si, ai+1) is incremented and the estimated Q-value is updated as\nQ̂(si, ai+1)← Q̂(si, ai+1) + Ri − Q̂(si, ai+1) n(si, ai+1) , (2)\nwhere Ri = ∑k−1 j=i R(sj , aj+1, sj+1).\n• recommend-action: Interestingly, the action recommendation protocol of UCT was never properly specified, and different applications of UCT\nadopt different decision rules, including maximization of the estimated Q-value, of the augmented estimated Q-value as in Eq. 1, of the number of times the action was selected during the simulation, as well as randomized protocols based on the information collected at the root.\nThe key property of MCT-based algorithms is that their exploration of the search space is obtained by considering a hierarchy of forecasters, each minimizing its own cumulative regret. Each such pseudo-agent forecaster corresponds to a state/steps-to-go pair (s, h). In that respect, according to Theorem 6 of Kocsis and Szepesvári [12], UCT achieves the optimal logarithmic cumulative regret. However, the cumulative regret does not seem to be the right way to base MC planning on, and this is because the rewards “collected” at the simulation phase are fictitious. In contrast, the same Theorem 6 of Kocsis and Szepesvári [12] claims only a polynomial-rate reduction of the probability of choosing a non-optimal action, and the recent results by Bubeck, Munos, and Stoltz [4] on simple regret minimization in MABs with stochastic rewards imply that UCT achieves only polynomial reduction of the simple regret over time. Some attempts have recently been made to adapt UCT, and MCT-based planning in general, to optimizing decisions in online MDP planning [19, 9]. As we show later on, some of these attempts were actually very successful empirically. However, to the best of our knowledge, none of them breaks the barrier for the formal performance guarantees provided by UCT.\n3 THE BRUE ALGORITHM\nThe work of Bubeck et al. [4] was probably the first systematic attempt to analyze pure exploration in MABs, showing that the minimal simple regret in MAB can increase as the bound on the cumulative regret is getting smaller. The analysis of Bubeck et al. led us to suspect that the major deficiency of the current MC planning algorithms is in their very reliance on the MCT scheme, and to look for alternative search schemes that would better suit simple regret optimization in MDP planning. As a result, here we introduce a novel scheme for MC planning in MDPs, MCTer (see Figure 2), and then describe a concrete instance of this scheme that (1) guarantees that the probability of recommending non-optimal arm convergences to zero at exponential rate, and (2) achieves over time exponential rate of the simple regret reduction.1\n1Note that we are not claiming that no instance of MCT can achieve such guarantees, but only that so far no such instance has been discovered. This leaves a very interesting\nSimilarly to MCT, MCTer iteratively constructs a tree rooted at the current state s0, starting each iteration with issuing a probe from s0 and expanding the tree based on the outcome ρ of the probe. The probes ρ are issued to varying depth in a round-robin fashion, from the full depth of H to just a basic lookahead of 1. Importantly, instead of updating the statistics stored at the nodes based on ρ and proceeding with the next iteration as in MCT, a new probe ρ is issued from the end state of ρ, and the information at the tree nodes is updated only after this “complementary probe” ρ. Importantly, the two type of probes can be generated according to different strategies, dubbed in Figure 2 as probe-S and probe-R.\nThe intuition behind what we try to achieve with the non-standard flow of MCTer is as follows. As we already mentioned, the origin of the standard MCT scheme, as well as of its various instantiations, is in online optimization in the context of various MAB problems. Considering each state/steps-to-go pair (s, h) as a pseudo-agent, the sole task of the root pseudo-agent (s0, H) is to minimize its own simple regret in a stochastic MAB setting induced by the applicable actions A(s0). Therefore, if an oracle would provide (s0, H) with an optimal action π∗(s0, H), then no further planning will be needed until after the execution of π∗(s0, H). However, the task characteristics of (s0, H) is an exception rather than a rule. Suppose that an oracle provides us with optimal actions for all pseudo-agents (s, h) but (s0, H). Despite the richness of this information, (s0, H) in some sense remains as clueless as it was before. To choose between the alternatives A(s0), (s0, H) needs at least\nquestion for future work.\nrelative information about the expected value of these alternatives. Hence, each non-root pseudo-agent (s, h) is devoted to two tasks: (1) identifying an optimal action π∗(s, h), and (2) estimating the actual value of that action, because this information is needed by the predecessor(s) of (s, h) in T .\nThis is the key point that MCTer aims at targeting differently from MCT. While both schemes incrementally collect information about the search space by sampling it, they differ in the roles that different samples play in that process. In MCT, each probe ρ issued at (s, h) is a priori devoted both to increasing the confidence in that some current candidate a† for π∗(s, h) is indeed π∗(s, h), as well as to improving the estimate of Qh(s, a\n†), as if assuming that π∗(s, h) = a†. Such an overloading of the probes is unavoidable in the “learning while acting” setup of reinforcement learning (RL) where agents should naturally care about their cumulative performance. However, while the second objective of (s, h) in online planning does prescribe (s, h) to act as to maximally exploit its current knowledge, this similarity to RL is somewhat misleading as RL-style exploitation per se is irrelevant in MC planning.\nIn principle, nothing requires our sampling mechanism to balance between the two contradicting objectives at the level of individual probes, the way instances of MCT are forced to do. In MCTer, the two roles are fulfilled by different sets of probes: the probes issued by probe-S aim at exploration while the probes issued by probe-R aim at improving the value estimates for the current candidates for π∗. In particular, this separation allows us to introduce a specific MCTer instance, BRUE,2 that is tailored to simple regret minimization. Inspired by the positive results of Bubeck et al. [4] for MABs, the BRUE setting of MCTer is as follows:\n• probe-S: The probes ρ = 〈s0, a1, . . . , sh−1, ah, sh〉 are all issued from the root node s0, with actions along the probe being selected at random according to uniform distribution. Each node/action pair (s, a) is associated with n(s, a) and Q̂(s, a) as in UCT; while counters n(s, a) are initialized to 0, value accumulators Q̂(s, a) are schematically initialized to −∞.\n• expand-tree: T is expanded with the suffix of state sequence s1, . . . , sh−1 that is new to T .\n• probe-R: In the complementary probe ρ issued at node sh, ρ = 〈sh = s1, a2, s2, . . . , ak, sk〉, each action ai is sampled uniformly at random, but only among the actions a ∈ A(si−1) that maximize Q̂(si, a).\n2Short for Best Recommendation with Uniform Exploration.\n• update-statistics: The complementary probe ρ as above is used to update statistics on the state/action pair (sh−1, ah) according to Eq. 2, but now with Ri = ∑k−1 j=1 R(sj , aj+1, sj+1). Note that the information\nobtained by ρ is not pushed further up the probe ρ. While that may appear wasteful and even counterintuitive, this locality of update is required to satisfy the formal guarantees of BRUE discussed later on.\n• recommend-action: The action recommended by BRUE is chosen uniformly at random among the actions a maximizing Q̂(s0, a).\nFor the sake of simplicity, in our formal analysis we assume uniqueness of the optimal policy π∗; that is, at each state s and each number h of stepsto-go, there is a single optimal action, and it is π∗(s, h). Likewise, let πBn be the (possibly stochastic) policy induced by the value accumulators Q̂ after n iterations of BRUE: denoting by Tn the graph obtained by BRUE after n iterations, and by Q̂h(s, a) the accumulated value Q̂(s, a) for s at depth H − h, for all state/steps-to-go pairs (s, h) ∈ Tn, πBn (s, h) is a randomized strategy, uniformly choosing among actions a maximizing Q̂h(s, a). We also use some auxiliary notation:\nK = maxs∈S |A(s)|, i.e., the maximal number of actions per state.\np = mins,a,s′:Tr(s,a,s′)>0 Tr(s, a, s ′), i.e., the likelihood of the least likely\n(but still possible) outcome of an action in our problem.\nd = mins,a ∆1[s, a], i.e., the smallest difference between the value of the optimal and a second-best action at a state with just one step-to-go.\nTheorem 1 Let BRUE be called on a state s0 of an MDP 〈S,A, Tr,R〉 with rewards in [0, 1] and finite horizon H. For any 1 ≤ h ≤ H, there exist parameters ch, c ′ h, c ′′ h <∞, dependent of p, d and h, but independent of s0, |S|, and the number of BRUE iterations n, such that, for each state s reachable from s0 in H − h steps, for all n ≥ c′′h, we have\n• Simple Regret E∆h[s, πBn (s, h)] ≤ che−c ′ hn, (3)\n• Error Probability\nP { πBn (s, h) 6= π∗(s, h) } ≤ ch\nh e−c ′ hn. (4)\nCorollary 1 The simple regret of the action πBn (s0, H), recommended by BRUE after n > c1 iterations, is bounded by c2 · exp (−n · c3) , where ci = fi(p, d,H) <∞.\nIn rest of this section we prove Theorem 1. To shorten the equations, when considering in what follows a state s with h steps-to-go, by a∗ we denote the optimal action π∗(s, h). When considering in that context an action a ∈ A(s), by Q we denote Qh(s, a), and by Q̂ and n(a) we denote the respective value accumulator Q̂h(s, a) and counter n (s, a); Q ∗ and Q̂∗ denote Qh(s, a ∗) and Q̂h(s, a\n∗), respectively. Likewise, by ph we denote the minimal probability of reaching a (reachable this way) state with still h steps-to-go on a uniform sample from s0.\nThe proof is by induction on h. Recall that the recommended action πBn (s, h) is an action maximizing Q̂h(s, a). Starting with proving the induction basis for h = 1, note that the probability of choosing an action a 6= a∗ is bounded by\nP { Q̂ ≥ Q̂∗ } ≤ P { Q̂∗ ≤ Q∗ − ∆1[s, a]\n2\n} + P { Q̂ ≥ Q+ ∆1[s, a]\n2\n} (5)\nThe analysis of the two summation terms in Eq. 5 is identical, and thus we detail it here only for the first term.\nP { Q̂∗ ≤ Q∗ − ∆1[s, a]\n2\n} ≤\nP {n(a∗) ≤ n0}+ P { Q̂∗ ≤ Q∗ − ∆1[s, a]\n2 , n(a∗) > n0 } (6) To bound the first term of this summation, we note that, for each search node (s, h) and each action a ∈ A(s), n(a) is a sum of n independent Bernouli trials with success probability ph ≥ 1K ( p K )\nH−h, and E [n(a)] ≥ nph. By choosing n0 = n p1 2 and employing Chernoff-Hoefdding bound, we obtain\nP {n(a∗) ≤ n0} = P { n(a∗)\nn ≤ E [n(a\n∗)] n − ( E [n(a∗)] n − n0 n )}\n≤ exp −2n(p(H−1) 2KH )2 = exp(−np21 2 ) .\n(7)\nThe second term in Eq. 6 can be bounded as: P { Q̂∗ ≤ Q∗ − ∆1[s, a]\n2 , n(a∗) > n0 } =\nn∑ t=n0+1 P { Q̂∗ ≤ Q∗ − ∆1[s, a] 2 , n(a∗) = t }\n≤ n∑\nt=n0+1\nP { Q̂∗ ≤ Q∗ − ∆1[s, a]\n2 ∣∣∣∣ n(a∗) = t}P {n(a∗) = t} =\nn∑ t=n0+1 P { Q̂∗ ≤ E [ Q̂∗ ] − ∆1[s, a] 2 ∣∣∣∣ n(a∗) = t}P {n(a∗) = t} since E [ Q̂∗ ] = Q∗\n≤ n∑\nt=n0+1\nexp ( −2t∆1[s, a] 2\n4\n) P {n(a∗) = t}\nby Chernoff-Hoefdding bound\n≤ e−2n0 ∆1[s,a]\n2\n4\nn∑ t=n0+1 P {n(a∗) = t}\n≤ e−n d2p1 4 . (8) Putting together Eqs. 5-8, we obtain P { Q̂ ≥ Q̂∗ } ≤ 4 exp ( −nd\n2p21 4\n) ,\nand thus, for all n ≥ 1,\nE∆1[s, πBn (s, 1)] ≤ ∑ a6=a∗ ∆1[s, a] · P { Q̂ ≥ Q̂∗ } ≤ 4Ke−n d2p21 4 (9)\nand\nP { πBn (s, 1) 6= π∗(s, 1) } ≤ ∑ a6=a∗ P { Q̂ ≥ Q̂∗ } ≤ 4Ke−n d2p21 4 (10)\nNow, assuming the claim holds for h ≥ 1, we prove it for h + 1. The probability of choosing a sub-optimal action a 6= a∗ when still h + 1 stepsto-go is bounded similarly to Eq. 5, with ∆1[s, a] replaced by ∆h+1[s, a]. Similarly, we obtain\nP { Q̂∗ ≤ Q∗ − ∆h+1[s, a]\n2\n} ≤\nP {n(a∗) ≤ n0}+ P { Q̂∗ ≤ Q∗ − ∆h+1[s, a]\n2 , n(a∗) > n0\n} (11)\nand\nP {n(a∗) ≤ n0} ≤ exp ( −n\np2h+1 2\n) . (12)\nHowever, bounding the second term of summation in Eq. 11 differs from this for h = 1 in Eq. 8 for two reasons.\n(F1) For h = 1, Q̂ is an unbiased estimator of Q, that is, EQ̂ = Q. In contrast, the estimates inside the tree (at nodes with h > 1) are biased. This bias stems from Q̂ possibly being based on numerous sub-optimal choices in the sub-tree rooted in (s, h).\n(F2) For h = 1, the summands accumulated by Q̂ are independent. This is not so for h > 1 since in this case the accumulated reward depends on the selection of actions in subsequent nodes, which in turn depends on previous rewards.\nWe now show that these deficiencies of h > 1 can still be overcome. In what follows, we first tackle the issues F1-F2 by a different bounding of the quantity\nP { Q̂∗ ≤ Q∗ − ∆h+1[s, a]\n2 ∣∣∣∣ n(a∗) = t} , and then use the outcome of this analysis to bound the second summand\nof Eq. 11.\nLemma 1 (Expected accumulated rewards) Considering the policy πBn induced by n iterations3 of BRUE on an MDP 〈S,A, Tr,R〉, for each state s ∈ S, each action a ∈ A(s), and each number of steps-to-go h ∈ {1, . . . ,H}, let X = R (s, a, s1) + ∑h−1 i=1 R ( si, π B n (si, h− i), si+1 ) be a random variable corresponding to the reward accumulated by taking a at s, and then following πBn for the rest h− 1 steps. Then δn,h(s, a) , Q(s, a)− E [X] is\nδn,h(s, a) = h−1∑ i=1 E∆h−i[si, πBn (si, h− i)]. (13)\nProof: For any state/steps-to-go pair (s, h) ∈ S × {1, . . . ,H}, we have EB,s′ [ R ( s, πBn (s, h), s ′)] = EB [ Q(s, πBn (s, h)) ] − EB,s′ [ Q(s′, π∗(s′, h− 1)) ] . (14)\n3Iteration = full run of the most-inner loop of BRUE.\nUsing that, we obtain a telescopic series that yields\nE [X] = EB,s1:sh [ R (s, a, s1) + h−1∑ i=1 R ( si, π B n (si, h− i), si+1 )] = Q(s, a)− Es1 [Q(s1, π∗(s1, h− 1))] +\nh−1∑ i=1 EB,s1:si [ Q(si, π B n (si, h− i)) ] −\nh−1∑ i=1 EB,s1:si+1 [Q(si+1, π ∗(si+1, h− i− 1))]\n= Q(s, a)− h−1∑ i=1 EB,s1:si [ ∆h−i[si, π B n (s, h− i)] ] .\n(15)\nIn what follows, we refer to the supremum of δn,h(s, a) from Lemma 1 as δn,h , maxs,a{δn,h(s, a)}.\nWith Lemma 1 in hand, let {Xt}n(a ∗) t=1 be the summands of the value accumulator Q̂∗ = Q̂h+1(s, a ∗). Approaching (F1) first, we exploit the fact that the entire bias in our estimate of Q∗ is due to the erroneous recommendations of πB at the successors of s, and the damage of these erroneous recommendations is fully captured by the simple regret of using πB at the immediate successors of s. When bounding Q∗ − EXt, we note that this difference is largest when each Xt is sampled at iteration t, that is, each Xt is acquired at the earliest possible (and thus least informed) stage of the algorithm. Hence, Q∗ − EXt ≤ δt,h+1, and this allows us to bound the influence of the bias as follows.\nP { Q̂∗ ≤ Q∗ − ∆h+1[s, a]\n2 ∣∣∣∣ n(a∗) = t} = P { Q̂∗ ≤ EQ̂∗ − ( ∆h+1[s, a] 2 − ( Q∗ − EQ̂∗\n)) ∣∣∣∣ n(a∗) = t} ≤ P { Q̂∗ ≤ EQ̂∗ − ( ∆h+1[s, a]\n2 − 1 t t∑ τ=1 δτ,h+1 ) ∣∣∣∣∣ n(a∗) = t }\nsince Q∗ − EQ̂∗ = 1 t t∑ τ=1 (Q∗ − EXτ ) ≤ 1 t t∑ τ=1 δτ,h+1\n(16)\nWe now proceed with (F2), bounding the influence of the Q̂∗ summands dependency. At high level, we achieve that by modifying the ChernoffHoefdding bound for independent random variables to certain sequences of dependent random variables {Yt} such that Yt|Yt−1, . . . , Y1 concentrates around its expectation with a probability that approaches 1 at rate exponential in t.\nWe begin with establishing some auxiliary notions and properties. Let At be the event in which Xt is sampled along the optimal actions in each of the h subsequent nodes4. We note that\nP {¬At} ≤ h∑ i=1 P { πBt (si, i) 6= π∗(si, i) } . (17)\nDenoting cp,h = ∑h i=1 ci i and cδ,h = ∑h i=1 ci, from (stemming from induction hypothesis) Eq. 10, monotonic decrease of c′i in i, and monotonic increase of c′′i in i, for all t ≥ c′′h we have\nP {¬At} ≤ h∑ i=1 ci i e−c ′ it ≤ cp,he−c ′ ht, (18)\nand\nδt,h+1 ≤ h∑ i=1 cie −c′it ≤ cδ,he−c ′ ht. (19)\nNow, for all ω ∈ At, E [Xt | X1, . . . , Xt−1] (ω) = Q∗, and thus [Q∗ −Xt | X1, . . . , Xt−1] (ω) is a random variable with expectation 0, and support [Q∗ − (h+ 1), Q∗]. Hence, for any λ ≥ 0,\nE [ eλ(EXt−Xt) ∣∣∣ X1, . . . , Xt−1] = eλ(EXt−Q ∗)E [ eλ(Q ∗−Xt) ∣∣∣ X1, . . . , Xt−1]\n≤ e−λδt,h+1E [ eλ(Q ∗−Xt) ∣∣∣ X1, . . . , Xt−1]\n≤ e−λδt,h+1+ λ2(h+1)2 8 (20)\nusing Fact 1 below.\n4It is easy to see that At ∈ σ (X1, . . . , Xt−1)\nFact 1 Let Z be a random variable satisfying a ≤ Z ≤ b and E [Z] = 0. Then, E [exp(λZ)] ≤ exp( (b−a) 2λ2\n8 ) for any λ ∈ R.\nProceeding now with developing Eq. 16, let α = ∆h+1[s,a] 2 − 1 t ∑t τ=1 δτ,h.\nIf α > 0, then for all λ ≥ 0, from Markov inequality it holds that (16) = P { Q̂∗ ≤ EQ̂∗ − α ∣∣∣ n(a∗) = t} = P { tQ̂∗ ≤ tEQ̂∗ − tα\n∣∣∣ n(a∗) = t} ≤ e−λtαE [ eλ ∑t i=1(EXi−Xi) ] . (21)\nFocusing on the second multiplicand in Eq. 21, E [ eλ ∑t i=1(EXi−Xi) ] = EAt [ eλ ∑t i=1(EXi−Xi) ] + E¬At [ eλ ∑t i=1(EXi−Xi)\n] ≤ e−λδt,h+1+ λ2(h+1)2 8 E [ eλ ∑t−1 i=1(EXi−Xi) ] + P {¬At} eλtQ ∗\n≤ e λ2(h+1)2 8 E [ eλ ∑t−1 i=1(EXi−Xi) ] + cp,he t(λ(h+1)−c′h) by Eq. 18, and Q∗ ≤ h+ 1 ≤ e λ2(h+1)2 8 (t−c ′′ h)eλc ′′ h(h+1) +\nt∑ τ=c′′h+1 e λ2(h+1)2 8 (t−τ)cp,he τ(λ(h+1)−c′h) (22)\nby iterating until c′′h along Eq. 24 as described below.\nConsidering the recursion\nf (t) = θf (t− 1) + g (t) , (23)\nit can be shown that, by iterating until t = c, Eq. 23 is equivalent to\nf (t) = θt−cf (c) + t∑ τ=c+1 θt−τg (τ) . (24)\nGiven that, the last bound in Eq. 22 is obtained by setting f (t) = E [ eλ ∑t i=1(EXi−Xi) ] ,\nθ = e λ2(h+1)2 8 , and g (t) = cp,he t(λ(h+1)−c′h).\nDenoting now ξ = αc′h\n(h+1) , and choosing λ = ξ 2 (h+1) ,\n(22) = eξ 2 (t−c\n′′ h)\n2 e2ξc ′′ h + t∑ τ=c′′h+1 eξ 2 (t−τ) 2 cp,he τ(2ξ−c′h)\n≤ eξ2 (t−c′′h) 2 e2ξc ′′ h + t∑ τ=c′′h+1 eξ 2 (t−τ) 2 cp,h\nsince, by definition of α, it holds that ξ ≤ c′h 2\n= eξ 2 (t−c\n′′ h)\n2 e2ξc ′′ h + cp,he ξ2 t 2 t∑ τ=c′′h+1 e−ξ 2 τ 2\n≤ eξ2 (t−c′′h) 2 e2ξc ′′ h + cp,he ξ2 t 2\n( 2\nξ2 e−\nξ2\n2 c′′h ) since\n∞∑ t=c+1 e−kt ≤ 1 k e−ck\n≤ eξ2 t 2 e2ξc ′′ h + 2cp,h ξ2 eξ 2 t 2 . (25)\nUsing the bound provided by Eq. 25 for λ = ξ 2(h+1) into Eq. 21, we obtain\nP { Q̂∗ ≤ EQ̂∗ − α ∣∣∣ n(a∗) = t} ≤ e− 3α2c′h 2(h+1)2 t ( e 2αc′hc ′′ h h+1 + 2cp,h (h+ 1) 2\nα2c′2h\n) .\n(26)\nSince this bound is true only for α > 0, it holds only starting from n such\nthat ∆h+1[s,a] 2 > 1 t ∑t τ=1 δτ,h+1 for all t ≥ n0 = nph+1 2 . Recalling Eq. 19, by induction hypothesis, for t ≥ c′′h it holds that\n∞∑ τ=1 δτ,h+1 = c′′h∑ τ=1 δτ,h+1 + ∞∑ τ=c′′h+1 δτ,h+1\n≤ c′′h(h+ 1) + ∞∑\nτ=c′′h+1\ncδ,he −c′hτ\n≤ c′′h(h+ 1) + cδ,h c′h e−c ′ hc ′′ h , β.\n(27)\nTherefore, for t ≥ 2dβ, we have that α > 0. Furthermore, to simplify dealing with α in the denominator in Eq. 26, for t ≥ 4dβ, we have α ≥ ∆h+1[s,a] 4 . Hence, for all n ≥ c′′h+1 = 8 dph+1 β, we have that α ≥ ∆h+1[s,a]4 for all t > n0. Consequently, from Eq. 26, for all n ≥ c′′h+1,\nP { Q̂∗ ≤ Q∗ − ∆h+1[s, a]\n2 ∣∣∣∣ n(a∗) = t} ≤ e− 3c′hd 2 8(h+1)2 t e 3c′h 2(h+1) β ( e 2αc′hc ′′ h h+1 + 2cp,h (h+ 1) 2\nα2c′2h\n)\nsince −tα2 ≤ −t ( ∆h+1[s, a] 2\n4 −∆h+1[s, a]\nβ\nt\n) ≤ − td 2\n4 + (h+ 1)β\n≤ e− 3c′hd 2 8(h+1)2 t e 3c′h 2(h+1) β ( ec ′ hc ′′ h + cp,h 32 (h+ 1)2\nd2c′2h\n) , γ(t).\nsince d 4 ≤ α ≤ h+ 1 2\nThis now provides us with the desired bound on the second summand in Eq. 11 as\nP { Q̂∗ ≤ Q∗ − ∆h+1[s, a]\n2 , n(a∗) > n0 } ≤\nn∑ t=n0 γ(t) · P {n(a∗) = t} ≤ γ(n0) n∑ t=n0 P {n(a∗) = t}\n≤ γ(n0) = e −n\n3c′hd 2ph+1 16(h+1)2 e 3c′h 2(h+1) β ( ec ′ hc ′′ h + cp,h 32 (h+ 1)2\nd2c′2h\n)\nThis finalizes the proof of induction hypothesis with\nch+1 = 2\n( 1 + e 3c′h 2(h+1) β ( ec ′ hc ′′ h + cp,h 32 (h+ 1)2\nd2c′2h\n)) ,\nc′h+1 = 3c′hd 2p2h+1\n16 (h+ 1)2 ,\nc′′h+1 = 8\ndph+1 β."
    }, {
      "heading" : "4 EXPERIMENTAL EVALUATION",
      "text" : "We have evaluated BRUE empirically on the MDP sailing domain [13] that was used in previous works for evaluating MC planning algorithms [13, 12, 19], as well as on random game trees used in the original empirical evaluation of UCT [12].\nIn sailing domain, a sailboat navigates to a destination on an 8-connected grid representing a marine environment, under fluctuating wind conditions. The goal is to reach the destination in as short time as possible, by choosing at each grid location a neighbor location to move to. The duration of each such move depends on the direction of the move (ceteris paribus, diagonal moves take √ 2 more time than straight moves), the direction of the wind relative to the sailing direction (the sailboat cannot sail against the wind\nand moves fastest under tail wind), and the tack. The direction of the wind changes over time, but its strength is assumed to be fixed. This sailing problem can be formulated as a goal-driven MDP over finite state space and finite set of actions, with each state capturing the position of the sailboat, wind direction, and tack. In a goal-driven MDP, the lengths of the paths to a terminal state are not necessarily bounded, and thus it is not entirely clear to what depth BRUE shall construct its tree. In the sailing domain, we chose H to be 4 × n, where n is the grid-size of the problem instance, as it is unlikely that the optimal path between any two locations on the grid will be larger than a complete encircling of the considered area. We note, however, that the recommendation-oriented samples ρ̄ always end at a terminal state, similar to the rollouts issued by UCT and GCT.\nSnapshots of the results for different grid sizes are shown in Figure 3. The comparison was made with two MCT-based algorithms: the UCT algorithm, and a recent modification of UCT, GCT, obtained from the former by replacing the UCB1 policy at the root node with the -greedy policy [19]. The motivation behind the design of GCT was to improve the empirical simple regret of UCT, and the results for GCT reported by [19] (and confirmed by our experiments here) are very impressive. All three algorithms were implemented within a single software infrastructure, with the parameters for UCT and GCT being set as in the previously reported evaluations on the sailing domain. Each algorithm was run on 1000 randomly chosen initial states s0, and the performance of the algorithm was assessed in terms of the average error Q(s0, a) − V (s0), that is, the difference between the true values of\nthe action a chosen by the algorithm and this of the optimal action π∗(s0). Consistently with the results reported by Tolpin and Shimony ([19]), GCT outperformed UCT by a very large margin, with the latter exhibiting very poor performance improvement over time even on the smallest, 5× 5, grids. In turn, BRUE substantially outperformed GCT, with the improvement being consistent except for relatively short planning deadlines.\nThe above allows us to conclude that BRUE is not only attractive in formal terms of performance guarantees, but can also very effective in practice of online planning. Under the same parameter setting of UCT and GCT, we have also evaluated the three algorithms in a domain of random game trees that aims at a simple modeling of two-person zero-sum games such as Go, Amazons and Globber. In such games, the winner is decided by a global evaluation of the end board, with the evaluation employing this or another feature counting procedure; the rewards thus are associated only with the terminal states. The rewards are calculated by first assigning values to moves, and then summing up these values along the paths to the terminal states. Note that the move values are used for the tree construction only and are not made available to the players. The values are chosen uniformly from [0, 127] for the moves of MAX, and from [−127, 0] for the moves of MIN. The players act so to (depending on the role) maximize/minimize their individual payoff: the aim of MAX is to reach terminal s with as high R(s) as possible, and the objective of MIN is similar, mutatis mutandis. This simple game tree model is similar in spirit to many other game tree models used in previous work [12, 16], except for that the success/failure of the players in measured not on a ternary scale of win/lose/draw, but via the actual payoffs received by the players. We have ran some preliminary experiments with two different settings of the branching factor (B) and tree depths (D). As in the sailing domain, we compared the convergence rate obtained by BRUE, UCT and GCT. Figure 4 plots the average error rate for two configurations, B = 6, D = 6 and B = 2, D = 16, with the average in each setting obtained over 20 trees and 100 runs for each tree. The results here appear encouraging as well, with BRUE taking over the other two algorithms faster on the deeper trees."
    }, {
      "heading" : "5 SUMMARY AND DISCUSSION",
      "text" : "We have introduced BRUE, a simple Monte-Carlo algorithm for online planning in MDPs that guarantees exponential-over-time reduction of the performance measures of interest, namely the simple regret and the probability\nof erroneous action choice. This improves over previous algorithms such as UCT that guarantee only polynomial-over-time reduction of these measures. The algorithm has been formalized for finite horizon MDPs, and it was analyzed as such. However, our empirical evaluation shows that it performs well also on goal-driven MDPs and two-person games.\nOur work leaves a few questions for future work. Considering γ-discounted MDPs with infinite horizon, a straightforward way to employ BRUE in that setting is to fix a horizon H, use the algorithm as it is, and derive guarantees on the aforementioned measures of interest by simply accounting for the additive gap of γHRmax/(1−γ) between the state/action values under horizon H and these under infinite horizon. However, this is not necessarily the best way to plan online for infinite-horizon MDPs, and thus this setting requires further introspection. Second, it is not unlikely that the state-space independent factors ch, c ′ h, and c ′′ h in the guarantees of BRUE can be substantially improved by employing more sophisticated combinations of exploration and recommendation probes, and currently we examine this issue. Finally, the core tree sampling scheme employed by BRUE differ from the more standard scheme employed in previous work. While this difference plays a critical role in establishing the formal guarantees of BRUE, it is still unclear whether that difference is necessary for establishing exponential-over-time reduction of the performance measures."
    }, {
      "heading" : "Acknowledgements",
      "text" : "Work supported by and carried out at the Technion-Microsoft Electronic Commerce Research Center."
    } ],
    "references" : [ {
      "title" : "Finite-time analysis of the multiarmed bandit problem",
      "author" : [ "P. Auer", "N. Cesa-Bianchi", "P. Fischer" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2002
    }, {
      "title" : "UCT for tactical assault planning in real-time strategy games",
      "author" : [ "R. Balla", "A. Fern" ],
      "venue" : "In IJCAI,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2009
    }, {
      "title" : "Lower bounding Klondike Solitaire with Monte-Carlo planning",
      "author" : [ "R. Bjarnason", "A. Fern", "P. Tadepalli" ],
      "venue" : "In ICAPS,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2009
    }, {
      "title" : "Pure exploration in finitely-armed and continuous-armed bandits",
      "author" : [ "S. Bubeck", "R. Munos", "G. Stoltz" ],
      "venue" : "Theor. Comput. Sci.,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2011
    }, {
      "title" : "Nested Monte-Carlo search",
      "author" : [ "T. Cazenave" ],
      "venue" : "In IJCAI, pages 456–461,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2009
    }, {
      "title" : "Bandit algorithms for tree search",
      "author" : [ "P-A. Coquelin", "R. Munos" ],
      "venue" : "In Proceedings of the 23rd Conference on Uncertainty in Artificial Intelligence (UAI),",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2007
    }, {
      "title" : "High-quality policies for the Canadian Traveler’s problem",
      "author" : [ "P. Eyerich", "T. Keller", "M. Helmert" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2010
    }, {
      "title" : "Monte-Carlo tree search and rapid action value estimation in computer Go",
      "author" : [ "S. Gelly", "D. Silver" ],
      "venue" : "AIJ, 175(11):1856–1875,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2011
    }, {
      "title" : "Metareasoning for Monte Carlo tree search",
      "author" : [ "N. Hay", "S.J. Russell" ],
      "venue" : "Technical Report UCB/EECS-2011-119, EECS Department,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2011
    }, {
      "title" : "A sparse sampling algorithm for near-optimal planning in large Markov decision processes",
      "author" : [ "M.J. Kearns", "Y. Mansour", "A.Y. Ng" ],
      "venue" : "In IJCAI,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1999
    }, {
      "title" : "Probabilistic planning based on UCT",
      "author" : [ "T. Keller", "P. Eyerich" ],
      "venue" : "In ICAPS,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2012
    }, {
      "title" : "Bandit based Monte-Carlo planning",
      "author" : [ "L. Kocsis", "C. Szepesvári" ],
      "venue" : "In ECML, pages 282–293,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2006
    }, {
      "title" : "On-line search for solving Markov decision processes via heuristic sampling",
      "author" : [ "L. Péret", "F. Garcia" ],
      "venue" : "In ECAI,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2004
    }, {
      "title" : "Some aspects of the sequential design of experiments",
      "author" : [ "H. Robbins" ],
      "venue" : "Bull. Amer. Math. Soc.,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1952
    }, {
      "title" : "Nested rollout policy adaptation for Monte Carlo tree search",
      "author" : [ "C.D. Rosin" ],
      "venue" : "In IJCAI,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2011
    }, {
      "title" : "An analysis of forward pruning",
      "author" : [ "S.J. Smith", "D.S. Nau" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1994
    }, {
      "title" : "An analysis of UCT in multi-player games",
      "author" : [ "N. Sturtevant" ],
      "venue" : "In CCG, page 3749,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2008
    }, {
      "title" : "Reinforcement Learning: An Introduction",
      "author" : [ "R.S. Sutton", "A.G. Barto" ],
      "venue" : null,
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 1998
    }, {
      "title" : "Doing better than UCT: Rational Monte Carlo sampling in trees",
      "author" : [ "D. Tolpin", "S.E. Shimony" ],
      "venue" : "CoRR, arXiv:1108.3711v1 [cs.AI],",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "An MDP 〈S,A, Tr,R〉 is defined by a set of states S, a set of actions A, a stochastic transition function Tr : S × A × S → [0, 1], and a reward function",
      "startOffset" : 122,
      "endOffset" : 128
    }, {
      "referenceID" : 9,
      "context" : "The sparse sampling algorithm by Kearns, Mansour, and Ng [10] offered near-optimal action selection in time exponential in H but independent of the state space size.",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 17,
      "context" : "Several other works, closer to our focus here, introduced interruptable algorithms that are designed to provide convergence to the best action if enough time is given, and small performance loss if the algorithm is stopped prematurely [18, 13, 12, 6, 5, 15, 19].",
      "startOffset" : 235,
      "endOffset" : 261
    }, {
      "referenceID" : 12,
      "context" : "Several other works, closer to our focus here, introduced interruptable algorithms that are designed to provide convergence to the best action if enough time is given, and small performance loss if the algorithm is stopped prematurely [18, 13, 12, 6, 5, 15, 19].",
      "startOffset" : 235,
      "endOffset" : 261
    }, {
      "referenceID" : 11,
      "context" : "Several other works, closer to our focus here, introduced interruptable algorithms that are designed to provide convergence to the best action if enough time is given, and small performance loss if the algorithm is stopped prematurely [18, 13, 12, 6, 5, 15, 19].",
      "startOffset" : 235,
      "endOffset" : 261
    }, {
      "referenceID" : 5,
      "context" : "Several other works, closer to our focus here, introduced interruptable algorithms that are designed to provide convergence to the best action if enough time is given, and small performance loss if the algorithm is stopped prematurely [18, 13, 12, 6, 5, 15, 19].",
      "startOffset" : 235,
      "endOffset" : 261
    }, {
      "referenceID" : 4,
      "context" : "Several other works, closer to our focus here, introduced interruptable algorithms that are designed to provide convergence to the best action if enough time is given, and small performance loss if the algorithm is stopped prematurely [18, 13, 12, 6, 5, 15, 19].",
      "startOffset" : 235,
      "endOffset" : 261
    }, {
      "referenceID" : 14,
      "context" : "Several other works, closer to our focus here, introduced interruptable algorithms that are designed to provide convergence to the best action if enough time is given, and small performance loss if the algorithm is stopped prematurely [18, 13, 12, 6, 5, 15, 19].",
      "startOffset" : 235,
      "endOffset" : 261
    }, {
      "referenceID" : 18,
      "context" : "Several other works, closer to our focus here, introduced interruptable algorithms that are designed to provide convergence to the best action if enough time is given, and small performance loss if the algorithm is stopped prematurely [18, 13, 12, 6, 5, 15, 19].",
      "startOffset" : 235,
      "endOffset" : 261
    }, {
      "referenceID" : 11,
      "context" : "Numerous concrete instances of MCT have been proposed, with UCT [12]",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 7,
      "context" : "probably being the most popular such algorithm these days [8, 17, 3, 2, 7, 11].",
      "startOffset" : 58,
      "endOffset" : 78
    }, {
      "referenceID" : 16,
      "context" : "probably being the most popular such algorithm these days [8, 17, 3, 2, 7, 11].",
      "startOffset" : 58,
      "endOffset" : 78
    }, {
      "referenceID" : 2,
      "context" : "probably being the most popular such algorithm these days [8, 17, 3, 2, 7, 11].",
      "startOffset" : 58,
      "endOffset" : 78
    }, {
      "referenceID" : 1,
      "context" : "probably being the most popular such algorithm these days [8, 17, 3, 2, 7, 11].",
      "startOffset" : 58,
      "endOffset" : 78
    }, {
      "referenceID" : 6,
      "context" : "probably being the most popular such algorithm these days [8, 17, 3, 2, 7, 11].",
      "startOffset" : 58,
      "endOffset" : 78
    }, {
      "referenceID" : 10,
      "context" : "probably being the most popular such algorithm these days [8, 17, 3, 2, 7, 11].",
      "startOffset" : 58,
      "endOffset" : 78
    }, {
      "referenceID" : 0,
      "context" : "Given si, the next-on-the-probe action ai+1 is determined by the deterministic UCB1 policy [1], originally proposed for optimal cumulative regret minimization in stochastic multiarmed bandit (MAB) problems [14]: If n(si, a) > 0 for all a ∈ A(si), then",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 13,
      "context" : "Given si, the next-on-the-probe action ai+1 is determined by the deterministic UCB1 policy [1], originally proposed for optimal cumulative regret minimization in stochastic multiarmed bandit (MAB) problems [14]: If n(si, a) > 0 for all a ∈ A(si), then",
      "startOffset" : 206,
      "endOffset" : 210
    }, {
      "referenceID" : 11,
      "context" : "In that respect, according to Theorem 6 of Kocsis and Szepesvári [12], UCT achieves the optimal logarithmic cumulative regret.",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 11,
      "context" : "In contrast, the same Theorem 6 of Kocsis and Szepesvári [12] claims only a polynomial-rate reduction of the probability of choosing a non-optimal action, and the recent results by Bubeck, Munos, and Stoltz [4] on simple regret minimization in MABs with stochastic rewards imply that UCT achieves only polynomial reduction of the simple regret over time.",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 3,
      "context" : "In contrast, the same Theorem 6 of Kocsis and Szepesvári [12] claims only a polynomial-rate reduction of the probability of choosing a non-optimal action, and the recent results by Bubeck, Munos, and Stoltz [4] on simple regret minimization in MABs with stochastic rewards imply that UCT achieves only polynomial reduction of the simple regret over time.",
      "startOffset" : 207,
      "endOffset" : 210
    }, {
      "referenceID" : 18,
      "context" : "Some attempts have recently been made to adapt UCT, and MCT-based planning in general, to optimizing decisions in online MDP planning [19, 9].",
      "startOffset" : 134,
      "endOffset" : 141
    }, {
      "referenceID" : 8,
      "context" : "Some attempts have recently been made to adapt UCT, and MCT-based planning in general, to optimizing decisions in online MDP planning [19, 9].",
      "startOffset" : 134,
      "endOffset" : 141
    }, {
      "referenceID" : 3,
      "context" : "[4] was probably the first systematic attempt to analyze pure exploration in MABs, showing that the minimal simple regret in MAB can increase as the bound on the cumulative regret is getting smaller.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "[4] for MABs, the BRUE setting of MCTer is as follows: • probe-S: The probes ρ = 〈s0, a1, .",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 0,
      "context" : "Theorem 1 Let BRUE be called on a state s0 of an MDP 〈S,A, Tr,R〉 with rewards in [0, 1] and finite horizon H.",
      "startOffset" : 81,
      "endOffset" : 87
    }, {
      "referenceID" : 12,
      "context" : "We have evaluated BRUE empirically on the MDP sailing domain [13] that was used in previous works for evaluating MC planning algorithms [13, 12, 19], as well as on random game trees used in the original empirical evaluation of UCT [12].",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 12,
      "context" : "We have evaluated BRUE empirically on the MDP sailing domain [13] that was used in previous works for evaluating MC planning algorithms [13, 12, 19], as well as on random game trees used in the original empirical evaluation of UCT [12].",
      "startOffset" : 136,
      "endOffset" : 148
    }, {
      "referenceID" : 11,
      "context" : "We have evaluated BRUE empirically on the MDP sailing domain [13] that was used in previous works for evaluating MC planning algorithms [13, 12, 19], as well as on random game trees used in the original empirical evaluation of UCT [12].",
      "startOffset" : 136,
      "endOffset" : 148
    }, {
      "referenceID" : 18,
      "context" : "We have evaluated BRUE empirically on the MDP sailing domain [13] that was used in previous works for evaluating MC planning algorithms [13, 12, 19], as well as on random game trees used in the original empirical evaluation of UCT [12].",
      "startOffset" : 136,
      "endOffset" : 148
    }, {
      "referenceID" : 11,
      "context" : "We have evaluated BRUE empirically on the MDP sailing domain [13] that was used in previous works for evaluating MC planning algorithms [13, 12, 19], as well as on random game trees used in the original empirical evaluation of UCT [12].",
      "startOffset" : 231,
      "endOffset" : 235
    }, {
      "referenceID" : 18,
      "context" : "The comparison was made with two MCT-based algorithms: the UCT algorithm, and a recent modification of UCT, GCT, obtained from the former by replacing the UCB1 policy at the root node with the -greedy policy [19].",
      "startOffset" : 208,
      "endOffset" : 212
    }, {
      "referenceID" : 18,
      "context" : "The motivation behind the design of GCT was to improve the empirical simple regret of UCT, and the results for GCT reported by [19] (and confirmed by our experiments here) are very impressive.",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 18,
      "context" : "Consistently with the results reported by Tolpin and Shimony ([19]), GCT outperformed UCT by a very large margin, with the latter exhibiting very poor performance improvement over time even on the smallest, 5× 5, grids.",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 11,
      "context" : "This simple game tree model is similar in spirit to many other game tree models used in previous work [12, 16], except for that the success/failure of the players in measured not on a ternary scale of win/lose/draw, but via the actual payoffs received by the players.",
      "startOffset" : 102,
      "endOffset" : 110
    }, {
      "referenceID" : 15,
      "context" : "This simple game tree model is similar in spirit to many other game tree models used in previous work [12, 16], except for that the success/failure of the players in measured not on a ternary scale of win/lose/draw, but via the actual payoffs received by the players.",
      "startOffset" : 102,
      "endOffset" : 110
    } ],
    "year" : 2017,
    "abstractText" : "We consider online planning in Markov decision processes. An algorithm for this problem should explore the set of possible policies from the current state, and, when interrupted, recommend an action to follow based on the outcome of the exploration. The performance of such an algorithm is assessed in terms of its simple regret, that is the loss in performance resulting from choosing the recommended action instead of an optimal one, and/or in terms of probability that the recommended action is not an optimal one. The best guarantees provided by the state-of-the-art algorithms for reduction of these measures over time are only polynomial. We introduce a new algorithm, BRUE, that achieves over time exponential reduction of these two measures. The algorithm is based on a simple yet non-standard state-space sampling scheme in which different samples are dedicated to different objectives. Our preliminary empirical evaluation shows that BRUE not only provides superior performance guarantees, but is also very effective in practice and favorably compares to state-of-the-art.",
    "creator" : "LaTeX with hyperref package"
  }
}