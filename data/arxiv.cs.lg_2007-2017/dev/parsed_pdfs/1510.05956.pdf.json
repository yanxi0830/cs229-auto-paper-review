{
  "name" : "1510.05956.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Optimal Cluster Recovery in the Labeled Stochastic Block Model",
    "authors" : [ "Se-Young Yun", "Alexandre Proutiere" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n51 0.\n05 95\n6v 6\n[ m\nat h.\nPR ]\nClustering under the SBM and their extensions has attracted much attention recently. Most existing work aimed at characterizing the set of parameters such that it is possible to infer clusters either positively correlated with the true clusters, or with a vanishing proportion of misclassified items, or exactly matching the true clusters. We find the set of parameters such that there exists a clustering algorithm with at most s misclassified items in average under the general LSBM and for any s = o(n), which solves one open problem raised in [2]. We further develop an algorithm, based on simple spectral methods, that achieves this fundamental performance limit within O(npolylog(n)) computations and without the a-priori knowledge of the model parameters."
    }, {
      "heading" : "1 Introduction",
      "text" : "Community detection consists in extracting (a few) groups of similar items from a large global population, and has applications in a wide spectrum of disciplines including social sciences, biology, computer science, and statistical physics. The communities or clusters of items are inferred from the observed pair-wise similarities between items, which, most often, are represented by a graph whose vertices are items and edges are pairs of items known to share similar features.\nThe stochastic block model (SBM), introduced three decades ago in [13], constitutes a natural performance benchmark for community detection, and has been, since then, widely studied. In the SBM, the set of items V = {1, . . . , n} are partitioned into K non-overlapping clusters V1, . . . ,VK , that have to be recovered from an observed realization of a random graph. In the latter, an edge between two items belonging to clusters Vi and Vj , respectively, is present with probability p(i, j), independently of other edges. The analyses presented in this paper apply to the SBM, but also to the labeled stochastic block model (LSBM) [12], a more general model to describe the similarities of items. There, the observation of the similarity between two items comes in the form of a label taken from a finite set L = {0, 1, . . . , L}, and label ℓ is observed between two items in clusters Vi and Vj , respectively, with probability p(i, j, ℓ), independently of other labels. The standard SBM can be seen as a particular instance of its labeled\ncounterpart with two possible labels 0 and 1, and where the edges present (resp. absent) in the SBM correspond to item pairs with label 1 (resp. 0). The problem of cluster recovery under the LSBM consists in inferring the hidden partition V1, . . . ,VK from the observation of the random labels on each pair of items.\nOver the last few years, we have seen remarkable progresses for the problem of cluster recovery under the SBM (see [8] for an exhaustive literature review), highlighting its scientific relevance and richness. Most recent work on the SBM aimed at characterizing the set of parameters (i.e., the probabilities p(i, j) that there exists an edge between nodes in clusters i and j for 1 ≤ i, j ≤ K) such that some qualitative recovery objectives can or cannot be met. For sparse scenarios where the average degree of items in the graph is O(1), parameters under which it is possible to extract clusters positively correlated with the true clusters have been identified [5, 20, 18]. When the average degree of the graph is ω(1), one may predict the set of parameters allowing a cluster recovery with a vanishing (as n grows large) proportion of misclassified items [25, 19], but one may also characterize parameters for which an asymptotically exact cluster reconstruction can be achieved [1, 24, 9, 19, 2, 3, 14].\nIn this paper, we address the finer and more challenging question of determining, under the general LSBM, the minimal number of misclassified items given the parameters of the model. Specifically, for any given s = o(n), our goal is to identify the set of parameters such that it is possible to devise a clustering algorithm with at most s misclassified items. Of course, if we achieve this goal, we shall recover all the aforementioned results on the SBM.\nMain results. We focus on the labeled SBM as described above, and where each item is assigned to cluster Vk with probability αk > 0, independently of other items. We assume w.l.o.g. that α1 ≤ α2 ≤ · · · ≤ αK . We further assume that α = (α1, . . . , αK) does not depend on the total population of items n. Conditionally on the assignment of items to clusters, the pair or edge (v,w) ∈ V2 has label ℓ ∈ L = {0, 1, . . . , L} with probability p(i, j, ℓ), when v ∈ Vi and w ∈ Vj . W.l.o.g., 0 is the most frequent label, i.e., 0 = argmaxℓ ∑K i=1 ∑K j=1 αiαjp(i, j, ℓ). Throughout the paper, we typically assume that p̄ = o(1) and p̄n = ω(1) where p̄ = maxi,j,ℓ≥1 p(i, j, ℓ) denotes the maximum probability of observing a label different than 0. We shall explicitly state whether these assumption are made when deriving our results. In the standard SBM, the second assumption means that the average degree of the corresponding random graph is ω(1). This also means that we can hope to recover clusters with a vanishing proportion of misclassified items. We finally make the following assumption: there exist positive constants η and ε such that for every i, j, k ∈ [K] = {1, . . . ,K},\n(A1) ∀ℓ ∈ L, p(i, j, ℓ) p(i, k, ℓ)\n≤ η and (A2) ∑K k=1 ∑L ℓ=1(p(i, k, ℓ) − p(j, k, ℓ))2\np̄2 ≥ ε.\n(A2) imposes a certain separation between the clusters. For example, in the standard SBM with two communities, p(1, 1, 1) = p(2, 2, 1) = ξ, and p(1, 2, 1) = ζ , (A2) is equivalent to 2(ξ − ζ)2/ξ2 ≥ ǫ. In summary, the LSBM is parametrized by α and p = (p(i, j, ℓ))1≤i,j≤K,0≤ℓ≤L, and recall that α does not depend on n, whereas p does.\nFor the above LSBM, we derive, for any arbitrary s = o(n), a necessary condition under which there exists an algorithm inferring clusters with s misclassified items. We further establish that under this condition, a simple extension of spectral algorithms extract communities with less than s misclassified items. To formalize these results, we introduce the divergence of (α, p). We denote by p(i) the K × (L + 1) matrix whose element on the j-th row and the (ℓ + 1)-th column is p(i, j, ℓ) and denote by p(i, j) ∈ [0, 1]L+1 the vector describing the probability distribution of the label of a pair of items in Vi and Vj , respectively. Let PK×(L+1) denote the set of K × (L + 1) matrices such that each row represents a probability distribution. The divergence D(α, p) of (α, p) is defined as follows: D(α, p) =\nmini,j:i 6=j DL+(α, p(i), p(j)) with\nDL+(α, p(i), p(j)) = min y∈PK×(L+1) max\n{\nK ∑\nk=1\nαkKL(y(k), p(i, k)),\nK ∑\nk=1\nαkKL(y(k), p(j, k))\n}\nwhere KL denotes the Kullback-Leibler divergence between two label distributions, i.e., KL(y(k), p(i, k)) =\n∑L ℓ=0 y(k, ℓ) log y(k,ℓ) p(i,k,ℓ) . Finally, we denote by ε π(n) the number of misclassified items under the clustering algorithm π, and by E[επ(n)] its expectation (with respect to the randomness in the LSBM and in the algorithm).\nWe first derive a tight lower bound on the average number of misclassified items when the latter is o(n). Note that such a bound was unknown even for the SBM [2].\nTheorem 1 Assume that (A1) and (A2) hold, and that p̄n = ω(1). Let s = o(n). If there exists a clustering algorithm π misclassifying in average less than s items asymptotically, i.e., lim supn→∞ E[επ(n)] s ≤ 1, then the parameters (α, p) of the LSBM satisfy:\nlim inf n→∞\nnD(α, p) log(n/s) ≥ 1. (1)\nTo state the corresponding positive result (i.e., the existence of an algorithm misclassifying only s items), we make an additional assumption to avoid extremely sparse labels: (A3) there exists a constant κ > 0 such that np(j, i, ℓ) ≥ (np̄)κ for all i, j and ℓ ≥ 1.\nTheorem 2 Assume that (A1), (A2), and (A3) hold, and that p̄ = o(1), p̄n = ω(1). Let s = o(n). If the parameters (α, p) of the LSBM satisfy (1), then the Spectral Partition (SP ) algorithm presented in Section 4 misclassifies at most s items with high probability, i.e., limn→∞ P[εSP (n) ≤ s] = 1.\nThese theorems indicate that under the LSBM with parameters satisfying (A1) and (A2), the number of misclassified items scales at least as n exp(−nD(α, p)(1 + o(1)) under any clustering algorithm, irrespective of its complexity. They further establish that the Spectral Partition algorithm reaches this fundamental performance limit under the additional condition (A3). We note that the SP algorithm runs in polynomial time, i.e., it requires O(n2p̄ log(n)) floating-point operations.\nWe further establish a necessary and sufficient condition on the parameters of the LSBM for the existence of a clustering algorithm recovering the clusters exactly with high probability. Deriving such a condition was also open [2].\nTheorem 3 Assume that (A1) and (A2) hold. If there exists a clustering algorithm that does not misclassify any item with high probability, then the parameters (α, p) of the LSBM satisfy: lim infn→∞ nD(α,p) log(n) ≥ 1. If this condition holds, then under (A3), the SP algorithm recovers the clusters exactly with high probability.\nThe paper is organized as follows. Section 2 presents the related work and example of application of our results. In Section 3, we sketch the proof of Theorem 1, which leverages change-of-measure and coupling arguments. We present in Section 4 the Spectral Partition algorithm, and analyze its performance (we outline the proof of Theorem 2). All results are proved in details in the supplementary material."
    }, {
      "heading" : "2 Related Work and Applications",
      "text" : ""
    }, {
      "heading" : "2.1 Related work",
      "text" : "Cluster recovery in the SBM has attracted a lot of attention recently. We summarize below existing results, and compare them to ours. Results are categorized depending on the targeted level of performance. First, we consider the notion of detectability, the lowest level of performance requiring that the extracted clusters are just positively correlated with the true clusters. Second, we look at asymptotically accurate recovery, stating that the proportion of misclassified items vanishes as n grows large. Third, we present existing results regarding exact cluster recovery, which means that no item is misclassified. Finally, we report recent work whose objective, like ours, is to characterize the optimal cluster recovery rate.\nDetectability. Necessary and sufficient conditions for detectability have been studied for the binary symmetric SBM (i.e., L = 1, K = 2, α1 = α2, p(1, 1, 1) = p(2, 2, 1) = ξ, and p(1, 2, 1) = p(2, 1, 1) = ζ). In the sparse regime where ξ, ζ = o(1), and for the binary symmetric SBM, the main focus has been on identifying the phase transition threshold (a condition on ξ and ζ) for detectability: It was conjectured in [5] that if n(ξ − ζ) < √\n2n(ξ + ζ) (i.e., under the threshold), no algorithm can perform better than a simple random assignment of items to clusters, and above the threshold, clusters can partially be recovered. The conjecture was recently proved in [20] (necessary condition), and [18] (sufficient condition). The problem of detectability has been also recently studied in [27] for the asymmetric SBM with more than two clusters of possibly different sizes. Interestingly, it is shown that in most cases, the phase transition for detectability disappears.\nThe present paper is not concerned with conditions for detectability. Indeed detectability means that only a strictly positive proportion of items can be correctly classified, whereas here, we impose that the proportion of misclassified items vanishes as n grows large.\nAsymptotically accurate recovery. A necessary and sufficient condition for asymptotically accurate recovery in the SBM (with any number of clusters of different but linearly increasing sizes) has been derived in [25] and [19]. Using our notion of divergence specialized to the SBM, this condition is nD(α, p) = ω(1). Our results are more precise since the minimal achievable number of misclassified items is characterized, and apply to a broader setting since they are valid for the generic LSBM.\nAsymptotically exact recovery. Conditions for exact cluster recovery in the SBM have been also recently studied. [1, 19, 9] provide a necessary and sufficient condition for asymptotically exact recovery in the binary symmetric SBM. For example, it is shown that when ξ = a log(n)n and ζ = b log(n) n for a > b,\nclusters can be recovered exactly if and only if a+b2 − √ ab ≥ 1. In [2, 3], the authors consider a more general SBM corresponding to our LSBM with L = 1. They define CH-divergence as:\nD+(α, p(i), p(j)) = n\nlog(n) max λ∈[0,1]\nK ∑\nk=1\nαk\n( (1− λ)p(i, k, 1) + λp(j, k, 1) − p(i, k, 1)1−λp(j, k, 1)λ ) ,\nand show that mini 6=j D+(α, p(i), p(j)) > 1 is a necessary and sufficient condition for asymptotically exact reconstruction. The following claim, proven in the supplementary material, relates D+ to DL+.\nClaim 4 When p̄ = o(1), we have for all i, j:\nDL+(α, p(i), p(j)) n→∞∼ max\nλ∈[0,1]\nL ∑\nℓ=1\nK ∑\nk=1\nαk\n( (1− λ)p(i, k, ℓ) + λp(j, k, ℓ) − p(i, k, ℓ)1−λp(j, k, ℓ)λ ) .\nThus, the results in [2, 3] are obtained by applying Theorem 3 and Claim 4. In [14], the authors consider a symmetric labeled SBM where communities are balanced (i.e., αk =\n1 K for all k) and where label probabilities are simply defined as p(i, i, ℓ) = p(ℓ) for all i and p(i, j, ℓ) = q(ℓ) for all i 6= j. It is shown that nIlog(n) > 1 is necessary and sufficient for asymptotically exact recovery, where I = − 2K log ( ∑L ℓ=0 √ p(ℓ)q(ℓ) ) . We can relate I to D(α, p):\nClaim 5 In the LSBM with K clusters, if p̄ = o(1), and for all i, j, ℓ such that i 6= j, αi = 1K , p(i, i, ℓ) = p(ℓ), and p(j, k, ℓ) = q(ℓ), we have: D(α, p)\nn→∞∼ − 2K log ( ∑L ℓ=0 √ p(ℓ)q(ℓ) ) .\nAgain from this claim, the results derived in [14] are obtained by applying Theorem 3 and Claim 5.\nOptimal recovery rate. In [6, 21], the authors consider the binary SBM in the sparse regime where the average degree of items in the graph is O(1), and identify the minimal number of misclassified items for very specific intra- and inter-cluster edge probabilities ξ and ζ . Again the sparse regime is out of the scope of the present paper. [26, 8] are concerned with the general SBM corresponding to our LSBM with L = 1, and with regimes where asympotically accurate recovery is possible. The authors first characterize the optimal recovery rate in a minimax framework. More precisely, they consider a (potentially large) set of possible parameters (α, p), and provide a lower bound on the expected number of misclassified items for the worst parameters in this set. Our lower bound (Theorem 1) is more precise as it is model-specific, i.e., we provide the minimal expected number of misclassified items for a given parameter (α, p) (and for a more general class of models). Then the authors propose a clustering algorithm, with time complexity O(n3 log(n)), and achieving their minimax recovery rate. In comparison, our algorithm yields an optimal recovery rate O(n2p̄ log(n)) for any given parameter (α, p), exhibits a lower running time, and applies to the generic LSBM."
    }, {
      "heading" : "2.2 Applications",
      "text" : "We provide here a few examples of application of our results, illustrating their versatility. In all examples, f(n) is a function such that f(n) = ω(1), and a, b are fixed real numbers such that a > b.\nThe binary SBM. Consider the binary SBM where the average item degree is Θ(f(n)), and represented by a LSBM with parameters L = 1, K = 2, α = (α1, 1 − α1), p(1, 1, 1) = p(2, 2, 1) = af(n)n , and p(1, 2, 1) = p(2, 1, 1) = bf(n)n . From Theorems 1 and 2, the optimal number of misclassified vertices scales as n exp(−g(α1, a, b)f(n)(1 + o(1))) when α1 ≤ 1/2 (w.l.o.g.) and where g(α1, a, b) := max\nλ∈[0,1] (1− α1 − λ+ 2α1λ)a+ (α1 + λ− 2αλ)b− α1aλb(1−λ) − (1− α1)a(1−λ)bλ.\nIt can be easily checked that g(α1, a, b) ≥ g(1/2, a, b) = 12( √ a− √ b)2 (letting λ = 12 ). The worst case is hence obtained when the two clusters are of equal sizes. When f(n) = log(n), we also note that the condition for asymptotically exact recovery is g(α1, a, b) ≥ 1. Recovering a single hidden community. As in [10], consider a random graph model with a hidden community consisting of αn vertices, edges between vertices belonging the hidden community are present with probability af(n)n , and edges between other pairs are present with probability bf(n) n . This is modeled by a LSBM with parameters K = 2, L = 1, α1 = α, p(1, 1, 1) = af(n) n , and p(1, 2, 1) = p(2, 1, 1) = p(2, 2, 1) = bf(n)n . The minimal number of misclassified items when searching for the hidden community scales as n exp(−h(α, a, b)f(n)(1 + o(1))) where\nh(α, a, b) := α\n(\na− (a− b)1 + log(a− b)− log(a log(a/b)) log(a/b)\n)\n.\nWhen f(n) = log(n), the condition for asymptotically exact recovery of the hidden community is h(α, a, b) ≥ 1. Optimal sampling for community detection under the SBM. Consider a dense binary symmetric SBM with intra- and inter-cluster edge probabilities a and b. In practice, to recover the clusters, one might not be able to observe the entire random graph, but sample its vertex (here item) pairs as considered in [25]. Assume for instance that any pair of vertices is sampled with probability δf(n)n for some fixed δ > 0, independently of other pairs. We can model such scenario using a LSBM with three labels, namely ×, 0 and 1, corresponding to the absence of observation (the vertex pair is not sampled), the observation of the absence of an edge and of the presence of an edge, respectively, and with parameters for all i, j ∈ {1, 2}, p(i, j,×) = 1 − δf(n)n , p(1, 1, 1) = p(2, 2, 1) = a δf(n) n , and p(1, 2, 1) = p(2, 1, 1) = b δf(n)n . The minimal number of misclassified vertices scales as n exp(−l(δ, a, b)f(n)(1 + o(1))) where l := δ(1 − √ ab − √\n(1− a)(1 − b)). When f(n) = log(n), the condition for asymptotically exact recovery is l(α, a+, a−, b+, b−) ≥ 1. Signed networks. Signed networks [16, 23] are used in social sciences to model positive and negative interactions between individuals. These networks can be represented by a LSBM with three possible labels, namely 0, + and -, corresponding to the absence of interaction, positive and negative interaction, respectively. Consider such LSBM with parameters: K = 2, α1 = α2, p(1, 1,+) = p(2, 2,+) = a+f(n) n , p(1, 1,−) = p(2, 2,−) = a−f(n)n , p(1, 2,+) = p(2, 1,+) = b+f(n)\nn , and p(1, 2,−) = p(2, 1,−) = b−f(n)\nn , for some fixed a+, a−, b+, b− such that a+ > b+ and a− < b−. The minimal number of misclassified individuals here scales as n exp(−m(α, a+, a−, b+, b−)f(n)(1 + o(1))) where\nm(α, a+, a−, b+, b−) := 1\n2\n( ( √ a+ − √ b+) 2 + ( √ a− − √ b−) 2 ) .\nWhen f(n) = log(n), the condition for asymptotically exact recovery is l(α, a+, a−, b+, b−) ≥ 1."
    }, {
      "heading" : "3 Fundamental Limits: Change of Measures through Coupling",
      "text" : "In this section, we explain the construction of the proof of Theorem 1. The latter relies on an appropriate change-of-measure argument, frequently used to identify upper performance bounds in online stochastic optimization problems [15]. In the following, we refer to Φ, defined by parameters (α, p), as the true stochastic model under which all the observed random labels are generated, and denote by PΦ = P (resp. EΦ[·] = E[·]) the corresponding probability measure (resp. expectation). In our change-of-measure argument, we construct a second stochastic model Ψ (whose corresponding probability measure and expectation are PΨ and EΨ[·], respectively). Using a change of measures from PΦ to PΨ, we relate the expected number of misclassified items EΦ[επ(n)] under any clustering algorithm π to the expected (w.r.t. PΨ) log-likelihood ratio Q of the observed labels under PΦ and PΨ. Specifically, we show that, roughly, log(n/EΦ[επ(n)]) must be smaller than EΨ[Q] for n large enough. Construction of ψ. Let (i⋆, j⋆) = argmini,j:i<j DL+(α, p(i), p(j)), and let v⋆ denote the smallest item index that belongs to cluster i⋆ or j⋆. If both Vi⋆ and Vj⋆ are empty, we define v⋆ = n. Let q ∈ PK×(L+1) such that: D(α, p) = ∑Kk=1 αkKL(q(k), p(i⋆, k)) = ∑K k=1 αkKL(q(k), p(j\n⋆, k)). The existence of such q is proved in Lemma 7 in the supplementary material. Now to define the stochastic model Ψ, we couple the generation of labels under Φ and Ψ as follows.\n1. We first generate the random clusters V1, . . . ,VK under Φ, and extract i⋆, j⋆, and v⋆. The clusters generated under Ψ are the same as those generated under Φ. For any v ∈ V , we denote by σ(v) the cluster of item v.\n2. For all pairs (v,w) such that v 6= v⋆ and w 6= v⋆, the labels generated under Ψ are the same as those generated under Φ, i.e., the label ℓ is observed on the edge (v,w) with probability p(σ(v), σ(w), ℓ).\n3. Under Ψ, for any v 6= v⋆, the observed label on the edge (v, v⋆) under Ψ is ℓ with probability q(σ(v), ℓ).\nLet xv,w denote the label observed for the pair (v,w). We introduce Q, the log-likelihood ratio of the observed labels under PΦ and PΨ as:\nQ = v⋆−1 ∑\nv=1\nlog q(σ(v), xv⋆ ,v)\np(σ(v⋆), σ(v), xv⋆ ,v) +\nn ∑\nv=v⋆+1\nlog q(σ(v), xv⋆ ,v)\np(σ(v⋆), σ(v), xv⋆ ,v) . (2)\nLet π be a clustering algorithm with output (V̂k)1≤k≤K , and let E = ⋃ 1≤k≤K V̂k \\ Vk be the set of misclassified items under π. Note that in general in our analysis, we always assume without loss of generality that |⋃1≤k≤K V̂k \\ Vk| ≤ | ⋃\n1≤k≤K V̂γ(k) \\ Vk| for any permutation γ, so that the set of misclassified items is indeed E . By definition, επ(n) = |E|. Since under Φ, items are interchangeable (remember that items are assigned to the various clusters in an i.i.d. manner), we have: nPΦ{v ∈ E} = EΦ[ε\nπ(n)] = E[επ(n)]. Next, we establish a relationship between E[επ(n)] and the distribution of Q under PΨ. For any\nfunction f(n), we can prove that: PΨ{Q ≤ f(n)} ≤ exp(f(n)) EΦ[ε π(n)]\n(αi⋆+αj⋆)n + αj⋆ αi⋆+αj⋆ . Using this\nresult with f(n) = log (n/EΦ[επ(n)]) − log(2/αi⋆), and Chebyshev’s inequality, we deduce that: log (n/EΦ[ε π(n)])− log(2/αi⋆) ≤ EΨ[Q] + √\n4 αi⋆ EΨ[(Q− EΨ[Q])2], and thus, a necessary condition for E[επ(n)] ≤ s is:\nlog (n/s)− log(2/αi⋆) ≤ EΨ[Q] + √ 4\nαi⋆ EΨ[(Q− EΨ[Q])2]. (3)\nAnalysis of Q. In view of (3), we can obtain a necessary condition for E[επ(n)] ≤ s if we evaluate EΨ[Q] and EΨ[(Q − EΨ[Q])2]. To evaluate EΨ[Q], we can first prove that v⋆ ≤ log(n)2 with high probability. From this, we can approximate EΨ[Q] by EΨ[ ∑n v=v⋆+1 log q(σ(v),xv⋆,v)\np(σ(v⋆),σ(v),xv⋆,v) ], which is\nitself well-approximated by nD(α, p). More formally, we can show that:\nEΨ[Q] ≤ ( n+ 2 log(η) log(n)2 ) D(α, p) + log η\nn3 . (4)\nSimilarly, we prove that EΨ[(Q − EΨ[Q])2] = O(np̄), which in view of Lemma 8 (refer to the supplementary material) and assumption (A2), implies that: EΨ[(Q− EΨ[Q])2] = o(nD(α, p)).\nWe complete the proof of Theorem 1 by putting the above arguments together: From (3), (4) and the above analysis of Q, when the expected number of misclassified items is less than s (i.e., E[επ(n)] ≤ s), we must have: lim infn→∞ nD(α,p) log(n/s) ≥ 1."
    }, {
      "heading" : "4 The Spectral Partition Algorithm and its Optimality",
      "text" : "In this section, we sketch the proof of Theorem 2. To this aim, we present the Spectral Partition (SP) algorithm and analyze its performance. The SP algorithm consists in two parts, and its detailed pseudocode is presented at the beginning of the supplementary document (see Algorithm 1).\nThe first part of the algorithm can be interpreted as an initialization for its second part, and consists in applying a spectral decomposition of a n × n random matrix A constructed from the observed labels. More precisely, A =\n∑L ℓ=1 wℓA ℓ, where Aℓ is the binary matrix identifying the item pairs with\nobserved label ℓ, i.e., for all v,w ∈ V , Aℓvw = 1 if and only if (v,w) has label ℓ. The weight wℓ for label ℓ ∈ {1, . . . , L} is generated uniformly at random in [0, 1], independently of other weights. From the spectral decomposition of A, we estimate the number of communities and provide asymptotically accurate estimates S1, . . . , SK of the hidden clusters asymptotically accurately, i.e., we show that when np̄ = ω(1), with high probability, K̂ = K and there exists a permutation γ of {1, . . . ,K} such that 1 n ∣ ∣∪Kk=1Vk \\ Sγ(k) ∣ ∣ = O ( log(np̄)2 np̄ ) . This first part of the SP algorithm is adapted from algorithms proposed for the standard SBM in [4, 25] to handle the additional labels in the model without the knowledge of the number K of clusters.\nThe second part is novel, and is critical to ensure the optimality of the SP algorithm. It consists in first constructing an estimate p̂ of the true parameters p of the model from the matrices (Aℓ)1≤ℓ≤L and the estimated clusters S1, . . . , SK provided in the first part of SP. We expect p to be well estimated since S1, . . . , SK are asymptotically accurate. Then our cluster estimates are iteratively improved. We run ⌊log(n)⌋ iterations. Let S(t)1 , . . . , S (t) K denote the clusters estimated after the t-th iteration, initialized with (S (0) 1 , . . . , S (0) K ) = (S1, . . . , SK). The improved clusters S (t+1) 1 , . . . , S (t+1) K are obtained by assigning each item v ∈ V to the cluster maximizing a log-likelihood formed from p̂, S(t)1 , . . . , S (t) K , and the observations (Aℓ)1≤ℓ≤L: v is assigned to S (t+1) k⋆ where k ⋆ = argmaxk{ ∑K i=1 ∑\nw∈S(t−1)i\n∑L ℓ=0 A ℓ vw log p̂(k, i, ℓ)}.\nPart 1: Spectral Decomposition. The spectral decomposition is described in Lines 1 to 4 in Algorithm 1. As usual in spectral methods, the matrix A is first trimmed (to remove lines and columns corresponding to items with too many observed labels – as they would perturb the spectral analysis). To this aim, we estimate the average number of labels per item, and use this estimate, denoted by p̃ in Algorithm 1, as a reference for the trimming process. Γ and AΓ denote the set of remaining items after trimming, and the corresponding trimmed matrix, respectively.\nIf the number of clusters K is known and if we do not account for time complexity, the two step algorithm in [4] can extract the clusters from AΓ: first the optimal rank-K approximation A(K) of AΓ is derived using the SVD; then, one applies the k-mean algorithm to the columns of A(K) to reconstruct the clusters. The number of misclassified items after this two step algorithm is obtained as follows. Let M ℓ = E[AℓΓ], and M = ∑L ℓ=1wℓM\nℓ (using the same weights as those defining A). Then, M is of rank K . If v and w are in the same cluster, Mv = Mw and if v and w do not belong to the same cluster, from (A2), we must have with high probability: ‖Mv −Mw‖2 = Ω(p̄ √ n). Thus, the k-mean algorithm misclassifies v only if ‖A(K)v −Mv‖2 = Ω(p̄ √ n). By leveraging elements of random graph and random matrix theories, we can establish that ∑\nv ‖A (k) v −Mv‖22 = ‖A(k)−M‖2F = O(np̄) with high probability.\nHence the algorithm misclassifies O(1/p̄) items with high probability. Here the number of clusters K is not given a-priori. In this scenario, Algorithm 2 estimates the rank of M using a singular value thresholding procedure. To reduce the complexity of the algorithm, the singular values and singular vectors are obtained using the iterative power method instead of a direct SVD. It is known from [11] that with Θ(log(n)) iterations, the iterative power method find singular values and the rank-K approximation very accurately. Hence, when np̄ = ω(1), we can easily estimate the rank of M by looking at the number of singular values above the threshold √ np̃ log(np̃), since we know from random matrix theory that the (K+1)-th singular value of AΓ is much less than √ np̃ log(np̃) with high probability. In the pseudo-code of Algorithm 2, the estimated rank of M is denoted by K̃ . The rank-K̃ approximation of AΓ obtained by the iterative power method is Â = Û V̂ = Û Û⊤AΓ. From the columns of Â, we can estimate the number of clusters and classify items. Almost every column\nof Â is located around the corresponding column of M within a distance 12\n√\nnp̃2\nlog(np̃) , since ∑ v ‖Âv − Mv‖22 = ‖Â − M‖2F = O(np̄ log(np̄)2) with high probability (we rigorously analyze this distance in\nthe supplementary material Section D.2). From this observation, the columns can be categorised into K groups. To find these groups, we randomly pick log(n) reference columns and for each reference column, search all columns within distance √ np̃2\nlog(np̃) . Then, with high probability, each cluster has at least one reference column and each reference column can find most of its cluster members. Finally, the K groups are identified using the reference columns. To this aim, we compute the distance of n log(n) column pairs Âv, Âw. Observe that ‖Âv− Âw‖2 = ‖V̂v− V̂w‖2 for any u, v ∈ Γ, since the columns of Û are orthonormal. Now V̂v is of dimension K̃, and hence we can identify the groups using O(nK̃ log(n)) operations.\nTheorem 6 Assume that (A1) and (A2) hold, and that np̄ = ω(1). After Step 4 (spectral decomposition) in the SP algorithm, with high probability, K̂ = K and there exists a permutation γ of {1, . . . ,K} such that: ∣\n∣∪Kk=1Vk \\ Sγ(k) ∣\n∣ = O ( log(np̄)2\np̄\n)\n.\nPart 2: Successive clusters improvements. Part 2 of the SP algorithm is described in Lines 5 and 6 in Algorithm 1. To analyze the performance of each improvement iteration, we introduce the set of items H as the largest subset of V such that for all v ∈ H: (H1) e(v,V) ≤ 10ηnp̄L; (H2) when v ∈ Vk, ∑K\ni=1 ∑L ℓ=0 e(v,Vi, ℓ) log p(k,i,ℓ) p(j,i,ℓ) ≥ np̄ log(np̄)4 for all j 6= k; (H3) e(v,V \\ H) ≤ 2 log(np̄)2, where for any S ⊂ V and ℓ, e(v, S, ℓ) = ∑w∈S Aℓvw , and e(v, S) = ∑L ℓ=1 e(v, S, ℓ). Condition (H1) means that there are not too many observed labels ℓ ≥ 1 on pairs including v, (H2) means that an item v ∈ Vk must be classified to Vk when considering the log-likelihood, and (H3) states that v does not share too many labels with items outside H .\nWe then prove that |V \\ H| ≤ s with high probability when nD(α, p) − np̄log(np̄)3 ≥ log(n/s) + √\nlog(n/s). This is mainly done using concentration arguments to relate the quantity ∑K\ni=1 ∑L ℓ=0 e(v,Vi, ℓ) log p(k,i,ℓ) p(j,i,ℓ) involved in (H2) to nD(α, p).\nFinally, we establish that if the clusters provided after the first part of the SP algorithm are asymptotically accurate, then after log(n) improvement iterations, there is no misclassified items in H . To that aim, we denote by E(t) the set of misclassified items after the t-th iteration, and show that with high probability, for all t, |E\n(t+1)∩H| |E(t)∩H| ≤ 1√ np̄ . This completes the proof of Theorem 2, since after log(n) iterations,\nthe only misclassified items are those in V \\H ."
    }, {
      "heading" : "A The SP Algorithm",
      "text" : "In this section, we present the Spectral Partition (SP) algorithm. The main pseudo-code of SP is presented in Algorithm 1. The SP algorithm consists in two parts. In the first part, corresponding to Lines 1-4 in the pseudo-code, we apply a spectral decomposition of the matrix A =\n∑L ℓ=1 wℓA ℓ constructed from the observed labels. This matrix is first trimmed, and then treated by applying the spectral decomposition algorithm, whose pseudo-code is presented in Algorithm 2. The second part of the SP algorithm, corresponding to Lines 5 and 6 in Algorithm 1, consists in improving the clusters initially identified in the first step.\nAlgorithm 1 Spectral Partition\nInput: Observation matrices Aℓ for every label ℓ (Aℓuv = 1 if ℓ is observed between u and v). 1. Estimated average degree. p̃ ← ∑L ℓ=1 ∑ u,v A ℓ uv n(n−1) 2. Random Weights. A ← ∑Lℓ=1wℓAℓ where the weights wℓ’s are i.i.d and uniformly distributed on [0, 1]. 3. Trimming. Construct AΓ = (Avw)v,w∈Γ, where Γ is the set of nodes obtained after removing ⌊n exp(−np̃)⌋ nodes having the largest ∑ℓ ∑ w∈V A ℓ vw. 4. Spectral Decomposition. Run Algorithm 2 with input AΓ, p̃, and output (Sk)k=1,...,K̂ . 5. Estimated parameters. p̂(i, j, ℓ) ← ∑ u∈Si ∑ v∈Sj Aℓuv\n|Si||Sj | for all 1 ≤ i, j ≤ K̂ and 0 ≤ ℓ ≤ L. 6. Improvement. S (0) k ← Sk, for all k for t = 1 to log n do S (t) k ← ∅, for all k\nfor v ∈ V do Find k⋆ = argmax1≤k≤K̂{ ∑K̂ i=1 ∑\nw∈S(t−1)i\n∑L ℓ=0A ℓ vw log p̂(k, i, ℓ)} (tie broken uniformly at\nrandom) S (t) k⋆ ← S (t) k⋆ ∪ {v} end for end for V̂k ← S(logn)k , for all k Output: (V̂k)k=1,...,K̂ ."
    }, {
      "heading" : "B Properties of the divergence D(α, p) and related quantities",
      "text" : "In this section, we prove the two claims of Section 2, as well as other results on the divergence D(α, p) that will be instrumental in the proofs of Theorems.\nAlgorithm 2 Spectral decomposition Input: AΓ, p̃ 1. Iterative Power Method with singular value thresholding (Initialization) χ ← n, k ← 0, and Û ← 0n×1 while χ ≥ √np̃ log(np̃) do\nk ← k + 1, U0 ← n× 1 Gaussian random vector (Iterative power method) Ut ← (AΓ)⌈2 log(n)⌉U0 (Orthonormalizing Ut) Ûk ←\nUt−Û1:k−1(Û⊤1:k−1Ut) ‖Ut−Û1:k−1(Û⊤1:k−1Ut)‖2\n(Estimating the k-th singular value) χ ← ‖AΓÛk‖2 end while K̃ ← k − 1, V̂ ← Û⊤\n1:K̃ AΓ\n2. Clustering VR ← a subset of Γ obtained by randomly selecting ⌈log(n)⌉ items of Γ Qv ← {w ∈ Γ : ‖V̂w − V̂v‖22 ≤ np̃ 2\nlog(np̃)} for all v ∈ VR (Initialization) S0 ← ∅, k ← 0, and ρ ← |Γ| while ρ ≥ log(np̃)4p̃ do\nk ← k + 1, v⋆k ← argmaxv∈VR |Qv \\ ⋃k−1 l=0 Sl|, Sk ← Qv⋆k \\ ⋃k−1 l=0 Sl and ρ ← |Sk|. end while K̂ ← k − 1 for v ∈ Γ \\⋃K̂k=1 Sk do\nk⋆ ← argmink ‖V̂v⋆ k − V̂v‖2, Sk⋆ ← Sk⋆ ∪ {v}\nend for Output: (Sk)k=1,...,K̂ ."
    }, {
      "heading" : "B.1 Proof of Claim 4",
      "text" : "DL+(p(i), p(j)) is the minimum of the objective function of the following convex optimization problem:\nmin y∈PK×(L+1)\nK ∑\nk=1\nαk\n(\nL ∑\nℓ=1\ny(k, ℓ) log\n(\ny(k, ℓ)\np(i, k, ℓ)\n) + (1− L ∑\nℓ=1\ny(k, ℓ)) log\n(\n1−∑Lℓ=1 y(k, ℓ) 1−∑Lℓ=1 p(i, k, ℓ)\n))\ns.t. K ∑\nk=1\nαkKL(y(k), p(i, k)) ≥ K ∑\nk=1\nαkKL(y(k), p(j, k)).\n(5)\nNote that we define y(k, 0) = 1 −∑Lℓ=1 y(k, ℓ) for all k. Since p̄ = o(1), one can easily check that the solution of (5) has to be\n∑L ℓ=1 y(k, ℓ) = o(1) for all k. The objective function converges to infinity when\n∑L ℓ=1 y(k, ℓ) = Ω(1), while it has o(p̄) when y(k, ℓ) = p(j, k, ℓ) for all k and ℓ. Thus, we consider\n∑L ℓ=1 y(k, ℓ) = o(1). The associated Lagrangian is:\ng(y, λ) = K ∑\nk=1\nαk\n(\nL ∑\nℓ=1\ny(k, ℓ) log\n(\ny(k, ℓ)\np(i, k, ℓ)\n) + (1− L ∑\nℓ=1\ny(k, ℓ)) log\n(\n1−∑Lℓ=1 y(k, ℓ) 1−∑Lℓ=1 p(i, k, ℓ)\n))\n+\nK ∑\nk=1\nαkλ\n(\nL ∑\nℓ=1\ny(k, ℓ) log\n(\np(i, k, ℓ)\np(j, k, ℓ)\n) + (1− L ∑\nℓ=1\ny(k, ℓ)) log\n(\n1−∑Lℓ=1 p(i, k, ℓ) 1−∑Lℓ=1 p(j, k, ℓ)\n))\n.\n(6)\nThe derivative of g(y, λ) w.r.t. y(k, ℓ) is computed as follows:\n∂g(y, λ) ∂y(k, ℓ) =αk\n(\nlog\n(\ny(k, ℓ)\np(i, k, ℓ)\n) − log ( 1−∑Lm=1 y(k,m) 1−∑Lm=1 p(i, k,m) )) +\nαkλ\n(\nlog\n(\np(i, k, ℓ)\np(j, k, ℓ)\n) − log ( 1−∑Lm=1 p(i, k,m) 1−∑Lm=1 p(j, k,m) )) .\nObserve that, since (A1) holds, p̄ = o(1) and ∑L ℓ=1 y(k, ℓ) = o(1), as n grows large, log (\n1− ∑L\nm=1 y(k,m)\n1− ∑L\nm=1 p(i,k,m)\n)\nand log ( 1−∑Lm=1 p(i,k,m) 1−∑Lm=1 p(j,k,m) ) converges to 0. Thus, (6) is minimized at\ny(k, ℓ) = p(i, k, ℓ)\n(\np(j, k, ℓ)\np(i, k, ℓ)\n)λ\n(1 + o(1)). (7)\nWhen we put (7) onto (6) and use the approximation limx→0 log(1 + x) = x (again using p̄ = o(1)),\nmin y∈PK×{0,1} g(y, λ)\n= min y∈PK×{0,1}\nK ∑\nk=1\nL ∑\nℓ=1\nαk (o(p̄)+\n(1− L ∑\nℓ=1\ny(k, ℓ)) log\n(\n1−∑Lℓ=1 y(k, ℓ) 1−∑Lℓ=1 p(i, k, ℓ)\n)(\n1−∑Lℓ=1 p(i, k, ℓ) 1−∑Lℓ=1 p(j, k, ℓ)\n)λ \n\n= min y∈PK×{0,1}\nK ∑\nk=1\nL ∑\nℓ=1\nαk (o(p̄)−\nL ∑\nℓ=1\ny(k, ℓ)(1 + o(1)) + (1− λ) L ∑\nℓ=1\np(i, k, ℓ)(1 + o(1)) + λ\nL ∑\nℓ=1\np(j, k, ℓ)(1 + o(1))\n)\n.\nTherefore, the minimum value of (5) is equivalent to\nmax λ∈[0,1]\nK ∑\nk=1\nL ∑\nℓ=1\nαk\n( (1− λ)p(i, k, ℓ) + λp(j, k, ℓ) − p(i, k, 1)1−λp(j, k, ℓ)λ ) + o(p̄)."
    }, {
      "heading" : "B.2 Proof of Claim 5",
      "text" : "When p̄ = o(1), for all i 6= j, αi = 1K , p(i, i, ℓ) = p(ℓ), and p(i, j, ℓ) = q(ℓ), from Claim 4,\nDL+(α, p(i), p(j)) = max λ∈[0,1]\nK ∑\nk=1\nL ∑\nℓ=1\nαk\n( (1− λ)p(i, k, ℓ) + λp(j, k, ℓ) − p(i, k, ℓ)1−λp(j, k, ℓ)λ )\n= 1\nK max λ∈[0,1]\nL ∑\nℓ=1\n( p(ℓ) + q(ℓ)− p(ℓ)1−λq(ℓ)λ − p(ℓ)λq(ℓ)1−λ )\n= 1\nK\nL ∑\nℓ=1\n(\np(ℓ) + q(ℓ)− 2 √ p(ℓ)q(ℓ) ) . (8)\nNow, since √ 1 + x = 1 + x2 (1 + o(1)) and log(1 + x) = x(1 + o(1)) when x = o(1),\n− 2 K log\n(\nL ∑\nℓ=0\n√\np(ℓ)q(ℓ)\n)\n= − 2 K log\n(\n√\np(0)q(0) +\nL ∑\nℓ=1\n√\np(ℓ)q(ℓ)\n)\n= − 2 K log\n(\n1− ∑L ℓ=1 p(ℓ) + q(ℓ)\n2 (1 + o(1)) +\nL ∑\nℓ=1\n√\np(ℓ)q(ℓ)\n)\n= 2\nK\n(\n∑L ℓ=1 p(ℓ) + q(ℓ)\n2 −\nL ∑\nℓ=1\n√\np(ℓ)q(ℓ)\n)\n(1 + o(1)). (9)\nThe claim follows from (8) and (9)."
    }, {
      "heading" : "B.3 Other properties",
      "text" : "Lemma 7 Let (i⋆, j⋆) = argmini,j DL+(p(i), p(j)) and i⋆ < j⋆. Then, there exists q ∈ PK×(L+1) such that\nD(α, p) = K ∑\nk=1\nαkKL(q(k), p(i ⋆, k)) =\nK ∑\nk=1\nαkKL(q(k), p(j ⋆, k)).\nProof. We check by contradiction that such a q exists. Indeed, assume that\nD(α, p) = K ∑\nk=1\nαkKL(q(k), p(i ⋆, k)) >\nK ∑\nk=1\nαkKL(q(k), p(j ⋆, k)).\nThen there exists k0 such that KL(q(k0), p(i⋆, k0)) > KL(q(k0), p(j⋆, k0)). Observe that by positivity of the KL divergence, q(k0) 6= p(i⋆, k0). Hence by continuity of the KL divergence, we can construct q′ such that q(k) = q′(k) for all k 6= k0, and such that: KL(q(k0), p(i⋆, k0)) − ǫ < KL(q′(k0), p(i⋆, k0)) < KL(q(k0), p(i⋆, k0)) and KL(q′(k0), p(j⋆, k0)) < KL(q(k0), p(j⋆, k0)) + ǫ for some 0 < ǫ < (KL(q(k0), p(i⋆, k0))−KL(q(k0), p(j⋆, k0)))/2. With this choice of q′, we get:\nD(α, p) >\nK ∑\nk=1\nαkKL(q ′(k), p(i⋆, k)) >\nK ∑\nk=1\nαkKL(q ′(k), p(j⋆, k)),\nwhich contradicts the definition of D(α, p).\nLemma 8 When p̄ = o(1),\nlim n→∞ D(α, p) ∑K\nk=1 αk 2\n(\n∑L ℓ=1(\n√ p(i⋆, k, ℓ) − √ p(j⋆, k, ℓ))2 ) ≥ 1.\nProof. Let (i⋆, j⋆) = argmini,j DL+(α, p(i), p(j)) and i⋆ < j⋆. From Lemma 7, there exists q satisfying that\nD(α, p) = K ∑\nk=1\nαkKL(q(k), p(i ⋆, k)) =\nK ∑\nk=1\nαkKL(q(k), p(j ⋆, k)).\nThen,\nnD(α, p) = n\n∑K k=1 (αkKL(q(k), p(i ⋆, k)) + αkKL(q(k), p(j ⋆, k)))\n2\n= −n K ∑\nk=1\nαk\nL ∑\nℓ=0\nq(k, ℓ) log\n(\n√\np(i⋆, k, ℓ)p(j⋆, k, ℓ)\nq(k, ℓ)\n)\n≥ n K ∑\nk=1\nαk\nL ∑\nℓ=0\n(\nq(k, ℓ)− √ p(i⋆, k, ℓ)p(j⋆, k, ℓ) )\n= n K ∑\nk=1\nαk\n(\n∑L ℓ=1(p(i ⋆, k, ℓ) + p(j⋆, k, ℓ))\n2 −\nL ∑\nℓ=1\n√\np(i⋆, k, ℓ)p(j⋆, k, ℓ)\n)\n(1− o(1))\n= n\nK ∑\nk=1\nαk 2\n(\nL ∑\nℓ=1\n( √ p(i⋆, k, ℓ) − √ p(j⋆, k, ℓ))2\n)\n(1− o(1)) .\nLemma 9 Under condition (A1), when p̄ = o(1), lim supn→∞ D(α,p) ηp̄L ≤ 1.\nProof. From the definition of D(α, p), for any i 6= j,\nD(α, p) ≤ max { K ∑\nk=1\nαkKL(p(i, k), p(i, k)),\nK ∑\nk=1\nαkKL(p(i, k), p(j, k))\n}\n=\nK ∑\nk=1\nαkKL(p(i, k), p(j, k))\n≤ K ∑\nk=1\nαk\nL ∑\nℓ=1\n(p(i, k, ℓ) − p(j, k, ℓ))2 p(j, k, ℓ) (1 + o(1))\n≤ K ∑\nk=1\nαk\nL ∑\nℓ=1\nηp̄(1 + o(1))\n= ηp̄L(1 + o(1)),\nwhere we use log(1 + x) = x(1 + o(1)) when x = o(1)."
    }, {
      "heading" : "C Proof of Theorem 1",
      "text" : "The proof consists in an appropriate change-of-measure argument. The originality of the proof stems from the fact that the change of measures is obtained by a judicious coupling argument [17]. In the following, we refer to Φ as the true stochastic model under which all the observed random labels are generated, and denote by PΦ = P (resp. EΦ[·] = E[·]) the corresponding probability measure (resp. expectation). We recall that Φ is defined by the parameters (α, p), and that under Φ, the nodes are first\nattached to the various clusters according to the distribution α, and the labels between two nodes are then generated using distributions p. The proof consists in constructing a perturbed stochastic model Ψ coupling the labels generated under Φ with those generated under Ψ. We denote by PΨ (resp. EΨ[·] = E[·]) the probability measure (resp. expectation) under the perturbed model Ψ. We then relate the proportion of misclassified nodes under any given clustering algorithm π to the distribution under PΨ of a quantity Q that resembles the log-likelihood ratio of the observed labels under PΦ and PΨ. The analysis of the likelihood ratio finally provides the desired lower bound on the expected misclassified nodes under π. Next, we detail each step of the proof.\nCoupling and the perturbed stochastic model Ψ. Let (i⋆, j⋆) = argmini,j:i<j DL+(p(i), p(j)), and let v⋆ denote the smallest node index that belongs to cluster i⋆ or j⋆. If both Vi⋆ and Vj⋆ are empty, we define v⋆ = n. Let q ∈ [0, 1]K×(L+1) satisfy:\nD(α, p) =\nK ∑\nk=1\nαkKL(q(k), p(i ⋆, k)) =\nK ∑\nk=1\nαkKL(q(k), p(j ⋆, k)).\nThere exists such a q from Lemma 7. Now to define the perturbed stochastic model Ψ, we couple the generation of labels under Φ and Ψ as follows.\n1. We first generate construct the random clusters V1, . . . ,VK under Φ, and extract i⋆, j⋆, and v⋆. The clusters generated under Ψ are the same as those generated under Φ. For any v ∈ V , we denote by σ(v) the cluster of node v.\n2. For all nodes v,w 6= v⋆, the labels generated under Ψ are the same as those generated under Φ, i.e., the label ℓ is observed on the edge (v,w) with probability p(σ(v), σ(w), ℓ).\n3. Under Ψ, for any v 6= v⋆, the observed label on the edge (v, v⋆) under Ψ is ℓ with probability q(σ(v), ℓ).\nThe log-likelihood ratio and its connection to the expected number of misclassified nodes. Let xv,w denote the label observed on the edge (v,w). We introduce Q, referred to as the pseudo-log-likelihood ratio of the observed labels under PΦ and PΨ) as:\nQ = v⋆−1 ∑\nv=1\nlog q(σ(v), xv⋆ ,v)\np(σ(v⋆), σ(v), xv⋆ ,v) +\nn ∑\nv=v⋆+1\nlog q(σ(v), xv⋆ ,v)\np(σ(v⋆), σ(v), xv⋆ ,v) . (10)\nLet π denote a clustering algorithm with output (V̂k)1≤k≤K , and let E = ⋃ 1≤k≤K V̂k \\ Vk be the set of misclassified nodes under π. Note that in general in our proofs, we always assume without loss of generality that |⋃1≤k≤K V̂k \\ Vk| ≤ | ⋃\n1≤k≤K V̂γ(k) \\ Vk| for any permutation γ, so that the set of misclassified nodes is really E . We denote by επ(n) = |E|. Since under Φ, nodes are interchangeable (remember that nodes are assigned to the various clusters in an i.i.d. manner), we have:\nnPΦ{v ∈ E} = EΦ[επ(n)] = E[επ(n)].\nNext, we establish a relationship between E[επ(n)] and the distribution of Q under PΨ. For any function f(n), we have:\nPΨ{Q ≤ f(n)} = PΨ{Q ≤ f(n), v⋆ ∈ E}+ PΨ{Q ≤ f(n), v⋆ /∈ E}. (11)\nUsing Q, we get:\nPΨ{Q ≤ f(n), v⋆ ∈ E} = ∫\n{Q≤f(n),v⋆∈E} dPΨ\n=\n∫\n{Q≤f(n),v⋆∈E} exp(Q)dPΦ\n≤ exp(f(n))PΦ{Q ≤ f(n), v⋆ ∈ E} ≤ exp(f(n))PΦ{v⋆ ∈ E} ≤ exp(f(n)) EΦ[ε π(n)]\n(αi⋆ + αj⋆)n , (12)\nwhere the last inequality is obtained from the fact that we cannot distinguish between v⋆ and any other v ∈ Vσ(v⋆). Indeed,\nPΦ{v⋆ ∈ E} = PΦ{v ∈ E|v ∈ Vi⋆ ∪ Vj⋆} =\nPΦ{v ∈ E , v ∈ Vi⋆ ∪ Vj⋆} PΦ{v ∈ Vi⋆ ∪ Vj⋆}\n≤ PΦ{v ∈ E} PΦ{v ∈ Vi⋆ ∪ Vj⋆} = EΦ[ε π(n)] (αi⋆ + αj⋆)n .\nFurthermore, since under the stochastic model Ψ, the observed labels do not depend on whether v⋆ belongs to cluster i⋆ or j⋆, we have:\nPΨ{v⋆ ∈ V̂i⋆ |v⋆ ∈ Vi⋆} = PΨ{v⋆ ∈ V̂i⋆ |v⋆ ∈ Vj⋆} and PΨ{v⋆ ∈ V̂j⋆|v⋆ ∈ Vi⋆} = PΨ{v⋆ ∈ V̂j⋆|v⋆ ∈ Vj⋆}.\nFinally, since PΨ{v⋆ ∈ V̂i⋆ |v⋆ ∈ Vi⋆}+ PΨ{v⋆ ∈ V̂j⋆|v⋆ ∈ Vi⋆} ≤ 1, we also have:\nPΨ{Q ≤ f(n), v⋆ /∈ E} ≤ PΨ{v⋆ /∈ E} = αi⋆\nαi⋆ + αj⋆ PΨ{v⋆ ∈ V̂i⋆ |v⋆ ∈ Vi⋆}+\nαj⋆\nαi⋆ + αj⋆ PΨ{v⋆ ∈ V̂j⋆|v⋆ ∈ Vj⋆}\n= αi⋆\nαi⋆ + αj⋆ PΨ{v⋆ ∈ V̂i⋆ |v⋆ ∈ Vi⋆}+\nαj⋆\nαi⋆ + αj⋆ PΨ{v⋆ ∈ V̂j⋆|v⋆ ∈ Vi⋆}\n≤ αj⋆ αi⋆ + αj⋆ . (13)\nCombining (11), (12), and (13), we conclude that:\nPΨ{Q ≤ f(n)} ≤ exp(f(n)) EΦ[ε\nπ(n)]\n(αi⋆ + αj⋆)n +\nαj⋆\nαi⋆ + αj⋆ . (14)\nThe previous equation provides the desired generic relationship between EΦ[επ(n)] and PΨ{Q ≤ f(n)} from which can deduce a necessary condition for E[επ(n)] ≤ s. Applying (14) with f(n) = log (n/EΦ[ε π(n)])− log(2/αi⋆), we have:\nPΨ{Q ≤ log (n/EΦ[επ(n)])− log(2/αi⋆)} ≤ 1− αi⋆ 2 < 1− αi⋆ 4 . (15)\nIn addition, from Chebyshev’s inequality,\nPΨ\n{ Q ≤ EΨ[Q] + √ 4\nαi⋆ EΨ[(Q− EΨ[Q])2]\n}\n≥ 1− αi⋆ 4 . (16)\nFrom (15) and (16), we deduce that:\nlog (n/EΦ[ε π(n)]) − log(2/αi⋆) ≤ EΨ[Q] +\n√\n4\nαi⋆ EΨ[(Q− EΨ[Q])2],\nand thus, a necessary condition for E[επ(n)] ≤ s is:\nlog (n/s)− log(2/αi⋆) ≤ EΨ[Q] + √ 4\nαi⋆ EΨ[(Q− EΨ[Q])2]. (17)\nAnalysis of the log-likelihood ratio. In view of (17), we can obtain a necessary condition for E[επ(n)] ≤ s if we evaluate EΨ[Q] and EΨ[(Q− EΨ[Q])2]. (i) We first compute EΨ[Q]. Note that in view of the definition of v⋆, a node whose index is smaller than v⋆ cannot be in Vi⋆ or Vj⋆ , whereas a node whose index v is larger than v⋆ can be in any cluster (and the cluster of such a v is drawn according to the distribution α independently of other nodes). This slightly complicates the computation of the expectation of the two sums defining Q in (10). To circumvent this problem, we can observe that v⋆ is rather small, i.e., less log(n)2 with high probability, and that hence, we can approximate EΨ[Q] by EΨ[ ∑n v=v⋆+1 log q(σ(v),xv⋆,v)\np(σ(v⋆),σ(v),xv⋆,v) ], which is itself well-approximated by\nnD(α, p). More formally, since P{v⋆ ≤ m} = 1− (1− αi⋆ − αj⋆)m,\nP{v⋆ ≤ log(n)2} ≥ 1− 1 n4 . (18)\nHence from condition (A1), (18), and the definition of Q,\nEΨ[Q] = P{v⋆ > log(n)2}EΨ[Q|v⋆ > log(n)2] + P{v⋆ ≤ log(n)2}EΨ[Q|v⋆ ≤ log(n)2] ≤ log η\nn3 + EΨ[Q|v⋆ ≤ log(n)2]\n≤ log η n3 + EΨ\n[ v⋆−1 ∑\nv=1\nlog q(σ(v), xv⋆ ,v)\np(σ(v⋆), σ(v), xv⋆ ,v) |v⋆ ≤ log(n)2\n]\n+ nD(α, p)\n≤ log η n3 + EΨ\n\n(v⋆ − 1) ∑\nk/∈{i⋆,j⋆}\nαkKL(q(k), p(σ(v ⋆, k)))\n1− αi⋆ − αj⋆ |v⋆ ≤ log(n)2\n\n+ nD(α, p)\n≤ ( n+ 2 log(n)2 log η ) D(α, p) + log η\nn3 , (19)\nwhere the last inequlaity stems from the fact that 2KL(q(i), p(σ(v⋆, i))) log η ≥ KL(q(j), p(σ(v⋆, j))) for all i and j from condition (A1).\n(ii) To compute EΨ[(Q − EΨ[Q])2], we evaluate EΨ[(Q − nD(α, p))2|σ(v⋆) = i⋆] and EΨ[(Q − nD(α, p))2|σ(v⋆) = j⋆]. From condition (A1), (18), and the definition of Q,\nEΨ[(Q− nD(α, p))2|σ(v⋆) = i⋆] = P{v⋆ ≤ log(n)2}EΨ[(Q− nD(α, p))2|σ(v⋆) = i⋆, v⋆ ≤ log(n)2]\n+P{v⋆ > log(n)2}EΨ[(Q− nD(α, p))2|σ(v⋆) = i⋆, v⋆ > log(n)2] ≤ EΨ[(Q− nD(α, p))2|σ(v⋆) = i⋆, v⋆ ≤ log(n)2]\n+ 1\nn4 EΨ[(Q− nD(α, p))2|σ(v⋆) = i⋆, v⋆ > log(n)2]\n= O(np̄).\nTo derive the above inequality, we have used:\nEΨ\n\n\n(\nn ∑\nv=v⋆+1\n(\nlog q(σ(v), xv⋆ ,v)\np(σ(v⋆), σ(v), xv⋆ ,v) −D(α, p)\n)\n)2 |σ(v⋆) = i⋆  \n= n ∑\nv=v⋆+1\nEΨ\n[\n(\nlog q(σ(v), xv⋆ ,v)\np(i⋆, σ(v), xv⋆ ,v) −D(α, p)\n)2 |σ(v⋆) = i⋆ ]\n= O(np̄) and\nEΨ\n\n\n( v⋆−1 ∑\nv=1\n(\nlog q(σ(v), xv⋆ ,v)\np(σ(v⋆), σ(v), xv⋆ ,v) −D(α, p)\n)\n)2 |σ(v⋆) = i⋆  \n= O(v⋆p̄+ (v⋆p̄)2),\nwhere we use (A1) and the fact that every label is generated independently. Using the same approach, we can also conclude that EΨ[(Q− nD(α, p))2|σ(v⋆) = j⋆] = O(np̄). In summary, we have:\nEΨ[(Q− EΨ[Q])2] = O(np̄). (20)\nWe are ready to complete the proof of Theorem 1. From (17), (19), (20), and Lemma 8, when the expected number of misclassified nodes is less than s (i.e., E[επ(n)] ≤ s ), we must have:\nlim inf n→∞\nnD(α, p) log (n/s) ≥ 1."
    }, {
      "heading" : "D Performance of the SP Algorithm – Proof of Theorem 2",
      "text" : "Notations. We use the standard matrix norm ‖A‖ = sup x:‖x‖2=1 ‖Ax‖2. We denote by M ℓ the expectation of the matrix of Aℓ, i.e., M ℓu,v = p(i, j, ℓ) when u ∈ Vi and v ∈ Vj . Let M = ∑L ℓ=1 wℓM ℓ. We define AΓ to denote the adjacency matrix obtained after trimming (Step 3 in Algorithm 1). For any matrix R ∈ R n×n, we define the matrix RΓ the square matrix formed by the lines and columns of R whose indexes are in Γ. Hence, we can define AℓΓ, M ℓ Γ, and MΓ where Γ is the set of items obtained after the trimming process (Line 3) in the SP algorithm (when taking the expectation to get for example MΓ, we condition on Γ). We introduce the noise matrices XℓΓ = A ℓ Γ − M ℓΓ and XΓ = ∑L ℓ=1wℓX ℓ Γ. We also denote by e(v, S, ℓ) = ∑\nw∈S A ℓ vw the total number of item pairs with observed label ℓ including the item v and an\nitem from S and µ(v, S, ℓ) = e(v,S,ℓ)|S| the empirical density of label ℓ. Let e(v, S) = ∑L\nℓ=1 e(v, S, ℓ) and µ(v, S) = [µ(v, S, ℓ)]0≤ℓ≤L. In what follows, e(v,V) is referred to as the degree of item v (the number of observed labels different than 0 of pairs of items including v).\nOutline of the proof. To analyze the performance of the SP algorithm, we first state preliminary lemmas. Lemma 10 is concerned with the concentration of the degree of the various items. Lemma 11 provides an upper bound of the matrix norm of random noise matrix XℓΓ. From these two lemmas, we analyze the performance of the first part of the SP algorithm, and prove Theorem 6. To analyze the second part of the SP algorithm consisting of log(n) improvement iterations, we introduce an appropriate set of items H such that that V \\H is of cardinality less than s with high probability under the condition that\nnD(α, p)− np̄ log(np̄)3 ≥ log(n/s)+ √ log(n/s). We further bound the rate of improvement of our cluster estimates in each iteration when restricted to the set of items H , and deduce that after log(n) iterations, no item in H is misclassified."
    }, {
      "heading" : "D.1 Preliminary lemmas",
      "text" : "Lemma 10 For every v ∈ V and c ≥ 1, we have\nP{e(v,V) ≥ 10cnp̄L} ≤ exp(−10cnp̄L).\nProof. From Markov inequality,\nP{e(v,V) ≥ 10np̄L} ≤ inf θ>0\n∏K k=1 E [exp(θe(v,Vk))]\nexp(θ10cnp̄L)\n≤ inf θ>0\n∏K k=1\n( 1 + p̄L(exp(θ)− 1) )αkn\nexp(θ10cnp̄L)\n≤ inf θ>0\n∏K k=1\n( exp(p̄L(exp(θ)− 1)) )αkn\nexp(θ10cnp̄L) ≤ exp(−10cnp̄L),\nwhere we derive the last inequality choosing θ = 2.\nLemma 11 (Lemma 8.5 of [4]) When e(v,V, ℓ) ≤ ∆ for all v ∈ Γ, with high probability,\n‖XℓΓ‖ = O( √ np̄+∆).\nThe proof of Lemma 11 relies on arguments used in the spectral analysis of random graphs, see [7] and [4].\nLemma 12 For all v ∈ Vk and D ≥ 0,\nP\n{(\nK ∑\ni=1\n|Vi|KL(µ(v,Vi), p(k, i)) ≥ nD ) ∩ ( e(v,V) ≤ 10ηnp̄L ) }\n≤ exp ( −nD +KL log(10ηLnp̄) + 100η 2np̄2L2\nα1\n)\n.\nProof. Let X be a set of K × (L+ 1) matrices such that\nX = { x ∈ ZK×(L+1) : K ∑\ni=1\nL ∑\nℓ=1\nxi,ℓ ≤ 10ηnp̄L, and L ∑\nℓ=0\nxi,ℓ = |Vi| for all 1 ≤ i ≤ K } .\nFor notational simplicity, we use [xi,ℓ|Vi| ] instead of [ xi,ℓ |Vi| ]0≤ℓ≤L to represent the probability mass vector on labels defined by xi. With a slight abuse of notation, we denote by e(v) the K × (L + 1) matrix whose (i, ℓ) element is e(v,Vi, ℓ). Then, for v ∈ Vk,\nP\n{(\nK ∑\ni=1\n|Vi|KL(µ(v,Vi), p(k, i)) ≥ nD ) ∩ ( e(v,V) ≤ 10np̄L ) }\n= ∑\nx∈X P {e(v) = x}P\n{\nK ∑\ni=1\n|Vi|KL(µ(v,Vi), p(k, i)) ≥ nD ∣ ∣ ∣\n∣\ne(v) = x\n}\n≤ ∑\nx∈X P{e(v) = x}\nexp (\n∑K i=1 |Vi|KL([ xi,ℓ |Vi| ], p(k, i))\n)\nexp(nD)\n≤ ∑\nx∈X P{e(v) = x}\n∏K i=1 ∏L ℓ=0\n(\nxi,ℓ |Vi|p(k,i,ℓ)\n)xi,ℓ\nexp(nD)\n(a) ≤ 1 exp(nD) ∑\nx∈X\nK ∏\ni=1\n((\n1− ∑L\nℓ=1 xi,ℓ |Vi|\n)xi,0\nexp(\nL ∑\nℓ=1\nxi,ℓ)\n)\n= 1\nexp(nD)\n∑\nx∈X\nK ∏\ni=1\nexp\n(\n(|Vi| − L ∑\nℓ=1\nxi,ℓ) log\n(\n1− ∑L\nℓ=1 xi,ℓ |Vi|\n)\n+\nL ∑\nℓ=1\nxi,ℓ\n)\n≤ 1 exp(nD) ∑\nx∈X\nK ∏\ni=1\nexp\n(\n( ∑L ℓ=1 xk,ℓ) 2\n|Vi|\n)\n≤ (10ηnp̄L) KL exp(100η2np̄2L2/α1)\nexp(nD)\n= exp\n(\n−nD +KL log(10ηLnp̄) + 100η 2np̄2L2\nα1\n)\n,\nwhere (a) stems from the following inequality:\nP{e(v,Vi, ℓ) = xi,ℓ for all i, ℓ}\n≤ K ∏\ni=1\n(\np(k, i, 0)xi,0 L ∏\nℓ=1\n(|Vi| xi,ℓ ) p(k, i, ℓ)xk,ℓ\n)\n≤ K ∏\ni=1\n(\np(k, i, 0)xi,0 L ∏\nℓ=1\n( e|Vi| xi,ℓ )xi,ℓ p(k, i, ℓ)xi,ℓ\n)\n."
    }, {
      "heading" : "D.2 Part 1 of the SP algorithm – Proof of Theorem 6",
      "text" : "Recall that Â = Û V̂ = Û Û⊤AΓ and ‖Âu−Âv‖ = ‖V̂u−V̂v‖. We can bound the number of misclassified items as follows:\n• with high probability, we have ‖Â−MΓ‖2F = ∑\nv∈Γ ‖Âv −Mv,Γ‖22 = O(np̄ log(np̄)2); (21)\n• with high probability, every item pair u and v satisfies that when σ(v) represents the cluster of v and Mv,Γ denotes the column vector of MΓ on v,\n‖Mu,Γ −Mv,Γ‖22 = Ω ( np̄2 ) when σ(u) 6= σ(v), (22)\nsince every wℓ is generated uniformly at random in [0, 1] and (A2) holds;\n• (22) suggests that if v is misclassified by Algorithm 2, then we should have:\n‖Âv −Mv,Γ‖22 = Ω ( np̄2 ) ; (23)\n• from (21) and (23), with high probability, ∣\n∣ ∣ ∣ ∣\nK ⋃\nk=1\n(Vk \\ Sk) ∣ ∣ ∣ ∣\n∣\n= O\n(\nlog(np̄)2\np̄\n)\n.\nNext, we prove (21) and (23).\nProof of (21). First observe that from the definition of Γ,\nP\n{\nmax v∈Γ\ne(v,V) ≥ 10np̄L } = P {|{v : e(v,V) ≥ 10np̄L}| > ⌊n exp(−np̃)⌋}\n≤ n exp(−10np̄L)⌊n exp(−np̃)⌋+ 1 ≤ exp(−5np̄L),\nwhere the first inequality stems from Lemma 10 and Markov inequality. Therefore, with high probability,\nmax v∈Γ\ne(v,V) ≤ 10np̄L. (24)\nWhen the degrees of items are bounded, the standard matrix norm of each noise matrix XℓΓ can be bounded using Lemma 11. From (24) and Lemma 11,\n‖XΓ‖ ≤ L ∑\nℓ=1\nwℓ‖XℓΓ‖\n= L ∑\nℓ=1\nO(wℓ √ np̄+ 10np̄L)\n= O( √ np̄). (25)\nLet K̃ be the number of columns of Û . Since Â is the K̃-rank approximation of AΓ obtained by the iterative power method with 2 log(n) iterations, from Theorem 9.1 and Theorem 9.2 in [11], with high probability,\n1 2 sk(AΓ) ≤ ‖AΓÛk‖ ≤ sk(AΓ) and ‖AΓ(I − Û1:kÛ⊤1:k)‖ ≤ 2sk+1(AΓ). (26)\nSince ‖AΓÛK‖ ≤ sK+1(AΓ) ≤ ‖XΓ‖ = O( √ np̄) from Lemma 11 and (26), K̃ ≤ K and thus the rank of (Â−MΓ) is less than 2K . Therefore,\n‖Â−MΓ‖2F ≤ 2K‖Â−MΓ‖2\n≤ 4K ( ‖Â−AΓ‖2 + ‖AΓ −MΓ‖2 ) ≤ O(np̄ log(np̄)2), (27)\nwhere the last inequality stems from the fact that ‖AΓ − MΓ‖ = ‖XΓ‖ = O( √ np̄) and ‖Â − AΓ‖ ≤\n2sK̃+1(AΓ) = O( √ np̄ log(np̄)) from (26). Proof of (23). Define the following sets:\nIk = {v ∈ Vk ∩ Γ : ‖Âv −MkΓ‖2 ≤ 1\n4\nnp̃2\nlog(np̃) }\nO = {v ∈ Γ : ‖Âv −MkΓ‖2 ≥ 4 np̃2\nlog(np̃) for all 1 ≤ k ≤ K}.\nThese sets are designed so that\n(i) |(∪Kk=1Ik)∩Qv| = 0 for all v ∈ O∩VR, since ‖Âv − Âw‖2 ≥ 12‖Âv −MkΓ‖2 −‖Âw −MkΓ‖2 > np̃2\nlog(np̃) for all w ∈ Ik;\n(ii) |Γ \\ (∪Kk=1Ik)| ≤ ‖Â−MΓ‖2F\nmin v∈Γ\\(∪K\nk=1 Ik)\n‖Âv−MkΓ‖2 = O\n(\nlog(np̄)3\np̄\n)\n;\n(iii) Ik ⊂ Qv for all v ∈ Ik ∩VR, since ‖Âv − Âw‖2 ≤ 2‖Âv −MkΓ‖2 +2‖Âw −MkΓ‖2 ≤ np̃ 2\nlog(np̃) for all w ∈ Ik;\n(iv) If |Qv ∩ Ik| ≥ 1, |Qv ∩ Ij| = 0 for all j 6= k, since ‖MkΓ −M j Γ‖ = Ω(np̄2) is much larger than\nthe radius np̃ 2 log(np̃) = O( np̄2 log(np̄));\nFrom the properties of Ik and O, we state the following results. • From (i) and (ii), we deduce that\n|Qv| = O ( log(np̄)3\np̄\n)\nfor all v ∈ O ∩ VR, (28)\nsince every w ∈ (∪Kk=1Ik) is outside of Qv (i.e., w ∈ Γ \\ (∪Kk=1Ik) is necessary for w ∈ Qv);\n• since αk is a constant for all k and |Γ\\(∪ K k=1Ik)| |Γ| = o(1) from (ii), with high probability,\n|Ik ∩ VR| ≥ 1 for all 1 ≤ k ≤ K; (29)\n• The properties (ii), (iii), and (iv) and (29) imply that\n|Qv \\ ∪k−1l=0 Sl| ≥ mk, ∃v ∈ (∪Km=1Ik ∩ VR) \\ (∪k−1l=0 Sl), (30)\nwhere mk is the k-th largest value among {|I1|, . . . , |IK |} ;\n• since |Ik| ≥ |Vk ∩ (Γ \\ O)| ≥ αkn(1− o(1)) from (ii) and (iii),\n|Ik| ≥ |Vk ∩ (Γ \\ O)| ≥ αkn(1− o(1)). (31)\nThus, we can conclude that K̂ = K from (30) and (31) and the property (ii); and from (28), there exists a permutation γ such that ‖Âv⋆ k −Mγ(k)Γ ‖2 ≤ 4 np̃ 2 log(np̃) for all k. Hence from (22), ‖Âv−Mv,Γ‖2 = Ω ( np̄2 ) when v is misclassified."
    }, {
      "heading" : "D.3 Proof of Theorem 2",
      "text" : "From Chernoff bound, with high probability,\n||Vk| − αkn| ≤ √ n log(n) for all k. (32)\nIn what follows, we hence just prove the theorem assuming that (32) holds. Let H be the largest set of items v ∈ V satisfying:\n(H1) e(v,V) ≤ 10ηnp̄L,\n(H2) When v ∈ Vk, ∑K\ni=1 ∑L ℓ=0 e(v,Vi, ℓ) log p(k,i,ℓ) p(j,i,ℓ) ≥ np̄ log(np̄)4 for all j 6= k.\n(H3) e(v,V \\H) ≤ 2 log(np̄)2. (H1) regularizes degrees, (H2) means that v ∈ H is correctly classified when using the log-likelihood estimate, and (H3) means that v does not share too many labels with items outside H .\nThe proof of the theorem follows from the following propositions. The first provides an upper bound of |V \\ H|, and the second provides the rate at which our estimated clusters improve in each iteration when we restrict our attention to items in H .\nProposition 13 When nD(α, p)− np̄log(np̄)3 ≥ log(n/s)+ √ log(n/s), |V \\H| ≤ s with high probability.\nProposition 14 If |⋃Kk=1(S (0) k \\ Vk) ∩H|+ |V \\H| = O(1/p̄), with high probability, the following statement holds |⋃Kk=1(S (t+1) k \\ Vk) ∩H|\n|⋃Kk=1(S (t) k \\ Vk) ∩H|\n≤ 1√ np̄ for all t ≥ 0.\nFrom Proposition 14, after log(n) iterations (remember that np̄ = ω(1), so when n is large enough 1/ √ np̄ ≤ e−2), no item in H can be misclassified with high probability. Hence the number of misclassified items cannot exceed |V \\H| ≤ s, nD(α, p) − np̄log(np̄)3 ≥ log(n/s) + √\nlog(n/s). The proof is completed by remarking that if the previous condition on D(α, p) holds, then\n1 ≤ lim n→∞\nnD(α, p)− np̄ log(np̄)3\nlog(n/s) + √ log(n/s) = lim n→∞ nD(α, p) log(n/s) ,\nwhere we used D(α, p) = Ω(p̄) from condition (A2) and Lemma 8."
    }, {
      "heading" : "D.3.1 Proof of Proposition 13 – Size of V \\H",
      "text" : "We compute the number of items satisfying (H1), (H2), and (H3) in (33), (34), and Lemma 15, respectively.\nNumber of items satisfying (H1): From Lemma 10, we get:\nP{e(v,V) ≤ 10ηnp̄L} ≥ 1− exp(−10ηnp̄L). (33)\nNumber of items satisfying (H2): We shall prove that when v satisfies (H1), v satisfies (H2) as well with probability at least\n1− exp ( −nD(α, p) + np̄ 2 log(np̄)3 ) . (34)\nTo this aim, we first establish that if v satisfies\nK ∑\ni=1\n|Vi|KL(µ(v,Vi), p(k, i)) ≤ ( 1− log(n) 2\n√ n\n)\nnD(α, p)− np̄ log(np̄)4 , (35)\nthen v satisfies (H2). Indeed, assume that (35) holds, then\n(i) ∑K i=1 αinKL(µ(v,Vi), p(k, i)) ≤ ( 1 + log(n) 2 √ n ) ∑K i=1 |Vi|KL(µ(v,Vi), p(k, i)) < nD(α, p),\nsince ||Vi| − αin| ≤ √ n log(n) and (35) holds;\n(ii) ∑K i=1 αinKL(µ(v,Vi), p(j, i)) ≥ nD(α, p), since max {\n∑K i=1 αiKL(µ(v,Vi), p(j, i)), ∑K i=1 αiKL(µ(v,Vi), p(k, i))\n}\n≥ D(α, p) and ∑K\ni=1 αiKL(µ(v,Vi), p(k, i)) < D(α, p);\n(iii) ∑K i=1 |Vi|KL(µ(v,Vi), p(j, i)) ≥ ( 1− log(n)2√ n )\nnD(α, p), from ii) and the fact that ||Vi|−αin| ≤√ n log(n);\n(iv) from (35) and iii), for all j 6= i, K ∑\ni=1\nL ∑\nℓ=0\ne(v,Vi, ℓ) log p(k, i, ℓ)\np(j, i, ℓ) =\nK ∑\ni=1\n|Vi| (KL(µ(v,Vi), p(j, i)) −KL(µ(v,Vi), p(k, i)))\n≥ np̄ log(np̄)4 .\nHence v satisfies (H2). It remains to evaluate the probability of the event (35), which is done by applying Lemma 12 and proves (34).\nNumber of items satisfying (H3): From (33), (34), and the Markov inequality, we deduce that with probability at least 1 − exp ( − √ log(n/s) ) , the number of items that do not satisfy either (H1) or (H2) is\nless than s/3 when nD(α, p)− np̄log(np̄)3 ≥ log(n/s) + √ log(n/s), since\nE{The number of items that do not satisfy either (H1) or (H2)} s/3\n≤ n exp(−10ηnp̄L) + n exp\n(\n−nD(α, p) + np̄ 2 log(np̄)3\n)\ns/3\n≤ n s exp\n(\n−nD(α, p) + np̄ log(np̄)3\n)\n≤ exp ( − √ log(n/s) ) , (36)\nwhere we have used Lemma 9 for the last inequality. Lemma 15 allows us to complete the proof of Proposition.\nLemma 15 When the number of items that do not satisfy either (H1) or (H2) is less than s/3, |V\\H| ≤ s, with high probability.\nProof. Let e(S, S) = ∑\nv∈S e(S, S). Next we prove the following intermediate claim: there is no subset S ⊂ V such that e(S, S) ≥ s log(np̄)2 and |S| = s with high probability. For any subset S ∈ V such that |S| = s, by Markov inequality,\nP{e(S, S) ≥ s log(np̄)2} ≤ inf t≥0\nE[exp(e(S, S)t)]\nst log(np̄)2\n≤ inf t≥0\n∏s2/2 i=1 (1 + Lp̄ exp(t))\nst log(np̄)2\n≤ inf t≥0 exp\n(\ns2Lp̄\n2 exp(t)− st log(np̄)2\n)\n≤ exp ( −np̄s ( log np̄− sL 2n exp( np̄ log np̄ ) ) ) ≤ exp (\n−np̄s log np̄ 2\n)\n, (37)\nwhere, in the last two inequalities, we have set t = np̄lognp̄ and used the fact that: n s ≥ exp( np̄ lognp̄), which comes from the assumptions made in the theorem. Since the number of subsets S ⊂ V with size s is (n s ) ≤ (ens )s, from (37), we deduce:\nE[|{S : e(S, S) ≥ s log(np̄)2 and |S| = s}|] ≤ (en s )s exp\n(\n−np̄s log np̄ 2\n)\n= exp\n(\n−s(np̄ log np̄ 2 − log en s )\n)\n≤ exp ( −np̄s log np̄ 4 ) .\nTherefore, by Markov inequality, we can conclude that there is no S ⊂ V such that e(S, S) ≥ s log(np̄)2 and |S| = s with high probability.\nTo conclude the proof of the lemma, we build the following sequence of sets. Let Z1 denote the set of items that do not satisfy at least one of (H1) and (H2). Let {Z(t) ⊂ V}1≤t≤t⋆ be generated as follows:\n• Z(0) = Z1.\n• For t ≥ 1, Z(t) = Z(t− 1) ∪ {vt} if there exists vt ∈ V such that e(vt, Z(t− 1)) > 2 log(np̄)2 and vt /∈ Z(t− 1). If such an item does not exist, the sequence ends.\nThe sequence ends after the construction of Z(t⋆). We show that if we assume that the cardinality of items that do not satisfy (H3) is strictly larger than s/2, then one the set of the sequence {Z(t) ⊂ V}1≤t≤t⋆ contradicts the claim we just proved.\nAssume that the number of items do not satisfy (H3) is strictly larger than s/2, then these items will be at some point added to the sets Z(t), and by definition, each of these node contributes with more than 2 log(np̄)2 in e(Z(t), Z(t)). Hence if starting from Z1, we add s/2 items not satisfying (H3), we get a set Z(t) of cardinality less than s/3+ s/2 and such that e(Z(t), Z(t)) > s log(np̄)2. We can further add arbitrary items to Z(t) so that it becomes of cardinality s, and the obtained set contradicts the claim."
    }, {
      "heading" : "D.3.2 Proof of Proposition 14",
      "text" : "Recall that {S(t)j }1≤j≤K is the partition after the t-th improvement iteration. Also recall that with loss of generality, we assume that the set of misclassified items in H after the t-th step is E(t) =\n( ∪k(S(t)k \\ Vk) )\n∩ H (it should be defined through an appropriate permutation γ of {1, . . . ,K} by E(t) = (∪k(S(t)k \\ Vγ(k))) ∩ H , but we omit γ). With this notational convention, we can define E(t)jk = (S (t) j ∩ Vk) ∩ H and E(t) = ⋃ j,k:j 6=k E (t) jk . At each improvement step, items move to the most likely cluster (according to the log-likelihood defined in the SP algorithm). Thus, for all i,\n0 ≤ ∑\nj,k:j 6=k\n∑\nv∈E(t+1) jk\nK ∑\ni=1\nL ∑\nℓ=0\ne(v, S (t) i , ℓ) log\np̂(j, i, ℓ)\np̂(k, i, ℓ)\n≤ ∑\nj,k:j 6=k\n∑\nv∈E(t+1) jk\nK ∑\ni=1\nL ∑\nℓ=0\ne(v, S (t) i , ℓ) log\np(j, i, ℓ) p(k, i, ℓ) + |E(t+1)|(np̄)1−κ log(np̄)3 (38)\n≤ ∑\nj,k:j 6=k\n∑\nv∈E(t+1) jk\nK ∑\ni=1\nL ∑\nℓ=0\ne(v,Vi, ℓ) log p(j, i, ℓ)\np(k, i, ℓ)\n+ ∑ w∈E(t+1) e(w, E(t)) log(2η) + 2|E(t+1)|(np̄)1−κ log(np̄)3 (39)\n≤− np̄ log(np̄)4 |E(t+1)|+ ∑ w∈E(t+1) e(w, E(t), ℓ) log(2η) + 2|E(t+1)|(np̄)1−κ log(np̄)3 (40) ≤− np̄ log(np̄)4 |E(t+1)|+ √ |E(t)||E(t+1)|np̄ log np̄+ 3|E(t+1)|(np̄)1−κ log(np̄)3. (41)\nTherefore, from the above inequalities, we conclude that\n|E(t+1)| |E(t)| ≤ log(np̄)10 np̄ ≤ 1√ np̄ .\nNext we prove all the steps of the previous analysis. Proof of (38): From log(1 + x) ≤ x, when p(j, i, ℓ) − |p̂(j, i, ℓ) − p(j, i, ℓ)| > 0,\n∣ ∣ ∣ ∣ log p̂(j, i, ℓ)\np(j, i, ℓ)\n∣ ∣ ∣ ∣ ≤ |p̂(j, i, ℓ) − p(j, i, ℓ)| p(j, i, ℓ) − |p̂(j, i, ℓ) − p(j, i, ℓ)| .\nThus, we just provide an upper bound of |p̂(j, i, ℓ)− p(j, i, ℓ)| to show (38). From the triangle inequality,\n|p̂(j, i, ℓ) − p(j, i, ℓ)|\n=\n∣ ∣ ∣e(S (0) i , S (0) j , ℓ)− p(j, i, ℓ)|S (0) i ||S (0) j | ∣ ∣ ∣\n|S(0)i ||S (0) j |\n≤\n∣ ∣ ∣e(S (0) i , S (0) j , ℓ)− E[e(S (0) i , S (0) j , ℓ)] ∣ ∣ ∣ + ∣ ∣ ∣E[e(S (0) i , S (0) j , ℓ)]− p(j, i, ℓ)|S (0) i ||S (0) j | ∣ ∣ ∣\n|S(0)i ||S (0) j |\n. (42)\nWe first find an upper bound of ∣ ∣ ∣ e(S\n(0) i , S (0) j , ℓ)− E[e(S (0) i , S (0) j , ℓ)]\n∣ ∣ ∣ . Let S be the of partitions such\nthat ∣\n∣∪Kk=1Vk \\ Sk ∣\n∣ ≤ ξ = O ( log(np̄)2\np̄\n)\nfor all {Sk}1≤k≤K ∈ S.\nThen,\n|S| ≤ ( n\nξ\n)\nKξ\n≤ ( ken\nξ\n)ξ\n= exp\n(\nO\n(\nlog(np̄)3\np̄\n))\n. (43)\nFor all {Sk}1≤k≤K ∈ S and for all ℓ ≥ 1 and 1 ≤ i, j ≤ K , e(Si, Sj , ℓ) is the sum of |Si||Sj | (or |Si| 2\n2 when i = j) independent Bernoulli random variables. Since the variance of e(Si, Sj , ℓ) is always less than n2p̄, by Chernoff inequality (e.g., Theorem 2.1.3 in [22]), with probability at least 1− exp ( −Θ ( log(np̄)4\np̄\n))\n,\n|e(Si, Sj , ℓ)− E[e(Si, Sj , ℓ)]| ≤ n log(np̄)2 for all i, j, ℓ. (44)\nFrom (43) and (44), with high probability,\n|e(Si, Sj , ℓ)− E[e(Si, Sj , ℓ)]| ≤ n log(np̄)2 for all i, j, ℓ and {Sk}1≤k≤K ∈ S.\nSince {S(0)k }1≤k≤K ∈ S , from the above inequality, ∣\n∣ ∣e(S (0) i , S (0) j , ℓ)− E[e(S (0) i , S (0) j , ℓ)]\n∣ ∣ ∣ ≤ n log(np̄)2 for all i, j, ℓ. (45)\nWe now devote to the remaining part of (42). Since |E(0)| = O ( log(np̄)2\np̄\n)\nfrom Theorem 6,\n∣ ∣ ∣ E[e(S\n(0) i , S (0) j , ℓ)]− |S (0) i ||S (0) j |p(i, j, ℓ)\n∣ ∣ ∣ ≤ η|E(0)|np(i, j, ℓ) = O(n log(np̄)2). (46)\nFrom (42), (45) and (46), with high probability,\n|p̂(j, i, ℓ) − p(j, i, ℓ)| = O(log(np̄)2/n) for all i, j, ℓ,\nwhich implies that: ∣\n∣ ∣ ∣\nlog p̂(j, i, ℓ)\np(j, i, ℓ)\n∣ ∣ ∣ ∣ ≤ |p̂(j, i, ℓ) − p(j, i, ℓ)| p(j, i, ℓ) − |p̂(j, i, ℓ) − p(j, i, ℓ)| = O ( log(np̄)2 np(j, i, ℓ) ) for all i, j, ℓ.\nSince e(v, S(t)i , ℓ) ≤ e(v,V) ≤ 10ηnp̄L from (H1) and np(j, i, ℓ) ≥ (np̄)κ from (A3), we deduce that, for all v ∈ Γ and i, j, k,\nL ∑\nℓ=0\ne(v, S (t) i , ℓ)\n∣ ∣ ∣ ∣ log p̂(j, i, ℓ) p̂(k, i, ℓ) − log p(j, i, ℓ) p(k, i, ℓ) ∣ ∣ ∣ ∣ = O ( log(np̄)2(np̄)1−κ ) .\nProof of (39): Since log p(j,i,0)p(k,i,0) = O(p̄) for all i, j, k and |E(t)| = O(log(np̄)2/p̄),\nK ∑\ni=1\nL ∑\nℓ=0\ne(v, S (t) i , ℓ) log\np(j, i, ℓ)\np(k, i, ℓ)\n= K ∑\ni=1\n(\n|S(t)i | log p(j, i, 0)\np(k, i, 0) +\nL ∑\nℓ=1\ne(v, S (t) i , ℓ) log\np(j, i, ℓ)p(k, i, 0)\np(k, i, ℓ)p(j, i, 0)\n)\n≤ K ∑\ni=1\n(\n|Vi| log p(j, i, 0)\np(k, i, 0) +\nL ∑\nℓ=1\ne(v, S (t) i , ℓ) log\np(j, i, ℓ)p(k, i, 0)\np(k, i, ℓ)p(j, i, 0)\n)\n+ log(np̄)3\n≤ K ∑\ni=1\nL ∑\nℓ=0\ne(v,Vi, ℓ) log p(j, i, ℓ)\np(k, i, ℓ) +\nK ∑\ni=1\nL ∑\nℓ=1\ne(v,Vi \\ S(t)i , ℓ) log(2η) + log(np̄)3\n= K ∑\ni=1\nL ∑\nℓ=0\ne(v,Vi, ℓ) log p(j, i, ℓ)\np(k, i, ℓ) +\n( e(v, E(t)) + e(v,V \\H) ) log(2η) + log(np̄)3\n≤ K ∑\ni=1\nL ∑\nℓ=0\ne(v,Vi, ℓ) log p(j, i, ℓ)\np(k, i, ℓ) + log(2η)e(v, E(t)) + 2 log(np̄)3,\nwhere the last inequality stems from (H3), i.e., from e(v,V \\H) ≤ 2 log(np̄)2 when v ∈ H .\nProof of (40): Since E(t+1) ⊂ H and every v ∈ H satisfies (H2), every v ∈ E(i+1)jk satisfies:\nK ∑\ni=1\nL ∑\nℓ=0\ne(v,Vi, ℓ) log p(j, i, ℓ) p(k, i, ℓ) ≤ − np̄ log(np̄)4 .\nProof of (41): Let Γ̄ = {v : e(v,V) ≤ 10ηnp̄L} and Aℓ Γ̄ be the trimmed matrix of Aℓ whose elements in rows and columns corresponding to w /∈ Γ̄ are set to 0. Γ̄ is the set of all items that satisfy (H1) and H ⊂ Γ̄. Let XΓ̄ = ∑L ℓ=1(A ℓ Γ̄ −M ℓ Γ̄ ). We have:\n∑\nv∈E(t+1) (e(v, E(t))− E[e(v, E(t))]) ≤ 1TE(t) ·XΓ̄ · 1E(t+1) ,\nwhere 1S is the vector whose v-th component is equal to 1 if v ∈ S and to 0 otherwise. Since E[e(v, E(t))] ≤ p̄L|E(t)| and ‖XΓ̄‖2 ≤ √ np̄ log np̄ with high probability from Lemma 11,\n∑\nv∈E(t+1) e(v, E(t)) =\n∑\nv∈E(t+1)\n( e(v, E(t))− E[e(v, E(t))] ) + p̄L|E(t)||E(t+1)|\n≤ ‖1TE(t) ·XΓ̄ · 1E(t+1)‖2 + |E (t+1)| log(np̄) ≤ ‖1TE(t)‖2‖XΓ̄‖2‖1E(t+1)‖2 + |E (t+1)| log(np̄) ≤ √ |E(t)||E(t+1)|np̄ log(np̄) + |E(t+1)| log(np̄)."
    }, {
      "heading" : "E Proof of Theorem 3",
      "text" : "The positive result is obtained by applying Theorem 2 to s = 12 . When lim infn→∞ nD(α,p) log(n) ≥ 1, SP algorithm find clusters exactly with high probability. Thus, it suffices to show the negative result. We prove the negative part by contradiction. Consider a maximum a posteriori (MAP) estimation with full parameter information. When we observe a labeld information A, the MAP estimates the clusters as follows:\n(Ŝk)k=1,...,k = arg max (Sk)k=1,..,K\nP {(Sk)k=1,..,K|α, p,K,A} . (47)\nLet εMAP denote the number of misclassified nodes by the MAP estimation. From the definition of the MAP estimation, for any clustring algorithm π, we have\nP {επ ≥ 1} ≥ P { εMAP ≥ 1 } . (48)\nThus, in what follows, we show that when lim infn→∞ nD(α,p) log(n) < 1, the MAP estimation is failed to find the exact clusters with high probability. We start by Lemma 16 which finds a large deviation inequality for edge connections.\nLemma 16 Let x ∈ ZK×(L+1) whose (k, ℓ+ 1) element is xk,ℓ, and such that ∑L ℓ=0 xk,ℓ = |Vk| for all"
    }, {
      "heading" : "1 ≤ k ≤ K , ∑Lℓ=1 xk,ℓ = Θ(np̄) for all k, and",
      "text" : "K ∑\nk=1\n|Vk|KL(µ(v,Vk), p(i, k)) = nD when e(v) = x,\nwhere we denote by e(v) the K × (L+ 1) matrix whose (k, ℓ+ 1) element is e(v,Vk, ℓ). Then,\nlog (P {e(v) = x}) ≥ −nD(1 + o(1)) when v ∈ Vi and D = Ω(p̄).\nProof. When using the convention ∑b\nℓ=a as 0 when a > b, we have\nlog (P {e(v) = x})\n=\nK ∑\nk=1\n((\n|Vk| − L ∑\nℓ=1\nxk,ℓ\n)\nlog (p(i, k, 0)) +\nL ∑\nℓ=1\nlog\n(\np(i, k, ℓ)xk,ℓ (|Vk| − ∑ℓ−1 m=1 xk,m\nxk,ℓ\n)\n))\n≥ K ∑\nk=1\n\n\n(\n|Vk| − L ∑\nℓ=1\nxk,ℓ\n)\nlog (p(i, k, 0)) +\nL ∑\nℓ=1\nlog\n\np(i, k, ℓ)xk,ℓ\n(\n|Vk| − ∑L m=1 xk,m\n)xk,ℓ\nxk,ℓ!\n\n\n\n\n(a) ≥ K ∑\nk=1\n\n\n(\n|Vk| − L ∑\nℓ=1\nxk,ℓ\n)\nlog (p(i, k, 0)) +\nL ∑\nℓ=1\nlog\n\n\n\n p(i, k, ℓ)e xk,ℓ\n|Vk|− ∑L m=1 xk,m\n\n\nxk,ℓ\n1\ne √ xk,ℓ\n\n\n\n\n(b) =\nK ∑\nk=1\n\n\n(\n|Vk| − L ∑\nℓ=1\nxk,ℓ\n)\nlog (p(i, k, 0)) + L ∑\nℓ=1\nlog\n\n p(i, k, ℓ)e xk,ℓ\n|Vk|− ∑L m=1 xk,m\n\n\nxk,ℓ \n− o ( K ∑\nk=1\nL ∑\nℓ=1\nxk,ℓ\n)\n(c) ≥ K ∑\nk=1\n(\n|Vk| − L ∑\nℓ=1\nxk,ℓ\n)\nlog\n(\np(i, k, 0)\n(\n1 +\n∑L ℓ=1 xk,ℓ\n|Vk| − ∑L ℓ=1 xk,ℓ\n))\n+ K ∑\nk=1\n\n\nL ∑\nℓ=1\nxk,ℓ log\n\n\np(i, k, ℓ) xk,ℓ\n|Vk|− ∑L m=1 xk,m\n\n\n − o ( K ∑\nk=1\nL ∑\nℓ=1\nxk,ℓ\n)\n=\nK ∑\nk=1\n(\n|Vk| − L ∑\nℓ=1\nxk,ℓ\n)\nlog\n(\np(i, k, 0)\n(|Vk| − ∑L ℓ=1 xk,ℓ)/|Vk|\n)\n+\nK ∑\nk=1\n(\nL ∑\nℓ=1\nxk,ℓ log\n(\np(i, k, ℓ) xk,ℓ/|Vk|\n)\n)\n+ K ∑\nk=1\n(\nL ∑\nℓ=1\nxk,ℓ log\n(\n|Vk| − ∑L\nm=1 xk,m |Vk|\n)) − o ( K ∑\nk=1\nL ∑\nℓ=1\nxk,ℓ\n)\n(d) ≥ − nD − o ( K ∑\nk=1\nL ∑\nℓ=1\nxk,ℓ\n)\n(e) ≥ − nD(1 + o(1)),\nwhere (a) is obtained from n! ≤ e√n (\nn e )n ; (b) stems from ∑K k=1 ∑L ℓ=1 xk,ℓ = ω(1); to derive (c), we\nuse e ∑L ℓ=1 xk,ℓ ≥ ( 1 + ∑L ℓ=1 xk,ℓ\n|Vk|− ∑L ℓ=1 xk,ℓ\n)|Vk|− ∑L\nℓ=1 xk,ℓ since e ≥ (1 + 1/x)x for all x > 0; to prove (d),\nwe use the definition of x and the following inequality:\nL ∑\nℓ=1\nxk,ℓ log\n(\n|Vk| |Vk| − ∑L m=1 xk,m\n)\n=\n(\n∑L ℓ=1 xk,ℓ\n)2\n|Vk| − ∑L ℓ=1 xk,ℓ (1 + o(1)) = o(\nL ∑\nℓ=1\nxk,ℓ);\nand (e) is obtained from the definition of x that ∑L\nℓ=1 xk,ℓ = Θ(np̄) for all k.\nAssume that there exists a constant η > 0 such that nD(α,p)log(n) < 1− η. Let (i⋆, j⋆) = argmini,j:i<j DL+(p(i), p(j)) (i.e., it is the hardest case to discriminate cluster i⋆ and cluster j⋆). When n → ∞, one can easily check using the continuity of the KL divergence that there\nexists x⋆ such that when e(v) = x⋆,\nη 2 log n+\nK ∑\nk=1\n|Vk|KL(µ(v,Vk), p(j⋆, k)) < K ∑\nk=1\n|Vk|KL(µ(v,Vk), p(i⋆, k)) and (49)\nK ∑\nk=1\n|Vk|KL(µ(v,Vk), p(i⋆, k)) ≤ (1− η/2) log(n). (50)\nLet Ve = {v ∈ Vi⋆ : e(v) = x⋆}. From (50) and Lemma 16, E[|Ve|] ≥ nη/4. Thus, from Markov inequality, with probability at least 1− n−η/4, Ve is not empty (i.e., |Ve| ≥ 1).\nLet v⋆ ∈ Ve be a node in Ve. We denote by Φ the original partition and define a slightly modified partition Ψ as follows:\nV̂i⋆ = Vi⋆ \\ {i⋆}, V̂j⋆ = Vj⋆ ∪ {i⋆}, and V̂k = Vk otherwise.\nThen, Ψ is a more likely partition than Φ from (49), i.e.,\nP {Φ|α, p,K,A} ≥ P {Ψ|α, p,K,A} (51)\nwhich means that the MAP estimator does not select the exact partition when Ve is not empty. Therefore, from (48), every clustering algorithm π has the error probability that\nE {επ ≥ 1} ≥ 1− n−η/4\nwhen there exists a constant η > 0 such that nD(α,p)log(n) < 1− η."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "We consider the problem of community detection or clustering in the labeled Stochastic Block Model (LSBM) with a finite number K of clusters of sizes linearly growing with the global population of items n. Every pair of items is labeled independently at random, and label l appears with probability p(i, j, l) between two items in clusters indexed by i and j, respectively. The objective is to reconstruct the clusters from the observation of these random labels. Clustering under the SBM and their extensions has attracted much attention recently. Most existing work aimed at characterizing the set of parameters such that it is possible to infer clusters either positively correlated with the true clusters, or with a vanishing proportion of misclassified items, or exactly matching the true clusters. We find the set of parameters such that there exists a clustering algorithm with at most s misclassified items in average under the general LSBM and for any s = o(n), which solves one open problem raised in [2]. We further develop an algorithm, based on simple spectral methods, that achieves this fundamental performance limit within O(npolylog(n)) computations and without the a-priori knowledge of the model parameters.",
    "creator" : "LaTeX with hyperref package"
  }
}