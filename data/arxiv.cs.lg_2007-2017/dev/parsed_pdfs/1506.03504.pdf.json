{
  "name" : "1506.03504.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Data Generation as Sequential Decision Making",
    "authors" : [ "Philip Bachman", "Doina Precup" ],
    "emails" : [ "phil.bachman@gmail.com", "dprecup@cs.mcgill.ca" ],
    "sections" : [ {
      "heading" : null,
      "text" : "We connect a broad class of generative models through their shared reliance on sequential decision making. We show how changes motivated by our point of view can improve an already-strong model, and then explore this idea further in the context of data imputation – perhaps the simplest setting in which to investigate the relation between unconditional and conditional generative modelling. We formulate data imputation as an MDP and develop models capable of representing effective policies for it. We construct our models using neural networks and train them using a form of guided policy search [9]. Through empirical tests, we show that our approach can learn effective policies for imputation problems of varying difficulty and across multiple datasets."
    }, {
      "heading" : "1 Introduction",
      "text" : "Directed generative models are naturally interpreted as specifying sequential procedures for generating data. Traditionally, we think of this process as sampling, but one could view this process as a sequence of decisions for how to set the variables at the next node, conditioned on the settings of the parents, hence, procedurally generating data from the distribution. The large body of existing work on reinforcement learning provides a powerful set of tools with which to address such sequential decision making problems. Our objective, in this paper, is to encourage a more complete transfer of these tools to the extended processes currently driving advances in generative modelling.\nFirst, we reinterpret several recent generative models as sequential decision making processes, and show how changes inspired by our point of view can improve the performance of the LSTM-based model introduced in [3]. We then explore the connections between directed generative models and reinforcement learning more fully by developing an approach to training policies for sequential data imputation. We base our approach on formulating data imputation as a finite-horizon Markov Decision Process which can also be interpreted as a deep, directed graphical model. As the data to impute grows to cover the full observation, we show that our approach successfully transitions to classical (i.e. unconditional) generative modelling.\nWe propose two policy representations for the sequential imputation MDP. The first extends the model in [3] by adding an explicit feedback loop into the generative process, and the second implements the MDP more literally. We train our models/policies using techniques motivated by Guided Policy Search [9, 10, 11, 8]. We examine the qualitative and quantitative performance of our models across imputation problems covering a range of difficulties (i.e. different amounts of data to impute and different “missingness mechanisms”), and across multiple datasets. Given the relative paucity of existing approaches to the general imputation problem, we compare our models to each other and to two simple baselines. Our models significantly outperform the baselines. We propose that developing better models and better benchmarks for imputation is a worthwhile endeavor, as imputation includes both classification and generative modelling as special cases.\nar X\niv :1\n50 6.\n03 50\n4v 1\n[ cs\n.L G\n] 1\n0 Ju\nn 20\n15"
    }, {
      "heading" : "2 Directed Generative Models as Sequential Decision Processes",
      "text" : "Directed generative models have gained in popularity relative to their undirected counter-parts [7, 14, 12, 4, 6, 16, 15] (etc.). Reasons include: the development of efficient methods for training them, the ease of sampling from them, and the frequent availability of efficient bounds on their loglikelihoods. The rapid growth in available computing power compounds these benefits. While data generation is typically viewed as a process of sampling from a sequence of variables in the model, in the context of models with latent variables, one can also think of setting the latent variables to appropriate values, in a sequence of decisions conditioned on preceding decisions. In the rest of the section we develop this interpretation of directed models as sequential decision processes."
    }, {
      "heading" : "2.1 Deep AutoRegressive Networks",
      "text" : "The deep autoregressive networks investigated in [4] define distributions of the following form:\np(x) = ∑ z p(x|z)p(z), with p(z) = p∅(z0) T−1∏ t=0 pt(zt+1|z0, ..., zt) (1)\nin which x indicates a generated observation and z0, ..., zT represent the T + 1 latent variables in the model. The distribution p(x|z) may also be factored similarly to p(z). The factored form of p(z) in Eqn. 1 can represent arbitrary distributions over the latent variables, and work in [4] mainly concerned approaches to parameterizing the conditionals pt(zt|z0, ..., zt−1) that restricted representational power in exchange for computational tractability. To appreciate the generality of Eqn. 1, one might consider using zt that are univariate, multivariate, structured, etc. Constructing a model via this sequential factorization permits a natural approach to training it."
    }, {
      "heading" : "2.2 Generalized Guided Policy Search",
      "text" : "We adopt a broader interpretation of Guided Policy Search than one might initially take from, e.g., [9, 10, 11, 8]. We expand guided policy search to include any optimization of the general form:\nminimize p,q E iq∼Iq E ip∼Ip\n[ E\nτ∼q(τ |iq,ip) [`(τ, iq, ip)] + λ div (q(τ |iq, ip), p(τ |ip))\n] (2)\nin which p indicates the primary policy, q indicates the guide policy, Iq indicates a distribution over information available only to q, Ip indicates a distribution over information available to both p and q, `(τ, iq, ip) computes the cost of trajectory τ in the context of iq/ip, and div(q(τ |iq, ip), p(τ |ip)) computes some measure of dissimilarity between the trajectory distributions generated by p/q. As λ ∈ R+ goes to infinity, Eqn. 2 enforces the constraint p(τ |ip) = q(τ |iq, ip), ∀τ, ip, iq . Additional terms for controlling, e.g., the entropy of p/q can also be added. The power of this approach stems from two main points: the guide policy q can use information iq that is not available to the primary policy p, and the primary policy need only be trained to minimize the dissimilarity term.\nFor example, directed models structured as in Eqn. 1 can be interpreted as specifying policies for a finite-horizon Markov decision process whose terminal state distribution encodes p(x). In this MDP, the state at time t is given by zt ∈ Zt for 0 ≤ t ≤ T and x ∈ X for t = T +1. The (non-stationary) policy pt at time t places a distribution over Zt+1, or over X at time t = T , and conditions on all preceding decisions. I.e., the policy can be written as: pt(zt+1|z0, ..., zt) for 0 ≤ t < T and as pk(x|z0, ..., zT ) for t = T . The initial state z0 ∈ Z0 is drawn from p∅(z0). The authors of [4] train deep autoregressive networks by maximizing a variational lower-bound on the training set log-likelihood. To do this, they introduce a variational distribution q which provides q∅(z0|x∗) and qt(zt+1|z0, ..., zt, x∗) for 0 ≤ t < T , with the final step qT (x|z0, ..., zT , x∗) given by a Dirac-delta at x∗. Given these definitions, the training in [4] can be interpreted as an application of guided policy search to the MDP described in the previous paragraph. Specifically, the variational distribution q provides a guide policy q(τ |x∗) over trajectories τ = {z0, ..., zT , x∗}:\nq(τ |x∗) = qT (x|z0, ..., zT , x∗)q∅(z0|x∗) T−1∏ t=0 qt(zt+1|z0, ..., zt, x∗) (3)\nThe primary policy p generates trajectories distributed according to:\np(τ) = pt(x|z0, ..., zT )p∅(z0) T−1∏ t=0 pt(zt+1|z0, ..., zt) (4)\nwhich does not depend on x∗. In this case, x∗ corresponds to the guide-only information iq ∼ Iq in Eqn. 2. Mirroring the form of Eqn. 2, we can rewrite the variational optimization as:\nminimize p,q E x∗∼Dx\n[ E\nτ∼q(τ |x∗) [`(τ, x∗)] + KL(q || p)\n] (5)\nwhere `(τ, x∗) ≡ 0 and Dx indicates the target distribution for the terminal state of the primary policy p1. When expanded, the KL term in Eqn. 5 becomes:\nKL(q || p) = E τ∼q(τ |x∗)\n[ log\nq∅(z0|x∗) p∅(z0) + T−1∑ t=0 log qt(zt+1|z0, ..., zt, x∗) pt(zt+1|z0, ..., zt) − log pT (x∗|z0, ..., zT ) ] (6)\nThus, the variational approach used in [4] for training directed generative models can be interpreted as a form of generalized guided policy search. As the form in Eqn. 1 is broad enough to represent any finite directed generative model, the preceding derivation extends to all models we consider."
    }, {
      "heading" : "2.3 Time-reversible Stochastic Processes",
      "text" : "One can simplify Eqn. 1 by assuming suitable relationships among X and Z0, ...,ZT . E.g., the authors of [16] proposed a model in which Zt ≡ X for all t and p∅(x0) is a Gaussian. Under these assumptions we write the model as: p(xT ) = ∑ x0,...,xT−1 pT (xT |xT−1)p∅(x0) ∏T−1 t=1 pt(xt|xt−1), where p(xT ) indicates the terminal state distribution of the non-stationary, finite-horizon Markov process determined by {p∅(x0), p1(x1|x0), ..., pT (xT |xT−1)}. Note that, throughout this paper, we (ab)use sums over latent variables and trajectories which could/should be written as integrals.\nThe authors of [16] observed that, for any reasonably smooth target distribution Dx and sufficiently large T , one can define a (reverse-time) stochastic process qt(xt−1|xt) with simple, time-invariant dynamics that transforms the distribution qT (xT ) = Dx into the Gaussian distribution p∅(x0). We can write the distribution of q as: q∅(x0) = ∑ x1,...,xT q1(x0|x1)Dx(xT ) ∏T t=2 qt(xt−1|xt) ≈ p∅(x0). Next, we define q(τ) as the distribution over trajectories τ = {x0, ..., xT } generated by executing the reverse-time process determined by {q1(x0|x1), ..., qT (xT−1|xT ),Dx(xT )}. We define p(τ) as the distribution over trajectories generated by executing the forward-time process determined by {p∅(x0), p1(x1|x0), ...pT (xT |xT−1)}. With these definitions, the training in [16] performs guided policy search, using guide trajectories sampled from q. I.e., p/q are trained for:\nminimize p,q E τ∼q(τ)\n[ − log p∅(x0) +\nT∑ t=1 log qt(xt−1|xt) pt(xt|xt−1)\n] (7)\nIf the log-densities in Eqn. 7 are tractable, which can be readily obtained by construction, then this minimization can be done using basic Monte-Carlo. If, as in [16], the reverse-time process q is not trained, then Eqn. 7 simplifies to: minimizep Eq(τ) [ − log p∅(x0)− ∑T t=1 log pt(xt|xt−1) ] .\nThis trick for generating guide trajectories exhibiting a particular distribution over terminal states xT – i.e. running dynamics backwards in time starting from xT ∼ Dx – may prove useful in settings other than those considered in [16]. The appendix provides further discussion of the material in this subsection. In particular, we derive Eqn. 7 as an upper-bound on EDx [− log p(xT )] which can be interpreted as a KL divergence between the trajectory distributions generated by p/q.\n1As defined, this MDP involves no reward, which is peculiar. We could pull the − log pT (x∗|z0, ..., zT ) term from the KL and put it in the cost `(τ, x∗), but we prefer the “cost free” formulation for its elegance. We also abuse notation by defining KL(δ(x = x∗) || p(x)) as − log p(x∗)."
    }, {
      "heading" : "2.4 Learning Generative Stochastic Processes with LSTMs",
      "text" : "The authors of [3] introduced a model capable of learning temporally-deep sequential generative processes. We interpret their model as defining a primary policy p which generates trajectories τ = {z0, ..., zT , x} according to the following distribution:\np(τ) = p(x|sθ(τ<x))p∅(z0) T∏ t=1 pt(zt), with τ<x = {z0, ..., zT } (8)\nin which τ<x indicates a latent trajectory and sθ(τ<x) indicates a state trajectory {s0, ..., sT } computed recursively from τ<x using the update st ← fθ(st−1, zt) for t > 0, with s0 given by a constant. Each state st = [ht; vt] represents the joint hidden/visible state ht/vt of an LSTM and fθ(state, input) computes a standard LSTM update2. The authors of [3] used a Gaussian distribution N (0, I) for all pt(zt). The output distribution p(x|sθ(τ<x)) was defined as p(x|cT ) with cT = c0 + ∑T t=1 ωθ(vt), where c0 indicates a constant and ωθ(vt) indicates, e.g., an affine transform of the LSTM visible state vt. p is governed by parameters θ which affect fθ, ωθ, and s0/c0.\nTo train p, the authors of [3] introduced a guide policy q with trajectory distribution:\nq(τ |x∗) = q(x|sφ(τ<x), x∗)q∅(z0|x∗) T∏ t=1 qt(zt|s̃t, x∗), with τ<x = {z0, ..., zT } (9)\nin which sφ(τ<x) indicates a state trajectory {s̃0, ..., s̃T } computed recursively from τ<x using the guide policy’s state update s̃t = fφ(s̃t−1, gφ(sθ(τ<t), x∗)). In this update s̃t−1 is the previous guide state and gφ(sθ(τ<t), x∗) is a deterministic function of the target sample x∗ and the partial primary state trajectory sθ(τ<t) = {s0, ..., st−1}, which can be computed from τ<t = {z0, ..., zt−1} using the primary policy’s state update st = fθ(st−1, zt). The output distribution q(x|sφ(τ<x), x∗) is defined as a Dirac-delta at x∗3. Each step distribution qt(zt|s̃t, x∗) is a diagonal Gaussian distribution with means and log-variances given by an affine function Lφ(ṽt) of the LSTM visible state ṽt, and q∅(z0) is defined as equivalent to p∅(z0). The guide policy q is governed by its parameters φ, which affect the state updates fφ(s̃t−1, gφ(sθ(τ<t))) and the step distributions qt(zt|s̃t, x∗). The function gφ(sθ(τ<t), x ∗) encapsulates the “read” operation of the encoder network in [3].\nUsing our definitions for the primary/guide policies p/q, the training objective in [3] is given by:\nminimize p,q E x∗∼Dx E τ∼q(τ |x∗)\n[ log\nq∅(z0|x∗) p∅(z0) + T∑ t=1 log qt(zt|s̃t, x∗) pt(zt) − log p(x∗|s(τ<x))\n] (10)\nwhich can also be written more succinctly as Ex∗∼Dx KL(q || p). This objective upper-bounds Ex∗∼Dx [− log p(x∗)], where p(x) = ∑ τ<x p(x|sθ(τ<x))p(τ<x) using definitions from Eqn. 8."
    }, {
      "heading" : "2.5 Extending the LSTM-based Model",
      "text" : "We now change the primary policy p in Eqn. 8 to: p(τ) = p(x|sθ(τ<x))p∅(z0) ∏T t=1 pt(zt|st−1), in which we compute st−1 ∈ sθ(τ<x) recursively as described for Eqn. 8. We define pt(zt|st−1) as a diagonal Gaussian distribution with means and log-variances given by an affine function Lθ(vt−1) of LSTM visible state vt−1, and we define p∅(z0) asN (0, I). We set s0 using s0 ← fθ(z0), where θ indicates parameters governing p. Intuitively, our changes make the “decisions” of the model more explicit by conditioning zt on st−1, and upgrade it to an infinite mixture by adding a distribution over its initial state s0. We represent fθ(z0) using a feedforward network with one hidden layer of tanh units. We also consider an alternate form for p(x|sθ(τ<x)), given by p(x|cT ), where cT = Lθ(hT ) indicates an affine transformation of the hidden part of the final LSTM state sT ∈ sθ(τ<x). This turns ht into a working memory, in which the model constructs an observation sequentially.\nWe train our model to optimize the objective:\nminimize p,q E x∗∼Dx E τ∼q(τ |x∗)\n[ log\nq∅(z0|x∗) p∅(z0) + T∑ t=1 log qt(zt|s̃t, x∗) pt(zt|st−1) − log p(x∗|s(τ<x))\n] (11)\n2For those unfamiliar with LSTMs, a good introduction can be found in [2]. We use LSTMs including input gates, forget gates, output gates, and peephole connections for all tests presented in this paper.\n3A possible improvement to this model that we leave for future work is to relax this assumption.\nwhere we now have to care about pt(zt|st−1), p∅(z0), and q∅(z0|x∗), which could be treated as constants in the model from [3]. We define q∅(z0|x∗) as a diagonal Gaussian distribution whose means and log-variances are given by gφ(x∗), which we represent using a feedforward network with one hidden layer of tanh units.\nWhen trained for the binarized MNIST benchmark examined in [3], our extended model scores a negative log-likelihood of 85.3 on the test set4. For comparison, the score reported in [3] was 87.4. After fine-tuning the variational distribution (i.e. q) on the test set, our model’s score improved to 84.6, which is quite strong considering it is an upper-bound. For comparison, see the best upperbound reported for this benchmark in [15], which was 85.1. When the model is modified to use the alternate output distribution p(x|Lθ(hT )), the raw/fine-tuned test scores were 85.7/85.1. Fig. 1 shows samples from our model. We provide more details in the appendix, and model/test code is available at http://github.com/Philip-Bachman/Sequential-Generation."
    }, {
      "heading" : "3 Developing Models for Sequential Imputation",
      "text" : "ct = c0 +\n∑t\nt′=1 Lθ(vt′). The right block is analogous, for a model using ct = Lθ(ht), as described in Sec. 2.5\nImputation concerns the density p(x/|x.), where x = [x/;x.] indicates a complete observation with known values x. and missing values x/. By expanding x/ to cover all of x, one arrives at standard generative modelling. By shrinking x/ to cover just a single element of x, one arrives at standard classification/regression. We define a mask m ∈ M as a partition of x into x//x.5. Given distribution Dm over m ∈ M and distribution Dx over x ∈ X , the objective for imputation via p(x/|x.) is:\nminimize p E x∼Dx E m∼Dx [− log p(x/|x.)] (12)\nWe define an MDP with finite-horizon T , for which guided policy search minimizes a bound on Eqn. 12. This MDP is defined by mask distribution Dm, complete observation distribution Dx, and the state spaces {Z0, ...,ZT } associated with each step. Together, Dm and Dx define a joint distribution over initial states and rewards in the MDP. For the trial determined by x∗ ∼ Dx andm ∼ Dm, the initial state z0 ∼ p(z0|x.∗) is selected by the policy p based on the known values x.∗, and the reward r(τ, x∗,m) given to trajectory τ = {z0, ..., zT } in the context (x∗,m) is defined as the log-likelihood assigned to the missing values x/∗ by the policy’s imputation distribution p(x\n/|τ, x.∗). We consider a policy p(zt|z0, ..., zt−1, x.∗) which defines a distribution p(τ |x.∗) over trajectories τ = {z0, ..., zT }, as given by p(τ |x.∗) = p(z0|x.∗) ∏T t=1 p(zt|z0, ..., zt−1, x.∗). Here, x.∗ is determined by the x∗ and m sampled for the current trial. The policy p can’t observe the missing values x/∗. With these definitions, an optimal policy for the imputation MDP maximizes the expected reward:\nmaximize p E x∗∼Dx E m∼Dm E τ∼p(τ |x.∗) [log p(x/∗|τ, x.∗)] (13)\nwhich maximizes the expected log-likelihood of producing a correct imputation on a given trial. The appendix discusses how this reward maximization relates to the variational free-energy.\nAs in Sec. 2, we train p by introducing a guide policy q that uses additional information, e.g. x/∗. q produces trajectories from: q(τ |x/∗, x.∗) = q(z0|x/∗, x.∗) ∏T t=1 q(zt|z0, ..., zt−1, x/∗, x.∗). Given models for p and q, we use guided policy search to maximize the reward defined in Eqn. 13, i.e.:\nminimize p,q E x∗∼Dx E m∼Dm\n[ E\nτ∼q(τ |iq,ip) [− log q(x/∗|τ, iq, ip)] + KL(q(τ |iq, ip)||p(τ |ip))\n] (14)\nwhere we define iq ≡ x/∗, ip ≡ x.∗, and q(x/∗|τ, iq, ip) = p(x/∗|τ, ip).\n4Using train/validate/test splits from: http://www.cs.toronto.edu/˜larocheh/public/datasets/binarized_mnist 5In our setting, there will always be only one mask m in the scope of a statement, and the //. superscripts\nrefer to partitioning x using that m. We distinguish between observations to partition using subscripts."
    }, {
      "heading" : "3.1 A Direct Representation for Sequential Imputation Policies",
      "text" : "We define the imputation-trajectory cτ = {c0, ..., cT , cT+1}, where the partial imputation ct ∈ X is a function wθ(τ<t) of τ<t = {z0, ..., zt−1}. We consider two forms for wθ(τ<t), mirroring those described in Sec. 2 for the LSTM-based generative model. Both forms can be written generally as wθ(τ<t) = [c / <t;x . ∗]. In the first form, c<t = c∅ + ∑t−1 t′=0 ωθ(zt′), where c∅ is a bias controlled by θ. In the second form, c<t = ωθ(zt−1). For step t = 0, we define c0 = [c/∅;x . ∗]. I.e., the partial imputation ct encodes the guess for the target missing values x/∗ immediately prior to selecting step zt. We assume an independent Bernoulli model for x, with an imputation distribution p(x/∗|τ) = σ(c/T+1). For our tests, we use a feedforward network with two ReLU layers to represent ωθ.\nWe construct a primary policy p for the imputation MDP that generates trajectories through a state space Z . At each step t ≥ 0, we compute p(zt|z0, ..., zt−1, ct) using pθ(zt|σ(ct)), which provides the means and log-variances for a diagonal Gaussian distribution over Z . In our tests, we use a feedforward network with two ReLU layers to represent pθ(zt|σ(ct)). The step model pθ(zt|σ(ct)) and the imputation constructor wθ(τ<t) fully determine the behaviour of the primary policy.\nWe construct a guide policy q similarly to p. The guide policy shares the imputation constructor wθ(τ<t) with the primary policy. The additional information incorporated by the guide policy is ĉt = [x / ∗;x . ∗] − [σ(c/t );x.∗], which gives the gradient of the imputation log-likelihood with respect to the trajectory τ<t followed up to step t− 1. We compute the guide policy using qφ(zt|σ(ct), ĉt), which defines the means and log-variances of a diagonal Gaussian over Z . As with the primary policy, we represent the guide policy using a feedforward network with two ReLU layers.\nWe train the required networks ωθ, pθ, and qφ simultaneously on the objective:\nminimize θ,φ E x∗∼Dx E m∼Dm\n[ E\nτ∼(qφ,ωθ)\n[ xent(x/∗|σ(c/T+1)) ] +KL(q(τ |x.∗, x/∗) || p(τ |x.∗)) ] , (15)\nin which xent(a, b) indicates the cross-entropy between Bernoulli distributions a and b. We train our models using Monte-Carlo roll-outs of q, and stochastic backpropagation as in [7, 14]. The appendix provides further description of these models details. Full implementations and test code are available from http://github.com/Philip-Bachman/Sequential-Generation.\nSimply put, the primary policy p just repeats the following two steps: sample zt from pθ(zt|σ(ct)), then compute ct+1 using ct and ωθ(zt). The initial c0 was defined previously. The guide policy just adds conditioning on ĉt in the first step and computation of ĉt+1 in the second step."
    }, {
      "heading" : "3.2 Representing Sequential Imputation Policies using LSTMs",
      "text" : "To make it useful for imputation, which requires conditioning on the exogenous information x.∗, we modify the LSTM-based model from Sec. 2.5 to include a “read” operation in its primary policy p. We incorporate a read operation by spreading p over two LSTMs, pr and pw, which respectively “read” and “write” an imputation trajectory cτ = {c0, ..., cT , cT+1}. Conveniently, the guide policy q for this model takes precisely the same form as in Sec. 2.5.\nProcedurally, a single full step of execution for p involves the following substeps: first p updates the reader state using srt ← frθ (srt−1, ωrθ(ct)), then p samples a step zt ∼ pθ(zt|vrt ), then p updates the writer state using swt ← fwθ (swt−1, zt), and finally p updates the imputation state using ct+1 ← ct+ω w θ (v w t ) (or ct+1 ← ωwθ (hwt )). We denote models using the first imputation update with “-add” and tag those using the second update as “-jump”. We provide empirical results for both types. In these updates, sr,wt refers to the joint hidden/visible state [h r,w t ; v r,w t ] of the (reader,writer) LSTMs. The LSTM updates fr,wθ are governed by disjoint subsets of the policy parameters θ. For our tests the read/write operations ωr,wθ were both affine functions governed by disjoint subsets of θ, with ω r θ also applying an element-wise sigmoid σ(ct) prior to the affine transform.\nTo train p, we generate sample trajectories using a guide policy q in place of pr. This changes the first two steps in the preceding paragraph to: update the guide state sqt ← fφ(s q t−1, ω q φ(ct, x∗)) and then sample zt ∼ qφ(zt|vqt ). The subsequent updates for swt and ct+1 remain the same. We use diagonal Gaussian step distributions pθ/qφ whose means and log-variances are affine functions of vrt /v q t . The “read” function ω q φ of the guide policy observes the current imputation state ct, and the complete observation x∗. As described previously, we use this to compute ĉt – the log-likelihood\ngradient with respect to ct. The resulting training objective is given by:\nminimize p,q E x∗∼Dx E m∼Dm\n[ E\nτ∼q(τ |x/∗,x.∗) [− log p(x/∗|cT+1)] + KL(q(τ |x/∗, x.∗) || p(τ |x.∗))\n] (16)\nwhich we optimize using Monte-Carlo roll-outs of q and stochastic backpropagation through time.\n4 Experiments\nWe tested the performance of our sequential imputation models on three datasets: MNIST (28x28), SVHN (cropped, 32x32) [13], and TFD (48x48) [17]. We converted all images to grayscale and shift/scaled them to be in the range [0...1]. We measured the imputation log-likelihood using an “independent Bernoulli” interpretation of the missing values x/∗ and the imputations given by σ(c / T+1).\nWe tested imputation under two types of data masking: missing completely at random (MCAR) and missing at random (MAR). In MCAR, we masked pixels uniformly at random from the source images, and indicate removal of d% of the pixels by MCARd. In MAR, we masked square regions, with the occlusions located uniformly at random within the borders of the source image. We indicate occlusion of a d× d square by MAR-d. On MNIST, we tested MCAR-d for d ∈ {50, 60, 70, 80, 90}, where MCAR-100 corresponds to unconditional generation. On TFD and SVHN we only tested MCAR-80. On MNIST, we tested MARd for d ∈ {14, 16}. On TFD we tested MAR-25 and\non SVHN we tested MAR-17. We tested four of our models, i.e. both model types× both imputation constructor types, against three baselines. For tests, we sampled masks from the distribution used in training, and sampled complete observations from a held-out test set. Fig. 2 and Tab. 1 present test results. We indicate models described in Sec. 3.1 as GPSI-add/GPSI-jump6, and the models from Sec. 3.2 as LSTM-add and LSTM-jump. The GPSI models performed 6 steps of imputation in each test, and the LSTM models performed 16 steps.\nThe baselines were “variational autoencoder imputation”, honest template matching, and oracular template matching. VAE imputation ran multiple steps of VAE reconstruction on the partiallyoccluded input, with the known values fixed to their true values and the missing values re-estimated with each reconstruction step7. We did this for 16 steps and let the model oracularly choose its best\n6GPSI – i.e. Guided Policy Search Imputer 7We discuss some deficiencies of VAE imputation in the appendix\nimputation. Honest template matching imputed the missing values using values from the training image which most closely matched the test image’s known values. Oracular template matching was like honest template matching, but with matching performed directly on the missing values.\nOur models significantly outperformed the baselines. In general, the LSTM-based models outperformed the more direct GPSI models. We evaluated the log-likelihood of imputations produced by our models using the lower-bounds naturally provided by the variational objective with respect to which they were trained. Evaluating the template-based imputations is straightforward. For VAE imputation, we computed the expected log-likelihood of the imputations sampled from multiple runs of the 16-step imputation process. This provides a valid lower-bound on their log-likelihood.\nAs illustrated in Fig. 4, the imputations produced by our models appear quite promising. The imputations are generally of high quality, and the models are capable of capturing strongly multi-modal reconstruction distributions (see subfigure (a)). The behavior of GPSI models changes intriguingly when we swap the underlying imputation constructor. Using the non-additive imputation constructor, the sequential imputation policy learned by the direct model is rather inscrutable. We provide further qualitative results in the appendix, and animations of the policies learned by the LSTM models are provided in the supplemental material. When trained on the binarized MNIST benchmark discussed in Sec. 2.5, i.e. with binarized images and subject to MCAR-100, the LSTM-add model produced raw/fine-tuned scores of 86.2/85.7. The LSTM-jump scored 87.1/86.3. Anecdotally, these “closed-loop” models seemed more prone to overfitting than the “open-loop” models in Sec. 2.5."
    }, {
      "heading" : "5 Discussion",
      "text" : "We presented a point of view which links methods for training directed generative models with policy search in reinforcement learning. We showed how our perspective can guide improvements to existing models. The importance of these connections will only grow as generative models rapidly increase in structural complexity and effective temporal extent (i.e. increased maximum path length in their topologically-sorted graphs).\nWe introduced the notion of imputation as a natural generalization of standard, unconditional generative modelling. Depending on the relation between the data-to-generate and the available information, imputation spans from full unconditional generative modelling to classification/regression. We showed how to successfully train sequential imputation policies comprising millions of parameters using an approach based on Guided Policy Search [9]. Our approach outperforms the baselines quantitatively and appears qualitatively promising. Incorporating, e.g., the local read/write mechanisms from [3] should provide further improvements."
    }, {
      "heading" : "6 Appendix",
      "text" : ""
    }, {
      "heading" : "6.1 Additional Material for Section 2.3",
      "text" : "We now show that the objective in Eqn. 7 describes the KL divergence KL(qτ || pτ ), and that it provides an upper-bound on EDx [− log p(xT )]. First, for τ = {x0, ..., xT }, we define:\n• p(τ>0|x0) = p(x1, ..., xT |x0) = ∏T t=1 pt(xt|xt−1)\n• p(τ) = p(x1, ..., xT |x0)p0(x0) = p0(x0) ∏T t=1 pt(xt|xt−1)\n• q(τ<T |xT ) = q(x0, ..., xT−1|xT ) = ∏T t=1 qt(xt−1|xt)\n• q(τ) = q(x0, ..., xT−1|xT )D(xT ) = D(xT ) ∏T t=1 qt(xt−1|xt)\nNext, we derive: p(xT ) = ∑\nx0,...,xT−1\np0(x0)p(x1, ..., xT |x0) q(τ<T |xT ) q(τ<T |xT )\n(17)\n= ∑\nx0,...,xT−1\np0(x0)p(τ>0|x0) q(τ<T |xT ) q(τ<T |xT )\n(18)\n= ∑\nx0,...,xT−1\nq(τ<T |xT ) p0(x0)p(τ>0|x0) q(τ<T |xT )\n(19)\n= ∑\nx0,...,xT−1\nq(x0, ..., xT−1|xT ) · ( p0(x0)\nT∏ t=1 pt(xt|xt−1) qt(xt−1|xt)\n) (20)\nlog p(xT ) ≥ ∑\nx0,...,xT−1\nq(x0, ..., xT−1|xT ) · log ( p0(x0)\nT∏ t=1 pt(xt|xt−1) qt(xt−1|xt)\n) (21)\n≥ E q(τ<T |xT )\n[ log p0(x0)− log\nT∏ t=1 qt(xt−1|xt) pt(xt|xt−1)\n] (22)\n≥ E q(τ<T |xT )\n[ log p0(x0)− log\nq(τ<T |xT ) p(τ>0|x0)\n] (23)\n≥ E q(τ<T |xT ) [log p0(x0)]−KL(q(τ<T |xT ) || p(τ>0|x0)) (24)\nwhich provides a lower-bound on log p(xT ) based on sample trajectories produced by the reversetime process q when it is started at xT . The transition from equality to inequality is due to Jensen’s inequality. Though q(τ<T |xT ) and p(τ>0|x0) may at first seem incommensurable via KL, they both represent distributions over T -step trajectories through X space, and thus the required KL divergence is well-defined. Next, by adding an expectation with respect to xT ∼ Dx, we derive a lower-bound on the expected log-likelihood EDx [log p(xT )]:\nlog p(xT ) ≥ E q(τ<T |xT )\n[ log p0(x0)− log\nq(τ<T |xT ) p(τ>0|x0)\n] (25)\nE xT∼Dx [log p(xT )] ≥ E xT∼Dx\n[ E\nq(τ<T |xT )\n[ log p0(x0)− log\nq(τ<T |xT ) p(τ>0|x0)\n]] (26)\n≥ E q(τ)\n[ log p0(x0)− log\nq(τ<T |xT ) p(τ>0|x0)\n] (27)\n≥ E q(τ)\n[ − log D(xT )q(τ<T |xT )\np0(x0)p(τ>0|x0)\n] − EDx (28)\n≥ KL(q(τ) || p(τ))− EDx (29)\nThese steps follow directly from the definitions of q(τ<T |xT ) and q(τ). In the last two equations, we defineEDx ≡ Ex∼Dx [− logDx(x)], which gives the entropy ofDx. Given thatDx is a constant with respect to the trainable parameters, the training described in [16] is thus equivalent to minimizing the KL(q(τ) || p(τ))."
    }, {
      "heading" : "6.2 LSTM Model details",
      "text" : "For purely generative tests, all LSTMs had hidden and visible states in R250. We ran the LSTMs for 16 steps. For our extended model in Sec. 2.5, the variational distribution over z0 was computed using\na feedforward network with a single hidden layer of 250 tanh units. Samples of z0 were converted into initial hidden/visible states for the primary and guide LSTMs using a feedforward network with a single hidden layer of 250 tanh units. The latent variable z0 was in R20 and the latent variables zt for t > 0 were in R100.\nWe trained the models using minibatches of size 250. For each example in the minibatch we sampled a single trajectory from the guide policy. The necessary KL divergences were computed via partial Rao-Blackwellisation, i.e. at each step we computed a 1-step KL analytically, and the sum of these provides an estimator whose mean is the full-trajectory KL.\nIn the generative tests, we trained the “raw” model for 200k updates. The variational posterior finetuning stage lasted 50k updates. We used the ADAM algorithm for optimization [5], which includes both first-order momentum-like smoothing and second-order Adagrad-like rescaling. We used a learning rate 0.0002 for all models in all tests.\nThe imputation tests added a “reader” LSTM to the generative model (i.e. the primary policy). This had precisely the same structure as the guide LSTM. However, rather than inputting [ct; ĉt] at each step (which includes information about the target values in x∗), we simply input [ct; ct]. This was the first thing we tried, and worked alright, but could probably be improved.\nWe used the rather new Blocks framework for managing all of our LSTM-based models, though we only used the framework for managing the THEANO computation graph [18, 1]. All training and data management were done manually in our test scripts. In addition to the LSTM-based models, we also implemented the other models using THEANO."
    }, {
      "heading" : "6.3 GPSI Model Details",
      "text" : "We trained our GPSI models using the same basic setup as for the LSTM models. For MNIST tests, the three networks underlying the model were built using two hidden layers of 1000 ReLU units. For the TFD and SVHN tests the layers were increased to 1500 units. We used latent variables zt ∈ R100 for MNIST and zt ∈ R200 for TFD/SVHN. Batch sizes and optimization method were the same as for the LSTMs. Code is available on Github. Due to computation/time constraints we performed little/no hyperparameter search. The GPSI results should improve somewhat with better architecture choices. Adding the localized read/write mechanisms from [3] may help too."
    }, {
      "heading" : "6.4 Problems with VAE Imputation",
      "text" : "Variational autoencoder imputation proceeds by running multiple steps of iterative sampling from the approximate posterior q(z|x) and then from the reconstruction distribution p(x|z), with the known values replaced by their true values at each step. I.e. the missing values are repeatedly guessed based on the previous guessed values, combined with the true known values.\nConsider an extreme case in which the mutual information between z and x in the joint distribution p(x, z) = p(x|z)p(z), arising from combining p(x|z) with the latent prior p(z), is 0. In this case, even if the marginal over x, i.e. p(x) = ∑ z p(x|z)p(z), is equal to the target distribution Dx, each sample of new guesses for the missing values will be sampled independently from the marginal over those values in Dx. Thus, the new guesses will be informed by neither the previous guesses nor the known part of the observation for which imputation is being performed.\nIn addition to this fundamental defect, the VAE approach to imputation also suffers due to the posterior inference model q(z|x) lacking any prior experience with heavily perturbed observations. I.e., if all training is performed on unperturbed observations, then the response of q(z|x) can not be guaranteed to remain useful when presented with observations from a different, perturbed distribution.\nWhile one could train a basic VAE for imputation by sampling random “VAE imputation” trajectories and then backpropagating the imputation log-likelihood through those trajectories, we empirically found that this was largely ineffective. In a strong sense, the problem with this approach is analogous to that solved (in certain situations) by Guided Policy Search. I.e., the primary policy is initially so poor that an, e.g., policy gradient approach to training it will be uninformative and ineffective. By incorporating privileged information in the guide policy, one can slowly shepherd the initially poor primary policy toward gradually improving behavior.\n6.5 Additional Qualitative Results for GPSI Models"
    } ],
    "references" : [ {
      "title" : "Theano: A cpu and gpu math expression compiler",
      "author" : [ "J. Bergstra", "O. Breuleux", "F. Bastien", "P. Lamblin", "R. Pascanu", "G. Desjardins", "J. Turian", "D. Warde-Farley", "Y. Bengio" ],
      "venue" : "In Python for Scientific Computing Conference (SciPy),",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2010
    }, {
      "title" : "Generating sequences with recurrent neural networks",
      "author" : [ "Alex Graves" ],
      "venue" : "[cs.NE],",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2013
    }, {
      "title" : "Draw: A recurrent neural network for image generation",
      "author" : [ "Karol Gregor", "Ivo Danihelka", "Alex Graves", "Daan Wierstra" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2015
    }, {
      "title" : "Deep autoregressive networks",
      "author" : [ "Karol Gregor", "Ivo Danihelka", "Andriy Mnih", "Charles Blundell", "Daan Wierstra" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2014
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P Kingma", "Jimmy Lei Ba" ],
      "venue" : "In International Conference on Learning Representations (ICLR),",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2015
    }, {
      "title" : "Semi-supervised learning with deep generative models",
      "author" : [ "Diederik P Kingma", "Danilo J Rezende", "Shakir Mohamed", "Max Welling" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2014
    }, {
      "title" : "Auto-encoding variational bayes",
      "author" : [ "Diederik P Kingma", "Max Welling" ],
      "venue" : "In International Conference on Learning Representations (ICLR),",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2014
    }, {
      "title" : "Learning neural network policies with guided policy search under unknown dynamics",
      "author" : [ "Sergey Levine", "Pieter Abbeel" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2014
    }, {
      "title" : "Guided policy search",
      "author" : [ "Sergey Levine", "Vladlen Koltun" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2013
    }, {
      "title" : "Variational policy search via trajectory optimization",
      "author" : [ "Sergey Levine", "Vladlen Koltun" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2013
    }, {
      "title" : "Learning complex neural network policies with trajectory optimization",
      "author" : [ "Sergey Levine", "Vladlen Koltun" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2014
    }, {
      "title" : "Neural variational inference and learning in belief networks",
      "author" : [ "Andriy Mnih", "Karol Gregor" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2014
    }, {
      "title" : "Reading digits in natural images with unsupervised feature learning",
      "author" : [ "Yuval Netzer", "Tao Wang", "Adam Coates", "Alessandro Bissacco", "Bo Wu", "Andrew Y Ng" ],
      "venue" : "NIPS Workshop on Deep Learning and Unsupervised Feature Learning,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2011
    }, {
      "title" : "Stochastic backpropagation and approximate inference in deep generative models",
      "author" : [ "Danilo Rezende", "Shakir Mohamed", "Daan Wierstra" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2014
    }, {
      "title" : "Variational inference with normalizing flows",
      "author" : [ "Danilo J Rezende", "Shakir Mohamed" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2015
    }, {
      "title" : "Deep unsupervised learning using nonequilibrium thermodynamics",
      "author" : [ "Jascha Sohl-Dickstein", "Eric A. Weiss", "Niru Maheswaranathan", "Surya Ganguli" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2015
    }, {
      "title" : "The toronto face database",
      "author" : [ "Joshua Susskind", "Adam Anderson", "Geoffrey E Hinton" ],
      "venue" : null,
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2010
    }, {
      "title" : "Blocks and fuel: Frameworks for deep learning",
      "author" : [ "Bart van Merrienboer", "Dzimitry Bahdanau", "Vincent Dumoulin", "Dmitriy Serdyuk", "David Warde-Farley", "Jan Chorowski", "Yoshua Bengio" ],
      "venue" : null,
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "We construct our models using neural networks and train them using a form of guided policy search [9].",
      "startOffset" : 98,
      "endOffset" : 101
    }, {
      "referenceID" : 2,
      "context" : "First, we reinterpret several recent generative models as sequential decision making processes, and show how changes inspired by our point of view can improve the performance of the LSTM-based model introduced in [3].",
      "startOffset" : 213,
      "endOffset" : 216
    }, {
      "referenceID" : 2,
      "context" : "The first extends the model in [3] by adding an explicit feedback loop into the generative process, and the second implements the MDP more literally.",
      "startOffset" : 31,
      "endOffset" : 34
    }, {
      "referenceID" : 8,
      "context" : "We train our models/policies using techniques motivated by Guided Policy Search [9, 10, 11, 8].",
      "startOffset" : 80,
      "endOffset" : 94
    }, {
      "referenceID" : 9,
      "context" : "We train our models/policies using techniques motivated by Guided Policy Search [9, 10, 11, 8].",
      "startOffset" : 80,
      "endOffset" : 94
    }, {
      "referenceID" : 10,
      "context" : "We train our models/policies using techniques motivated by Guided Policy Search [9, 10, 11, 8].",
      "startOffset" : 80,
      "endOffset" : 94
    }, {
      "referenceID" : 7,
      "context" : "We train our models/policies using techniques motivated by Guided Policy Search [9, 10, 11, 8].",
      "startOffset" : 80,
      "endOffset" : 94
    }, {
      "referenceID" : 6,
      "context" : "Directed generative models have gained in popularity relative to their undirected counter-parts [7, 14, 12, 4, 6, 16, 15] (etc.",
      "startOffset" : 96,
      "endOffset" : 121
    }, {
      "referenceID" : 13,
      "context" : "Directed generative models have gained in popularity relative to their undirected counter-parts [7, 14, 12, 4, 6, 16, 15] (etc.",
      "startOffset" : 96,
      "endOffset" : 121
    }, {
      "referenceID" : 11,
      "context" : "Directed generative models have gained in popularity relative to their undirected counter-parts [7, 14, 12, 4, 6, 16, 15] (etc.",
      "startOffset" : 96,
      "endOffset" : 121
    }, {
      "referenceID" : 3,
      "context" : "Directed generative models have gained in popularity relative to their undirected counter-parts [7, 14, 12, 4, 6, 16, 15] (etc.",
      "startOffset" : 96,
      "endOffset" : 121
    }, {
      "referenceID" : 5,
      "context" : "Directed generative models have gained in popularity relative to their undirected counter-parts [7, 14, 12, 4, 6, 16, 15] (etc.",
      "startOffset" : 96,
      "endOffset" : 121
    }, {
      "referenceID" : 15,
      "context" : "Directed generative models have gained in popularity relative to their undirected counter-parts [7, 14, 12, 4, 6, 16, 15] (etc.",
      "startOffset" : 96,
      "endOffset" : 121
    }, {
      "referenceID" : 14,
      "context" : "Directed generative models have gained in popularity relative to their undirected counter-parts [7, 14, 12, 4, 6, 16, 15] (etc.",
      "startOffset" : 96,
      "endOffset" : 121
    }, {
      "referenceID" : 3,
      "context" : "The deep autoregressive networks investigated in [4] define distributions of the following form:",
      "startOffset" : 49,
      "endOffset" : 52
    }, {
      "referenceID" : 3,
      "context" : "1 can represent arbitrary distributions over the latent variables, and work in [4] mainly concerned approaches to parameterizing the conditionals pt(zt|z0, .",
      "startOffset" : 79,
      "endOffset" : 82
    }, {
      "referenceID" : 8,
      "context" : ", [9, 10, 11, 8].",
      "startOffset" : 2,
      "endOffset" : 16
    }, {
      "referenceID" : 9,
      "context" : ", [9, 10, 11, 8].",
      "startOffset" : 2,
      "endOffset" : 16
    }, {
      "referenceID" : 10,
      "context" : ", [9, 10, 11, 8].",
      "startOffset" : 2,
      "endOffset" : 16
    }, {
      "referenceID" : 7,
      "context" : ", [9, 10, 11, 8].",
      "startOffset" : 2,
      "endOffset" : 16
    }, {
      "referenceID" : 3,
      "context" : "The authors of [4] train deep autoregressive networks by maximizing a variational lower-bound on the training set log-likelihood.",
      "startOffset" : 15,
      "endOffset" : 18
    }, {
      "referenceID" : 3,
      "context" : "Given these definitions, the training in [4] can be interpreted as an application of guided policy search to the MDP described in the previous paragraph.",
      "startOffset" : 41,
      "endOffset" : 44
    }, {
      "referenceID" : 3,
      "context" : "(6) Thus, the variational approach used in [4] for training directed generative models can be interpreted as a form of generalized guided policy search.",
      "startOffset" : 43,
      "endOffset" : 46
    }, {
      "referenceID" : 15,
      "context" : ", the authors of [16] proposed a model in which Zt ≡ X for all t and p∅(x0) is a Gaussian.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 15,
      "context" : "The authors of [16] observed that, for any reasonably smooth target distribution Dx and sufficiently large T , one can define a (reverse-time) stochastic process qt(xt−1|xt) with simple, time-invariant dynamics that transforms the distribution qT (xT ) = Dx into the Gaussian distribution p∅(x0).",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 15,
      "context" : "With these definitions, the training in [16] performs guided policy search, using guide trajectories sampled from q.",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 15,
      "context" : "If, as in [16], the reverse-time process q is not trained, then Eqn.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 15,
      "context" : "running dynamics backwards in time starting from xT ∼ Dx – may prove useful in settings other than those considered in [16].",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 2,
      "context" : "The authors of [3] introduced a model capable of learning temporally-deep sequential generative processes.",
      "startOffset" : 15,
      "endOffset" : 18
    }, {
      "referenceID" : 2,
      "context" : "The authors of [3] used a Gaussian distribution N (0, I) for all pt(zt).",
      "startOffset" : 15,
      "endOffset" : 18
    }, {
      "referenceID" : 2,
      "context" : "To train p, the authors of [3] introduced a guide policy q with trajectory distribution:",
      "startOffset" : 27,
      "endOffset" : 30
    }, {
      "referenceID" : 2,
      "context" : "The function gφ(sθ(τ<t), x ∗) encapsulates the “read” operation of the encoder network in [3].",
      "startOffset" : 90,
      "endOffset" : 93
    }, {
      "referenceID" : 2,
      "context" : "Using our definitions for the primary/guide policies p/q, the training objective in [3] is given by:",
      "startOffset" : 84,
      "endOffset" : 87
    }, {
      "referenceID" : 1,
      "context" : "For those unfamiliar with LSTMs, a good introduction can be found in [2].",
      "startOffset" : 69,
      "endOffset" : 72
    }, {
      "referenceID" : 2,
      "context" : "where we now have to care about pt(zt|st−1), p∅(z0), and q∅(z0|x), which could be treated as constants in the model from [3].",
      "startOffset" : 121,
      "endOffset" : 124
    }, {
      "referenceID" : 2,
      "context" : "When trained for the binarized MNIST benchmark examined in [3], our extended model scores a negative log-likelihood of 85.",
      "startOffset" : 59,
      "endOffset" : 62
    }, {
      "referenceID" : 2,
      "context" : "For comparison, the score reported in [3] was 87.",
      "startOffset" : 38,
      "endOffset" : 41
    }, {
      "referenceID" : 14,
      "context" : "For comparison, see the best upperbound reported for this benchmark in [15], which was 85.",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 6,
      "context" : "We train our models using Monte-Carlo roll-outs of q, and stochastic backpropagation as in [7, 14].",
      "startOffset" : 91,
      "endOffset" : 98
    }, {
      "referenceID" : 13,
      "context" : "We train our models using Monte-Carlo roll-outs of q, and stochastic backpropagation as in [7, 14].",
      "startOffset" : 91,
      "endOffset" : 98
    }, {
      "referenceID" : 12,
      "context" : "We tested the performance of our sequential imputation models on three datasets: MNIST (28x28), SVHN (cropped, 32x32) [13], and TFD (48x48) [17].",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 16,
      "context" : "We tested the performance of our sequential imputation models on three datasets: MNIST (28x28), SVHN (cropped, 32x32) [13], and TFD (48x48) [17].",
      "startOffset" : 140,
      "endOffset" : 144
    }, {
      "referenceID" : 8,
      "context" : "We showed how to successfully train sequential imputation policies comprising millions of parameters using an approach based on Guided Policy Search [9].",
      "startOffset" : 149,
      "endOffset" : 152
    }, {
      "referenceID" : 2,
      "context" : ", the local read/write mechanisms from [3] should provide further improvements.",
      "startOffset" : 39,
      "endOffset" : 42
    } ],
    "year" : 2017,
    "abstractText" : "We connect a broad class of generative models through their shared reliance on sequential decision making. We show how changes motivated by our point of view can improve an already-strong model, and then explore this idea further in the context of data imputation – perhaps the simplest setting in which to investigate the relation between unconditional and conditional generative modelling. We formulate data imputation as an MDP and develop models capable of representing effective policies for it. We construct our models using neural networks and train them using a form of guided policy search [9]. Through empirical tests, we show that our approach can learn effective policies for imputation problems of varying difficulty and across multiple datasets.",
    "creator" : "LaTeX with hyperref package"
  }
}