{
  "name" : "1609.03348.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "thomas.holland.ward@gmail.com " ],
    "sections" : [ {
      "heading" : null,
      "text" : "A generic and scalable Reinforcement Learning scheme for Artificial Neural Networks is presented,                          providing a general purpose learning machine. By reference to a node threshold three features are                              described 1) A mechanism for Primary Reinforcement, capable of solving linearly inseparable problems 2)                            The learning scheme is extended to include a mechanism for Conditioned Reinforcement, capable of                            forming long term strategy 3) The learning scheme is modified to use a thresholdbased deep learning                                algorithm, providing a robust and biologically inspired alternative to backpropagation. The model may be                            used for supervised as well as unsupervised training regimes.   \n1 Introduction    This paper proposes that a general purpose learning machine can be achieved by implementing Reinforcement                              Learning in an Artificial Neural Network (ANN), a method which attempts to emulate the core mechanism of that                                    process is presented.     AI research has characteristically followed a bottomup approach; focusing on subsystems that address distinct,                            specialized and unrelated problem domains. In contrast the work presented follows a distinctly topdown approach                              attempting to model intelligence as a whole system; a causal agent interacting with the environment [6]. The agent is                                      not designed to solve a particular problem, but is instead assigned a reward condition. The reward condition serves                                    as a goal, and in the path a variety of unknown challenges may be present. To solve these problems efficiently the                                          agent requires intelligence.    This topdown approach assumes that the core self organizing mechanisms of learning that exist in natural                                organisms can be replicated in artificial autonomous agents. These can then be scaled up by endowing the agent with                                      more resources (sensors, neurons & motors). Given sufficient resources and learning opportunities an agent may                              provide a solution to a problem provided one exists. Also given the generalization properties of ANN’s the agent can                                      provide appropriate responses to novel stimuli.     A distinction is made between supervised, unsupervised and reinforcement training regimes. Supervised learning                          regimes use a (human) trainer to assign desired inputoutput pattern pairings. Unsupervised training regimes are                              typically used to cluster a data set into related groups. Reinforcement Learning (RL) may be considered a subtype of                                      unsupervised training; it is sometimes called learning with a critic rather than learning with a teacher as the feedback                                      is evaluative (right or wrong) rather than instructive (where a desired output action is prescribed). Significant RL                                  successes have been achieved in AI notably with the use of Qlearning [2][7].     First a definition of intelligence is required:   \n1 \nThe demonstration of beneficial behaviors acquired through learning.    A beneficial action/behavior being one that would result in a positive survival outcome (eg successful feeding,                                mating, self preservation) for the agent. For the most part our inherent internal reward systems encourage us to                                    perform beneficial behaviors, but this is not always the case (eg substance abuse may be rewarding but not                                    beneficial). The term ‘desirable behavior’ is avoided due to existing usage of the term ‘desired output’ in supervised                                    learning schemes.    Let’s revise our definition, and expectation, of intelligence:   \nThe demonstration of rewarding behaviors acquired through learning.   \nRewarding behaviors/actions will be selected for reinforcement (ie learnt) over non rewarding ones. Rewarding                            behaviors are those that allow the agent to achieve the reward condition, thereby achieving its goal(s) in an                                    acceptably efficient manner (eg elapsed time, steps taken, energy expended). Rewarding behaviors may lead to                              pleasure, or at least a reduction in pain. Goals are attained by achieving the preestablished reward condition, and                                    thereby satiating active desire(s). Behaviors need not be active they may be passive; inaction may lead to reward and                                      therefore be reinforced. \n  From initial state s t if action a t results in an immediate reward in the subsequent state s t+1 , action a t will be                                          reinforced. If the same (or similar from generalization) input pattern is encountered the learnt action will be                                  performed. This process of learning is termed Primary Reinforcement. Primary reinforcement reward conditions (eg                            hunger or thirst) typically drive some form of homeostatic, adaptive control function for the agent [5][8].   \n  This is in contrast to Secondary/Conditioned Reinforcement where from initial state s t action a t does not result in                                    immediate reward in subsequent state s t+1 . If later action a t+1 does result in a reward in state s t+1 , this will reinforce                                          actions a t+1 and a t . The number of actions learnt from start to goal is arbitrary and depends on the relative size of                                            the reward relative to the cost (or pain) in attaining it.     A learning scheme is proposed that enables an embodied neural network agent to autonomously determine and learn                                  desirable behaviors. The agent may be embodied in a real or artificial environment. Artificial environments may be                                  modelled on physical environments or even abstract problem domains. The environment may be any set of input                                  patterns, however there must be a causal relationship between the output (behavior) of the agent and the subsequent                                    input pattern.     This paper presents three features:   \n1. A Primary Reinforcement learning scheme is achieved by wrapping a (supervised) backpropagation                        network within an unsupervised framework.     Primary Reinforcement enables ‘desirable’ inputoutput mappings (behaviors) to be autonomously and                      continuously learnt; this is of particular benefit when a human supervisor is not available / does not know                                    what the desired output should be, or when the environment, or agent itself,  is changeable.   \n2. The framework is then extended to provide Conditioned (Secondary) Reinforcement.   \nConditioned (Secondary) Reinforcement enables an arbitrarily long sequence of chained behaviors to be                          autonomously learnt, in expectation of a primary reward condition; this provides long term strategy. \n2 \n  3. An algorithm is described, termed Threshold Assignment of Connections (TAC), that replaces                       \nbackpropagation within the framework, this algorithm can conversely also be used in supervised training                            schemes. \n  The Threshold Assignment of Connections algorithm provides a biologically inspired alternative to                        backpropagation.    The examples that follow are focused mainly on target tracking, however this approach may be applied to a wide                                      range of real and abstract problem spaces. The tasks are small in scale and primarily serve as proof of concept.                                        While learning rate performance has been documented, the model has not been performance tuned. The intent of this                                    work is to establish a biologically plausible working model of intelligence that is simple, scalable and generic.  \n   \n2 Primary Reinforcement   \nIn this section a learning scheme is described that provides Primary Reinforcement learning in an Artificial Neural                                  Network. Weight adjustments using backpropagation[3][9] are traditionally used in supervised learning schemes;                        that is desired activations (or training sets) are established prior to the learning phase. In this example                                  backpropagation will be used in an unsupervised learning scheme; desired activations will be calculated onthefly.                              This approach, termed Threshold Assignment of Patterns (TAP), relies on a threshold to determine the desired                                output pattern.  \nThe model presented is fundamentally a hedonistic one, learning is based on pleasure/pain drives. Although reward                                results in reinforcement the model is stochastic; punishment results in new candidate behaviors being randomly                              generated. Sensing utilizes a sensory input pattern and behavior arises from a motor output pattern. Thinking (or                                  processing) is implemented via layers of artificial neurons. \nThe learning scheme consists of the following components:   \n● Artificial Neural Network  ● Learning Algorithm  ● Framework   ● Environment \n  The network architecture is that of a familiar multilayer perceptron (MLP)[3]. An input (sensor) pattern representing                                the environment, produces activations that feed forward and result in an output (motor) pattern representing a                                behavior. Weight updates are derived using a standard (supervised) back propagation learning algorithm[3][10].     The framework sits between the network and environment. The framework acts as an interface between network and                                  environment, and is responsible for establishing a reward condition as Primary Reinforcer and determining (desired                              output) behavior based on that reward condition. The framework, network and learning algorithm constitute the                              agent.     Simulated environments consist of input patterns and ‘physics’ rules. The physics rules take the current (t) network                                  output and determine which input pattern is next (t+1) presented to the network. The environment consists of states,                                    and physics rules which define the relationship between states.        \n \n3 \n  \nfig1 State ‘t’ \n \n \nfig2  State ‘t+1’ \nBackpropagation requires a set of desirable output patterns to be established. Through learning the desired output                                patterns will inform the network what the required output node activations are for each input pattern. In this model                                      desired output patterns will be dynamically generated onthefly. But how can the agent determine potentially                              complex desired output patterns from a simple yes/no reward condition? The agent will decide by trial and error, it                                      does so by the following process (Box 1):   \nProcessing steps \n1. Input pattern for state ‘t’ is presented and forward propagated through the network (Box 2).                              Node output activation may be in the range 01 (Box 3). A threshold value of 0.5 is assigned                                    to each node in output layer. \n2. Agent’s behavior, based on whether output layer activations have exceeded threshold values                        (fig 3), determines subsequent input pattern ‘t+1’. \n3. Framework determines ‘reward’ value based on ‘t+1’ input pattern. \n4. Weight changes are made: \na. Desired ‘t’ activations at the output layer are calculated: \ni. If reward condition then the desired ‘t’ activation for that node is set to                            maximal value (1.0) for those that were above threshold, and set to                        minimal value (0.0) for those that were below threshold. \nii. If no reward then the desired ‘t’ activation for all nodes are random                          moderate values [0.45..0.55]. \nb. Weight changes for state ‘t’ are made according to error values which are back                            propagated through the network (Box 4). \n4 \n  Box 1 Threshold Assignment of Patterns processing steps \nDesired activation values are set depending on whether a reward or punishment occurred after the output response.                                  Thresholds for neuron activation are widely found in animals, where sufficient ‘excitation’ is required to result in an                                    electrical action potential that can be signalled to other neurons [11]. If a reward condition occurred all actual                                    activations above threshold (eg fig 4 node 1) will be reinforced by setting the corresponding desired activation to 1.0                                      and all activations below threshold (eg fig 4 node 2) will be reinforced by setting the corresponding desired                                    activation to 0.0. If a punishment condition occurred all actual activations, above or below threshold, will be                                  weakened by setting desired activations to near threshold values. \n \nfig3 Logistic Activation Function with threshold \n \nInput activation for unit u. \n= etinputn u ∑  \ni eightw ui ai  \n \n  Box 2  Input activation \n \nOutput activation for unit u.  = au 11 + e−netinputu  \n \n  Box 3  Logistic Activation Function \n \n1) Derive delta error value for an output layer node u, by finding difference between desired activation ( ) and                                  du     actual activation ( ).au   =(  ) )eltad u du au au 1( −au     2) Derive weight change for connection between a hidden unit h and an output unit u, using learning rate.   \n= weightΔ uh rate deltal u ah    \n3) Derive delta error value for a hidden unit h, using weighted sum of all units in output layer . \n5 \n  = eltad h ah 1  a )( −   h ∑  \nu eltad u eightw uh  \n  4) Derive weight change for connection between an input unit i and a hidden unit h, using learning rate.    = weightΔ hi rate deltal h ai    \n  Box 4  Backpropagation weight update \nIn this way randomized desired out patterns will generate a new candidate behavior on the next presentation of that                                      same (or similar) stimulus. In effect the response has been established before it is first manifested, and this                                    rewarding response will be reinforced on future presentations. Conversely nonrewarding behaviors will be                          destroyed in favour of a new candidate behavior. Akin to natural selection, only rewarding behaviors will survive.   \n \nfig4   Desired activation : Punishment vs Reward \nA mapping is formed from an input pattern stimuli to an output pattern motor response, dependent on the reward                                      condition that follows. No distinction is made between learning and testing phase; that is the agent in a continual                                      process of learning and evaluation. A behavior is deemed to have been learnt when all output node activations are                                      mature (eg above 0.9 or less than 0.1) for a given input pattern and results in a reward. \n2.1 Example: Target tracking (part I)   \n2.1.1 Problem description \nIn this example the agent must autonomously learn how to track a target (fig 5). \n6 \n  fig5 Target tracking example start state  ● The target may appear in any one of nine cells of a grid (3x3). The agent is required to focus on the target,                                             \nby moving it to the centre cell.  ● The network is rewarded only if it moves the target to it’s centremost cell. Once the target is moved to the                                         \ncentremost cell it is moved to a new starting position on the grid.  ● The agent has no prior knowledge. It does not initially know it will be rewarded by relocating the target                                     \nuntil it does so.  ● The agent may only move the target 1 step (ie to an adjacent cell) in each iteration.  ● To increase the task difficulty the agent may not move the target diagonally in one movement, two steps are                                     \nrequired.  \n \n2.1.2 Network configuration \nA network was created (table 1)(fig 6):   \nInput layer  9 input nodes; the centre node is designated as a special ‘reward’                        node \nHidden layer  12 hidden nodes \nOutput layer  4 motor nodes \nNotes  Each layer fully connected to the next via weights.  Weights initially randomised.  Each hidden and output nodes assigned an exclusive bias unit.  Learning rate = 1.0 \n \nTable 1 Target tracking network configuration \n7 \n  fig6 Target tracking network topology \n \nThe nine input nodes are mapped to each cell in the grid (fig7): \n \nfig7 Target tracking input mapping \nThe network has four output nodes, representing output motors (Up, Down, Left, Right). Depending on the output                                  the agent is able to move the target within the grid. In order for the agent to move the target in any given direction                                                there must be only one output node activation above threshold (> 0.5), otherwise the target will remain stationary.                                    Therefore there is only a 1in16 chance (24) the agent will move in the correct direction by chance. \nThe framework establishes the reward condition by assigning one the input nodes as a special ‘reward node’. The                                    framework consists of some rules to calculate previous (t) desired output patterns based on the current (t+1) input                                    pattern which contains the reward value. A reward indicator value of 1 is set if the network it to be rewarded, and                                            zero if it is to be punished. A reward occurs if the target is moved to the centre cell (fig 8):   \n     \n8 \nfig8 Target tracking reward condition \nThe behavior, or output response of the network, will causally influence the subsequent input pattern. Reward                                feedback will be given to the network, thereby informing whether the output was ‘correct’. The output patterns are                                    not predetermined. The network is presented a pattern and produces a response behavior. If the behavior was                                  rewarding then a stronger version of the actual output is assigned as the desired output pattern. If the behavior was                                        not rewarding a random pattern is set as the desired output pattern. A threshold is required at the output layer to                                          make this decision.    2.1.3 Results    Initially the agent moves the target randomly. Activations tend to be weak (eg 0.4  0.6) across all output nodes.                                        When rewarding behaviors are discovered these are further reinforced and the output matures (eg < 0.1, > 0.9).    With sufficient exposure the network learns the optimal behavior to achieve a reward; consistently moving one step                                  left/right/up/down towards the food reward from starting locations (cells 1,3,5,7) (table 2). If the target is placed                                  directly on the centre cell it will remain stationary.   \n \nScheme  # pattern presentations \nThreshold Assignment of Patterns  (Unsupervised backpropagation) \n116816 \n  Table 2 Target tracking results \nHowever, the agent is unable to learn how to move the target when placed in corners (cells 0,2,6,8). The physics                                        rules prevent the agent from moving the target diagonally in one step, instead two steps are required. Whilst the                                      agent was able to solve this ‘1step’ solution in 116816 presentations, it spent much of it’s time ‘stuck’ in corner                                        cells. The agent can only reinforce behaviors where there is an immediate reward in the subsequent input pattern.                                    The network is unable to solve the ‘temporal credit assignment’ problem. In order to learn two or more consecutive                                      behaviors secondary (conditioned) reinforcement is required (fig 9).     \n   \nfig9 Primary Reinforcement partial solution for target tracking task \nNote; In this example the reward condition was facilitated by assigning an existing input layer node as the ’Reward                                      Node’. However, the reward condition could be evaluated against a combination of existing input layer nodes, a                                  separate input layer node(s), or even no node at all (see XOR example below).  \n \n2.2 Example: XOR  2.2.1 Problem description \nThe agent will be required to solve the XOR problem (table 3) in order to test the network's ability to map non linear                                             \n9 \ntransformations that require multiple layers of neurons. This test will will also provide a performance comparison                                between unsupervised reinforcement learning and the supervised regime.    \n \n(input)  A  (input)  B \n(output)  XOR \n0  0  0 \n0  1  1 \n1  0  1 \n1  1  0 \n  Table 3 XOR task \nAny input pattern can be considered an environment, and any output pattern a behavior. Thus any set of mappings                                      can be learnt as they would under a conventional supervised learning regime. In contrast to the previous experiment,                                    the network will now receive controlled exposure to all input patterns in turn. The behavior, or output response of                                      the network will not influence the subsequent input pattern. However, reward feedback will be given to the network                                    thereby informing whether the output was correct according to the desired pattern in the supervised training set. This                                    tightly controlled presentation of input patterns is termed ‘guided’.    For clarity, and to allow for a close comparison with the supervised backpropagation training regime, no explicit                                  reward node will be established in the network topology. The framework is still responsible for setting the reward                                    condition. To allow exposure to all the input patterns they will be cycled through in sequence. The framework will                                      evaluate the output and decide whether it should be reinforced or not.    2.2.2 Network configuration    A network was created (table 4)(fig 10): \nInput layer  2 input nodes \nHidden layer  3 hidden nodes \nOutput layer  1 output node \nNotes  Each layer fully connected to the next via weights.  Weights initially randomised.  Each hidden and output nodes assigned an exclusive bias unit.  Learning rate = 1.0 \n \nTable 4 XOR network configuration \n \n10 \n   \nfig10 XOR network topology \n  2.2.3 Results   \n \nScheme  # pattern presentations \nSupervised backpropagation  2182 \nThreshold Assignment of Patterns   (unsupervised backpropagation) \n7550 \n  Table 5 XOR results \nThe Threshold Assignment of Patterns (unsupervised backpropagation) regime required significantly more pattern                        presentations to learn the XOR solution than the supervised regime (table 5). Both were using the same set of initial                                        weights and learning parameters (learning rate etc). Both are using the same backpropagation algorithm to perform                                weight updates. The difference in performance can be attributed to the manner in which desired output patterns are                                    provided. In the supervised scheme the desired patterns are known a priori , in the unsupervised (guided) learning                                  regime these must be discovered by the network through trial and error. This performance gap would be expected                                    to widen as the number of nodes in the output layer grows and with it the number of candidate combinations. \n \n2.3 Summary   Unsupervised Primary Reinforcement can be achieved, with reference to a threshold, by dynamically generating                            desired output activations and feeding these into a supervised learning algorithm. These experiments demonstrate                            the network's ability to learn and map arbitrary sets of patterns by reward, thereby producing behaviors that allow it                                      to reach it’s goal in an efficient manner. Primary Reinforcement may solve problems that are that are linearly                                    inseparable. It is capable of training weights deep within the network, thereby capable of forming complex abstract                                  representations. \nThe input patterns can be presented in any order. It can be exposed to all potential input patterns found in the                                          environment. However, in order for the network to identify which output patterns (or behaviors) were correct, the                                  network must be presented with subsequent reward feedback. The unsupervised Reinforcement Learning regime                          requires more iterations to learn compared to the supervised regime. The principal reason being that the                                unsupervised must discover the ‘correct’ solution through trial and error. And even when agent does not receive                                  reinforcement, the new random candidate behavior may be a repeat of a prior incorrect one. \n11 \nThe unsupervised Reinforcement Learning regime is limited to a binary maximal off/on (0 or 1) activation on each                                    node. Conversely the supervised regime may set desired activations of any value (0..1). \nThe unsupervised Reinforcement Learning regime is capable of finding solutions and adapting to change without                              the aid of a trainer. Alternate behaviors will be adopted in the face of unforeseen environmental hazards, or if the                                        agent suffers damage. Additional sensors and motors can be added to the agent without requiring a trainer. \nPrimary Reinforcement may only learn a single behavior sequence, therefore it is not suitable for acquiring long                                  term strategy. Problems requiring long term strategy must use Secondary/Conditioned Reinforcement. \n \n3 Secondary/Conditioned Reinforcement  Primary reinforcement provides a generic method of autonomously establishing beneficial output responses. But it                            has a significant limitation; it can only provide a one step mapping from start state(s) to goal. A more useful feature                                          is the ability to establish a series of behavior steps leading from start state(s) to a goal. This is the benefit provided                                            by Conditioned Reinforcement. The term ‘Conditioned’ Reinforcement is preferred over that of ‘Secondary’                          Reinforcement, since the latter may imply chained behaviors only 2 steps deep. In fact the number of chained                                    behaviors can be arbitrarily deep, depending on the strength and decay of the reward. This approach, termed                                  Threshold Assignment of Rewards (TAR), relies on building an association between a rewarding stimulus and an                                internal proxy reward. \nIn Primary Reinforcement a mapping is established between an input pattern stimuli to an output pattern motor                                  response. In Secondary Reinforcement a mapping is established between an input pattern stimuli to an output                                pattern motor response AND a reward node. With sufficient reinforcement the response activation on the reward                                node matures; the reward node is now available as a proxy reward condition (secondary/conditioned reinforcer) for                                a given input pattern. In turn this conditioned reinforcer can help to create further conditioned reinforcers, that are                                    activated only in response to a recognized input pattern stimuli. \nThe first conditioned response to be learnt will be closest to the primary reinforcer. Thereafter a chain of                                    conditioned reinforcers can be established, a series of ‘breadcrumbs’ that lead ultimately to the goal. Using this                                  mechanism planned or goal oriented tasks can be solved. \nThis approach is intended to overcome the temporal ‘hill climbing’ limitation outlined in the previous example. It                                  is based on the a similar architecture and learning rule as before but with one important addition: a special reward                                        node is added to the output layer. Unlike other nodes in the output layer the special reward node is not a motor                                            neuron. Once this becomes mature it behaves like an input reward node; deciding which behaviors should be learnt.                                    Behaviors leading to a distant reward can be chained together. Essentially mapping are now formed between input                                  patterns and rewards, rather than just input patterns and motor nodes (Box 5). \n \nProcessing steps    Differences to the Primary Reinforcement process described previously are highlighted in bold.   \n1. Input pattern for state ‘t’ is presented and forward propagated through the network (Box 2).                              Node output activation may be in the range 01 (Box 3). A threshold value of 0.5 is assigned                                    to each node in output layer. \n2. Agent’s behavior, based on whether output layer activations have exceeded threshold values                        (fig 3), determines subsequent input pattern ‘t+1’. \n12 \n3. Framework determines ‘reward’ value based on ‘t+1’ input pattern ‘t+1’ and on activation                          of special output reward node (eg > 0.8 ). \n4. Weight changes are made: \na. Desired ‘t’ activations at the output layer are calculated: \ni. If reward condition then the desired ‘t’ activation for that node is set to                            maximal value (1.0) for those that were above threshold, and set to                        minimal value (0.0) for those that were below threshold. Set the desired                        activation for the special output reward node to 95% of reward value. \nii. If no reward then the desired ‘t’ activation for all nodes are random                          moderate values [0.45..0.55].Set the desired activation for the special                  output reward node to minimal value (0.0). \nb. Weight changes for state ‘t’ are made according to error values which are back                            propagated through the network (Box 4). \n  Note: To achieve this both the current and previous activations must be stored. Weight changes are  derived using back propagation on previous activations. \n  Box 5 Threshold Assignment of Rewards processing steps \n3.1 Example: Target tracking (part II)  The same problem as described in section 3.1.1 is revisited. In this example the agent is equipped with the                                      resources required to facilitate Secondary (Conditioned) Reinforcement. \n \n3.1.1 Problem description \nIn this example the agent must autonomously learn how to track a target (fig 11). \n \nfig11 Target tracking example start state  ● The target may appear in any one of nine cells of a grid (3x3). The agent is required to focus on the target,                                             \nby moving it to the centre cell.  ● The network is rewarded only if it moves the target to it’s centremost cell. Once the target is moved to the                                         \ncentremost cell it is moved to a new starting position on the grid.  ● The agent has no prior knowledge. It does not initially know it will be rewarded by relocating the target                                     \nuntil it does so. \n13 \n● The agent may only move the target 1 step (ie to an adjacent cell) in each iteration.  ● To increase the task difficulty the agent may not move the target diagonally in one movement, two steps are                                     \nrequired.  \n \n3.1.2 Network configuration \nA network was created (table 6)(fig 12): \nInput layer  9 input nodes; the centre node is designated as a special ‘reward’                        node \nHidden layer  12 hidden nodes \nOutput layer  4 motor nodes + 1 reward node \nNotes  Each layer fully connected to the next via weights.  Weights initially randomised.  Each hidden and output nodes assigned an exclusive bias unit.  Learning rate = 1.0 \nTable 6 Target tracking network configuration \n \n \n  fig12  \n \n14 \nThe nine input nodes are mapped to each cell in the grid (fig 13): \n \nfig13 Target tracking input mapping \nThe network has four output nodes, representing output motors (Up, Down, Left, Right). Depending on the output                                  the agent is able to move the target within the grid. In order for the agent to move the target in any given direction                                                there must be only one output node activation above threshold (> 0.5), otherwise the target will remain stationary.                                    Therefore there is only a 1in16 chance (24) the agent will move in the correct direction by chance. \nThe framework establishes the reward condition by assigning one the input nodes as a special ‘reward node’. The                                    framework consists of some rules to calculate previous (t) desired output patterns based on the current (t+1) input                                    pattern which contains the reward value. A reward indicator value of 1 is set if the network it to be rewarded, and                                            zero if it is to be punished. A reward occurs if the target is moved to the centre cell (fig 14):   \n \nfig14 Target tracking reward condition \nThe network is also assigned an additional special reward node in the output layer. A reward also occurs if the  activation of this output reward node is above a given criteria (eg 0.8), which indicates it has been previously  reinforced. If the subsequent input reward node or subsequent output reward node is activated, connections to the  output reward node and motor output nodes will be reinforced.   \nNote; In this example a discrete special reward node was added to the output layer. It is possible to to have no                                            dedicated reward node at output layer, instead it possible to test if activations of all motor neurons are high (and                                        have therefore matured). Consequently activations, connections and therefore the strength of memories will be                            proportional to the reward. \n  3.1.3 Results    Initially the agent moves the target randomly. Activations tend to be weak (eg 0.4  0.6) across all output nodes.                                        When rewarding behaviors are discovered these are further reinforced and the output matures (eg < 0.1, > 0.9).   \n \nScheme  # pattern presentations \nThreshold Assignment of Reward  (Unsupervised backpropagation) \n110324 \n \n15 \nTable 7 Target tracking results     \n   \nfig15 Conditioned Reinforcement full solution for target tracking task \nThe agent is able to learn how to move the target when placed in corners (cells 0,2,6,8) (table 7). The agent has                                            effectively established proxy rewards in intermediate locations (cells 1,3,5,7) allowing chained sequences of                          behaviors to be learned (fig 15). The chaining of sequences of behaviors enables long term strategy to be acquired,                                      this is demonstrated more substantially in the following maze navigation problem. \n3.2 Example: Maze navigation \n3.2.1 Problem description \nIn this example the agent must autonomously learn how to navigate a target through a maze (fig 16). \n \n \nfig16 Maze task start state  ● The target may appear in any one of nine cells of a grid (3x3). The agent is required to focus on the target,                                             \nby moving it the top centre cell.  ● Once the target is moved to the target cell (cell 1) it is moved back to it’s original starting position on the                                           \ngrid (cell 3).  ● The agent has no prior knowledge. It does not know that food will lead to a reward until it happens.  ● The agent may only move the target 1 step (ie to an adjacent cell) in each iteration.  ● To increase the task difficulty the agent may not move the target diagonally in one movement, two steps are                                     \nrequired.   ● To further increase task difficulty there is an invisible barrier between the start state and the goal. The agent                                     \nmust learn to navigate around the barrier. \n \n3.2.2 Network configuration \n  A network was created (table 8) (fig 17): \n16 \nInput layer  9 input nodes; the centre node is designated as a special ‘reward’                        node \nHidden layer  12 hidden nodes \nOutput layer  4 motor nodes + 1 reward node \nNotes  Each layer fully connected to the next via weights.  Weights initially randomised.  Each hidden and output nodes assigned an exclusive bias unit.  Learning rate = 1.0 \n \nTable 8 Maze task network configuration \n  fig17 Maze task network topology \n   \nThe nine input nodes are mapped to each cell in the grid (fig 18): \n \n17 \nfig18 Maze task input mapping \nThe network has four output nodes, representing output motors (Up, Down, Left, Right). Depending on the output                                  the agent is able to move the target within the grid. In order for the agent to move the target in any given direction                                                there must be only one output node activation above threshold (> 0.5), otherwise the target will remain stationary.                                    Therefore there is only a 1in16 chance (24) the agent will move in the correct direction by chance. \nThe framework establishes the reward condition by assigning one the input nodes as a special ‘reward node’. The                                    framework consists of some rules to calculate previous (t) desired output patterns based on the current (t+1) input                                    pattern which contains the reward value. A reward indicator value of 1 is set if the network it to be rewarded, and                                            zero if it is to be punished. A reward occurs if the target is moved to the top centre cell (cell 1).    The network is also assigned an additional special reward node in the output layer. A reward also occurs if the  activation of this output reward node is above a given criteria (eg 0.8), which indicates it has been previously  reinforced. Connections to the output reward node will be reinforced in the same manner as motor output  nodes if  the subsequent input reward node or subsequent output reward node is activated.      3.2.3 Results    Initially the network moves randomly through the environment. In time the agent is able to learn how to move the  target around corners. The physics rules prevent the agent from moving the target diagonally in one step, instead two  steps are required. The agent can only reinforce behaviors where there is an immediate reward in the next input  pattern. The agent has effectively established proxy rewards in intermediate locations (cells 2,5,4) allowing chained  sequences of behaviors to be learned (fig 1922).       \n \nfig19 Agent encounters the Primary  Reinforcer and learns first  behavior. \n \nfig20 Agent establishes first  Conditioned Reinforcer. \n \n \nfig21 Agent establishes a subsequent  Conditioned Reinforcer by  reference to the the initial  Conditioned Reinforcer. \n \nfig22 Agent establishes a series of  Conditioned Reinforcers from  goal to starting position. \n      With sufficient exposure the network learns the optimal behavior to achieve a reward; consistently moving one step                                  left/right/up/down towards the goal from the starting location (cell 3).     The chaining of sequences of behaviors enables long term strategy to be acquired (table 9).     \n \nScheme  # pattern presentations \nThreshold Assignment of Reward  (Unsupervised backpropagation) \n80184 \n  Table 9 Target tracking results \n \n18 \nLearning rates can be improved by shaping [9], that is initially placing the agent closer to the reward and once learnt  the distance may be increased.   \n3.3 Summary  A considerable challenge facing Reinforcement Learning schemes is that rewards can be very temporally delayed.  Secondary/Conditioned Reinforcement provides an effective solution to this ‘Temporal’ Credit Assignment  Problem. The reward condition was facilitated by adding single reward node to the output layer in addition to  evaluating the reward at the input layer. The output reward desired activation experiences ‘decay’ as it becomes  temporally distant (number of iterations) from the target input reward. It is necessary to enforce decay on the output  reward node in order to prevent the agent from becoming infinitely rewarded on a proxy reward that has been  established. If the agent becomes fixated on a proxy reward it will become ‘stuck’, repeating the previous behavior. \nIt is also possible to achieve this effect without an explicit motivator node in output layer. Since only strong                                      (output) behaviors are those which have been reinforced, a test can be made against the strength of the entire output                                        pattern rather than a specific node.  \nWhen using a single node representation for the (mapped) output layer reward node and enforcing decay, a                                  diminishing desired activation is set for that node. The node is interpreted in an analogue (continuous) fashion. An                                    alternative representation would be to introduce a discrete output reward node for each time step away from the                                    input node reward.  \n \n4 Threshold Assignment of Connections  So far backpropagation, an algorithm intended for supervised learning, has been housed within an unsupervised                              framework; desired output patterns have been presented by a framework acting as a surrogate supervisor. Using the                                  same framework as described in earlier sections, the backpropagation learning algorithm was replaced with an                              alternative algorithm, termed Threshold Assignment of Connections (TAC). This approach relies on a threshold                            to determine direct ‘onnode’ delta values that can be used to calculate weight updates, rather than backpropagating                                  them (Box 6). \n \nProcessing steps    Differences to the Conditioned Reinforcement process described previously are highlighted in bold.   \n1. Input pattern for state ‘t’ is presented and forward propagated through the network (Box 2).                              Node output activation may be in the range 01 (Box 3). A threshold value of 0.5 is assigned                                    to all nodes in every layer. \n2. Agent’s behavior, based on whether output layer activations have exceeded threshold values                        (fig 3), determines subsequent input pattern ‘t+1’. \n3. Framework determines ‘reward’ value based on ‘t+1’ input pattern ‘t+1’ and on activation                          of special output reward node (eg > 0.8 ). \n4. Weight changes are made: \n19 \na. Desired ‘t’ activations on every node are calculated: \ni. If reward condition then the desired ‘t’ activation for that node is set to                            maximal value (1.0) for those that were above threshold, and set to                        minimal value (0.0) for those that were below threshold. Set the desired                        activation for the special output reward node to 95% of reward value. \nii. If no reward then the desired ‘t’ activation for all nodes are random                          moderate values [0.45..0.55]. Set the desired activation for the special                    output reward node to minimal value (0.0). \nb. Weight changes for state ‘t’ are made for all connections to each node in situ                              according to error values (Box 7); these are not back propagated. \n  Note: To achieve this both the current and previous activations must be stored.  \n \nBox 6 Threshold Assignment of Connections processing steps \n \n1) Calculate desired activation for unit u based on reward r and threshold t.    ¬r )(r a ) ¬a ))> 0⇒ ( u > t⇒ du = 1 ⋀ ( u > t⇒ du = 0 ⋀ ( > 0⇒ du ~ U 0.45, .55[ 0 ]     2) Derive delta error value for node u, by finding difference between desired activation ( ) and actual activation                            du         ( ).au   =(  ) )eltad u du au au 1( −au     3) Derive weight change for connection between unit h and unit u, using learning rate.    = weightΔ uh rate deltal u ah    \n \nBox 7 Threshold Assignment of Connections weight change \n \n4.1 Example: Maze navigation  The same problem as described in section 3.2.2 is revisited. In this example the agent is equipped with the TAC                                        algorithm rather than backpropagation. \n  4.1.1 Problem description \nIn this example the agent must autonomously learn how to navigate a target through a maze (fig 23). \n \n20 \n  fig23 Maze task start state  ● The target may appear in any one of nine cells of a grid (3x3). The agent is required to focus on the target,                                             \nby moving it the top centre cell.  ● Once the target is moved to the target cell (cell 1) it is moved back to it’s original starting position on the                                           \ngrid (cell 3).  ● The agent has no prior knowledge. It does not know that food will lead to a reward until it happens.  ● The agent may only move the target 1 step (ie to an adjacent cell) in each iteration.  ● To increase the task difficulty the agent may not move the target diagonally in one movement, two steps                                   \nare required.   ● To further increase task difficulty there is an invisible barrier between the start state and goal. The agent                                   \nmust learn to navigate around the barrier. \n \n4.1.2 Network configuration \nA network was created (table 10)(fig 23): \nInput layer  9 input nodes; the centre node is designated as a special ‘reward’                        node \nHidden layer  12 hidden nodes \nOutput layer  4 motor nodes + 1 reward node \nNotes  Each layer fully connected to the next via weights.  Weights initially randomised.  Each hidden and output nodes assigned an exclusive bias unit.  Learning rate = 1.0 \n \nTable 10 Maze task network configuration \n21 \n  fig24 Maze task network topology \n  The nine input nodes are mapped to each cell in the grid (fig 25): \n \nfig25 Maze task input mapping \nThe network has four output nodes, representing output motors (Up, Down, Left, Right). Depending on the output                                  the agent is able to move the target within the grid. In order for the agent to move the target in any given direction                                                there must be only one output node activation above threshold (> 0.5), otherwise the target will remain stationary.                                    Therefore there is only a 1in16 chance (24) the agent will move in the correct direction by chance. \nThe framework establishes the reward condition by assigning one the input nodes as a special ‘reward node’. The                                    framework consists of some rules to calculate previous (t) desired output patterns based on the current (t+1) input                                    pattern which contains the reward value. A reward indicator value of 1 is set if the network it to be rewarded, and                                            zero if it is to be punished. A reward occurs if the target is moved to the top centre cell (cell 1).    The network is also assigned an additional special reward node in the output layer. A reward also occurs if the                                        activation of this output reward node is above a given criteria (eg 0.8), which indicates it has been previously                                      reinforced. If the subsequent input reward node or subsequent output reward node is activated, connections to the                                  output reward node and motor output nodes will be reinforced. \n22 \n  4.1.3 Results    Initially the network moves randomly through the environment. In time the agent is able to learn how to move                                      around corners to the goal. The physics rules prevent the agent from moving the target diagonally in one step,                                      instead two steps are required. The agent can only reinforce behaviors where there is an immediate reward in the                                      next input pattern. The agent has effectively established proxy rewards in intermediate locations (cells 2,5,4)                              allowing chained sequences of behaviors to be learned (fig 2629).     \n \nfig26 Agent encounters the Primary  Reinforcer and learns first  behavior. \n \nfig27 Agent establishes first  Conditioned Reinforcer. \n \n \nfig28 Agent establishes a subsequent  Conditioned Reinforcer by  reference to the the initial  Conditioned Reinforcer. \n \nfig29 Agent establishes a series of  Conditioned Reinforcers from  goal to starting position. \n      With sufficient exposure the network learns the optimal behavior to achieve a reward; consistently moving one                                step left/right/up/down towards the food reward from the starting location (cell 3). \n \nThe chaining of sequences of behaviors enables long term strategy to be acquired (table 11).   \nScheme  # pattern presentations \nThreshold Assignment of Reward  (Unsupervised backpropagation) \n80184 \nThreshold Assignment of Connections  (Unsupervised) \n18212 \n  Table 11 Maze task tracking results \n  Learning rates can be improved by shaping [9], that is initially placing the agent closer to the reward and once                                        learnt increasing the distance.   \n4.2 Example: XOR  The same problem as described in section 3.1.2 is revisited. In this example the agent is equipped with the TAC                                        algorithm rather than backpropagation. \n4.2.1 Problem description \nThe agent will be required to solve the XOR problem (table 12) in order to test the network's ability to map non                                            linear transformations that require multiple layers of neurons. This test will will also provide a performance                                comparison between unsupervised reinforcement learning and the supervised regime.    \n \n23 \n(input)  A  (input)  B \n(output)  XOR \n0  0  0 \n0  1  1 \n1  0  1 \n1  1  0 \n  Table 12 XOR task \n  The behavior, or output response of the network will not influence the subsequent input pattern. However, reward                                  feedback will be given to the network thereby informing whether the output was correct. Once again the output                                    patterns are not predetermined. This tightly controlled presentation of input patterns is termed ‘guided’. The physics                                rules are modified in order to guide the agent.   \n4.2.2 Network configuration \nA network was created (table 13) (fig 30): \nInput layer  2 input nodes \nHidden layer  3 hidden nodes \nOutput layer  1 output node \nNotes  Each layer fully connected to the next via weights.  Weights initially randomised.  Each hidden and output nodes assigned an exclusive bias unit.  Learning rate = 1.0 \n \nTable 13 XOR Network configuration   \n  fig30 XOR topology \n24 \n4.2.3 Results \n \nScheme  # pattern presentations \nSupervised backpropagation  2182 \nThreshold Assignment of Connections  (unsupervised) \n311 \n  Table 14 XOR results \n  The Threshold Assignment of Connections (unsupervised) scheme required significantly less pattern presentations  to learn the XOR solution than the supervised regime (table 14). Both were using the same set of initial weights and  learning parameters (learning rate etc). Despite the fact that in the supervised scheme the desired patterns are known  a priori , while in the unsupervised (guided) learning regime these must be discovered by the network through trial  and error. However this performance gap would be expected to lessen as the number of nodes in the output layer  grows, increasing dimensionality and with it the number of candidate combinations. \n4.3 Summary  These experiments demonstrate TAC may solve linearly inseparable problems that are normally reserved for                            supervised training regimes. It is capable of training weights deep within the network, thereby capable of forming                                  complex abstract representations. TAC can also solve problems that require long term strategy, when applied in                                conjunction with Secondary (Conditioned) Reinforcement. Once TAC has replaced backpropagation, the                      framework is no longer required to act as a surrogate supervisor \n \n5 Discussion  The present solution to RL requires the network to learn appropriate mappings based on the temporal ordering of                                    events; these mappings are contingent on subsequent reward conditions. Learning is based on cause and effect. The                                  model does not require a past/present array of network or environment states. While Q learning with neural                                  networks have been used principally to determine policy rather than action[1][7], in the present model they are                                  integrated. The presented model can be easily scaled, and does not compromise biological plausibility in the process.                                  In contrast to some other ANN predictors of time series events (eg Elman networks[13], LSTM[12]) the present                                  solution does not rely on recurrency. While the examples presented are based on feedforward architectures the                                present model is not mutually exclusive with recurrency. Indeed it is envisaged recurrency could be used to augment                                    the presented model, providing information of prior state.  \nThe merits of the present model can be evaluated in terms of Machine Learning Applications as well as the broader                                        Cognitive Science implications (eg biological, psychological, philosophical).  \n5.1 Applications \n5.1.1 Primary Reinforcement \nOne of the key advantages of Primary Reinforcement over supervised learning schemes is that the agent is able to                                      autonomously discover solutions (behaviors) to problems, of use when a human may be unable to provide training                                  data. Another benefit of Primary Reinforcement is that learning is dynamic and continuous, there is no distinction                                  between training and classification phases. This is useful when the agent encounters novel stimuli, or when a once                                   \n25 \nuseful response is no longer so. \n \nfig31 ‘Rags the robot’  \n \n‘Rags the robot’ learns how to track objects with a  vision sensor. The robot has an onboard Arduino  microcontroller and is wirelessly controlled by desktop  computer. \n \nThe learning scheme can be readily applied to robotics. As proof of concept an agent was created to remotely control                                        ‘Rags the robot’; a mobile Arduino based device (fig 31). The task was to target an object, in this case a red frisbee.                                              For this purpose Rags was equipped with a vision sensor (PixyCam), and was capable of rotating via a pair of                                        wheels. The agent begins each learning session with randomised network weights. Similar to the previous targeting                                tasks, the robot initially moves randomly, rotating passed the target. Typically within a couple of minutes                                (particularly sensitive on lighting conditions) the robot successfully comes to a resting state aiming at the target. \nThe Primary Reinforcement solution presented is readily scalable; additional sensors, motors and hidden units can                              be easily added without reworking the underlying learning process. These additional resources will be utilized in                                problem solving. \nAlthough Primary Reinforcement enables autonomous agents to discover solutions where a human supervisor is                            unable, a potential issue is that the learned behavior may not be ideal but is sufficient to achieve a reward condition;                                          this can be termed a suboptimal behavior limitation . For example a ball may have been successfully kicked into a                                      goal, but the kicking technique itself was poor. To avoid this a higher standard can be achieved by setting a more                                          stringent reward condition, the tradeoff being more optimal behaviors are potentially slower to be discovered and                                learnt. \nAnother limitation of the present solution is that we are unable to set arbitrary desired output activations on output                                      nodes, a maximal or minimal value is set (ie 1.0 or 0.0). For example in a network with three output nodes we may                                              choose a desired output pattern vector of [0.6, 0.3, 0.7] for a supervised network, but in the present model the                                        desired output pattern vector would be limited to [1.0, 0.0, 1.0]. This shortcoming can be termed a binary limitation .                                      To workaround this limitation alternate output representations may be required to achieve the same level of                                granularity. \nOne of the challenges of scaling relates to the complexity of output responses; the more output nodes that are                                      required for a rewarding behavior to occur, the longer it will take for the agent to explore candidate behaviors and                                        discover the requisite combination. Also it should also be noted that not all node activations may be relevant to                                      producing a rewarding response. These may be included in the dynamically generated desired output pattern, and the                                  agent is unable to differentiate which nodes were responsible for achieving the reward. These redundant activations                                may be eliminated by conforming to what can be termed a laziness principle , where rewards are reduced in                                    proportion to output node activations.  \nNote; while Reinforcement Learning is essentially a trial and error approach to learning, it is possible to first part                                      train  in supervised mode, save the weights, and then switch to unsupervised mode. \n \n26 \n5.1.2 Secondary reinforcement \nA potential issue facing the presented model may be termed a suboptimal path limitation . At first glance this is                                      similar to the suboptimal behavior limitation described in Primary Reinforcement, but on a temporal level. The                                problem being that the while the acquired sequence of behaviors reliably leads to the goal, and the individual                                    behaviors may have been optimal, the route was not the most efficient one possible. Once learnt the agent has no                                        pressure to alter its preferred route. This limitation also afflicts natural organisms. To mitigate this issue behaviors                                  may be ‘shaped’  to elicit the optimal path [9]. \n5.1.3 Threshold Assignment of Connections (TAC) \nTAC is an alternative solution to the credit assignment problem in ANN’s, a learning algorithm to allow weight                                    updates in multi layer networks. In this regard it does not offer any more or less functional benefit than                                      backpropagation. However, TAC also suffers from the binary limitation  previously outlined. \nTAC is significantly easier to implement than backpropagation. This would have significant advantages in                            hardware implementations. It has fewer error derivation dependencies than backpropagation, and hence more                          resilient should damage/information loss occur during weight updates.  \nIn terms of performance, TAC was found to learn XOR in fewer iterations than supervised backpropagation.                                However this advantage is expected to diminish as the number of output nodes, and therefore potential candidate                                  behaviors, increase. \n5.2 Implications \nReinforcement Learning has already been achieved in the real world as evidenced in nature. Therefore if we want                                    to achieve a working, scalable and reliable solution it is expedient to use nature as a reference. We may also then                                          use the model as a basis for other Cognitive Science experiments. The model may predict, or be refuted by,                                      nature’s solution to Reinforcement Learning. However that does not preclude the possibility that there is more than                                  one underlying mechanism that provides Reinforcement Learning in nature.  \n5.2.1 Primary Reinforcement    Consider a simple organism ‘agent V1’ that does not possess any plastic connections, whose behaviors are inherited                                  and fixed. The only way this organism and members of it’s species can generate new behaviors is via mutation and                                        natural selection. Individuals with mutations that produce desirable behaviors are more likely to survive and pass on                                  their genes and behaviors to the next generation.    More mutations of this species eventually result in an individual with some plastic connections; ‘agent V2’. It is able                                      to use a reward stimulus, or Primary Reinforcer, to strengthen the connections that were responsible for those                                  behaviors. This process of Primary Reinforcement allows the individual to evaluate and learn desirable behaviors.                              These desirable behaviors enhances its chances of survival and put it at a competitive advantage relative to its peers,                                      making it more likely to pass on this trait to its progeny. An incremental evolutionary step has provided the agent                                        with the power to learn a rewarding behavior. Likewise the model presented uses a simple mechanic to achieve this                                      feature.   \n5.2.2 Secondary Reinforcement    While ‘agent V2’ is able to learn isolated behaviors it is incapable of planning; it cannot learn a sequence of                                        behaviors. In order to do so this species must evolve again, this time using a resultant reward stimulus to not only                                          inform the development of mappings between the previous sensory input pattern with its resultant motor output                                pattern, but also between the previous sensory input pattern and the reward. Once the association between the                                  sensory input pattern and the reward stimulus is sufficiently strong a secondary (conditioned) reinforcer can be                               \n27 \nestablished. This proxy reward can be used to establish more conditioned reinforcers and thereby chain behaviors                                together, by doing so goal oriented problem solving can be achieved. Likewise an incremental change to the model                                    provides the agent with the power to form long term strategy.   \n5.2.3 Threshold Assignment of Connections (TAC)    A biological plausibility concern faces supervised learning schemes in general: \n● How are desired output patterns selected and presented to the network ? \nIn previous examples, once backpropagation is placed in the Reinforcement Learning framework it is no longer a                                  truly supervised learning scheme; desired output patterns are not known a priori . However backpropagation faces                              the additional biological plausibility concern: \n● How is error back propagated ? \nThus far research has provided little neurobiological support for backpropagation [4]. TAC does not require                              backward connections or tenuous biochemical explanations. TAC could be explained by neuromodulators rather                          than at the neural computation level. This would provide a more robust solution with a simpler biological                                  explanation. \nIt should be noted that while the model suggests assigning random desired activations around the threshold value                                  [0.45..0.55] in punishment conditions, it was found to be at least as effective to set this in the range [01]. It should                                            be appreciated that (with moderate learning rates) this has the same effect of establishing an ‘immature’ response                                  around the threshold level [fig 3]. These alternatives have implications as to what we might expect to find in                                      natural organisms.  \n \n5.3 Final words \nThis work presents a self organizing model of cognition; internal drives enforce learning and behavior. The model                                  is based on a single continuum of pleasure and pain, there is a single learning mechanism for the two; pleasure                                        effectively being the alleviation of pain. Whilst the examples provided are configured with only a single drive, this                                    should be extended to multiple drives working in concert/competition. Also the examples provided entail the drive                                being always active, and therefore continually reinforcing or weakening behaviors. However, it should be taken                              that when not active (eg drives are satiated), behaviors will not be reinforced or weakened and the agent will be in                                          something of a ‘cruise mode’ with no active learning. Also as a rule of thumb the agent should conform to a                                          ‘laziness principle’, that is they should be punished for exerting unnecessary energy, thereby achieving their goal                                with greater efficiency. \nEthical concerns may arise with such models. The designer may be inclined to set malevolent directives for the                                    agents, for example setting the Primary Reinforcement reward condition to intentionally cause harm; the agent will                                be compelled to fulfill this goal. Even with benevolent directives, once assigned the agent is free to adopt any                                      behavior to achieve its goals, also yielding harmful behaviors. \nThe proposed model may not be deemed accurate as a model of Reinforcement Learning. Would an accurate model                                    of Reinforcement Learning also be an accurate model of consciousness? While an agreed definition of                              consciousness is more elusive than that of intelligence, such models may at least assist in refining and testing                                    concepts of consciousness. Does consciousness only arise at a certain scale and complexity? Is recurrency and                                introspection required? Are plastic connections required? Is embodiment required? Are there even different forms,                            or levels of consciousness? Providing answers is beyond the scope of this paper, but while it may seem premature to                                        conclude that such models may be conscious, neither can the question be ignored from an ethics perspective.                                  Experiments conducted on a valid working pleasurepain based model would likely inflict some degree of suffering                                on the agent. \n28 \nThe model’s true potential lies in embodiment, whether in real or artificial environments. Agents may also be placed                                    in abstract environments that do not physically exist (eg a stock market). The model is generic and readily scalable,                                      and given sufficient time and resources can tackle a variety of tasks. Tasks can be assigned either by setting of the                                          reward condition goal, or by placing as obstacles to that goal. If the presented model of Reinforcement Learning                                    should be deemed implausible from a Cognitive Science perspective, the practical benefits at least may be of utility.  \n5.4 Appendix \n5.4.1 Platform  Hardware: Desktop PC (i3 3.7GHz, 16GB ram) \nSoftware: OS Linux Mint 17.3, Neural Network software written in C (GCC) \n \n6 References  [1] Charles W. Anderson. 1986. Learning and Problem Solving with Multilayer Connectionist Systems.                       \nTechnical Report. University of Massachusetts, Amherst, MA, USA. \n[2] Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., and Riedmiller,                            M.(2013). Playing atari with deep reinforcement learning.arXiv preprint arXiv:1312.5602 \n[3] Rumelhart, David E., Hinton, Geoffrey E., Williams, Ronald J., “Learning representations by                        backpropagating errors”, Nature, 1986 \n[4] D. G. Stork, \"Is backpropagation biologically plausible?,\" Neural Networks, 1989. IJCNN., International                        Joint Conference on , Washington, DC, USA, 1989, pp. 241246 vol.2. \n[5] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 1998. \n[6] Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J Gershman. Building machines                              that learn and think like people. arXiv preprint arXiv:1604.00289, 2016. \n[7] Gerald Tesauro, ”Temporal difference learning and TDGammon”, Communications of the ACM CACM                        Homepage archive, Volume 38 Issue 3, March 1995, Pages 5868 \n[8] Andrew G. Barto, Richard S. Sutton, Charles W. Anderson, “Neuronlike adaptive elements that can solve                              difficult learning control problems”, IEEE Transactions on Systems, Man, and Cybernetics                      (Volume:SMC13 ,  Issue: 5) pages 834  846, 1983 \n[9] Skinner, B. F. (1953). Science and human behavior. New York: Macmillan, Pages. 92–3 \n[10]William Bechtel, Adele Abrahamsen, “Connectionism and the mind”, Wiley,1991, pages 7097Skinner,                      B. F. (1953). Science and human behavior. New York: Macmillan, Pages. 92–3 \n[11]Hodgkin, A. L., A. F. Huxley, and B. Katz. “Measurement of CurrentVoltage Relations in the Membrane                                of the Giant Axon of Loligo.” The Journal of Physiology 116.4 (1952): 424–448. \n[12]Sepp Hochreiter and Jürgen Schmidhuber (1997). \"Long shortterm memory\" (PDF). Neural                      Computation. 9 (8): 1735–1780. http://dx.doi.org/10.1162/neco.1997.9.8.1735 \n[13]Elman, J. L. (1990), Finding Structure in Time. Cognitive Science, 14: 179–211.                        doi:10.1207/s15516709cog1402_1 \n \n29 "
    } ],
    "references" : [ {
      "title" : "Learning and Problem Solving with Multilayer Connectionist Systems",
      "author" : [ "Charles W. Anderson" ],
      "venue" : "Technical Report",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1986
    }, {
      "title" : "Playing atari with deep reinforcement learning.arXiv preprint arXiv:1312.5602",
      "author" : [ "V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Graves", "I. Antonoglou", "D. Wierstra", "Riedmiller" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2013
    }, {
      "title" : "Learning representations by     back­propagating errors",
      "author" : [ "Rumelhart", "David E", "Hinton", "Geoffrey E", "Williams", "Ronald J" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1986
    }, {
      "title" : "Is backpropagation biologically plausible",
      "author" : [ "D.G. Stork" ],
      "venue" : "​Neural Networks, 1989. IJCNN., International      Joint Conference on  ​  , Washington, DC, USA, 1989, pp. 241­246 vol.2.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "Reinforcement learning: An introduction",
      "author" : [ "Richard S Sutton", "Andrew G Barto" ],
      "venue" : "MIT press,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1998
    }, {
      "title" : "Building machines   that learn and think like people",
      "author" : [ "Brenden M Lake", "Tomer D Ullman", "Joshua B Tenenbaum", "Samuel J Gershman" ],
      "venue" : "arXiv preprint arXiv:1604.00289,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2016
    }, {
      "title" : "Temporal difference learning and TD­Gammon”, Communications of the ACM CACM",
      "author" : [ "Gerald Tesauro" ],
      "venue" : "Homepage archive, Volume 38 Issue",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1995
    }, {
      "title" : "Connectionism and the mind”, Wiley,1991, pages 70­97Skinner",
      "author" : [ "William Bechtel", "Adele Abrahamsen" ],
      "venue" : "Science and human behavior",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1953
    }, {
      "title" : "Measurement of Current­Voltage Relations in the Membrane       of the Giant Axon of Loligo.",
      "author" : [ "A.L. Hodgkin", "A.F. Huxley", "B. Katz" ],
      "venue" : "The Journal of Physiology",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1952
    }, {
      "title" : "Long short­term memory\" (PDF). Neural     Computation",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber" ],
      "venue" : null,
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1997
    }, {
      "title" : "Finding Structure in Time",
      "author" : [ "J.L. Elman" ],
      "venue" : "Cognitive Science,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1990
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "In contrast the work presented follows a distinctly top­down approach attempting to model intelligence as a whole system; a causal agent interacting with the environment [6].",
      "startOffset" : 170,
      "endOffset" : 173
    }, {
      "referenceID" : 1,
      "context" : "Significant RL successes have been achieved in AI notably with the use of Q­learning [2][7].",
      "startOffset" : 85,
      "endOffset" : 88
    }, {
      "referenceID" : 6,
      "context" : "Significant RL successes have been achieved in AI notably with the use of Q­learning [2][7].",
      "startOffset" : 88,
      "endOffset" : 91
    }, {
      "referenceID" : 4,
      "context" : "Primary reinforcement reward conditions (eg hunger or thirst) typically drive some form of homeostatic, adaptive control function for the agent [5][8].",
      "startOffset" : 144,
      "endOffset" : 147
    }, {
      "referenceID" : 2,
      "context" : "Weight adjustments using backpropagation[3][9] are traditionally used in supervised learning schemes; that is desired activations (or training sets) are established prior to the learning phase.",
      "startOffset" : 40,
      "endOffset" : 43
    }, {
      "referenceID" : 2,
      "context" : "The learning scheme consists of the following components: ● Artificial Neural Network ● Learning Algorithm ● Framework ● Environment The network architecture is that of a familiar multilayer perceptron (MLP)[3].",
      "startOffset" : 207,
      "endOffset" : 210
    }, {
      "referenceID" : 2,
      "context" : "Weight updates are derived using a standard (supervised) back propagation learning algorithm[3][10].",
      "startOffset" : 92,
      "endOffset" : 95
    }, {
      "referenceID" : 7,
      "context" : "Weight updates are derived using a standard (supervised) back propagation learning algorithm[3][10].",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 8,
      "context" : "Thresholds for neuron activation are widely found in animals, where sufficient ‘excitation’ is required to result in an electrical action potential that can be signalled to other neurons [11].",
      "startOffset" : 187,
      "endOffset" : 191
    }, {
      "referenceID" : 0,
      "context" : "While ​Q learning with neural networks have been used principally to determine policy rather than action[1][7], in the present model they are integrated​.",
      "startOffset" : 104,
      "endOffset" : 107
    }, {
      "referenceID" : 6,
      "context" : "While ​Q learning with neural networks have been used principally to determine policy rather than action[1][7], in the present model they are integrated​.",
      "startOffset" : 107,
      "endOffset" : 110
    }, {
      "referenceID" : 10,
      "context" : "In contrast to some other ANN predictors of time series events (eg Elman networks[13], LSTM[12]) the present solution does not rely on recurrency.",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 9,
      "context" : "In contrast to some other ANN predictors of time series events (eg Elman networks[13], LSTM[12]) the present solution does not rely on recurrency.",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 3,
      "context" : "However backpropagation faces the additional biological plausibility concern: ● How is error back propagated ? Thus far research has provided little neurobiological support for backpropagation [4].",
      "startOffset" : 193,
      "endOffset" : 196
    } ],
    "year" : 0,
    "abstractText" : "A generic and scalable Reinforcement Learning scheme for Artificial Neural Networks is presented,   providing a general purpose learning machine. By reference to a node threshold three features are  described 1) A mechanism for Primary Reinforcement, capable of solving linearly inseparable problems 2)   The learning scheme is extended to include a mechanism for Conditioned Reinforcement, capable of   forming long term strategy 3) The learning scheme is modified to use a threshold­based deep learning   algorithm, providing a robust and biologically inspired alternative to backpropagation. The model may be   used for supervised as well as unsupervised training regimes.",
    "creator" : null
  }
}