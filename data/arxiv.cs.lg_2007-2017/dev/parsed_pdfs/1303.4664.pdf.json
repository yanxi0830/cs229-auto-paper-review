{
  "name" : "1303.4664.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Large-Scale Learning with Less RAM via Randomization",
    "authors" : [ "Daniel Golovin", "Brendan McMahan" ],
    "emails" : [ "dgg@google.com", "dsculley@google.com", "mcmahan@google.com", "mwyoung@google.com" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "As the growth of machine learning data sets continues to accelerate, available machine memory (RAM) is an increasingly important constraint. This is true for training massive-scale distributed learning systems, such as those used for predicting ad click through rates (CTR) for sponsored search (Richardson et al., 2007; Craswell et al., 2008; Bilenko & Richardson, 2011; Streeter & McMahan, 2010) or for filtering email spam at scale (Goodman et al., 2007). Minimizing RAM use is also important on a single machine if we wish to utilize the limited memory of a fast GPU processor, or to simply use fast L1-cache more effectively. After training, memory cost remains a key consideration at prediction time as real-world models are often replicated to multiple machines to minimize prediction latency.\nThis is an extend version of the paper of the same name which appeared in ICML 2013. The main addition is Appendix A.3, which contains additional proofs.\nEfficient learning at peta-scale is commonly achieved by online gradient descent (OGD) (Zinkevich, 2003) or stochastic gradient descent (SGD), (e.g., Bottou & Bousquet, 2008), in which many tiny steps are accumulated in a weight vector β ∈ Rd. For large-scale learning, storing β can consume considerable RAM, especially when datasets far exceed memory capacity and examples are streamed from network or disk.\nOur goal is to reduce the memory needed to store β. Standard implementations store coefficients in single precision floating-point representation, using 32 bits per value. This provides fine-grained precision needed to accumulate these tiny steps with minimal roundoff error, but has a dynamic range that far exceeds the needs of practical machine learning (see Figure 1).\nWe use coefficient representations that have more limited precision and dynamic range, allowing values to be stored cheaply. This coarse grid does not provide enough resolution to accumulate gradient steps without error, as the grid spacing may be larger than the updates. But we can obtain a provable safety guarantee through a suitable OGD algorithm that uses randomized rounding to project its coefficients onto the grid each round. The precision of the grid used on each round may be fixed in advance or changed adaptively as learning progresses. At prediction time, more aggressive rounding is possible because errors no longer accumulate.\nOnline learning on large feature spaces where some features occur very frequently and others are rare often benefits from per-coordinate learning rates, but this requires an additional 32-bit count to be stored for each coordinate. In the spirit of randomized rounding, we limit the memory footprint of this strategy by using an 8-bit randomized counter for each coordinate based on a variant of Morris’s algorithm (1978). We show the resulting regret bounds are only slightly worse than the exact counting variant (Theorem 3.3), and empirical results show negligible added loss.\nar X\niv :1\n30 3.\n46 64\nv1 [\ncs .L\nG ]\n1 9\nM ar\n2 01\nContributions This paper gives the following theoretical and empirical results:\n1. Using a pre-determined fixed-point representation of coefficient values reduces cost from 32 to 16 bits per value, at the cost of a small linear regret term.\n2. The cost of a per-coordinate learning rate schedule can be reduced from 32 to 8 bits per coordinate using a randomized counting scheme.\n3. Using an adaptive per-coordinate coarse representation of coefficient values reduces memory cost further and yields a no–regret algorithm.\n4. Variable-width encoding at prediction time allows coefficients to be encoded even more compactly (less than 2 bits per value in experiments) with negligible added loss.\nApproaches 1 and 2 are particularly attractive, as they require only small code changes and use negligible additional CPU time. Approaches 3 and 4 require more sophisticated data structures."
    }, {
      "heading" : "2. Related Work",
      "text" : "In addition to the sources already referenced, related work has been done in several areas.\nSmaller Models A classic approach to reducing memory usage is to encourage sparsity, for example via the Lasso (Tibshirani, 1996) variant of least-squares regression, and the more general application of L1 regularizers (Duchi et al., 2008; Langford et al., 2009; Xiao, 2009; McMahan, 2011). A more recent trend has been to reduce memory cost via the use of feature hashing (Weinberger et al., 2009). Both families of approaches are effective. The coarse encoding schemes reported here may be used in conjunction with these methods to give further reductions in memory usage.\nRandomized Rounding Randomized rounding schemes have been widely used in numerical computing and algorithm design (Raghavan & Tompson, 1987). Recently, the related technique of randomized counting has enabled compact language models\n(Van Durme & Lall, 2009). To our knowledge, this paper gives the first algorithms and analysis for online learning with randomized rounding and counting.\nPer-Coordinate Learning Rates Duchi et al. (2010) and McMahan & Streeter (2010) demonstrated that per-coordinate adaptive regularization (i.e., adaptive learning rates) can greatly boost prediction accuracy. The intuition is to let the learning rate for common features decrease quickly, while keeping the learning rate high for rare features. This adaptivity increases RAM cost by requiring an additional statistic to be stored for each coordinate, most often as an additional 32-bit integer. Our approach reduces this cost by using an 8-bit randomized counter instead, using a variant of Morris’s algorithm (Morris, 1978)."
    }, {
      "heading" : "3. Learning with Randomized Rounding and Probabilistic Counting",
      "text" : "For concreteness, we focus on logistic regression with binary feature vectors x ∈ {0, 1}d and labels y ∈ {0, 1}. The model has coefficients β ∈ Rd, and gives predictions pβ (x) ≡ σ(β · x), where σ(z) ≡ 1/(1+e−z) is the logistic function. Logistic regression finds the model that minimizes the logistic–loss L. Given a labeled example (x, y) the logistic–loss is\nL(x, y;β) ≡ −y log (pβ (x))− (1− y) log (1− pβ (x))\nwhere we take 0 log 0 = 0. Here, we take log to be the natural logarithm. We define ‖x‖p as the `p norm of a vector x; when the subscript p is omitted, the `2 norm is implied. We use the compressed summation notation g1:t ≡ ∑t s=1 gs for scalars, and similarly\nf1:t(x) ≡ ∑t s=1 fs(x) for functions.\nThe basic algorithm we propose and analyze is a variant of online gradient descent (OGD) that stores coefficients β in a limited precision format using a discrete set ( Z)d. For each OGD update, we compute each new coefficient value in 64-bit floating point representation and then use randomized rounding to project the updated value back to the coarser representation.\nA useful representation for the discrete set ( Z)d is the Qn.m fixed-point representation. This uses n bits for the integral part of the value, and m bits for the fractional part. Adding in a sign bit results in a total of K = n + m + 1 bits per value. The value m may be fixed in advance, or set adaptively as described below. We use the method RandomRound from Algorithm 1 to project values onto this encoding.\nThe added CPU cost of fixed-point encoding and randomized rounding is low. Typically K is chosen to correspond to a machine integer (say K = 8 or 16),\nAlgorithm 1 OGD-Rand-1d\ninput: feasible set F = [−R,R], learning rate schedule ηt, resolution schedule t define fun Project (β) = max(−R,min(β,R)) Initialize β̂1 = 0 for t=1, . . . , T do\nPlay the point β̂t, observe gt βt+1 = Project ( β̂t − ηtgt ) β̂t+1 ← RandomRound(βt+1, t)\nfunction RandomRound(β, ) a← ⌊ β ⌋ ; b← ⌈ β ⌉ return { b with prob. (β − a)/ a otherwise\nso converting back to a floating point representations requires a single integer-float multiplication (by = 2−m). Randomized rounding requires a call to a pseudo-random number generator, which may be done in 18-20 flops. Overall, the added CPU overhead is negligible, especially as many large-scale learning methods are I/O bound reading from disk or network rather than CPU bound."
    }, {
      "heading" : "3.1. Regret Bounds for Randomized Rounding",
      "text" : "We now prove theoretical guarantees (in the form of upper bounds on regret) for a variant of OGD that uses randomized rounding on an adaptive grid as well as per-coordinate learning rates. (These bounds can also be applied to a fixed grid). We use the standard definition\nRegret ≡ T∑ t=1 ft(β̂t)− arg min β∗∈F T∑ t=1 ft(β ∗)\ngiven a sequence of convex loss functions ft. Here the β̂t our algorithm plays are random variables, and since we allow the adversary to adapt based on the previously observed β̂t, the ft and post-hoc optimal β ∗ are also random variables. We prove bounds on expected regret, where the expectation is with respect to the randomization used by our algorithms (highprobability bounds are also possible). We consider regret with respect to the best model in the nondiscretized comparison class F = [−R,R]d.\nWe follow the usual reduction from convex to linear functions introduced by Zinkevich (2003); see also Shalev-Shwartz (2012, Sec. 2.4). Further, since we consider the hyper-rectangle feasible set F = [−R,R]d, the linear problem decomposes into n independent one-dimensional problems.1 In this setting, we consider OGD with randomized rounding to an adaptive\n1Extension to arbitrary feasible sets is possible, but\ngrid of resolution t on round t, and an adaptive learning rate ηt. We then run one copy of this algorithm for each coordinate of the original convex problem, implying that we can choose the ηt and t schedules appropriately for each coordinate. For simplicity, we assume the t resolutions are chosen so that −R and +R are always gridpoints. Algorithm 1 gives the onedimensional version, which is run independently on each coordinate (with a different learning rate and discretization schedule) in Algorithm 2. The core result is a regret bound for Algorithm 1 (omitted proofs can be found in the Appendix):\nTheorem 3.1. Consider running Algorithm 1 with adaptive non-increasing learning-rate schedule ηt, and discretization schedule t such that t ≤ γηt for a constant γ > 0. Then, against any sequence of gradients g1, . . . , gT (possibly selected by an adaptive adversary) with |gt| ≤ G, against any comparator point β∗ ∈ [−R,R], we have\nE[Regret(β∗)] ≤ (2R) 2\n2ηT +\n1 2 (G2 + γ2)η1:T + γR\n√ T .\nBy choosing γ sufficiently small, we obtain an expected regret bound that is indistinguishable from the nonrounded version (which is obtained by taking γ = 0). In practice, we find simply choosing γ = 1 yields excellent results. With some care in the choice of norms used, it is straightforward to extend the above result to d dimensions. Applying the above algorithm on a per-coordinate basis yields the following guarantee:\nCorollary 3.2. Consider running Algorithm 2 on the feasible set F = [−R,R]d, which in turn runs Algorithm 1 on each coordinate. We use percoordinate learning rates ηt,i = α/\n√ τt,i with α =√ 2R/ √ G2 + γ2, where τt,i ≤ t is the number of nonzero gs,i seen on coordinate i on rounds s = 1, . . . , t. Then, against convex loss functions ft, with gt a subgradient of ft at β̂t, such that ∀t, ‖gt‖∞ ≤ G, we have\nE[Regret] ≤ d∑ i=1 ( 2R √ 2τT,i(G2 + γ2) + γR √ τT,i ) .\nThe proof follows by summing the bound from Theorem 3.1 over each coordinate, considering only the rounds when gt,i 6= 0, and then using the inequality∑T t=1 1/ √ t ≤ 2 √ T to handle the sum of learning rates on each coordinate.\nThe core intuition behind this algorithm is that for features where we have little data (that is, τi is small, for\nchoosing the hyper-rectangle simplifies the analysis; in practice, projection onto the feasible set rarely helps performance.\nAlgorithm 2 OGD-Rand\ninput: feasible set F = [−R,R]d, parameters α, γ > 0 Initialize β̂1 = 0 ∈ Rd; ∀i, τi = 0 for t=1, . . . , T do\nPlay the point β̂t, observe loss function ft for i=1, . . . , d do\nlet gt,i = ∇ft(xt)i if gt,i = 0 then continue τi ← τi + 1 let ηt,i = α/ √ τi and t,i = γηt,i\nβt+1,i ← Project ( β̂t,i − ηt,igt,i ) β̂t+1,i ← RandomRound(βt+1,i, t,i)\nexample rare words in a bag-of-words representation, identified by a binary feature), using a fine-precision coefficient is unnecessary, as we can’t estimate the correct coefficient with much confidence. This is in fact the same reason using a larger learning rate is appropriate, so it is no coincidence the theory suggests choosing t and ηt to be of the same magnitude.\nFixed Discretization Rather than implementing an adaptive discretization schedule, it is more straightforward and more efficient to choose a fixed grid resolution, for example a 16-bit Qn.m representation is sufficient for many applications.2 In this case, one can apply the above theory, but simply stop decreasing the learning rate once it reaches say (= 2−m). Then, the η1:T term in the regret bound yields a linear term like O( T ); this is unavoidable when using a fixed resolution . One could let the learning rate continue to decrease like 1/ √ t, but this would provide no benefit; in fact, lower-bounding the learning-rate is known to allow online gradient descent to provide regret bounds against a moving comparator (Zinkevich, 2003).\nData Structures There are several viable approaches to storing models with variable–sized coefficients. One can store all keys at a fixed (low) precision, then maintain a sequence of maps (e.g., as hashtables), each containing a mapping from keys to coefficients of increasing precision. Alternately, a simple linear probing hash–table for variable length keys is efficient for a wide variety of distributions on key lengths, as demonstrated by Thorup (2009). With this data structure, keys and coefficient values can be treated as strings over 4-bit or 8-bit bytes, for example. Blandford & Blelloch (2008) provide yet another data structure: a compact dictionary for variable length keys. Finally, for a fixed model, one can write out the string\n2If we scale x → 2x then we must take β → β/2 to make the same predictions, and so appropriate choices of n and m must be data-dependent.\ns of all coefficients (without end of string delimiters), store a second binary string of length s with ones at the coefficient boundaries, and use any of a number of rank/select data structures to index into it, e.g., the one of Patrascu (2008)."
    }, {
      "heading" : "3.2. Approximate Feature Counts",
      "text" : "Online convex optimization methods typically use a learning rate that decreases over time, e.g., setting ηt proportional to 1/ √ t. Per-coordinate learning rates require storing a unique count τi for each coordinate, where τi is the number of times coordinate i has appeared with a non-zero gradient so far. Significant space is saved by using a 8-bit randomized counting scheme rather than a 32-bit (or 64-bit) integer to store the d total counts. We use a variant of Morris’ probabilistic counting algorithm (1978) analyzed by Flajolet (1985). Specifically, we initialize a counter C = 1, and on each increment operation, we increment C with probability p(C) = b−C , where base b is a parameter. We estimate the count as τ̃(C) = b C−b b−1 , which is an unbiased estimator of the true count. We then use learning rates ηt,i = α/ √ τ̃t,i + 1, which ensures that even when τ̃t,i = 0 we don’t divide by zero.\nWe compute high-probability bounds on this counter in Lemma A.1. Using these bounds for ηt,i in conjunction with Theorem 3.1, we obtain the following result (proof deferred to the appendix).\nTheorem 3.3. Consider running the algorithm of Corollary 3.2 under the assumptions specified there, but using approximate counts τ̃i in place of the exact counts τi. The approximate counts are computed using the randomized counter described above with any base b > 1. Thus, τ̃t,i is the estimated number of times gs,i 6= 0 on rounds s = 1, . . . , t, and the per–coordinate learning rates are ηt,i = α/ √ τ̃t,i + 1. With an appropriate choice of α we have\nE[Regret(g)] = o ( R √ G2 + γ2T 0.5+δ ) for all δ > 0,\nwhere the o-notation hides a small constant factor and the dependence on the base b.3"
    }, {
      "heading" : "4. Encoding During Prediction Time",
      "text" : "Many real-world problems require large-scale prediction. Achieving scale may require that a trained model be replicated to multiple machines (Buciluǎ et al., 2006). Saving RAM via rounding is especially attractive here, because unlike in training accumulated\n3Eq. (5) in the appendix provides a non-asymptotic (but more cumbersome) regret bound.\nroundoff error is no longer an issue. This allows even more aggressive rounding to be used safely.\nConsider a rounding a trained model β to some β̂. We can bound both the additive and relative effect on logistic–loss L(·) in terms of the quantity |β ·x− β̂ ·x|:\nLemma 4.1 (Additive Error). Fix β, β̂ and (x, y). Let δ = |β · x− β̂ · x|. Then the logistic–loss satisfies\nL(x, y; β̂)− L(x, y;β) ≤ δ.\nProof. It is well known that ∣∣∣∂L(x,y;β)∂βi ∣∣∣ ≤ 1 for all x, y, β and i, which implies the result.\nLemma 4.2 (Relative Error). Fix β, β̂ and (x, y) ∈ {0, 1}d × {0, 1}. Let δ = |β · x− β̂ · x|. Then\nL(x, y; β̂)− L(x, y;β) L(x, y;β) ≤ eδ − 1.\nSee the appendix for a proof. Now, suppose we are using fixed precision numbers to store our model coefficients such as the Qn.m encoding described earlier, with a precision of . This induces a grid of feasible model coefficient vectors. If we randomly round each coefficient βi (where |βi| ≤ 2n) independently up or down to the nearest feasible value β̂i, such that\nE[β̂i] = βi, then for any x ∈ {0, 1}d our predicted logodds ratio, β̂ ·x is distributed as a sum of independent random variables {β̂i | xi = 1}.\nLet k = ‖x‖0. In this situation, note that |β · x − β̂ · x| ≤ ‖x‖1 = k, since |βi − β̂i| ≤ for all i. Thus Lemma 4.1 implies\nL(x, y; β̂)− L(x, y;β) ≤ ‖x‖1.\nSimilarly, Lemma 4.2 immediately provides an upper bound of e k − 1 on relative logistic error; this bound is relatively tight for small k, and holds with probability one, but it does not exploit the fact that the randomness is unbiased and that errors should cancel out when k is large. The following theorem gives a bound on expected relative error that is much tighter for large k:\nTheorem 4.3. Let β̂ be a model obtained from β using unbiased randomized rounding to a precision grid as described above. Then, the expected logistic– loss relative error of β̂ on any input x is at most 2 √ 2πk exp ( 2k/2 ) where k = ‖x‖0.\nAdditional Compression Figure 1 reveals that coefficient values are not uniformly distributed. Storing these values in a fixed-point representation means that individual values will occur many times. Basic information theory shows that the more common val-\nues may be encoded with fewer bits. The theoretical bound for a whole model with d coefficients is − ∑d i=1 log p(βi) d bits per value, where p(v) is the probability of occurrence of v in β across all dimensions d. Variable length encoding schemes may approach this limit and achieve further RAM savings."
    }, {
      "heading" : "5. Experimental Results",
      "text" : "We evaluated on both public and private large data sets. We used the public RCV1 text classification data set, specifically from Chang & Lin (2011). In keeping with common practice on this data set, the smaller “train” split of 20,242 examples was used for parameter tuning and the larger “test” split of 677,399 examples was used for the full online learning experiments. We also report results from a private CTR data set of roughly 30M examples and 20M features, sampled from real ad click data from a major search engine. Even larger experiments were run on data sets of billions of examples and billions of dimensions, with similar results as those reported here.\nThe evaluation metrics for predictions are error rate for the RCV1 data, and AucLoss (or 1-AUC) relative to a control model for the CTR data. Lower values are better. Metrics are computed using progressive validation (Blum et al., 1999) as is standard for online learning: on each round a prediction is made for a given example and record for evaluation, and only after that is the model allowed to train on the example. We also report the number of bits per coordinate used.\nRounding During Training Our main results are given in Figure 2. The comparison baseline is online logistic regression using a single global learning rate and 32-bit floats to store coefficients. We also test the effect of per-coordinate learning rates with both 32- bit integers for exact counts and with 8-bit randomized counts. We test the range of tradeoffs available for fixed-precision rounding with randomized counts, varying the number of precision m in q2.m encoding to plot the tradeoff curve (cyan). We also test the range\nof tradeoffs available for adaptive-precision rounding with randomized counts, varying the precision scalar γ to plot the tradeoff curve (dark red). For all randomized counts a base of 1.1 was used. Other than these differences, the algorithms tested are identical.\nUsing a single global learning rate, a fixed q2.13 encoding saves 50% of the RAM at no added loss compared to the baseline. The addition of per-coordinate learning rates gives significant improvement in predictive performance, but at the price of added memory consumption, increasing from 32 bits per coordinate to 64 bits per coordinate in the baselines. Using randomized counts reduces this down to 40 bits per coordinate. However, both the fixed-precision and the adaptive precision methods give far better results, achieving the same excellent predictive performance as the 64-bit method with 24 bits per coefficient or less. This saves 62.5% of the RAM cost compared to the 64-bit method, and is still smaller than using 32-bit floats with a global learning rate.\nThe benefit of adaptive precision is only apparent on the larger CTR data set, which has a “long tail” distribution of support across features. However, it is useful to note that the simpler fixed-precision method also gives great benefit. For example, using q2.13 encoding for coefficient values and 8-bit randomized counters allows full-byte alignment in naive data structures.\nRounding at Prediction Time We tested the effect of performing coarser randomized rounding of a fully-trained model on the CTR data, and compared to the loss incurred using a 32-bit floating point representation. These results, given in Table 1, clearly support the theoretical analysis that suggests more aggressive rounding is possible at prediction time. Surprisingly coarse levels of precision give excellent results, with little or no loss in predictive performance. The memory savings achievable in this scheme are considerable, down to less than two bits per value for q2.7 with theoretically optimal encoding of the discrete values."
    }, {
      "heading" : "6. Conclusions",
      "text" : "Randomized storage of coefficient values provides an efficient method for achieving significant RAM savings both during training and at prediction time.\nWhile in this work we focus on OGD, similar randomized rounding schemes may be applied to other learning algorithms. The extension to algorithms that efficiently handle L1 regularization, like RDA (Xiao, 2009) and FTRL-Proximal (McMahan, 2011), is rela-\ntively straightforward.4 Large scale kernel machines, matrix decompositions, topic models, and other largescale learning methods may all be modifiable to take advantage of RAM savings through low precision randomized rounding methods."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We would like to thank Matthew Streeter, Gary Holt, Todd Phillips, and Mark Rose for their help with this work."
    }, {
      "heading" : "A. Appendix: Proofs",
      "text" : "A.1. Proof of Theorem 3.1\nOur analysis extends the technique of Zinkevich (2003). Let β∗ be any feasible point (with possibly infinite precision coefficients). By the definition of βt+1,\n‖βt+1−β∗‖2 = ‖β̂t−β∗‖2−2ηtgt · (β̂t−β∗)+η2t ‖gt‖2.\nRearranging the above yields\ngt · (β̂t − β∗)\n≤ 1 2ηt\n( ‖β̂t − β∗‖2 − ‖βt+1 − β∗‖2 ) + ηt 2 ‖gt‖2\n= 1\n2ηt\n( ‖β̂t − β∗‖2 − ‖β̂t+1 − β∗‖2 ) + ηt 2 ‖gt‖2 + ρt,\nwhere the ρt = 1\n2ηt\n( ‖β̂t+1 − β∗‖2 − ‖βt+1 − β∗‖2 ) terms will capture the extra regret due to the randomized rounding. Summing over t, and following Zinkevich’s analysis, we obtain a bound of\nRegret(T ) ≤ (2R) 2 2ηT + ‖gt‖22 2 η1:T + ρ1:T .\nIt remains to bound ρ1:T . Letting dt = βt+1 − β̂t+1 and at = dt/ηt, we have\nρ1:T = T∑ t=1 1 2ηt ( (β̂t+1 − β∗)2 − (βt+1 − β∗)2 ) ≤\nT∑ t=1 1 2ηt ( β̂2t+1 − β2t+1 ) + β∗a1:T\n≤ T∑ t=1 1 2ηt ( β̂2t+1 − β2t+1 ) +R |a1:T | .\nWe bound each of the terms in this last expression in expectation. First, note |dt| ≤ t ≤ γηt by definition of the resolution of the rounding grid, and so\n4Some care must be taken to store a discretized version of a scaled gradient sum, so that the dynamic range remains roughly unchanged as learning progresses.\n|at| ≤ γ. Further E[dt] = 0 since the rounding is unbiased. Letting W = |a1:T |, by Jensen’s inequality we have E[W ]2 ≤ E[W 2]. Thus, E[|a1:T |] ≤√\nE[(a1:T )2] = √\nVar(a1:T ), where the last equality follows from the fact E[a1:T ] = 0. The at are not independent given an adaptive adversary.5 Nevertheless, consider any as and at with s < t. Since both have expectation zero, Cov(as, at) = E[asat]. By construction, E[at | gt, βt,histt] = 0, where histt is the full history of the game up until round t, which includes as in particular. Thus\nCov(as, at) = E[asat] = E [ E[asat | gt, βt,histt] ] = 0.\nFor all t, |at| ≤ γ so Var(at) ≤ γ2, and Var(a1:T ) =∑ t Var(at) ≤ γ2T . Thus, E[|a1:T |] ≤ γ √ T .\nNext, consider E[β̂2t+1 − β2t+1 | βt+1]. Since E[β̂t+1 | βt+1] = βt+1, for any shift s ∈ R, we have E [ (β̂t+1 −\ns)2 − (βt+1 − s)2 | βt+1 ] = E [ β̂2t+1 − β2t+1 | βt+1 ] , and so taking s = βt+1,\n1 ηt E [ β̂2t+1 − β2t+1 | βt+1 ] = 1 ηt E [ (β̂t+1 − βt+1)2 | βt+1 ] ≤ 2 t\nηt ≤ γ 2η2t ηt = γ2ηt.\nCombining this result with E[|a1:T |] ≤ γ √ T , we have\nE [ρ1:T ] ≤ γ2η1:T + γR √ T ,\nwhich completes the proof.\nA.2. Approximate Counting\nWe first provide high–probability bounds for the approximate counter.\nLemma A.1. Fix T and t ≤ T . Let Ct+1 be the value of the counter after t increment operations using the approximate counting algorithm described in Section 3.2 with base b > 1. Then, for all c > 0, the estimated count τ̃(Ct+1) satisfies\nPr [ τ̃(Ct+1) <\nt bc log(T ) − 1 ] ≤ 1 T c−1 (1)\nand\nPr [ τ̃(Ct+1) > et\nb− 1 b √ 2c logb(T )+2\n] ≤ 1\nT c . (2)\nBoth T and c are essentially parameters of the bound; in the Eq. (2), any choices of T and c that keep T c\n5For example the adversary could ensure at+1 = 0 (by playing gt+1 = 0) iff at > 0.\nconstant produce the same bound. In the first bound, the result is sharpest when T = t, but it will be convenient to set T equal to the total number of rounds so that we can easily take a union bound (in the proof of Theorem 3.3).\nProof of Lemma A.1. Fix a sequence of T increments, and let Ci denote the value of the approximate counter at the start of increment number i, so C1 = 1. Let Xj = |{i : Ci = j}|, a random variable for the number of increments for which the counter stayed at j.\nWe start with the bound of Eq. (1). When C = j, the update probability is pj = p(j) = b\n−j , so for any `j we have Xj ≥ `j with probability at most (1− pj)`j ≤ exp(−pj)`j = exp(−pj`j) since (1− x) ≤ exp(−x) for all x. To make this at most T−c it suffices to take `j = c(log T )/pj = cb\nj log T . Taking a (rather loose) union bound over j = 1, 2, . . . , T , we have\nPr [ ∃j, Xj > cbj log T ] ≤ 1/T c−1.\nFor Eq. (1), it suffices to show that if this does not occur, then τ̃(Ct) ≥ t/(bc log(T ))−1. Note ∑Ct j=1Xj ≥ t. With our supposition that Xj ≤ cbj log T for all j, this implies t ≤ ∑Ct j=1 cb j log T = cb log T ( bCt−1 b−1 ) , and\nthus Ct ≥ logb ( t(b−1) bc log T + 1 ) . Since τ̃ is monotonically increasing and b > 1, simple algebra then shows τ̃(Ct+1) ≥ τ̃(Ct) ≥ t/(bc log(T ))− 1.\nNext consider the bound of Eq. (2). Let j0 be the minimum value such that p(j0) ≤ 1/et, and fix k ≥ 0. Then Ct+1 ≥ j0 + k implies the counter was incremented k times with an increment probability at most p(j0). Thus,\nPr[Ct ≥ j0 + k] ≤ ( t\nk ) j0+k−1∏ j=j0 p(j)\n≤ ( te\nk )kk−1∏ j=0 p(j0)b −j  = ( te\nk\n)k p(j0) k b−k(k−1)/2\n≤ k−k · b−k(k−1)/2\nNote that j0 ≤ dlogb (et)e. Taking k = √\n2c logb(T )+1 is sufficient to ensure this probability is at most T−c, since k−k ≤ 1 and k2 − k ≥ 2c logb T . Observing that τ̃ ( dlogb (et)e+ √ 2c logb(T ) + 1 ) ≤ etb−1b √ 2c logb(T )+2 completes the proof.\nProof of Theorem 3.3. We prove the bound for the one-dimensional case; the general bound then follows by summing over dimensions. Since we consider a single dimension, we assume |gt| > 0 on all rounds. This is without loss of generality, because we can implicitly skip all rounds with zero gradients, which means we don’t need to make the distinction between t and τt,i. We abuse notation slightly by defining τ̃t ≡ τ̃(Ct+1) ≈ t = τt for the approximate count on round t. We begin from the bound\nE[Regret] ≤ (2R) 2\n2ηT +\n1 2 (G2 + γ2)η1:t + γR\n√ T .\nof Theorem 3.1, with learning rates ηt = α/ √ τ̃t + 1. Lemma A.1 with c = 2.5 then implies\nPr[τ̃t + 1 < k1t] ≤ 1\nT 1.5 and Pr[τ̃t > k2t] ≤\n1\nT 2.5 ,\nwhere k1 = 1/(bc log T ) and k2 = eb √ 2c logb T+2\nb−1 . A union bound on t = 1, ..., T on the first bound implies with probability 1− 1√\nT we have ∀t, τ̃t + 1 ≥ k1t, so\nη1:T = T∑ t=1 α√ τ̃t + 1 ≤ 1√ k1 T∑ t=1 α√ t ≤ 2α √ T√ k1 , (3)\nwhere we have used the inequality ∑T t=1 1√ t ≤ 2 √ T . Similarly, the second inequality implies with probability at least 1− 1T 2.5 ,\nηT = α√ τ̃T + 1 ≥ α√ k2T + 1 . (4)\nTaking a union bound, Eqs. (3) and (4) hold with probability at least 1−2/ √ T , and so at least one fails with probability at most 2/ √ T . Since ft(β)−ft(β′) ≤ 2GR for any β, β′ ∈ [−R,R] (using the convexity of ft and the bound on the gradients G), on any run of the algorithm, regret is bounded by 2RGT . Thus, these failed cases contribute at most 4RG √ T to the expected regret bound.\nNow suppose Eqs. (3) and (4) hold. Choosing α = R√ G2+γ2 minimizes the dependence on the other constants, and note for any δ > 0, both 1√ k1 and √ k2 are o(T δ). Thus, when Eqs. (3) and (4) hold,\nE[Regret] ≤ (2R) 2\n2ηT +\n1 2 (G2 + γ2)η1:t + γR\n√ T\n≤ 2R 2 √ k2T + 1\nα + (G2 + γ2) α √ T√ k1 + γR √ T\n= o ( R √ G2 + γ2T 0.5+δ ) .\nAdding 4RG √ T for the case when the high-probability statements fail still leaves the same bound.\nIt follows from the proof that we have the more precise but cumbersome upper bound on E[Regret]:\n2R2 √ k2T + 1\nα + (G2 + γ2) α √ T√ k1 + γR √ T + 4RG √ T .\n(5)\nA.3. Encoding During Prediction Time\nWe use the following well–known inequality, which is a direct corollary of the Azuma–Hoeffding inequality. For a proof, see (Chung & Lu, 2006).\nTheorem A.2. Let X1, . . . , Xd be independent random variables such that for each i, there is a constant ci such that |Xi − E [Xi] | ≤ ci, always. Let X = ∑d i=1Xi. Then Pr[|X −E [X] | ≥ t] ≤\n2 exp{−t2/2 ∑ i c 2 i }.\nAn immediate consequence is the following large deviation bound on δ = |β · x− β̂ · x|:\nLemma A.3. Let β̂ be a model obtained from β using unbiased randomized rounding to a precision grid. Fix x, and let Z = β̂ · x be the random predicted logodds ratio. Then\nPr[|Z − β · x| ≥ t] ≤ 2 exp ( −t2\n2 2‖x‖0\n)\nLemmas 4.1 and 4.2 provide bounds in terms of the quantity |β ·x−β̂ ·x|. The former is proved in Section 4; we now provide a proof of the latter.\nProof of Lemma 4.2 We claim that the relative error is bounded as\nL(x, y; β̂)− L(x, y;β) L(x, y;β) ≤ eδ − 1, (6)\nor equivalently, that that L(x, y; β̂) ≤ eδL(x, y;β), where δ ≡ |β · x − β̂ · x| as before. We will argue the case in which y = 1; the y = 0 case is analogous. Let z = β · x, and ẑ = β̂ · x; then, when y = 1,\nL(x, y, β) = log(1 + exp(−z)),\nand similarly for β̂ and ẑ. If ẑ > z then L(x, y; β̂) is less than L(x, y;β), which immediately implies the claim. Thus, we need only consider the case when ẑ = z − δ. Then, the claim of Eq. (6) is equivalent to\nlog (1 + exp (−z + δ)) ≤ exp(δ) log (1 + exp (−z)) ,\nor equivalently,\n1 + exp (−z + δ) ≤ (1 + exp (−z))exp(δ) .\nLet w ≡ exp (δ) and u ≡ exp (−z). Then, we can rewrite the last line as 1+wu ≤ (1+u)w, which is true by Bernoulli’s inequality, since u ≥ 0 and w ≥ 1.\nProof of Theorem 4.3 Let R = L(x,y;β̂)−L(x,y;β)L(x,y;β) denote the relative error due to rounding, and let R(δ) be the worst case expected relative error given δ = |β̂ · x− β · x|. Let R̄ ≡ eδ − 1. Then, by Lemma 4.2, R(δ) ≤ R̄(δ). It is sufficient to prove a suitable upper bound on E [ R̄ ] . First, for r ≥ 0,\nPr [ R̄ ≥ r ] = Pr [ eδ − 1 ≥ r ] = Pr[δ ≥ log(r + 1)]\n≤ 2 exp ( − log2(r + 1)\n2 2‖x‖0\n) . [Lemma A.3]\nUsing this, we bound the expectation of R̄ as follows:\nE[R̄] = ∫ ∞ r=0 Pr [ R̄ ≥ r ] dr\n≤ 2 ∫ ∞ r=0 exp ( − log2(r + 1) 2 2‖x‖0 ) dr,\nand since the function being integrated is non-negative on (−1,∞),\n≤ 2 ∫ ∞ r=−1 exp ( − log2(r + 1) 2 2‖x‖0 ) dr\n= 2 √ 2π‖x‖0 exp ( 2‖x‖0\n2\n) ,\nwhere the last line follows after straightforward calculus. A slightly tighter bound (replacing the leading 2 with 1 + Erf( √ ‖x‖0/ √ 2)) can be obtained if one does not make the change in the lower limit of integration."
    } ],
    "references" : [ {
      "title" : "Predictive client-side profiles for personalized advertising",
      "author" : [ "Bilenko", "Mikhail", "Richardson", "Matthew" ],
      "venue" : "In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining,",
      "citeRegEx" : "Bilenko et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Bilenko et al\\.",
      "year" : 2011
    }, {
      "title" : "Compact dictionaries for variable-length keys and data with applications",
      "author" : [ "Blandford", "Daniel K", "Blelloch", "Guy E" ],
      "venue" : "ACM Trans. Algorithms,",
      "citeRegEx" : "Blandford et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Blandford et al\\.",
      "year" : 2008
    }, {
      "title" : "Beating the hold-out: bounds for k-fold and progressive crossvalidation",
      "author" : [ "Blum", "Avrim", "Kalai", "Adam", "Langford", "John" ],
      "venue" : "In Proceedings of the twelfth annual conference on Computational learning theory,",
      "citeRegEx" : "Blum et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Blum et al\\.",
      "year" : 1999
    }, {
      "title" : "The tradeoffs of large scale learning",
      "author" : [ "Bottou", "Léon", "Bousquet", "Olivier" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Bottou et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Bottou et al\\.",
      "year" : 2008
    }, {
      "title" : "LIBSVM: A library for support vector machines",
      "author" : [ "Chang", "Chih-Chung", "Lin", "Chih-Jen" ],
      "venue" : "ACM Transactions on Intelligent Systems and Technology,",
      "citeRegEx" : "Chang et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Chang et al\\.",
      "year" : 2011
    }, {
      "title" : "An experimental comparison of click positionbias models",
      "author" : [ "Craswell", "Nick", "Zoeter", "Onno", "Taylor", "Michael", "Ramsey", "Bill" ],
      "venue" : "In Proceedings of the international conference on Web search and web data mining,",
      "citeRegEx" : "Craswell et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Craswell et al\\.",
      "year" : 2008
    }, {
      "title" : "Efficient projections onto the l1ball for learning in high dimensions",
      "author" : [ "Duchi", "John", "Shalev-Shwartz", "Shai", "Singer", "Yoram", "Chandra", "Tushar" ],
      "venue" : "In Proceedings of the 25th international conference on Machine learning,",
      "citeRegEx" : "Duchi et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Duchi et al\\.",
      "year" : 2008
    }, {
      "title" : "Adaptive subgradient methods for online learning and stochastic optimization",
      "author" : [ "Duchi", "John", "Hazan", "Elad", "Singer", "Yoram" ],
      "venue" : "In COLT,",
      "citeRegEx" : "Duchi et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Duchi et al\\.",
      "year" : 2010
    }, {
      "title" : "Approximate counting: A detailed analysis",
      "author" : [ "Flajolet", "Philippe" ],
      "venue" : "BIT, 25(1):113–134,",
      "citeRegEx" : "Flajolet and Philippe.,? \\Q1985\\E",
      "shortCiteRegEx" : "Flajolet and Philippe.",
      "year" : 1985
    }, {
      "title" : "Spam and the ongoing battle for the inbox",
      "author" : [ "Goodman", "Joshua", "Cormack", "Gordon V", "Heckerman", "David" ],
      "venue" : "Commun. ACM, 50(2),",
      "citeRegEx" : "Goodman et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Goodman et al\\.",
      "year" : 2007
    }, {
      "title" : "Sparse online learning via truncated gradient",
      "author" : [ "Langford", "John", "Li", "Lihong", "Zhang", "Tong" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Langford et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Langford et al\\.",
      "year" : 2009
    }, {
      "title" : "Follow-the-Regularized-Leader and Mirror Descent: Equivalence Theorems and L1 Regularization",
      "author" : [ "McMahan", "H. Brendan" ],
      "venue" : "In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS),",
      "citeRegEx" : "McMahan and Brendan.,? \\Q2011\\E",
      "shortCiteRegEx" : "McMahan and Brendan.",
      "year" : 2011
    }, {
      "title" : "Adaptive bound optimization for online convex optimization",
      "author" : [ "McMahan", "H. Brendan", "Streeter", "Matthew" ],
      "venue" : "In COLT,",
      "citeRegEx" : "McMahan et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "McMahan et al\\.",
      "year" : 2010
    }, {
      "title" : "Counting large numbers of events in small registers",
      "author" : [ "Morris", "Robert" ],
      "venue" : "Communications of the ACM,",
      "citeRegEx" : "Morris and Robert.,? \\Q1978\\E",
      "shortCiteRegEx" : "Morris and Robert.",
      "year" : 1978
    }, {
      "title" : "Randomized rounding: a technique for provably good algorithms and algorithmic proofs",
      "author" : [ "Raghavan", "Prabhakar", "Tompson", "Clark D" ],
      "venue" : "Combinatorica, 7(4),",
      "citeRegEx" : "Raghavan et al\\.,? \\Q1987\\E",
      "shortCiteRegEx" : "Raghavan et al\\.",
      "year" : 1987
    }, {
      "title" : "Predicting clicks: estimating the click-through rate for new ads",
      "author" : [ "Richardson", "Matthew", "Dominowska", "Ewa", "Ragno", "Robert" ],
      "venue" : "In Proceedings of the 16th international conference on World Wide Web,",
      "citeRegEx" : "Richardson et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Richardson et al\\.",
      "year" : 2007
    }, {
      "title" : "Online learning and online convex optimization",
      "author" : [ "Shalev-Shwartz", "Shai" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "Shalev.Shwartz and Shai.,? \\Q2012\\E",
      "shortCiteRegEx" : "Shalev.Shwartz and Shai.",
      "year" : 2012
    }, {
      "title" : "Less regret via online conditioning",
      "author" : [ "Streeter", "Matthew J", "McMahan", "H. Brendan" ],
      "venue" : "CoRR, abs/1002.4862,",
      "citeRegEx" : "Streeter et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Streeter et al\\.",
      "year" : 2010
    }, {
      "title" : "String hashing for linear probing",
      "author" : [ "Thorup", "Mikkel" ],
      "venue" : "In Proceedings of the 20th ACM-SIAM Symposium on Discrete Algorithms,",
      "citeRegEx" : "Thorup and Mikkel.,? \\Q2009\\E",
      "shortCiteRegEx" : "Thorup and Mikkel.",
      "year" : 2009
    }, {
      "title" : "Regression shrinkage and selection via the lasso",
      "author" : [ "Tibshirani", "Robert" ],
      "venue" : "Journal of the Royal Statistical Society. Series B (Methodological),",
      "citeRegEx" : "Tibshirani and Robert.,? \\Q1996\\E",
      "shortCiteRegEx" : "Tibshirani and Robert.",
      "year" : 1996
    }, {
      "title" : "Probabilistic counting with randomized storage",
      "author" : [ "Van Durme", "Benjamin", "Lall", "Ashwin" ],
      "venue" : "In Proceedings of the 21st international jont conference on Artifical intelligence,",
      "citeRegEx" : "Durme et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Durme et al\\.",
      "year" : 2009
    }, {
      "title" : "Feature hashing for large scale multitask learning",
      "author" : [ "Weinberger", "Kilian", "Dasgupta", "Anirban", "Langford", "John", "Smola", "Alex", "Attenberg", "Josh" ],
      "venue" : "In Proceedings of the 26th Annual International Conference on Machine Learning,",
      "citeRegEx" : "Weinberger et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Weinberger et al\\.",
      "year" : 2009
    }, {
      "title" : "Dual averaging method for regularized stochastic learning and online optimization",
      "author" : [ "Xiao", "Lin" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Xiao and Lin.,? \\Q2009\\E",
      "shortCiteRegEx" : "Xiao and Lin.",
      "year" : 2009
    }, {
      "title" : "Online convex programming and generalized infinitesimal gradient ascent",
      "author" : [ "Zinkevich", "Martin" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Zinkevich and Martin.,? \\Q2003\\E",
      "shortCiteRegEx" : "Zinkevich and Martin.",
      "year" : 2003
    } ],
    "referenceMentions" : [ {
      "referenceID" : 15,
      "context" : "This is true for training massive-scale distributed learning systems, such as those used for predicting ad click through rates (CTR) for sponsored search (Richardson et al., 2007; Craswell et al., 2008; Bilenko & Richardson, 2011; Streeter & McMahan, 2010) or for filtering email spam at scale (Goodman et al.",
      "startOffset" : 154,
      "endOffset" : 256
    }, {
      "referenceID" : 5,
      "context" : "This is true for training massive-scale distributed learning systems, such as those used for predicting ad click through rates (CTR) for sponsored search (Richardson et al., 2007; Craswell et al., 2008; Bilenko & Richardson, 2011; Streeter & McMahan, 2010) or for filtering email spam at scale (Goodman et al.",
      "startOffset" : 154,
      "endOffset" : 256
    }, {
      "referenceID" : 9,
      "context" : ", 2008; Bilenko & Richardson, 2011; Streeter & McMahan, 2010) or for filtering email spam at scale (Goodman et al., 2007).",
      "startOffset" : 99,
      "endOffset" : 121
    }, {
      "referenceID" : 6,
      "context" : "Smaller Models A classic approach to reducing memory usage is to encourage sparsity, for example via the Lasso (Tibshirani, 1996) variant of least-squares regression, and the more general application of L1 regularizers (Duchi et al., 2008; Langford et al., 2009; Xiao, 2009; McMahan, 2011).",
      "startOffset" : 219,
      "endOffset" : 289
    }, {
      "referenceID" : 10,
      "context" : "Smaller Models A classic approach to reducing memory usage is to encourage sparsity, for example via the Lasso (Tibshirani, 1996) variant of least-squares regression, and the more general application of L1 regularizers (Duchi et al., 2008; Langford et al., 2009; Xiao, 2009; McMahan, 2011).",
      "startOffset" : 219,
      "endOffset" : 289
    }, {
      "referenceID" : 21,
      "context" : "A more recent trend has been to reduce memory cost via the use of feature hashing (Weinberger et al., 2009).",
      "startOffset" : 82,
      "endOffset" : 107
    }, {
      "referenceID" : 6,
      "context" : "Per-Coordinate Learning Rates Duchi et al. (2010) and McMahan & Streeter (2010) demonstrated that per-coordinate adaptive regularization (i.",
      "startOffset" : 30,
      "endOffset" : 50
    }, {
      "referenceID" : 6,
      "context" : "Per-Coordinate Learning Rates Duchi et al. (2010) and McMahan & Streeter (2010) demonstrated that per-coordinate adaptive regularization (i.",
      "startOffset" : 30,
      "endOffset" : 80
    }, {
      "referenceID" : 2,
      "context" : "Metrics are computed using progressive validation (Blum et al., 1999) as is standard for online learning: on each round a prediction is made for a given example and record for evaluation, and only after that is the model allowed to train on the example.",
      "startOffset" : 50,
      "endOffset" : 69
    } ],
    "year" : 2013,
    "abstractText" : "We reduce the memory footprint of popular large-scale online learning methods by projecting our weight vector onto a coarse discrete set using randomized rounding. Compared to standard 32-bit float encodings, this reduces RAM usage by more than 50% during training and by up to 95% when making predictions from a fixed model, with almost no loss in accuracy. We also show that randomized counting can be used to implement percoordinate learning rates, improving model quality with little additional RAM. We prove these memory-saving methods achieve regret guarantees similar to their exact variants. Empirical evaluation confirms excellent performance, dominating standard approaches across memory versus accuracy tradeoffs.",
    "creator" : "TeX"
  }
}