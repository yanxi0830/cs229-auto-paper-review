{
  "name" : "1206.6421.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Structured Learning from Partial Annotations",
    "authors" : [ "Xinghua Lou", "Fred A. Hamprecht" ],
    "emails" : [ "xinghua.lou@iwr.uni-heidelberg.de", "fred.hamprecht@iwr.uni-heidelberg.de" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Given a training set, structured learning extracts rules that allow the prediction of complex, structured output from structured input. It has improved conceptual clarity of, and boosted performance in, different tasks such as image segmentation, graph matching, word alignment, grammatical tagging, protein structure prediction or cell tracking (see (Bakir et al., 2006) and references therein). However, most classic structured learning algorithms require a strong prerequisite: the training data with complex structure needs to be completely annotated in order to apply those\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nalgorithms. This results in expensive, time-consuming data preparation and makes re-training very difficult.\nIn this paper, we study the problem of structured learning from partial annotations. Our contributions are the proposal of the new “bridge” loss function (section 3.2), a synthesis of previous work (section 3.4), a large margin problem formulation (section 3.5) with a suitable optimization strategy (section 4) and experimental validation (section 5)."
    }, {
      "heading" : "1.1. Prior Art",
      "text" : "We build on important previous work for multiclass classification with ambiguous labels. Here, each training sample x has an exact, unknown label c∗. However, the training set only comprises a set of candidate labels c∗ for each observation, where c∗ ∈ c∗. (Jin & Ghahramani, 2002) proposed an EM-like algorithm that iteratively estimates the label distribution and classifies using this distribution as a prior. Recently, (Cour et al., 2011) proposed convex loss for partial labels, which in turn resembles the one-versus-all loss (Zhang, 2004). We will extend this loss to structured data and discuss its properties in section 3.2.\nThis work is also closely related (section 3.3) to structured learning with latent variables (Yu & Joachims, 2009; Girshick et al., 2011). The main differences with (Yu & Joachims, 2009) are, theoretically, the derivation as an extension of multiclass learning with ambiguous labels and, practically, an improved optimization strategy (section 5). Structured learning for partially annotated sequences is studied in (Fernandes & Brefeld, 2011). These authors use a structured perceptron which, according to the experiments in section 5, does not generalize as well as the proposed largemargin method.\nFinally, note that structured learning from partial annotations is different from semi-supervised or unsupervised structured learning (Altun et al., 2006; Xu et al., 2006; Zien et al., 2007). In those settings, training samples are either completely annotated or complete-\nly unannotated."
    }, {
      "heading" : "2. Structured Learning from Partial Annotations",
      "text" : "We want to learn from a partially annotated training set {(xn, y∗n) ∈ Xn ×Yn : n = 1, . . . , N}. Here, xn is a structured input from a space Xn1. y∗ is a partially annotated structured output which induces a partitioning of structured output space Y into two sets Y∗ ∩ Y◦ = ∅, Y∗ ∪ Y◦ = Y . Y∗ comprises all outputs that are compatible with a partial annotation y∗, while Y◦ encompasses all those structured outputs that are not compatible with the partial annotation, see Fig. 1."
    }, {
      "heading" : "3. Large Margin Learning from Partial Annotations",
      "text" : "The aim of learning is to find a parameter vector w that minimizes the weighted sum of a regularization term Ω(w) and the empirical loss:\nmin w\nJ(w) := λΩ(w) + 1\nN N∑ n=1 L(xn, y ∗ n;w), (1)\nThe particular choice of regularizer and loss function leads to different learning methods, with the squared norm and hinge loss, respectively, yielding a structSVM (Tsochantaridis et al., 2006), which is particularly popular in structured output learning.\nIn the following, we propose two formulations of large margin learning from partial annotations. The first is a formal generalization of one-versus-all learning to\n1Note that the cardinality of the spaces Xn, Yn is typically different for each input n.\nstructured data which is instructive, but not viable in practice. The second is a tractable formulation which is a generalization of multiclass learning from ambiguous labels. Hurried readers may want to jump to this proposal in section 3.2."
    }, {
      "heading" : "3.1. Formulation I: One-Versus-All",
      "text" : "The recently proposed “convex loss for partial labels” (CLPL) (Cour et al., 2011) enjoys favorable properties, including convexity, consistency and demonstrably high performance for flat (unstructured) outputs. Using notations in Table 1, CLPL can be expressed as\nLclpl(x, c∗;w) = l\n( 1\n|c∗| ∑ c∈c∗ f(x, c;w) ) +∑\nc∈C\\c∗ l (−f(x, c;w)) (2)\nGeneralizing this loss to structured outputs gives\nLclpl−sl(x, y∗;w) = l  1 |Y∗| ∑ y∈Y∗ f(x, y;w) + ∑ y∈Y◦ l (−f(x, y;w)) (3)\nwhere l is the hinge loss function and f(x, y;w) the score of a prediction y given input x and parameter vector w.\nInserting this loss into Eq. 1 and using slack variables to prevent overfitting as in (Tsochantaridis et al.,\n2006), we obtain the following optimization problem:\nmin w\nλΩ(w) + 1\nN ∑ n ξn + 1 N ∑ n ∑ y◦n∈Y◦n ξn,y◦n\ns.t. ∀n, Ey|y∈Y∗n [f(xn, y;w)] ≥ 1− ξn ∀n, ∀y◦n ∈ Y ◦ n, −f(xn, y;w) ≥ 1− ξn,y◦n\n∀n, ξn ≥ 0 ∀y◦n ∈ Y ◦ n, ξn,y◦n ≥ 0\nThe above formulation is convex, yet intractable in practice. Firstly, there are exponentially many terms in the target function, which thus cannot be represented explicitly. Secondly, and more importantly, this formulation requires computing the conditional expectations of the scores over the entire spaces Y∗n which, similar to estimating the partition function of graphical models, is usually intractable. To circumvent this limitation, one could use pseudo-likelihood (maximizing the margin is similar to maximizing the likelihood ratio) to approximate this expectation as in (Lee et al., 2006). However, a recent study shows that this is only possible when the distribution of training sample is rich enough (Sontag et al., 2010). This assumption is in contradiction to our problem setting, where annotations are partial and rare."
    }, {
      "heading" : "3.2. Formulation II: Pairwise Comparison",
      "text" : "When formulated as a conventional multiclass learning problem, structured learning needs to discriminate a correct structured output from an exponential number of wrong structured outputs. As a way out, structSVM (Tsochantaridis et al., 2006) penalizes small or negative margins (differences) between the score of the correct structured output and the highest score among any of the wrong structured outputs.\nWe follow the same argument, by constructing a loss function that penalizes small margins between the current prediction (maximizer of the first term in the loss below) and the best scoring wrong prediction (maximizer of the second term in the following loss function):\nLpair(x, y∗;w) = l ( max y∈Y∗ f(x, y;w)− max y∈Y◦ f(x, y;w) ) Note that, unlike the CLPL in Eq. 3, this loss function is not convex. Even so, we show in section 5 that the resulting learning problem can still be solved efficiently with an improved concave-convex procedure.\n(Tsochantaridis et al., 2006) suggest to adjust the penalty levied for high-scoring wrong predictions according to just how wrong they are, as measured by a user-defined task loss function ∆(y∗, y). The idea is\nto push the decision boundary away from “bad” predictions. Following this idea, we obtain\nLbridge(x, y∗;w) = ∣∣∣∣maxy∈Y◦ [f(x, y;w) + ∆(y∗, y)] − max y∈Y∗ [f(x, y;w)] ∣∣∣∣ + (4)\nwhich we name the bridge loss, given that its margin is always computed across two disjoint spaces."
    }, {
      "heading" : "3.3. Connection to structSVMs with Latent Variables",
      "text" : "Even though the loss in Eq. 4 was motivated and derived from multiclass learning with ambiguous labels, it has a very similar structure to the loss defined in the context of structured learning with latent variables. Specifically, (Yu & Joachims, 2009) address the problem when a training sample (x, z∗, h) ∈ X × Z ×H consists of both observed variables z∗ ∈ Z and unknown hidden variables h ∈H. They propose a hinge loss with latent variables:\nLhinge(x, z∗;w) = max (z,h)∈Z×H [f(x, z, h;w) + ∆(z∗, z)]\n−max h∈H\n[f(x, z∗, h;w)] (5)\nIndeed, in our problem setting the un-annotated part of a structure can be considered as a collection of hidden variables. In that sense, Z ×H corresponds to Y in our setting and, by fixing the observed variables, z∗, z∗×H amounts to Y∗. Now the key difference between Eq. 4 and Eq. 5 is clear: the bridge loss searches the most misleading (high-scoring, but wrong) output over the space Y◦ of configurations that are incompatible with the provided partial annotation, while the hinge loss searches throughout the entire space, encompassing both feasible and infeasible outputs."
    }, {
      "heading" : "3.4. Synthesis of Loss Functions",
      "text" : "In fact, several other related loss functions, including ramp and max loss, have recently been proposed. They can be summarized in terms of a generic formulation:\nLgeneric(x, y∗;w) = ∣∣∣∣max y∈YP [f(x, y;w) + ∆(y∗, y)]\n− max y∈YR [f(x, y;w)] ∣∣∣∣ +\n(6)\nHere, YP is a “Penalty” space, since its members make a positive contribution to the loss. Accordingly, YR denotes a “Reward” space because it contains the correct configuration and brings a negative contribution.\nTable 2 provides a summary of related loss functions and shows how they fit into our generic formulation.\nThose loss functions bear different properties. For example, while needed for max loss and bridge loss, the | · |+ operator can be dropped for ramp loss and hinge loss provided that ∆(·, ·) is a positive function2. We refer the readers to (Zhang, 2004; McAllester & Keshet, 2011; McAllester et al., 2010) for more theoretical analysis on hinge/ramp loss."
    }, {
      "heading" : "3.5. Large Margin Learning Objective",
      "text" : "With a clear definition of loss function in Eq. 6, we now set off to define the learning objective function. Take any loss function in Table 2 and insert it into Eq. 1. We obtain our learning objective as\nmin w\nλΩ(w) + 1\nN ∑ n max y∈YPn\n[f(xn, yn;w) + ∆(y ∗ n, y)]︸ ︷︷ ︸\nP (w), convex\n− 1 N ∑ n max y∈YRn\n[f(xn, y;w)]︸ ︷︷ ︸ R(w), convex\n(7)\ns.t. each loss must be nonnegative.\nEq. 8 is a subtraction of two convex functions, namely λΩ(w) + P (w)−R(w). Note that such structured learning problems are generally computationally expensive because the maximizations therein have to be solved at each iteration of updating w for each training sample. Next, we will present an efficient method to address this problem.\n2For these losses, YR ⊆ YP, so the margin can never be negative."
    }, {
      "heading" : "4. Optimization with Bound Recycling",
      "text" : ""
    }, {
      "heading" : "4.1. Convex-Concave Problem and CCCP",
      "text" : "The difference of two convex functions forms a convexconcave optimization problem that can be solved by the CCCP procedure (Yuille & Rangarajan, 2003). Briefly, CCCP iterates between two steps: Step 1: At iteration t, estimate a linear upper bound on the concave function −R(w) using its subgradient at wt, viz. v = −∂wR(wt). Then,\n〈vt,w −wt〉 −R(wt) ≥ −R(w),∀w (8)\nStep 2: Update the model by\nwt+1 = argminwJ̃(w) := λΩ(w) + P (w) + 〈vt,w〉. (9) The procedure is guaranteed to converge to a local minimum or saddle point (Yuille & Rangarajan, 2003).\n(Yu & Joachims, 2009) used this strategy to optimize their structured SVM with latent variables, with a proximal bundle method (Kiwiel, 1990) for Step 2. (Girshick et al., 2011) and (Jie & Orabona, 2010) coined a similar procedure and applied stochastic gradient descent to speed up the training.\nTo construct the hyperplane for bounding −R(w), one first solves ỹn = arg maxy∈YRn [f(xn, yn;wt)] for every n, and then computes vt as\nvt = 1\nN ∑ n ∂wf(xn, ỹn;wt) (10)"
    }, {
      "heading" : "4.2. Speeding Up CCCP with Bounds Recycling",
      "text" : "Structured learning is computationally expensive due to the repetitive maximization problems one has to solve at every iteration to compute the subgradients (Tsochantaridis et al., 2006; Teo et al., 2010). This makes the above CCCP based optimization strategy particularly expensive because a complete structured learning has to be solved largely from scratch. We now introduce a novel method for speeding up CCCP when structured learning is required.\nWe first inspect the structure of the objective J̃(w) in Eq. 9 and obtain the following key observations:\nComplexity: J̃(w) consists of three terms with different complexity: a regularizer λΩ(w) (e.g., quadratic when using L2 regularization) and a linear term 〈v,w〉, both smooth and easy to solve, and a complicated, possibly non-smooth term P (w).\nConsistency: J̃(w) changes at each CCCP iteration, due to the update of v; however, the difficult function P (w) remains the same.\nThese two observations lead to two ideas for speedup.\nFirstly, we construct a piecewise linear lower bound on the difficult P (w) only, rather than on the entire objective J̃(w) as in (Yu & Joachims, 2009). Since the P (w) part of J̃(w) remains the same, we can reuse these bounds across multiple CCCP iterations and avoid recomputing them from scratch. When some “good” linear approximation for P (w) is provided at each iteration, solving J̃(w) is easy because the other two terms are simple. We name this technique bounds recycling, since the bounds will be reused to compute the approximation gap between the original objective and its linear approximation.\nSecondly, (Yuille & Rangarajan, 2003) showed that CCCP iteratively matches points on the two convex functions (i.e. λΩ(w) + P (w) and R(w)) which have the same subgradient, see Fig. 2 (left). Since we usually start with some w0 far from the optimum, it is not sensible to solve J̃(w) to high precision at early iterations. Otherwise, many bounds need be computed to achieve this precision at some immature w, which are mostly not reused at later iterations when precision really matters. Therefore, we propose to adaptively increase the precision of CCCP iteration until reaching the required precision. This procedure, named adaptive precision, is shown in Fig. 2 (right)."
    }, {
      "heading" : "4.3. Solving Model Update in the Dual",
      "text" : "To construct a lower bound approximation for P (w), we follow the bundle minimization method from (Teo et al., 2010). Briefly, at some wk, we compute the subgradient of P (w) and the corresponding offset,\na = 1\nN ∑ n ∂wf(xn, ŷ;wk) (11)\nb = 1\nN ∑ n [f(x, ŷ;wk) + ∆(y ∗, ŷ)]− 〈a,wk〉(12)\nwhere ŷ = arg maxy∈YP [f(x, y;wk) + ∆(y ∗, y)] is the expensive augmented inference problem (Tsochantaridis et al., 2006). Now, this lower bound sitting at wk can be expressed as 〈a,w〉+ b ≤ P (w),∀w.\nWe store all subgradents a as column vectors in A = [a0,a1, . . .] and the offsets b in b = [b0, b1, . . .]\n′. Given A and b, solving J̃(w) in Eq. 9 becomes\nmin w λΩ(w) + max (a,b)∈(A,b) (〈a,w〉+ b)︸ ︷︷ ︸ Linearly lower bounded P (w) +〈v,w〉(13)\nGiven regularizer Ω(w) = 12‖w‖ 2, this problem can be easily solved in its dual form:\nTheorem 1. Given a list of lower bounds for some convex function expressed by subgradients A = [a0,a1, . . .] and offsets b = [b0, b1, . . .] ′ , the dual form of the primal minimization problem in Eq. 13 is\nmax α\n− 1 2λ α′A′Aα+\n( b′ − 1\nλ v′A\n) α\ns.t. α′1 = 1,α ≥ 0. (14)\nThe primal variable w is connected to α by\nw = − 1 λ (v +Aα) (15)\nProof. Very similar to Theorem 2 in (Teo et al., 2010).\nThis dual form can be easily inserted into popular QP solvers such as CPLEX3 and libqp4."
    }, {
      "heading" : "4.4. Pseudocode and Implementation Details",
      "text" : "Pseudocode of our optimization method is illustrated in Algorithm 1. We use t to index CCCP iterations and k to index lower bounds. Given rate ρ ∈ (0, 1), line 5 gradually increases the desired precision at each iteration until min (smaller means higher precision). Line 8 shows the accumulation of bounds that are reused every time at line 9. The approximation gap ̂ is the margin between the original objective J̃t(w) and its lower bounded approximation, i.e. the minimum value of Eq. 13. We refer the readers to (Teo et al., 2010) for more details. Finally, the algorithm terminates when the decrease of the objective J̃t(w) between two consecutive CCCP iterations is smaller than some threshold η. Matlab code will be available to the public at http://xinghua-lou.org/research/.\nIt is important to point out that the nonnegativity constraint on each empirical loss must not be violated\n3http://www.ibm.com/software/ 4http://cmp.felk.cvut.cz/~xfrancv/libqp/html/\nAlgorithm 1 CCCP with Bounds Recycling\n1: Input: {xn, y∗n}, w0, η, { , min, ρ} 2: Initialize t = 0, k = 0,A = ∅, b = ∅,w = w0 3: repeat 4: Compute vt as in Eq. 10 5: Set = max( × ρ, min) 6: repeat 7: Compute ak and bk as in Eq. 11 and Eq. 12 8: Set A = A ∪ ak and b = b ∪ bk 9: Update w using Eq. 13 with A, b and vt\n10: Compute approximation gap ̂ 11: Set k = k + 1 12: until ̂ ≤ 13: Set wt+1 = w 14: Set t = t+ 1 15: until J̃(wt−1)− J̃(wt) ≤ η 16: Output: w\nthroughout the entire CCCP procedure. This is satisfied for hinge loss and ramp loss by their definition (McAllester & Keshet, 2011). For max loss and bridge loss, this can be achieved by ignoring samples that violate this constraint from the subgradient computation, as in usual SVM."
    }, {
      "heading" : "5. Experiments",
      "text" : "We evaluate our method on a very challenging real world problem: cell tracking. Robust tracking is of fundamental importance for, i.a., molecular, cell and developmental biology. Recently, (Lou & Hamprecht, 2011) proposed a structured learning for cell tracking which allows to learn the parameters of an energy function from manually annotated tracks, leading to significantly improved performance especially if the number of parameters becomes large. However, their learning strategy was based on classic structured learning, requiring exhaustive assignment annotations of pairs of frames. This is a tedious task at best, and becomes impossible for large scale problems."
    }, {
      "heading" : "5.1. Model, Data and Comparison Setup",
      "text" : "(Lou & Hamprecht, 2011) formulate tracking by assignment as a constrained binary energy minimization problem. A foregoing detection step finds potential cells/targets in either of two consecutive frames. Based on these detections, a set E of possible events (such as motion, division, etc.), described by features φec,c′ is compiled. The indicator variables yec,c′ state if an event is realized or not. Many events are mutually exclusive according to conservation laws: each detected cell must have a unique history and a unique fate. In\nsummary, given the learned parameters w, a predicted tracking is obtained as the minimizer of\nmin y f(x, y;w) := ∑ e∈E ∑ c∈C ∑ c′∈C′ 〈φec,c′ ,we〉yec,c′ (16)\ns.t. ∀c′ ∈ C ′, ∑ e∈E ∑ c∈C yec,c′ = 1 (conservation) (17)\n∀c ∈ C, ∑ e∈E ∑ c′∈C′ yec,c′ = 1 (conservation) (18)\n∀e ∈ E, c ∈ C, c′ ∈ C ′, yec,c′ ∈ {0, 1} (Booleanity)\nHere, C and C ′ are power sets of all detections from the respective frames to accommodate the description of events such as division, where one cell in the first frame can be assigned to two cells from the second frame.\nTo make the problem realistic and even harder, training5 and test6 data (both publicly available) from different (!) experiments and labs were used. The cardinality of the structured output (number of indicator variables) ranges from 400 at early stages to over 5000 at late stages, and the inference problem involves higher-order constraints up to order 50."
    }, {
      "heading" : "5.2. Comparison to Structured Perceptron and Full Annotation",
      "text" : "To obtain a baseline, 20 randomly selected but fully annotated pairs of frames were used to train the model from (Lou & Hamprecht, 2011) using bundle minimization. Next, to obtain partial annotations, a variable fraction of all events was selected using stratified7 random sampling. For better statistics, each experiment was repeated 10 times using different stratified random samples. To make all experiments comparable, the same precision (i.e., approximation gap, see Algorithm 1) was used for bundle minimization and the method proposed here. The structured perceptron with partial annotations was trained until the task loss became zero, or stopped improving, and no early stopping was used.\nFig. 3 shows a comparison of the average test loss. One surprising result is that the model learned from partial annotations as suggested here apparently can outperform the model learned from full annotation (Lou & Hamprecht, 2011) when only around 40% of all data is annotated. Our interpretation is that significantly\n5http://www.cbi-tmhs.org/Dcelliq/files/051606_ HeLaMCF10A_DMSO_1.rar\n6http://www.mitocheck.org/cgi-bin/mtc?action= show_movie;query=24386\n7Making sure that rare events such as division could become part of a partial annotation.\nless data may not be enough to optimally train the tracking model with its around 40 parameters; while significantly more data may lead to overfitting. Note that this phenomenon was also observed by (Fernandes & Brefeld, 2011). Secondly, the proposed method consistently outperforms the structured perceptron with partial annotation. We attribute this to the perceptron’s lack of regularization, and resulting overfitting.\nFig. 4 shows a comparison of training times. Once the proportion of partial annotation exceeds 20%, our method requires roughly twice as much time as the bundle method for risk minimization that is working on full annotations only. Training the structured perceptron appears to be more expensive, but its runtimes have a lower variance.\n50 45 40 35 30 25 20 15 10 5 0.005\n0.01\n0.015\n0.02\n0.025\nPercentage of Annotation\nA ve\nra ge\nT as\nk Lo\nss\nPerceptron w/ Partial Annotation BMRM w/ Full Annotation Ours w/ Partial Annotation\nFigure 3. Comparison of average test loss. 50 45 40 35 30 25 20 15 10 5\n0\n500\n1000\n1500\nPercentage of Annotation\nA ve\nra ge\nR un\ntim e\n(s )\nPerceptron w/ Partial Annotation BMRM w/ Full Annotation Ours w/ Partial Annotation\nFigure 4. Comparison of training time."
    }, {
      "heading" : "5.3. Comparison of Surrogate Losses",
      "text" : "Table 3 shows a comparison of various loss functions w.r.t. prediction accuracy and runtime for the partially annotated data. We see that bridge (proposed here) and hinge loss yield very similar prediction performance, with somewhat faster runtime of the former.\nSurprisingly, despite their very similar formulations, both ramp and max loss give much lower accuracy (around threefold higher test loss), but allow two- or threefold faster training. Recall that Table 2 shows the key difference between max/ramp loss and hinge/bridge loss: the former search through the entire space for the best configuration, while the latter only search within a subspace that is consistent with the partial annotation. Our result suggests that constraining the search to a feasible subspace that is compatible with the available annotations is crucial for the accuracy of the learned model.\nTo support this argument, we modify the ramp/max losses to make them aware of available partial annotations in their search for the highest-scoring configuration, i.e. when solving the second maximization in Eq. 6. This can be achieved by inserting a −∆(y∗, y) into the second maximization (McAllester & Keshet,\n2011), as\nLnew(x, y∗;w) = ∣∣∣∣max y∈YP [f(x, y;w) + ∆(y∗, y)]\n− max y∈YR [f(x, y;w)−∆(y∗, y)] ∣∣∣∣ +\nThe performance of such modified ramp/max losses is shown in the last rows of Table 3. Their learning accuracy is significantly improved, and brought to the level of the hinge/bridge loss. Note that this modification has no effect on hinge/bridge loss."
    }, {
      "heading" : "5.4. Comparison of Optimization Strategy",
      "text" : "We compare our optimization strategy to the CCCP procedure from (Yu & Joachims, 2009) which does not use the bounds recycling and adaptive precision proposed here. In a lesion study, we also study the effect of omitting either bounds recycling or/and adaptive precision.\nFig. 5 shows the convergence of the objective function. All optimization methods converge to the same objective value. Using both bounds recycling and adaptive precision, we achieve a speed-up of a factor of 5 or so. Note that we implemented (Yu & Joachims, 2009)’s CCCP procedure using the BMRM method (Teo et al., 2010) whose complexity O( 1 ) is actually better than that of the proximal bundle method used in the original paper, O( 1 3 ).\nFig. 6 shows the total number of bounds computed across the CCCP iterations. By using bounds recycling, our method only requires ca. 100 bounds until convergence, while (Yu & Joachims, 2009)’s approach computes almost 100 bounds at its first iteration."
    }, {
      "heading" : "6. Conclusions and Outlook",
      "text" : "We conclude that structured learning from partial annotations is practically possible. With a proper choice of loss function and optimization strategy, the model learned from partial annotations has an accuracy that compares well with that obtained from exhaustive annotation.\nOverall, we witness a fundamental tradeoff: in our experiments, successful learning from partial annotations\n0 200 400 600 800 1000 1200 0\n0.5\n1\n1.5\n1019 1227\nRuntime (s)\nO bj\nec tiv\ne F\nun ct\nio n\n(Yu & Joachims, 2009) Ours w/ only Bounds Recycling Ours w/ only Adaptive Precision Ours with Both\nFigure 5. Decrease of the objective function. 0 5 10 15 20 25 30 35\n0\n100\n200\n300\n400\n500\n600\n700\nCCCP Iteration\nN um\nbe r\nof B\nou nd\ns A\ndd ed\n(Yu & Joachims, 2009) Ours w/ only Bounds Recycling Ours w/ only Adaptive Precision Ours with Both\nFigure 6. Total Number of bounds before convergence.\nis cheaper for the human by a factor of 2-4 (lower labeling effort), but more expensive for the computer by a similar factor. Given that we value human time more highly, and that labeling takes of the order of hours whereas computations are in the order of minutes, we believe that learning from partial annotations as proposed here and in (Fernandes & Brefeld, 2011), as well as implicitly in (Yu & Joachims, 2009), is fundamentally a sound idea that is worth pursuing.\nIn the future, we are interested in marrying our approach with active learning: that is, let the computer identify relevant partial structures which, if annotated, can reduce ambiguity in the current labeling, and help achieve steeper learning curves."
    } ],
    "references" : [ {
      "title" : "Maximum margin semi-supervised learning for structured variables",
      "author" : [ "Y. Altun", "D. McAllester", "M. Belkin" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Altun et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Altun et al\\.",
      "year" : 2006
    }, {
      "title" : "Predicting Structured Data",
      "author" : [ "G. Bakir", "T. Hofmann", "B. Schoelkopf", "A.J. Smola", "B. Taskar", "S.V.N. Vishwanathan" ],
      "venue" : null,
      "citeRegEx" : "Bakir et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Bakir et al\\.",
      "year" : 2006
    }, {
      "title" : "Tighter bounds for structured estimation",
      "author" : [ "C.B. Do", "Q. Le", "C.H. Teo", "O. Chapelle", "A. Smola" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Do et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Do et al\\.",
      "year" : 2008
    }, {
      "title" : "Learning from partially annotated sequences",
      "author" : [ "E. Fernandes", "U. Brefeld" ],
      "venue" : "In ECML/PKDD,",
      "citeRegEx" : "Fernandes and Brefeld,? \\Q2011\\E",
      "shortCiteRegEx" : "Fernandes and Brefeld",
      "year" : 2011
    }, {
      "title" : "Object Detection with Grammar Models",
      "author" : [ "R.B. Girshick", "P.F. Felzenszwalb", "D. McAllester" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Girshick et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Girshick et al\\.",
      "year" : 2011
    }, {
      "title" : "Learning from Candidate Labeling Sets",
      "author" : [ "L. Jie", "F. Orabona" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Jie and Orabona,? \\Q2010\\E",
      "shortCiteRegEx" : "Jie and Orabona",
      "year" : 2010
    }, {
      "title" : "Learning with Multiple Labels",
      "author" : [ "R. Jin", "Z. Ghahramani" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Jin and Ghahramani,? \\Q2002\\E",
      "shortCiteRegEx" : "Jin and Ghahramani",
      "year" : 2002
    }, {
      "title" : "Proximity control in bundle methods for convex nondifferentiable minimization",
      "author" : [ "K.C. Kiwiel" ],
      "venue" : "Math Program,",
      "citeRegEx" : "Kiwiel,? \\Q1990\\E",
      "shortCiteRegEx" : "Kiwiel",
      "year" : 1990
    }, {
      "title" : "Learning to model spatial dependency: Semi-supervised discriminative random fields",
      "author" : [ "C.H. Lee", "S. Wang", "F. Jiao", "D. Schuurmans", "R. Greiner" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Lee et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2006
    }, {
      "title" : "Structured learning for cell tracking",
      "author" : [ "X. Lou", "F.A. Hamprecht" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Lou and Hamprecht,? \\Q2011\\E",
      "shortCiteRegEx" : "Lou and Hamprecht",
      "year" : 2011
    }, {
      "title" : "Generalization Bounds and Consistency for Latent Structural Probit and Ramp Loss",
      "author" : [ "D. McAllester", "J. Keshet" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "McAllester and Keshet,? \\Q2011\\E",
      "shortCiteRegEx" : "McAllester and Keshet",
      "year" : 2011
    }, {
      "title" : "Direct loss minimization for structured prediction",
      "author" : [ "McAllester", "David", "Hazan", "Tamir", "Keshet", "Joseph" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "McAllester et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "McAllester et al\\.",
      "year" : 2010
    }, {
      "title" : "More data means less inference: A pseudo-max approach to structured learning",
      "author" : [ "D. Sontag", "O. Meshi", "T.S. Jaakkola", "A. Globerson" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Sontag et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Sontag et al\\.",
      "year" : 2010
    }, {
      "title" : "Bundle methods for regularized risk",
      "author" : [ "C.H. Teo", "S.V.N. Vishwanthan", "A.J. Smola", "Q.V. Le" ],
      "venue" : "minimization. JMLR,",
      "citeRegEx" : "Teo et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Teo et al\\.",
      "year" : 2010
    }, {
      "title" : "Large Margin Methods for Structured and Interdependent Output Variables",
      "author" : [ "I. Tsochantaridis", "T. Joachims", "T. Hofmann", "Y. Altun" ],
      "venue" : null,
      "citeRegEx" : "Tsochantaridis et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Tsochantaridis et al\\.",
      "year" : 2006
    }, {
      "title" : "Structured output regression for detection with partial truncation",
      "author" : [ "A. Vedaldi", "A. Zisserman" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Vedaldi and Zisserman,? \\Q2009\\E",
      "shortCiteRegEx" : "Vedaldi and Zisserman",
      "year" : 2009
    }, {
      "title" : "A discriminative latent model of object classes and attributes",
      "author" : [ "Y. Wang", "G. Mori" ],
      "venue" : "In ECCV,",
      "citeRegEx" : "Wang and Mori,? \\Q2010\\E",
      "shortCiteRegEx" : "Wang and Mori",
      "year" : 2010
    }, {
      "title" : "Discriminative unsupervised learning of structured predictors",
      "author" : [ "L. Xu", "D. Wilkinson", "F. Southey", "D. Schuurmans" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Xu et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2006
    }, {
      "title" : "Learning Structural SVMs with Latent Variables",
      "author" : [ "C.N.J. Yu", "T. Joachims" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Yu and Joachims,? \\Q2009\\E",
      "shortCiteRegEx" : "Yu and Joachims",
      "year" : 2009
    }, {
      "title" : "The Concave-Convex Procedure",
      "author" : [ "A.L. Yuille", "A. Rangarajan" ],
      "venue" : "Neural Comput,",
      "citeRegEx" : "Yuille and Rangarajan,? \\Q2003\\E",
      "shortCiteRegEx" : "Yuille and Rangarajan",
      "year" : 2003
    }, {
      "title" : "Statistical Analysis of Some Multi-category Large Margin Classification",
      "author" : [ "T. Zhang" ],
      "venue" : "Methods. JMLR,",
      "citeRegEx" : "Zhang,? \\Q2004\\E",
      "shortCiteRegEx" : "Zhang",
      "year" : 2004
    }, {
      "title" : "Latent hierarchical structural learning for object detection",
      "author" : [ "L.L. Zhu", "Y. Chen", "A. Yuille", "W. Freeman" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "Zhu et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2010
    }, {
      "title" : "Transductive support vector machines for structured variables",
      "author" : [ "A. Zien", "U. Brefeld", "T. Scheffer" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Zien et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Zien et al\\.",
      "year" : 2007
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "It has improved conceptual clarity of, and boosted performance in, different tasks such as image segmentation, graph matching, word alignment, grammatical tagging, protein structure prediction or cell tracking (see (Bakir et al., 2006) and references therein).",
      "startOffset" : 215,
      "endOffset" : 235
    }, {
      "referenceID" : 20,
      "context" : ", 2011) proposed convex loss for partial labels, which in turn resembles the one-versus-all loss (Zhang, 2004).",
      "startOffset" : 97,
      "endOffset" : 110
    }, {
      "referenceID" : 4,
      "context" : "3) to structured learning with latent variables (Yu & Joachims, 2009; Girshick et al., 2011).",
      "startOffset" : 48,
      "endOffset" : 92
    }, {
      "referenceID" : 0,
      "context" : "Finally, note that structured learning from partial annotations is different from semi-supervised or unsupervised structured learning (Altun et al., 2006; Xu et al., 2006; Zien et al., 2007).",
      "startOffset" : 134,
      "endOffset" : 190
    }, {
      "referenceID" : 17,
      "context" : "Finally, note that structured learning from partial annotations is different from semi-supervised or unsupervised structured learning (Altun et al., 2006; Xu et al., 2006; Zien et al., 2007).",
      "startOffset" : 134,
      "endOffset" : 190
    }, {
      "referenceID" : 22,
      "context" : "Finally, note that structured learning from partial annotations is different from semi-supervised or unsupervised structured learning (Altun et al., 2006; Xu et al., 2006; Zien et al., 2007).",
      "startOffset" : 134,
      "endOffset" : 190
    }, {
      "referenceID" : 14,
      "context" : "The particular choice of regularizer and loss function leads to different learning methods, with the squared norm and hinge loss, respectively, yielding a structSVM (Tsochantaridis et al., 2006), which is particularly popular in structured output learning.",
      "startOffset" : 165,
      "endOffset" : 194
    }, {
      "referenceID" : 8,
      "context" : "To circumvent this limitation, one could use pseudo-likelihood (maximizing the margin is similar to maximizing the likelihood ratio) to approximate this expectation as in (Lee et al., 2006).",
      "startOffset" : 171,
      "endOffset" : 189
    }, {
      "referenceID" : 12,
      "context" : "However, a recent study shows that this is only possible when the distribution of training sample is rich enough (Sontag et al., 2010).",
      "startOffset" : 113,
      "endOffset" : 134
    }, {
      "referenceID" : 14,
      "context" : "As a way out, structSVM (Tsochantaridis et al., 2006) penalizes small or negative margins (differences) between the score of the correct structured output and the highest score among any of the wrong structured outputs.",
      "startOffset" : 24,
      "endOffset" : 53
    }, {
      "referenceID" : 14,
      "context" : "(Tsochantaridis et al., 2006) suggest to adjust the penalty levied for high-scoring wrong predictions according to just how wrong they are, as measured by a user-defined task loss function ∆(y∗, y).",
      "startOffset" : 0,
      "endOffset" : 29
    }, {
      "referenceID" : 21,
      "context" : "Loss Y Y Appeared in Literature hinge Y Y∗ (Yu & Joachims, 2009; Fernandes & Brefeld, 2011; Zhu et al., 2010; Vedaldi & Zisserman, 2009; Wang & Mori, 2010) ramp Y Y (Do et al.",
      "startOffset" : 43,
      "endOffset" : 155
    }, {
      "referenceID" : 2,
      "context" : ", 2010; Vedaldi & Zisserman, 2009; Wang & Mori, 2010) ramp Y Y (Do et al., 2008; Girshick et al., 2011) max Y◦ Y (Jie & Orabona, 2010) bridge Y◦ Y∗ This paper",
      "startOffset" : 63,
      "endOffset" : 103
    }, {
      "referenceID" : 4,
      "context" : ", 2010; Vedaldi & Zisserman, 2009; Wang & Mori, 2010) ramp Y Y (Do et al., 2008; Girshick et al., 2011) max Y◦ Y (Jie & Orabona, 2010) bridge Y◦ Y∗ This paper",
      "startOffset" : 63,
      "endOffset" : 103
    }, {
      "referenceID" : 20,
      "context" : "We refer the readers to (Zhang, 2004; McAllester & Keshet, 2011; McAllester et al., 2010) for more theoretical analysis on hinge/ramp loss.",
      "startOffset" : 24,
      "endOffset" : 89
    }, {
      "referenceID" : 11,
      "context" : "We refer the readers to (Zhang, 2004; McAllester & Keshet, 2011; McAllester et al., 2010) for more theoretical analysis on hinge/ramp loss.",
      "startOffset" : 24,
      "endOffset" : 89
    }, {
      "referenceID" : 7,
      "context" : "(Yu & Joachims, 2009) used this strategy to optimize their structured SVM with latent variables, with a proximal bundle method (Kiwiel, 1990) for Step 2.",
      "startOffset" : 127,
      "endOffset" : 141
    }, {
      "referenceID" : 4,
      "context" : "(Girshick et al., 2011) and (Jie & Orabona, 2010) coined a similar procedure and applied stochastic gradient descent to speed up the training.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 14,
      "context" : "Structured learning is computationally expensive due to the repetitive maximization problems one has to solve at every iteration to compute the subgradients (Tsochantaridis et al., 2006; Teo et al., 2010).",
      "startOffset" : 157,
      "endOffset" : 204
    }, {
      "referenceID" : 13,
      "context" : "Structured learning is computationally expensive due to the repetitive maximization problems one has to solve at every iteration to compute the subgradients (Tsochantaridis et al., 2006; Teo et al., 2010).",
      "startOffset" : 157,
      "endOffset" : 204
    }, {
      "referenceID" : 13,
      "context" : "To construct a lower bound approximation for P (w), we follow the bundle minimization method from (Teo et al., 2010).",
      "startOffset" : 98,
      "endOffset" : 116
    }, {
      "referenceID" : 14,
      "context" : "n [f(x, ŷ;wk) + ∆(y ∗, ŷ)]− 〈a,wk〉 (12) where ŷ = arg maxy∈YP [f(x, y;wk) + ∆(y ∗, y)] is the expensive augmented inference problem (Tsochantaridis et al., 2006).",
      "startOffset" : 132,
      "endOffset" : 161
    }, {
      "referenceID" : 13,
      "context" : "Very similar to Theorem 2 in (Teo et al., 2010).",
      "startOffset" : 29,
      "endOffset" : 47
    }, {
      "referenceID" : 13,
      "context" : "We refer the readers to (Teo et al., 2010) for more details.",
      "startOffset" : 24,
      "endOffset" : 42
    }, {
      "referenceID" : 13,
      "context" : "Note that we implemented (Yu & Joachims, 2009)’s CCCP procedure using the BMRM method (Teo et al., 2010) whose complexity O( 1 ) is actually better than that of the proximal bundle method used in the original paper, O( 1 3 ).",
      "startOffset" : 86,
      "endOffset" : 104
    } ],
    "year" : 2012,
    "abstractText" : "Structured learning is appropriate when predicting structured outputs such as trees, graphs, or sequences. Most prior work requires the training set to consist of complete trees, graphs or sequences. Specifying such detailed ground truth can be tedious or infeasible for large outputs. Our main contribution is a large margin formulation that makes structured learning from only partially annotated data possible. The resulting optimization problem is non-convex, yet can be efficiently solve by concave-convex procedure (CCCP) with novel speedup strategies. We apply our method to a challenging trackingby-assignment problem of a variable number of divisible objects. On this benchmark, using only 25% of a full annotation we achieve a performance comparable to a model learned with a full annotation. Finally, we offer a unifying perspective of previous work using the hinge, ramp, or max loss for structured learning, followed by an empirical comparison on their practical performance.",
    "creator" : "LaTeX with hyperref package"
  }
}