{
  "name" : "1605.05628.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Detecting Novel Processes with CANDIES – An Holistic Novelty Detection Technique based on Probabilistic Models",
    "authors" : [ "Christian Gruhl", "Bernhard Sick" ],
    "emails" : [ "cgruhl@uni-kassel.de).", "bsick@uni-kassel.de)." ],
    "sections" : [ {
      "heading" : null,
      "text" : "Index Terms—ovelty Detection Gaussian Mixture Models CANDIES Online Goodness-of-Fitovelty Detection Gaussian Mixture Models CANDIES Online Goodness-of-FitN"
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "Today, so-called “smart” or “intelligent” technical systems are often equipped with abilities to act in real environments that are termed to be “dynamic” in the sense that their characteristics are time-variant (change over time). But typically, knowledge about the basic nature of these changes is built into these systems and it is assumed that only the time when these changes occur cannot be predicted. Future systems, however, have to evolve over time. Not all knowledge about any situations the system will face at run-time will be available at design-time. That is, the system has to detect and react on fundamental changes in time-variant environments. As\nC. Gruhl and B. Sick are with the University of Kassel, Department of Electrical Engineering and Computer Science, Wilhelmshoeher Allee 73, 34121 Kassel, Germany (email: cgruhl,bsick@uni-kassel.de).\nan example, we may consider technical systems that make observations of their environment with sensors and classify these observations (samples). In a timeinvariant environment, there may be different causes for different kinds of observations (called “processes” in the following). The data are modeled (e.g., by means of probabilistic models such as Gaussian mixtures) and then a classifier is built (e.g., by gradually assigning components of the Gaussian mixture to classes). At runtime, the data model and the classifier are not adapted.\nAn example for such a system could be a machine, that produces various parts (cf. left part of Figure 1). A process in this hypothetical environment would be similar to the production of a certain part. If the system is monitored with multiple sensors (e.g., S1 and S2), the resulting sensor signals span a two-dimensional input space (as shown in the middle part of Figure 1). Each value pair is called an observation or sample. With suitable machine learning techniques we can approximate the resulting distribution of samples. On the right side of Figure 1 a Gaussian mixture model (GMM, cf. 4.1) is used to approximate, and thus modeling, the sample distribution. Ideally, each component of the GMM describes a physical process in the environment. Reasons to rely on GMM for this purpose is that arbitrary continuous densities can be approximated by GMM (with any desired precision, based on the number of components) and the generalized central limit theorem, which states that the sum of i.i.d. random samples tends to be normally distributed (assumed, that the variance is finite). In technical system this is frequently the case, since observed sensor values are often the outcome of various random parameters that influence each other. In a time-variant environment, there may eventually be some conspicuous samples (cf. Fig. 2). Then (if the system is able to detect such a situation), some questions come up: Are these samples outliers of existing processes or not? If not, is there an anomaly in the observed environment or did a new process emerge that was unknown at design-time? And how can we build novelty detection methods to identify such novel processes? How can we adapt the data model to suit the changed environment and when (in order to find ar X iv :1 60 5. 05 62 8v 1\n[ cs\n.L G\n] 1\n8 M\nay 2\n2 S1\nS2 -3\n-2\n-1\n0\n1\n2\n3\n-3 -2 -1 0 1 2 3\nS1\nS2\n-3\n-2\n-1\n0\n1\n2\n3\n-3 -2 -1 0 1 2 3\nS1\nS2\nFig. 1. Hypothetical scenario of a monitored machine. On the left: abstract machine with monitoring sensors S1 and S2. In the middle: the two-dimensional input space consisting of measured sensor signals from S1 and S2. In this case, the outcomes of three different processes are gathered in three clusters. On the right: approximated density model, the ellipses are called components and correspond to multivariate Gaussians that represent the physical processes.\na trade-off between fast and accurate reaction)? And if there are new model components, to which class do we have to assign them?\nWith CANDIES (Combined Approach for Novelty Detection in Intelligent Embedded Systems), some of the questions can be answered. One major challenge is the reliable detection of (possibly multiple) novel processes in the complete input space. Herby we assume that the input space is divided into two parts:\n1) High-density regions (HDR): These are regions that are already covered by one or more components of the mixture model (i.e., the support of the kernels, in our case Gaussians, is high). This implies that normal observations are expected to appear in these regions and thus forming the normal model. However, new processes might also emerge here (e.g., “close” to, or between existing components, or even\n-2\n-1\n0\n1\n2\n3\n4\n-3 -2 -1 0 1 2 3\nThe transition between HDR and LDR is not strictly defined and is application dependent. Caused by their different characteristics, different problems are faced to detect novel processes. Since LDR have a potentially infinite support, the main difficulty is to efficiently find spatial relations (i.e., clusters) between suspicious observations. On the other hand, for HDR, two issues must be addressed: 1) which observations are assumed to be normal (outcome of an already known and modeled process) and which are suspicious (i.e., outcome of a novel process, or anomalies). 2) When is a novel process present.\nIn a preliminary article (see [15]), we presented 2- SNDR, an approach to solve the novelty detection problem sketched above for situations, where novel processes start to “generate” data in LDR of a probabilistic knowledge model (based on Gaussian mixtures). Figure 2 depicts such an exemplary scenario (where a novel process is emerging in a LDR). To detect novelty in HDR CANDIES relies on a new approach that is premised on statistical goodness-of-fit testing (i.e., measuring how\n3 well observed samples fit the assumed distribution), adjusted to suite Gaussian mixture models (GMM, cf. Section 4.1) and online environments. Figure 3 shows a different situation, where two novel processes started to “generate” samples in a HDR, but are not yet represented in the current model.\nAltogether, it is possible to address a specific kind of time-variance in the observed environment which is useful for many applications. We may imagine other kinds of situations where processes disappear (obsoleteness) or change some basic parameters (concept shift or concept drift). Our current research addresses these situations as well.\nThe remainder of this article is organized as follows: Section 3 gives a broad overview of related work, including other common novelty detection techniques, related topics, and what are the distinctions to CANDIES. Section 4 briefly summarizes methodical foundations essential for this article. Preliminary to the technical in depth details, a simplified overview of the idea behind the proposed technique is given in Section 2. The main body, introducing CANDIES in detail, is contained in Section 5. In Section 6 a small case study based on the KDD Cup 99 Computer Intrusion data set is presented. Finally, a conclusion and outlook to future work is given in Section 7."
    }, {
      "heading" : "2 OVERVIEW OF CANDIES",
      "text" : "With CANDIES we aim on three main goals: 1) Detecting clusters of suspicious samples (i.e., those that differ notably from what is expected). 2) Detecting such clusters in the complete input space, that is, in LDR and in HDR. 3) using the discovered clusters to model new processes.\nThe algorithms consists of multiple detectors for HDR and a single one for the LDR. It works (simplified) in the following manner. The foundation of the whole approach is a GMM, that provides a density estimate of the expected data. An advantage of GMM is that they can easily be extended to a classifier and that they belong to the family of generative models, thus additional structural information about the expected observations can be deduced (in contrast to discriminative classifiers, e.g., SVM). At first a new sample x′ located either in a HDR or in a LDR. Depending on the location it is marked as normal (located in HDR) or suspicious (located in LDR) (suspicious is what comparable algorithms mark as novelties). Depending on that decision either the LDR detector or one of the HDR detectors is responsible for handling the new sample. If the sample is marked as suspicious the sample is stored in a ring buffer on which a nonparametric clustering is performed. If a cluster in the buffer reaches a certain size the detector will report the detection of a novel process. Otherwise, when the sample is regarded as normal, it is used to update one of the HDR-Detectors (there is one HDR-Detector for each individual component of the GMM), the decision\nwhich detector is updated is made at random. The HDRDetector works by testing how well the last m samples are fitting the estimated Gaussian bell. This is done by using a χ2 test. If the t-value exceeds the critical value the detector reports the detection of a novel process."
    }, {
      "heading" : "3 RELATED WORK",
      "text" : "The main task for a Novelty Detector is to distinguish if a previously unseen sample belongs either to a normal model or if it is different in some way so that it does not belong to the normal data and is therefore novel. Closely related to the topic are the fields of anomaly and outlier detection. Over a decade ago it was sufficient to roughly group novelty detection approaches into two classes: either statistical (cf. [24]) or neural network based (cf. [25]).\nMost of the statistical approaches are relying on a model of the processed data. Observations are identified as (potentially) novel if they differ to much from what is expected, e.g., described by an appropriate model. Further, these approaches can be discerned based on the models they are using – either parametric or nonparametric models. Novelty detection techniques based on nonparametric density modeling are, for example, those using k-nearest neighbors approaches or kernel density estimators, see [37] for a sample application in intrusion detection. Parametric models on the other side make assumptions about the distribution of the observed samples, e.g. Gaussian mixture models. In preliminary work [12] we detect novelty based on a parametric Gaussian mixture model and a state variable which monitors how well the observations fit the model. The approach is used for comparison to CANDIES and briefly presented in the case study in Section 6.\nThe second group comprises detection techniques that are based on neural networks, e.g., multi-layer perceptrons, radial basis function neural networks, [4], [6] but, according to Markou and Singh [25], also include methods based on support vector machines, e.g. OneClass SVM as described by Tax and Duin [35].\nSince the early 2000s, the topic draw much attention as objective of research and changed considerably (i.e. new ranges of applications or whole new techniques, due to advances in computing power). Now, a more recent survey [29] suggest five different categories to group novelty detection approaches: i) probabilistic, ii) distance-based, iii) reconstruction-based, iv) domainbased, and v) information theoretical.\nThe first category covers a large part of the approaches that where previously affiliated with the statistical group. Typically these techniques are build upon a parametric density estimation of training data as a model. Frequently used are mixtures of Gaussians ([12] [20], [38], for instance). Novelty is usually detected if samples are observed in low-density-regions (i.e., the density for the observed sample is below a selected threshold). Several method to define a threshold are based on Extreme\n4 Value Theory (EVT, cf. [8], [18], [32]). The idea in EVT is to estimate the distribution of extreme values (i.e., maximum or minimum for legit samples) for a given density model and a given sample size. Then, samples that exceed the expected maximum or surpass the expected minimum are identified as novel. Recently Extreme Learning Machines with decision making depending on EVT where proposed by Al-Behadili et al. [2] to implement incremental semi-supervised learning based on novelty detection. Thus, probabilistic approaches are not limited to generative models, cf. [10], for example, where Support Vector Machines are used for detection and resulting novelty values calibrated in order to be interpreted as class-conditional probabilities.\nTo the second category belong approaches that are based on distances. Popular representatives of this category are approaches based on k-nearest-neighbors (knn). E.g., [7] or [17], [27], where the latter use the density of a k-neighborhood (i.e., a radius required to enclose k neighbors) to identify novel samples. A sample is novel if its neighborhood density is considerably lower than the density of its neighbors. Clustering based approaches refer also to category ii). Typically, normal samples are aggregated to form clusters, novelty is then determined by the minimal distance of an unseen sample to any centroid (e.g. [33], [36]). It is questionable whether category i) and ii) are sharply differentiable. Gaussian Mixture Models for example, consists of multiple location invariant kernels and the density is finally greatly dependent on the applied distance measure.\nOur new CANDIES approach does not fit into a single category but is a hybrid in the sense that it belongs to the first two categories: probabilistic and distance-based. For a detailed summary of the remaining categories iii), iv), and v) cf. [29].\nHowever, most of the introduced paradigms are designed to spot only single samples as novelties and do not relate those samples to one another. Thus, potential new knowledge (structural information in form of a cluster, that is evidence of a novel physical process) is unexploited and discarded. In some common applications such as medical condition monitoring [9], [31], [34] or machinery monitoring [30], this is not a real drawback, since anomalies might arose everywhere in the input space and are very specifically stuck to a concrete application (i.e., monitoring a specific patient or a specific engine). But in other fields, such as network intrusion detection, this discovered knowledge has great potential to be used to detect future attacks.\nThe contributions of this article are: 1) CANDIES is trimmed to detect novel processes\n(clusters of suspicious observations, cf. knowledge) in such a way, that the process can easily be integrated as new component into the existing GMM. This leads to the result, that learning does not only happen in a isolated training phase, off-line at design-time, but it is also conducted at run-time [16].\n2) Novelty is not only detected in low-density regions (where normal observations are not likely to appear), but also in high-density regions, i.e., where normal observations are expected."
    }, {
      "heading" : "4 METHODICAL FOUNDATIONS",
      "text" : "For the purpose of a self-contained article, we briefly recap the most important techniques used to implement our approach. This includes an overview of Gaussian mixtures, a method to extend those to a classifier, a short introduction to nonparametric density estimation as foundation for cluster analysis, and a brief description of statistical goodness-of-fit testing."
    }, {
      "heading" : "4.1 Gaussian Mixtures",
      "text" : "One frequently used approach to generative modeling is the Gaussian mixture model (GMM). That is, a superposition of multiple multivariate normal distributions (denoted as N and commonly referred to as Gaussian) and a mixing coefficients πj (Eq. (1)). Each Gaussian is called a component and has its own set of parameters which are the mean vector µj ∈ RD and a covariance matrix Σj (with dim(Σj) = D × D) that describes its shape. The mixing coefficients πj (with constraints ∑J j=1 πj = 1, πj ∈ R+) ensure that the resulting p(x) (x ∈ RD is the random variable) still fulfills the requirements for a density function. They may also be interpreted as priors for each component (i.e, the probability that an unobserved sample is generated by the corresponding component). Altogether, we get:\np(x) = J∑ j=1 πj · N (x|µj ,Σj). (1)\n5 An ordinary GMM models only the density of an associated training set and can be trained in an unsupervised manner (i.e., labels are not required). Since the sufficient statistics for the components cannot be computed in closed form, we pursue this goal with an expectationmaximization (EM) like approach that uses 2nd order (or hyper-) distributions and is heavily based on variational Bayesian inference (VI). An extensive introduction to VI is given by [5]. For clarification, a trained GMM for a two-dimensional data set is shown in Figure 4(a).\nRelying on VI gives rise to two advantages: (1) prior knowledge about the data can be included, which is especially valuable in real-world applications, and (2) multiple GMM can be fused into one model as described in [14]. The final GMM is obtained from the expectations (a point estimate from the second-order distributions) of the hyper-distributions after the VI training finishes.\nSince we assume a certain functional form of the underlying distribution and estimate its parameters, GMM are parametric density models."
    }, {
      "heading" : "4.2 Classification Paradigm",
      "text" : "To derive a classifier h(x) from the trained density model p(x), we estimate the class posteriors p(c|x) in a second, supervised (i.e., with respect to class labels) iteration. The classification of a given sample x is then done, as shown in Eq. (2) by selecting the maximum a-posteriori (MAP) of the class probabilities:\nh(x) = argmax c {p(c|x)} , (2)\nwith\np(c|x) = J∑ j=1 p(c|j) · p(j|x) = J∑ j=1 ξj,c · γx,j , (3)\nwhere\nγx,j = πjN (x|µj ,Σj)∑J\nj′=1 πj′N (x|µj′ ,Σj′) , (4)\nξj,c = 1\nNj ∑ xn∈Xc γxn,j . (5)\nEq. (4) shows the responsibilities γx,j which are the probability that a given sample x was generated by the j-th component. For each component j and class c the conclusion is determined by Eq. (5), which is the fraction of all responsibilities for samples xn ∈ Xc that are labeled with class c and the effective number of samples (denoted as Nj = ∑N n=1 γxn,j) belonging to the j-th component (X is the overall set of labeled samples, Xc the subset of X associated with class c).\nFinally, the class posteriors p(c|x) given in Eq. (3) are a composition of the responsibilities γx,j and the class conclusions ξj,c. The resulting decision boundary, which describes the classifier for the previously estimated density model, is shown in Figure 4(b)."
    }, {
      "heading" : "4.3 Density Based Clustering",
      "text" : "Rather than assuming a specific functional form such as parametric methods, nonparametric techniques provide a point estimate for the density p(x) at a given point x. One well-known nonparametric method is the Parzen window (or kernel) density estimator, here with Gaussian kernel:\np(x) = 1 N · hD N∑ n=1 k ( x− xn h ) . (6)\nIt is the sum of a finite set of N samples xn of an underlying training set to which an appropriate kernel function is applied to. The kernel is placed at the point x where the density should be estimated. The parameter h is a smoothing factor that controls how smooth the estimation is while D is the number of dimensions. Closely related to the Parzen window are histograms (cf. [5]).\nThe DBSCAN clustering algorithm (cf. [11]) uses a density estimation that is quite similar to a Parzen window estimator. Based on the density at each sample the algorithm decides whether a sample belongs to, lies at the edge, or is outside a cluster (in that case it is considered as noise). To do so, the kernel in Eq. (7):\nk(x) = { 1, if dist(x,0) ≤ 0, otherwise\n(7)\nis used which forms an D-dimensional sphere around the point x with radius . Typically, dist is realized with an Euclidean metric. If a sample is part of a cluster, all samples inside the sphere are also assigned to the same cluster. The advantage of this approach is that clusters of arbitrary shapes can be identified."
    }, {
      "heading" : "4.4 Statistical Goodness-of-Fit Tests",
      "text" : "To validate whether an observed sample matches a hypothesized distribution or not, goodness-of-fit tests can be applied. A fast and reliable method is Pearson’s chi-squared (χ2) test [28]. The test compares observed frequencies from mutually exclusive events (finite set of possible outcomes/values of a discrete random variable) against expected theoretical frequencies (obtained from a suitable fitted distribution) of these events. The test statistic (or t-value) is calculated by:\nt = k∑ i (xi − ei)2 ei (8)\nwhere xi is the observed event frequency of event i and k is the total number of different events. The expected frequencies of events i are given by ei:\nei = Pfit(i|Θ). (9)\nwhere Pfit is the fitted distribution. The test aggregates the squared deviations between observed and expected frequencies and weights them by the expected frequency.\n6 0 0.02 0.04 0.06 0.08 0.1\n0 5 10 15 20 25 30\nχ2(x), k = 11 upper 5%\nFig. 5. Distribution of t-values for χ2 test with 11 degrees of freedom. The marked region at the right is the rejection area for a significance level of α = 5%. The beginning of the region is equal to the critical value χ2,upper5%,11 .\nThis leads to stronger penalties when only small frequencies are expected. To accept or reject the null-hypothesis that the sample is drawn from the hypothesized distribution the t-value must be less than the critical value:\nχ 2,upper α,k = F −1 χ2ν (1− α) (10)\nThe critical value is calculated by evaluating the inverse cumulative density function F−1 of the χ2 distribution with ν degrees of freedom at point 1 − α. Where α is the significance level which implies that the error rate for type I errors is at most α (often α = 5% or 1%). The degrees of freedom ν are given by the number of events minus the number p of covariate parameters Θ of the fitted density Pfit (e.g., p = 2 for univariate Gaussian with Θ = (µ, σ)). Figure 5 emphasizes the relation between the χ2 distribution of t-values and the critical value χ2,upper for a significance level of α = 5%."
    }, {
      "heading" : "5 ONLINE NOVELTY DETECTION",
      "text" : "This section is split into three parts: the first part is based on our previous work [15] and discusses novelty detection and reaction in LDR with 2SND. The second part treats novelty detection in HDR with online capable χ2 goodness-of-fit tests. The last part then introduces CANDIES a detector which is able to detect novelties in the whole input space by combining both previously mentioned techniques. All techniques share the property to be applicable to online environments (i.e., soft realtime)."
    }, {
      "heading" : "5.1 Novelty Detection in Low-Density Regions",
      "text" : "To detect novel processes in sparse LDR we developed the 2 Stage Novelty Detection (2SND) algorithm. The algorithm works on top of an existing GMM or CMM (as described in Section 4.2) and extends it with novelty detection capabilities. Further, with 2SND it is possible to update the underlying GMM/CMM and to enhance them by including components that model the detected novel processes.\nThe algorithm itself consists of two procedures: a main procedure 2SND (Alg. 1) and an auxiliary procedure\nPROPAGATE, that propagates the cluster id to all affiliated samples using a modified breadth-first search.\nTo detect novel processes, we propose a two-stage approach which identifies suspicious samples in the first stage and novel processes in the latter. Each assessed sample is individually tested how well it suits the current model by determining whether it resides in a high- (HDR) or low-density region (LDR). This is done by exploiting the fact that the squared Mahalanobis distances between samples from a Gaussian j to its mean µj :\n∆2j (x) = (x− µj)TΣ −1 j (x− µj) (11)\nare χ2D-distributed, where Σ −1 j is the inverted covariance (or the precision) matrix. With the quantile function F−1 χ2D of the χ2D distribution, we can determine a squared Mahalanobis distance ρ = F−1\nχ2D (α) such that a fraction α\nof samples (which belong to the Gaussian) have a smaller squared Mahalanobis distance to the mean as ρ. Figure 6 depicts the relationship for one- and two-dimensional Gaussians.\nSeparating the input space into HDR and LDR simplifies the model considerably. Legitimate samples are assumed to appear in the dense regions while samples in the low-density regions are less likely to be observed. To detect novel processes in HDR additional detectors are required (cf. Section 5.2), since 2SND focuses only on LDR. By selecting α we specify how much of the total probability mass is covered by HDR, thus defining the transition between HDR and LDR. Samples with a Mahalanobis distance of ∆j(x) ≤ ρ to at least one of the component centers µj are located within a HDR and therefore seen as not suspicious. Samples with a higher distance ∆j(x) > ρ to all centers are regarded as being suspicious. This complies to the first stage of our novelty detection.\nFigures 6(b) and (c) are illustrate an exemplary situation for a GMM with a single component and different values of α. Observations inside the α-region (which is equal to a HDR) are depicted by circles ◦, while suspicious samples (located in a LDR) are shown as triangles M. The first stage is implemented in the first part of the procedure 2SND given in Alg. 1.\nThe second stage utilizes a density based clustering approach. Each sample x′ that is identified as being suspicious is cached in a circular buffer B with size b̃. Based on the distance to the nearest neighbor of x′ in B, the algorithm decides if x′ belongs to an already existing cluster. This behavior depends on the kernel given in Eq. (7) with being the maximum distance between a sample x′ and its nearest neighbor and it is implemented in the second part of the 2SND procedure, given in Alg. 1. If the sample is associated with a cluster C, the cluster is extended to include all -neighbors (i. e., all buffered samples with a distance dist(x′,xB) ≤ ). This is achieved with the procedure PROPAGATE, which is basically a breadth-first search with constraints. In fact,\n7 -0.025 0 0.025 0.05 0.075 0.1 0.125 0.15 0.175\n-2.5 -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 2.5\nα=0.9 α=0.66 √ρ=1.64 √ρ=0.95\n(a) A normal distribution with mean µ = 0 and variance σ2 = 1. The darker green area shows the region where 66% of the probability mass is located. The combination of both areas corresponds to a mass of 90%. Since the squared distances are χ21 distributed, the radii of the areas are equal to the root of the quantile function F−1\nχ2 D\nof the χ2D distribution which is √ ρ ≈ 0.95 for the darker green area\n(blue line) and √ ρ ≈ 1.64 for the combined area (green line).The marked areas are identical to the respective high-density regions.\n-2.5 -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 2.5\n-2 -1 0 1 2\n√ρ=2.14\n(b) Bivariate Gaussian with 90% α-region and maximum Mahalanobis distance of √ ρ = 2.15.\n-2.5 -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 2.5\n-2 -1 0 1 2\n√ρ=1.46\n(c) Bivariate Gaussian with 66% α-region and maximum Mahalanobis distance of √ ρ = 1.47.\nFig. 6. Relation between normal distribution and χ2D distributed distances. The dashed ellipses in (b) and (c) are level curves with a Mahalanobis distance of 1 while the black ellipses have a distance of √ ρ = √ F−1 χ22 (α) to their center. Samples displayed as red triangles M are suspicious (potentially novel), while samples depicted as blue circles ◦ are not suspicious. High-density regions (HDR) are colored in blue, low-density regions (LDR) in red.\nexpanding a cluster can lead to a merger of multiple clusters and, thus, create a much larger cluster.\nA novel process is detected as soon as a cluster C fulfills the adaptation criterion |C| ≥ minPts which corresponds to the number of samples that are associated with the cluster C. In Section 5.3.2 a measure is proposed to represent the current amount of novelty in LDR in human readable form."
    }, {
      "heading" : "5.1.1 Model Adaptation",
      "text" : "The last part of the 2SND procedure is responsible for deciding whether a novel process exists in the monitored LDR and how the model is adapted. If a new process in form of a cluster is identified, the underlying GMM needs to be updated. This is done by performing a VI training on all samples that are associated with the corresponding cluster. After that training step, the\nAlgorithm 1 2SND\nInput: sample x′, parameters α, ,minPts,b̃ Global: model M, buffer B Initialize ρ = F−1\nχ2D (α).\n{1st stage – detection of suspicious samples} for all components j in M do\nif ∆2j (x ′) ≤ ρ then\n{The observation is not suspicious} return classification of x′ based on M.\nend if end for if |B| = b̃ then\nRemove oldest sample from buffer B. end if Add suspicious sample x′ to buffer B.\n{2nd stage – detection of novel processes} Find nearest neighbor nnx′ of x′ if dist(x′,nnx′) ≤ then if nnx′ belongs to noise cluster then\nCreate new cluster Cnew with samples x′ and nnx′\nC = Cnew else\nAssign x′ to the same cluster Cnn as nnx′ C = Cnn PROPAGATE C to -neighborhood of x′.\nend if\n{Process detected – model adaptation} if |C| ≥ minPts then\nTrain GMM Mnovel of process C with VI. Update M and fuse it with Mnovel. Remove C and delete all samples of C from B return Classification of x′ based on updated M.\nend if end if return classification of x′ based on M.\nnovel process is represented by another GMM. To update the model, we exploit the properties of the hyperdistributions and use a fusion technique proposed in [14]. To fuse two given GMM we measure the pairwise divergence between each component and fuse only those which exceed a given threshold (0.5). In this case, we may assume that both components model the same process. This might happen, if a process emerges close to the border if the α-region (which also separates LDR from HDR). As divergence measure the Hellinger distance is used (cf. [3], [19]). The actual fusion combines the hyper-parameters of both components. Non-overlapping components are simply inserted into the existing model. In each case the hyper-distributions of the mixing coefficients must be adjusted, such that the mixing coefficients of the combined GMM still form a distribution. After this fusion step, the model is adapted to the changes in its\n8 environment and all samples belonging to the cluster C are removed from buffer B. If the updated model is used as the base for a classifier, the conclusion for the new component must be determined. Possible solutions to this problem are the involvement of a human domain expert, if meaningful labels are required, or the automatic generation of new, unique labels. As investigated by [12], it is also possible to exchange knowledge with other systems so that a novel process can be faster detected by another system. This kind of behavior is especially interesting for cyber-physical systems that share knowledge about their environment.\nFor clarification, an exemplary scenario that highlights the important operational phases of the new approach is shown in Figure 7."
    }, {
      "heading" : "5.2 Novelty Detection in High-Density Regions",
      "text" : "The above presented method detects suspicious samples in LDR only. However, observations in HDR are always considered as being normal, thus making this approach unable to detect novel processes there. For the moment we focus on the detection of overlapping processes for single components and later extend the idea to also suit GMM (where multiple components are present). The difficulty in detecting novel processes in dense regions is that we cannot decide if an observed sample is the legitimate outcome of a known (i.e., the existing component) or from an unknown overlapping process without knowing its affiliation (which in this case is a latent variable). We therefore make use of a sliding window that keeps track of the last ω observed samples, no matter if they are actually novel or not. It is clear, that if a novel process is present (that deviates at least in it’s mean or covariance from the existing component) the observed sample population (i.e., the content of the sliding window) will not match the distribution (described by the component) anymore and a noticeable difference between population and component has to be measurable. Due to their high computational complexity divergence measures such as the Kullback-Leibler divergence [22] or Hellinger distance [19] are intractable for the measurements, especially if the input space is of high dimensionality. To tackle this problem we do not measure the divergence of the sliding window and the existing component directly, instead we test how well the distances between samples in the buffer an the component’s center suit the expected distribution (which is a χ2 distribution as stated in Section 5.1). This task can be performed by using the χ2 test described in Section 4.4. Since the test is performed on the sliding window, the approach is suitable for online environments."
    }, {
      "heading" : "5.2.1 Transformation of the distance distribution",
      "text" : "One of the requirements of the χ2 goodness-of-fit test is, that the different events must be mutually exclusive. Therefore the continuous distance density must be transformed into a discrete one. Since any distribution\n-5 -4 -3 -2 -1 0 1 2 3 4 5\n-5 -4 -3 -2 -1 0 1 2 3 4 5\n(a) Initial training set with samples from two different classes, green circle ◦ and blue cross +. The density model is trained with VI and extended to a classifier as described in Section 4.2.\n-5 -4 -3 -2 -1 0 1 2 3 4 5\n-5 -4 -3 -2 -1 0 1 2 3 4 5\n(b) Resulting initial GMM with two components after VI training. The black line is the combination of the decision boundary and the α-regions. Samples that appear in the outer (cyan colored) LDR are identified as suspicious.\n9 is normalized (i.e., ∫ p(x)dx = 1) and, furthermore distances (here x) are always strictly positive it is quite easy to transform the density of distances into a discrete, uniform distribution.\nChoosing a finite uniform distribution with λ events (also cells, or buckets) brings several advantages including constant calculation of expected frequencies:\nunifλ(i) = 1\nλ , 1 ≤ i ≤ λ, i ∈ N. (12)\nThe fitted uniform distribution has only one free parameter (λ), therefore the degree of freedom is given by:\nk = λ− 1. (13)\nTo do the actual transformation the boundaries of the individual cells (celli) must be estimated so that each cell is equally likely. This is done with the inverse cumulative density function F−1 of the continuous density (as stated before χ2d for multivariate Gaussians with d dimensions) by dividing the density into λ areas of equal size:\ncelli = { [li, ri) 1 ≤ i < λ [li,∞) i = λ , (14)\nli = F −1 χ2d\n( (i− 1) · 1\nλ\n) , (15)\nri = li+1. (16)\nThus the first cell always begins at 0, and the last one is unbounded (right boundary is →∞).\nNow, if a previously unseen sample x′ is observed it is stored in the sliding window buffer and a counter bi for the responsible cell celli is incremented. The lookup for the correct cell can be done in O(log n) (e.g., using a tree structure). The t-value for the current buffer configuration at time point n is then calculated as follows:\ntn = λ∑ i=1 (bi,n − ei)2 ei (17)\nwith\nei = unifλ(i) · λ∑ j=1 bj ≈ ω λ . (18)\nThe expected value ei is only approximated by ωλ since the buffer can be not completely filled (i.e., contains less than ω samples). If the buffer is at capacity when a new observation x′ is processed, the oldest element gets removed. The t-value is compared with significance of α = 0.01 = 1% against the critical value:\nχ2,upperk = F −1 χ2k (1− α) = F−1 χ2k (0.99). (19)\nIf t > χ2,upperk the threshold is exceeded and the detector reports novelty. Figure 8 shows on the left an exemplary data set with a single component trained to fit the green circles ◦. Additionally the set contains samples from three more processes (purple + crosses around (0.5, 1.5), red 4 triangles around (−2, 3), and blue ◦ circles close to the components center around (−2, 1.5)).\n-1\n0\n1\n2\n3\n4\n-4 -3 -2 -1 0 1 0\n25\n50\n75\n100\n125\n0 2 500 5 000 7 500\nFig. 8. Test data set and corresponding test statistics t. First crosses appear, then triangles, and finally circles. The parameters are: w = 50, λ = 12, α = 0.99, p = 0.01. The red line is are the test statistics t. The horizontal, black line indicates the critical value.\nAll additional processes represent novelties and appear in the given order. The image on the right shows the curve of the calculated t-values for the sliding window in red. When samples from the novel processes appear the curve changes and exceeds the critical value (given as black line) considerably.\nThe signal is however noisy, so that at some points of time the critical value is slightly exceeded even though no novel processes are present. To compensate for this effect smoothing the t-values with a moving average:\ntma,n = 1\nM M∑ i=0 tn−i, (20)\nwhere M is the number of considered previous t-values, is a promising approach as the blue curve on the right image of Figure 8 illustrates."
    }, {
      "heading" : "5.2.2 Learning of the distance distribution",
      "text" : "Real world data sets or sensory data from embedded systems often differ from their assumed distributions. Whereas this is not a problem for classification for our goodness-of-fit approach to novelty detection it is, as the t-value curve in Figure 9 highlights. Here a single Gaussian is fitted based on the depicted samples, which are uniformly distributed rather than normal. Therefore the distances are not χ2 distributed as it is presumed by our test and the critical value is almost permanently exceeded by the test statistics.\nThe problem can be solved by estimating the cell boundaries directly from the samples Xtrain that are used to train the component:\ncelli =  [li, ri) 1 < i < λ [0, ri) i = 1\n[li,∞) i = λ , (21)\nli = ∆(xdi·wλ e) , xj ∈ Sort(Xtrain), (22) ri = li+1. (23)\nwhere ∆(x) is the Mahalanobis distance to the center µ of the component. By using the distance of every di · wλ e\n10\nelement of the order samples each cell contains approximately the same number of entries, thus forming again a discrete uniform distribution. The computationally most complex part is the sorting of the training samples Xtrain, which can be done in O(n · log n). Note that the last cellλ might be underestimated due to the rounding.\nIn Figure 10 the ordered training set Xtrain is depicted. The y-axis (index of the sorted samples) is divided into λ = 12 equally sized parts (each part corresponding to a cell) the associated function arguments on the x-axis (distances to component center) are equal to the interval boundaries. If the curve is normalized an approximation of the cumulative density function of the real distance distribution can be obtained.\nFigure 11 shows the resulting t-value curves of two uniform distributed data sets (on the left the same example as in Figure 9 with 2 dimensions, on the right another sample set with 5 dimensions) where the expected frequencies for the test are estimated according to Equation (21). The (moving-average) curves are now clearly below\nthe critical value (black line), thus not indicating any novelty."
    }, {
      "heading" : "5.2.3 Extension to Gaussian Mixture Models",
      "text" : "To extend the high-density approach to GMM with multiple components, each component needs its own detector.\nIf a new sample x′ is observed it should be used to update the detector of its affiliated component. The affiliation is however a latent variable and thus not known at run-time. One method to estimate the affiliations is (Monte Carlo) random sampling, which requires only the evaluation of the unnormalized (without mixing coefficient πj) densities Pj(x′) = N (x′|µj ,Σj) for each component j and a continuous uniform pseudo random number generator unif(0, 1) for the unit interval [0, 1]. The sampling works by partitioning the unit interval into J parts. Each partition mj is associated with exactly one component j and the boundaries are given by:\nmj =  [0, rj) j = 1 [lj , rj) 1 < j < J\n[lj , 1] j = J\n, (24)\npx′,j = Pj(x ′)∑J k=1 Pk(x ′) (25)\nlj = rj−1 j > 1, (26) rj = lj + px′,j . (27)\nwhere px′,j are the normalized densities to ensure that the support of the individual parts sum up to 1. To identify a winner component (i.e., the one that will be affiliated with the observation x′) a random value r′ is drawn from the uniform generator unif(0, 1). The partition mj that covers the drawn value r′ indicates the winning component j.\nFigure 12(a) shows clouds (5000 samples, 2 dimensions, 2 classes) a widely used artificial data set from the UCI Repository [23], with trained classifier. The CMM is trained in 5-fold cross-validation fashion, where four\n11\nfolds are used for the actual training and the remaining fold for testing. This leads to an experiment where no novel (unknown) processes are present since all folds contain samples from all four known processes. The test result for one experiment is displayed in Figure 12(b). The curve shows the average high-density novelty measure (discussed in Section 5.3.2), which does not exceeded the critical value that is given by the black line and has a constant value of 1, thus indicating that no novel processes are present. The test statistics of the individual component detectors are depicted in Figure 13.\nA modified test setup for clouds is illustrated in Figure 14(a). Here the training is performed on 2000 samples and the remaining 3000 samples are interspersed with 400 samples from two overlapping novel processes (red 4 triangles). The novel processes appear around time steps 1000 and 2200. The corresponding high-density novelty measure curve for the experiment is given in Figure 14(b) and indicates novelty (blue bars rising to 1) in the regions where the novel samples are interspersed. The test statistics of the individual component detectors\nof this experiment are depicted in Figure 15. From the curves it can be inferred that the main contributions for the detection is from the component (bottom right) that represents blue + crosses."
    }, {
      "heading" : "5.3 CANDIES",
      "text" : "CANDIES is our holistic approach to detect novelty in high- as well as in low-density regions of a GMM. This is achieved by using a single 2SND (cf. Section 5.1) detector combined with multiple HDR detectors for each component (cf. Section 5.2.3)."
    }, {
      "heading" : "5.3.1 Requirements to merge LDR and HDR detectors",
      "text" : "For the system to operate some adjustments are necessary. At first a previously unseen sample x′ is checked whether it is located in an HDR or not (similar to the first stage of 2SND). If this is not the case, the sample is passed to the second stage of 2SND and the density based clustering is refreshed. At this point a novel process might be detected. Otherwise, the in Section 5.2.3 described random sampling is executed and x′ gets affiliated with exactly one of the components J . Then\n12\nAlgorithm 2 CANDIES\nInput: sample x′, parameters α, ω , ,minPts,b̃ Global: model M, buffer B Initialize ρ = F−1\nχ2D (α).\n{decide if x′ is located in high- or low-density region}\nfor all components j in M do if ∆2j (x\n′) ≤ ρ then {The observation is in dense region} {get winner according to Section 5.2.3} j = winner component for x′ update χ2 detector of component j {Compare t-value with critical value} if tj > χ2,upper then {Process detected} end if return classification of x′ based on M.\nend if end for {The observation is in low-density region} return 2SND(x′, α, ,minPts, b̃).\nfor this component j a new t-value is estimated and compared against the critical value. At this point an overlapping novel process might be detected.\nSince samples x′ with a distance ∆j(x′) > ρ for all j ∈ J are always processed by the LDR detection part, the assumed distance distribution of the components HDR detectors will not match the observed samples. However, by establishing the following dependency λ = 1 1−α between the 2SND LDR detector and the HDR detectors, and adjusting the calculation of the t-values to:\ntn,j = λ−2∑ i=1 (bi,n − ei)2 ei , (28)\nei ≈ ω\nλ− 1 , (29)\nthe last cells cellλ are representing exactly the fraction α of samples that are located in LDR (but with eλ = 0) while the first λ − 1 cells cover the remaining 1 − α percentage. The critical value is changed to:\nχ2,upperλ−2 = F −1 χ2λ−2 (0.99). (30)\nThus the goodness-of-fit test is adjusted to evaluate only the frequencies of samples expected to appear in HDR. The whole approach is summarized and commented in Algorithm 2.\nFigure 16 shows another modification of the previously used clouds data set. Again, additional samples (red 4 triangles) from two novel processes are interspersed. The locations are chosen in a way so that one process (centered around (1, 1)) shares a large fraction of its support with two of the known processes, while the other one (centered around (−2,−2)) is positioned in a\nlow-density region. The t-value curves of the four known components are illustrated in Figure 17. Furthermore the novelty measures (discussed in Section 5.3.2) given in Figure 18 clearly indicate novelty in the expected time ranges (blue bars rising to 1). The left curve gives the novelty value in low-density regions, where the first novel process starts generating samples around time stamp ≈ 1000. On the right curve the average novelty measure for high-density regions is illustrated. Here, the novel process gets also detected but a small delay between appearance of novel samples (around ts ≈ 2000) and the detection can be observed. This is most likely due to the random sampling, which disperses novel samples to multiple components."
    }, {
      "heading" : "5.3.2 Novelty Measure – Human Readability",
      "text" : "We propose two novelty measures to quantify how much novelty is present in different regions (i.e. HDR or LDR) in a way that is comprehensible for (data scientists). Therefore, the measures should express the absence of a novel process (or novelty) with a value near 0, while the presence of such a process should be expressed by a value ≥ 1.\nThe measure ν2snd for LDR is given by:\nν2snd,n = 1− |C|+ |Noise|\n|B| (31)\n13\nWhere |B| is the number of observations currently stored in the buffer, |C| is the number of different cluster, and |Noise| the number of samples associated with the noise cluster. If a single cluster that contains most samples currently kept in the buffer is present (which is a strong indicator for a novel process), the measure will be close to 1. On the other hand, if all samples are considered to be noise or multiple clusters with only a few samples are present, the novelty value will be closer to 0.\nThe HDR measure ν̄ (average novelty) is based on the geometric mean of the normalized t-values νj of the individual components:\nν̄n =  J∏ j w(νj)  1J , (32) νj = tn,j\nχ2,upperk . (33)\nThe normalization constant is given by the critical value. As Equation (32) shows, the νj are passed to a function w which is a non-linear transform that boosts values near 1:\nw(x) = x · (2− comp(1− x, 1000)), (34)\ncomp(x, µ) = log(1 + µ · x)\nlog(1 + µ) . (35)\nThe idea here is that if multiple components approach the critical value (an indicator for novel process located between these components) the novelty measure should also express this. If the model however consists of considerably more components (with t-values distant from the critical value), the mean is dominated by these components. Thus boosting values already close to 1, allows to overcome the normal components to increase the mean, so that novelty is also expressed there. Exemplary curves for both measures are given in Figure 18 (novel processes present) and Figure 19 (only normal processes observed)."
    }, {
      "heading" : "5.3.3 Overview of Parameters",
      "text" : "CANDIES comes along with a considerable amount of adjustable parameters. Table 1 gives an overview of all parameters present in CANDIES including a short description, recommendations for (good) default values (if possible), and which detector is influenced by the parameter. Note, that especially the buffer-size parameters are application-dependent on how many novel processes are expected to appear at once, and how many samples they will generate."
    }, {
      "heading" : "5.3.4 Handling of Noise",
      "text" : "Since the novelty detection is designed to detect novel processes and not single observations, it is rather robust against distributed noise in the input space. While novel samples of a novel process will appear in a dense form, random noise is scattered across the input space so that it is quite unlikely to form sufficiently large clusters.\nFor the LDR detection part (based on 2SND) the robustness is achieved by the two stage architecture that suspicious samples pass through. Figure 20 shows a scenario that includes uniformly distributed noise that is mixed into a test set with observations from one known process and one novel process (located to the right, outside of the α-zone in low-density region). Depending on its parametrization, the LDR approach only detects a novel process where the novel observations are actually located.\nFigure 21(a) depicts the same exemplary data set that is already used in Section 5.2.1 interspersed with uniform random noise (purple + crosses). The corresponding t-value curve is displayed in Figure 21(b) and shows a recognizable up-shift, introduced by the noise. Nevertheless, this undesired effect can be circumvented by adjusting the distance distribution according to Section 5.2.2. The curve of the adjusted test is illustrated in Figure 21(c). The course of the moving-average is now clearly below the threshold in intervals where no novelty is present, but rises clearly - although weaker as compared to the application without noise - above the critical value, when the novel processes start to generate samples. Therefore the high-density approach\n14\nis essentially capable of handling noise, but requires the presence of noise in the training data."
    }, {
      "heading" : "5.3.5 An alternative view on CANDIES",
      "text" : "At first the two different approaches to novelty detection for LDR and HDR might seem quite different. It is, however, possible to get a consistent view by interpreting one detector by means of the other. As mentioned before, the last bins cellλ of each HDR detector matches the lowdensity parts of the input space. Therefore the ring buffer B used for 2SND can be seen as a shared cell across all HDR detectors. On the other hand, the individual buffers of each HDR detector allow an interpretation as clusters (with a different adaptation predicate P ), and thus suiting the 2nd stage of 2SND."
    }, {
      "heading" : "6 CASE STUDY",
      "text" : "To validate that the presented approach can be used to real-world applications, we show experimental results\nbased on the well-known KDD Cup 1999 network intrusion data set [21]. Even though it is pointed out that there are some serious flaws in the data set, which makes it inappropriate for the evaluation of real intrusion detection systems, its properties are still suitable for our purposes, since we are not interested in building a state of the art intrusion detection system.\n15"
    }, {
      "heading" : "6.1 Setup",
      "text" : "As mentioned before, our new approach is compared to a novelty detection technique that is proposed in [12]. Here, novel samples are also identified using a GMM and the squared Mahalanobis distance between processed samples and the mean of the different components. Each time a new sample is processed, an internal state variable Sn is updated, such that Sn = Sn−1 + χ2nov , with:\nχ2nov(x) = η J∑ j=1 p(j|x) ( δα,j(x)− α 1− α (1− δα,j(x)) ) (36) being a penalty or reward, depending on how well the new sample fits the model. To compute whether the state variable is rewarded or punished, the indicator functions:\nδα,j(x) =\n{ 1, ∆2j (x) ≤ ρ = F\n−1 χ2D (α)\n0, sonst (37)\nof each component are evaluated and the results are multiplied with the responsibilities of the components. If the algorithm is processed in an environment without emerging processes, the expectation of the state variable will be equal to its initial value E[Sn] = 1. The presence of a novel process will lead to a decrease of the value of the state variable Sn. This can be exploited to detect novel processes as soon as the state variable underflows a given threshold (here: 0.2). The parameter η controls how fast the state variable changes (here: 0.001). This causes a model adaptation that uses the last 500 observations to retrain the model, which is done with a modified VI algorithm, that allows to insert new components into an existing GMM and train only those, keeping the existing components “fixed”. After the model is adapted to its changed environment, the state variable is reset to its initial value. We refer to this approach as CSND (χ2novelty detection)\nOriginating from the various recorded connections in the KDD99 data set, different attack scenarios are sampled (these are: ipsweep, neptune, nmap, satan, and smurf). Each scenario consist of background connections (legitimate network traffic) and connections related to the specific attack. A dimension reduction to 6 out of the 41 dimensions is performed as preprocessing step. Additionally and due to the massive support in terms of categories, we interpret the discrete attributes as nearly continuous. Each scenario consists of three parts with an overall of 25000 connections. The first part contains 10000 connections drawn from a pool of background connections only. The second part is a mixture of background and attack connections (with the attack name as label) with a ratio of 3:1 and a total of 10000 connections. The last 5000 connections form the third part, which again consists only of legitimate traffic.\nBoth adaptive classifiers are trained with the first 5000 samples of the first part of each scenario to learn an initial GMM with VI. The experiments themselves are\nconducted in a 5-fold cross-validation fashion, with independent folds for the train sets, which consist of connections from the first and third parts of each scenario, and a single test set that is equal to the second part of each scenario.\nAdditionally, to get a baseline for the classification performance, a static classifier (as described in Section 4.2, referred to as GMM-Static) is trained on samples of all classes (background connections and attacks). That is, this classifier can be seen as omniscient as it anticipates future attacks that are completely new and unpredictable for the two adaptive classifiers above. In order to get meaningful results, a stratified 5-fold cross-validation, with all connections mixed together, is carried out. Then the accuracy and the F1-score of the class assigned to samples of the novel process are used to evaluate the classification performance."
    }, {
      "heading" : "6.1.1 Results",
      "text" : "The resulting averaged classification performances are summarized in Table 2, which states that both adaptive approaches are able to identify the attacks and perform model adaptations that integrate the acquired knowledge. In all scenarios, the accuracy and the observed F1score of CANDIES is equal or higher compared to those of the CSND approach. In three out of five scenarios our approach performs comparably well as the static baseline and still satisfiable on the other two.\nThe higher performance of CANDIES over CSND is explained by Table 3, which shows the average number of actual novel samples (samples actually belonging to the attack) that are processed before the novel process is detected and a model adaptation triggered. Here CANDIES displays its strength to exploit spatial information between suspicious samples in LDR the form of clusters, which accelerates the detection compared to the slowly changing state variable of CSND.\nThe algorithm is designed to be processed in an online mode. Therefore, the number of triggered model adaptation steps and the number of inserted components are also investigated. Table 4 shows the averaged number of adaptation and insertion steps for each scenario. As\n16\nwe can see, both approaches tend only to a single model adaptation, which is the optimum here. The CSND approach has fewer model adaptations on average than the CANDIES, but has a higher average number of inserted components, which is not negligible since the number of components in the GMM has a direct influence on the run-time of both algorithms."
    }, {
      "heading" : "7 CONCLUSION AND OUTLOOK",
      "text" : "We introduced CANDIES, a holistic approach to novelty detection for (new) emerging processes throughout the complete input space of a probabilistic classifier. To achieve this, different novelty detectors for low-density regions (LDR, where it is less likely to observe samples) and high-density regions (HDR, samples are expected to be observed here) are combined and thus able to cover the complete input space. For LDR we resort on 2SND, this algorithm works with two stages. First, suspicious observations are identified with the help of a GMM (which are based on parametric densities). In the second stage, suspicious samples are then clustered in a nonparametric way (inspired by DBSCAN). A novel process is recognized as soon as one of the (nonparametric) clusters reaches a sufficient size. The detection in HDR on the other hand is purely based on parametric density estimation. We showed how to use multiple detectors (one detector per component) to identify novelty in GMM. The presence of novel processes in HDR is\ndirectly identified. This is accomplished by maintaining a sliding window of recent observations and performing statistical goodness-of-fit tests between sliding window and the affiliated component.\nIn a compact case study in the field of computer network intrusion detection, we could show that CANDIES is applicable to real-world data sets. We tested it on a subset of the well-known KDD Cup ’99 Intrusion Detection data set, where rather promising results were obtained. So far, first experiments on artificial laboratory data sets lead us to the conclusion that CANDIES will be a satisfactory solution to novelty detection with model adaptation in the near future.\nIn our future work we will focus on extending the described novelty detector further, this includes in particular reaction procedures for the HDR detection. We will elaborate the performance of CANDIES on more sample applications, e.g., in the fields of robotics or video based surveillance. Detection and handling of obsoleteness or concept shift will be accomplished with techniques similar to the ones proposed here. The same holds for concept drift, but here, it will be quite difficult to effect the trade-off between under- and overreaction (too early or too late). The accuracy of our techniques must be set in relation to a “degree” of time-variance in the observed system. It will be possible to detect emergent phenomena in the observed environment and to numerically assess the degree of emergence (cf. [13]). Also, these techniques allow for an application to various anomaly detection problems. Furthermore, the design of the approach is not necessarily limited to GMM but applicable to other mixture models as well.\nAnother possible application field that could benefit from our proposed technique are systems equipped with awareness capabilities. Often, terms such as locationaware, context-aware, self-aware, or environment-aware are used in the literature (see, e.g., [1], [26]). In our opinion, awareness is essentially • the capability to compare knowledge about the\nself, the environment, other systems etc. to current observations in order to detect when expectations concerning current observations do not meet the actual observations anymore and • the ability to adapt the knowledge model in a way such that the system meets some performance requirements which includes a solution to the problem when to adapt the model in odrder to avoid a performance loss either due to too fast or too slow reactions.\nAltogether, awareness techniques will be a key to develop new kinds of technical systems that could actually be termed to be “intelligent” or “smart” with some higher degree of justification."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "The authors would like to thank the German Research Foundation (DFG) for support within the DFG project CYPHOC (SI 674/9-1).\n17"
    } ],
    "references" : [ {
      "title" : "Towards a better understanding of context and context-awareness",
      "author" : [ "G. Abowd", "A. Dey", "P. Brown", "N. Davies", "M. Smith", "P. Steggles" ],
      "venue" : "H.-W. Gellersen, editor, Handheld and Ubiquitous Computing, volume 1707 of Lecture Notes in Computer Science, pages 304–307. Springer Berlin Heidelberg",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Extreme Learning Machine based Novelty Detection for Incremental Semi- Supervised Learning",
      "author" : [ "H. Al-Behadili", "A. Grumpe", "C. Dopp", "C. Wohler" ],
      "venue" : "International Conference on Image Information Processing, pages 230–235",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "On a measure of divergence between two multinomial populations",
      "author" : [ "A. Bhattacharyya" ],
      "venue" : "Sankhyā: The Indian Journal of Statistics, pages 401–406",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 1946
    }, {
      "title" : "Novelty detection and neural network validation",
      "author" : [ "C.M. Bishop" ],
      "venue" : "Vision, Image and Signal Processing, volume 141, pages 217–222",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Pattern Recognition and Machine Learning (Information Science and Statistics)",
      "author" : [ "C.M. Bishop" ],
      "venue" : "Springer-Verlag",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Neural networks applied in intrusion detection systems",
      "author" : [ "J. Bonifacio", "A. Cansian", "A. Carvalho", "E. Moreira" ],
      "venue" : "Proc. of IJCNN, volume 1, pages 205–210",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "LOF: Identifying Density-Based Local Outliers",
      "author" : [ "M.M. Breunig", "H.-P. Kriegel", "R.T. Ng", "J. Sander" ],
      "venue" : "ACM SIGMOD Record, 29(2):93–104",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Novelty detection with multivariate extreme value statistics",
      "author" : [ "D.A. Clifton", "S. Hugueny", "L. Tarassenko" ],
      "venue" : "Journal of Signal Processing Systems, 65(3):371–389",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Identification of patient deterioration in vital-sign data using one-class support vector machines",
      "author" : [ "L. Clifton", "D.A. Clifton", "P.J. Watkinson", "L. Tarassenko" ],
      "venue" : "2011 Federated Conference on Computer Science and Information Systems (FedCSIS), (2):125–131",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Probabilistic novelty detection with support vector machines",
      "author" : [ "L. Clifton", "D.A. Clifton", "Y. Zhang", "P. Watkinson", "L. Tarassenko", "H. Yin" ],
      "venue" : "IEEE Transactions on Reliability, 63(2):455–467",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "S",
      "author" : [ "M. Ester", "H. Kriegel" ],
      "venue" : "J., and X. Xu. A density-based algorithm for discovering clusters in large spatial databases with noise. In Proc. of KDD-96, pages 226–231. AAAI Press",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Techniques for knowledge acquisition in dynamically changing environments",
      "author" : [ "D. Fisch", "M. Jänicke", "E. Kalkowski", "B. Sick" ],
      "venue" : "TAAS, 7(1):1–25",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Quantitative emergence – a refined approach based on divergence measures",
      "author" : [ "D. Fisch", "M. Jänicke", "B. Sick", "C. Müller-Schloer" ],
      "venue" : "SASO, pages 94–103",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Knowledge fusion for probabilistic generative classifiers with data mining applications",
      "author" : [ "D. Fisch", "E. Kalkowski", "B. Sick" ],
      "venue" : "TKDE, 26(3):652–666",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "A building block for awareness in technical systems: Online novelty detection and reaction with an application in intrusion detection",
      "author" : [ "C. Gruhl", "B. Sick", "A. Wacker", "S. Tomforde", "J. Hähner" ],
      "venue" : "IEEE iCAST, pages 194–200. IEEE",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Runtime Self-Integration as Key Challenge for Mastering Interwoven Systems",
      "author" : [ "J. Haehner", "U. Brinkschulte", "P. Lukowicz", "S. Mostaghim", "B. Sick", "S. Tomforde" ],
      "venue" : "pages 1–8",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Outlier detection using k-nearest neighbour graph",
      "author" : [ "V. Hautamäki", "I. Kärkkäinen", "P. Fränti" ],
      "venue" : "Proc. – International Conference on Pattern Recognition, 3(09):430–433",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Extreme value statistics for vibration spectra outlier detection",
      "author" : [ "A. Hazan", "J. Lacaille", "K. Madani" ],
      "venue" : "International Conference on Condition Monitoring and Machinery Failure Prevention Technologies, pages 736–744, London, UK",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Neue Begründung der Theorie quadratischer Formen von unendlich vielen Veränderlichen",
      "author" : [ "E. Hellinger" ],
      "venue" : "Journal für die reine und angewandte Mathematik, 136:210–271",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 1909
    }, {
      "title" : "Gaussian mixture pdf in one-class classifcation: computing and utilizing confidence values",
      "author" : [ "J. Ilonen", "P. Paalanen", "J. Kamarainen", "H. Kälviäinen" ],
      "venue" : "ICPR, volume 2, pages 577–580",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "KDD Cup 1999 Data – Data Set",
      "author" : [ "KDD Cup" ],
      "venue" : "http://kdd.ics.uci. edu/databases/kddcup99/kddcup99.html, 1999. ",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "On information and sufficiency",
      "author" : [ "S. Kullback", "R.A. Leibler" ],
      "venue" : "The Annals of Mathematical Statistics, 22:79–86",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 1951
    }, {
      "title" : "Novelty Detection: a review – part 1: statistical approaches",
      "author" : [ "M. Markou", "S. Singh" ],
      "venue" : "Signal Processing, 83:2481–2497",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Novelty Detection: a review – part 2: neural network based approaches",
      "author" : [ "M. Markou", "S. Singh" ],
      "venue" : "Signal Processing, 83:2499–2521",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Organic Computing – A Paradigm Shift for Complex Systems",
      "author" : [ "C. Müller-Schloer", "H. Schmeck", "T. Ungerer" ],
      "venue" : "Springer-Verlag Berlin Heidelberg",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "LOCI: Fast Outlier Detection Using the Local Correlation Integral",
      "author" : [ "S. Papadimitriou", "H. Kitagawa", "P.B. Gibbons", "C. Faloutsos" ],
      "venue" : "In Data Engineering,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2003
    }, {
      "title" : "On the criterion that a given system of deviations from the probable in the case of a correlated system of variables is such that it can be reasonably supposed to have arisen from random sampling",
      "author" : [ "K. Pearson" ],
      "venue" : "Philosophical Magazine Series 5, 50(302):157–175",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 1900
    }, {
      "title" : "A review of novelty detection",
      "author" : [ "M.A. Pimentel", "D.A. Clifton", "L. Clifton", "L. Tarassenko" ],
      "venue" : "Signal Processing,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2014
    }, {
      "title" : "Unsupervised condition change detection in large diesel engines",
      "author" : [ "N.H. Pontoppidan", "J. Larsen" ],
      "venue" : "Neural Networks for Signal Processing – Proc. of the IEEE XIII Workshop, pages 565–574",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Extreme value statistics for novelty detection in biomedical data processing",
      "author" : [ "S. Roberts" ],
      "venue" : "IEE Proc. – Science, Measurement and Technology, 147(6):363–367",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Novelty detection using extreme value statistics",
      "author" : [ "S.J. Roberts" ],
      "venue" : "Vision, Image and Signal Processing, IEEE Proc., 146(3):124–129",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "F",
      "author" : [ "E.J. Spinosa" ],
      "venue" : "de Carvalho, A. deLeon, and J. Gama. Novelty detection with application to data streams. Intelligent Data Analysis, 13(3):405–422",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Novelty detection for the identification of masses in mammograms",
      "author" : [ "L. Tarassenko", "P. Hayton", "N. Cerneaz", "M. Brady" ],
      "venue" : "Artificial Neural Networks, 1995., Fourth International Conference on, (10):442– 447",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Uniform Object Generation for Optimizing One-class Classifiers",
      "author" : [ "D. Tax", "R. Duin" ],
      "venue" : "The Journal of Machine Learning Research, 2:155–173",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Outlier identification and market segmentation using kernel-based clustering techniques",
      "author" : [ "C.H. Wang" ],
      "venue" : "Expert Systems with Applications, 36(2):3744–3750",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Parzen-window network intrusion detectors",
      "author" : [ "D. Yeung", "C. Chow" ],
      "venue" : "Proc. of ICPR, volume 4, pages 385–388",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Novelty detection for practical pattern recognition in condition monitoring of multivariate processes: A case study",
      "author" : [ "F. Zorriassatine", "A. Al-Habaibeh", "R.M. Parkin", "M.R. Jackson", "J. Coy" ],
      "venue" : "International Journal of Advanced Manufacturing Technology, 25(9- 10):954–963",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2005
    } ],
    "referenceMentions" : [ {
      "referenceID" : 14,
      "context" : "In a preliminary article (see [15]), we presented 2SNDR, an approach to solve the novelty detection problem sketched above for situations, where novel processes start to “generate” data in LDR of a probabilistic knowledge model (based on Gaussian mixtures).",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 22,
      "context" : "[24]) or neural network based (cf.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 23,
      "context" : "[25]).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 35,
      "context" : "Novelty detection techniques based on nonparametric density modeling are, for example, those using k-nearest neighbors approaches or kernel density estimators, see [37] for a sample application in intrusion detection.",
      "startOffset" : 164,
      "endOffset" : 168
    }, {
      "referenceID" : 11,
      "context" : "In preliminary work [12] we detect novelty based on a parametric Gaussian mixture model and a state variable which monitors how well the observations fit the model.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 3,
      "context" : ", multi-layer perceptrons, radial basis function neural networks, [4], [6] but, according to Markou and Singh [25], also include methods based on support vector machines, e.",
      "startOffset" : 66,
      "endOffset" : 69
    }, {
      "referenceID" : 5,
      "context" : ", multi-layer perceptrons, radial basis function neural networks, [4], [6] but, according to Markou and Singh [25], also include methods based on support vector machines, e.",
      "startOffset" : 71,
      "endOffset" : 74
    }, {
      "referenceID" : 23,
      "context" : ", multi-layer perceptrons, radial basis function neural networks, [4], [6] but, according to Markou and Singh [25], also include methods based on support vector machines, e.",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 33,
      "context" : "OneClass SVM as described by Tax and Duin [35].",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 27,
      "context" : "Now, a more recent survey [29] suggest five different categories to group novelty detection approaches: i) probabilistic, ii) distance-based, iii) reconstruction-based, iv) domainbased, and v) information theoretical.",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 11,
      "context" : "Frequently used are mixtures of Gaussians ([12] [20], [38], for instance).",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 19,
      "context" : "Frequently used are mixtures of Gaussians ([12] [20], [38], for instance).",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 36,
      "context" : "Frequently used are mixtures of Gaussians ([12] [20], [38], for instance).",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 7,
      "context" : "[8], [18], [32]).",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 17,
      "context" : "[8], [18], [32]).",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 30,
      "context" : "[8], [18], [32]).",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 1,
      "context" : "[2] to implement incremental semi-supervised learning based on novelty detection.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 9,
      "context" : "[10], for example, where Support Vector Machines are used for detection and resulting novelty values calibrated in order to be interpreted as class-conditional probabilities.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 6,
      "context" : ", [7] or [17], [27], where the latter use the density of a k-neighborhood (i.",
      "startOffset" : 2,
      "endOffset" : 5
    }, {
      "referenceID" : 16,
      "context" : ", [7] or [17], [27], where the latter use the density of a k-neighborhood (i.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 25,
      "context" : ", [7] or [17], [27], where the latter use the density of a k-neighborhood (i.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 31,
      "context" : "[33], [36]).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 34,
      "context" : "[33], [36]).",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 27,
      "context" : "[29].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 8,
      "context" : "In some common applications such as medical condition monitoring [9], [31], [34] or machinery monitoring [30], this is not a real drawback, since anomalies might arose everywhere in the input space and are very specifically stuck to a concrete application (i.",
      "startOffset" : 65,
      "endOffset" : 68
    }, {
      "referenceID" : 29,
      "context" : "In some common applications such as medical condition monitoring [9], [31], [34] or machinery monitoring [30], this is not a real drawback, since anomalies might arose everywhere in the input space and are very specifically stuck to a concrete application (i.",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 32,
      "context" : "In some common applications such as medical condition monitoring [9], [31], [34] or machinery monitoring [30], this is not a real drawback, since anomalies might arose everywhere in the input space and are very specifically stuck to a concrete application (i.",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 28,
      "context" : "In some common applications such as medical condition monitoring [9], [31], [34] or machinery monitoring [30], this is not a real drawback, since anomalies might arose everywhere in the input space and are very specifically stuck to a concrete application (i.",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 15,
      "context" : "This leads to the result, that learning does not only happen in a isolated training phase, off-line at design-time, but it is also conducted at run-time [16].",
      "startOffset" : 153,
      "endOffset" : 157
    }, {
      "referenceID" : 4,
      "context" : "An extensive introduction to VI is given by [5].",
      "startOffset" : 44,
      "endOffset" : 47
    }, {
      "referenceID" : 13,
      "context" : "Relying on VI gives rise to two advantages: (1) prior knowledge about the data can be included, which is especially valuable in real-world applications, and (2) multiple GMM can be fused into one model as described in [14].",
      "startOffset" : 218,
      "endOffset" : 222
    }, {
      "referenceID" : 4,
      "context" : "[5]).",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 10,
      "context" : "[11]) uses a density estimation that is quite similar to a Parzen window estimator.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 26,
      "context" : "A fast and reliable method is Pearson’s chi-squared (χ) test [28].",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 14,
      "context" : "This section is split into three parts: the first part is based on our previous work [15] and discusses novelty detection and reaction in LDR with 2SND.",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 13,
      "context" : "To update the model, we exploit the properties of the hyperdistributions and use a fusion technique proposed in [14].",
      "startOffset" : 112,
      "endOffset" : 116
    }, {
      "referenceID" : 2,
      "context" : "[3], [19]).",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 18,
      "context" : "[3], [19]).",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 11,
      "context" : "As investigated by [12], it is also possible to exchange knowledge with other systems so that a novel process can be faster detected by another system.",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 21,
      "context" : "Due to their high computational complexity divergence measures such as the Kullback-Leibler divergence [22] or Hellinger distance [19] are intractable for the measurements, especially if the input space is of high dimensionality.",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 18,
      "context" : "Due to their high computational complexity divergence measures such as the Kullback-Leibler divergence [22] or Hellinger distance [19] are intractable for the measurements, especially if the input space is of high dimensionality.",
      "startOffset" : 130,
      "endOffset" : 134
    }, {
      "referenceID" : 9,
      "context" : "1000 Uniformly distributed 2D samples in the interval [0, 10] and trained Gaussian component.",
      "startOffset" : 54,
      "endOffset" : 61
    }, {
      "referenceID" : 0,
      "context" : "One method to estimate the affiliations is (Monte Carlo) random sampling, which requires only the evaluation of the unnormalized (without mixing coefficient πj) densities Pj(x) = N (x|μj ,Σj) for each component j and a continuous uniform pseudo random number generator unif(0, 1) for the unit interval [0, 1].",
      "startOffset" : 302,
      "endOffset" : 308
    }, {
      "referenceID" : 20,
      "context" : "based on the well-known KDD Cup 1999 network intrusion data set [21].",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 11,
      "context" : "As mentioned before, our new approach is compared to a novelty detection technique that is proposed in [12].",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 12,
      "context" : "[13]).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 0,
      "context" : ", [1], [26]).",
      "startOffset" : 2,
      "endOffset" : 5
    }, {
      "referenceID" : 24,
      "context" : ", [1], [26]).",
      "startOffset" : 7,
      "endOffset" : 11
    } ],
    "year" : 2016,
    "abstractText" : "In this article, we propose CANDIES (Combined Approach for Novelty Detection in Intelligent Embedded Systems), a new approach to novelty detection in technical systems. We assume that in a technical system several processes interact. If we observe these processes with sensors, we are able to model the observations (samples) with a probabilistic model, where, in an ideal case, the components of the parametric mixture density model we use, correspond to the processes in the real world. Eventually, at run-time, novel processes emerge in the technical systems such as in the case of an unpredictable failure. As a consequence, new kinds of samples are observed that require an adaptation of the model. CANDIES relies on mixtures of Gaussians which can be used for classification purposes, too. New processes may emerge in regions of the models’ input spaces where few samples were observed before (low-density regions) or in regions where already many samples were available (high-density regions). The latter case is more difficult, but most existing solutions focus on the former. Novelty detection in lowand high-density regions requires different detection strategies. With CANDIES, we introduce a new technique to detect novel processes in high-density regions by means of a fast online goodness-of-fit test. For detection in low-density regions we combine this approach with a 2SND (Two-Stage-Novelty-Detector) which we presented in preliminary work. The properties of CANDIES are evaluated using artificial data and benchmark data from the field of intrusion detection in computer networks, where the task is to detect new kinds of attacks.",
    "creator" : "LaTeX with hyperref package"
  }
}