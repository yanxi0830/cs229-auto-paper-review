{
  "name" : "1512.05947.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Kathrin Bujna" ],
    "emails" : [ "kathrin.bujna}@uni-paderborn.de" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n51 2.\n05 94\n7v 1\n[ cs\n.L G\n] 1\n8 D\nec 2\n01 5"
    }, {
      "heading" : "1 Introduction",
      "text" : "Clustering is a widely used technique in unsupervised machine learning. Simply speaking, its goal is to group similar objects. This problem occurs in a wide range of practical applications in many fields such as image analysis, information retrieval, and bioinformatics. We call a grouping of objects into a given number of clusters a hard clustering if each object is assigned to exactly one cluster. A popular example for a hard clustering problem is the well known K-means problem. In contrast, in a soft clustering each object belongs to each cluster with a certain degree of membership. There is a continuous generalization of the K-means problem that leads to a such a soft clustering problem, known as the fuzzy K-means problem.\n1.1 Fuzzy K-Means\n[1] was the first to present a fuzzy K-means objective function, which was later extended by [2]. Today, fuzzy K-means has found a wide range of practical applications, for example in image segmentation [3] and biological data analysis [4].\nFuzzy K-Means Problem Let X = {(x1, w1), . . . , (xN , wN )} be a set of data points xn ∈ RD weighted by wn ∈ R≥0. We want to group X into some predefined number of clusters K. These clusters are represented by mean vectors {µ1, . . . , µK} ⊂ RD. In a fuzzy clustering, each data point xn belongs to each cluster, represented by a µk, with a certain membership value rnk ∈ [0, 1]. The fuzzy K-means problem has an additional parameter, the so-called fuzzifier m ∈ N>1, which is chosen in advance and is not subject to optimization. In simple terms, the fuzzifier m determines how much clusters are allowed to overlap, i.e. how soft the clustering is.\nProblem 1 (Fuzzy K-means). Given X = {(xn, wn)}n∈[N ] ⊂ RD × R≥0, K ≥ 1 and m ≥ 2, find C = {µk}k∈[K] ⊂ RD and R = {rnk}n∈[N ],k∈[K] ⊂ [0, 1] minimizing\nφ (m) X (C,R) =\nN∑\nn=1\nK∑\nk=1\nrmnkwn ‖xn − µk‖22 ,\nsubject to: ∑K\nk=1 rnk = 1 for all n ∈ [N ]. We denote the costs of an optimal solution by φOPT(X,K,m).\nFor m = 1 this problem would coincide with the classical K-means problem, while for m → ∞ the memberships converge to a uniform distribution and the centers converge to the center of the data set X . Our problem definition is a generalization of the original definition presented in [2] in that we consider weighted data sets. By setting all weights to 1, we obtain the original definition.\nFuzzy K-Means (FM) Algorithm The most widely used heuristic for the fuzzy K-means problem is an alternating optimization algorithm known as fuzzy K-means (FM) algorithm. It is defined by the following first-order optimality conditions [5]: Fixing the means {µk}k∈[K], optimal memberships are given by\nrnk = ‖xn − µk‖\n− 2 m−1\n2 ∑K\nl=1 ‖xn − µl‖ − 2 m−1 2\n(1)\nif xn 6= µl for all l ∈ [K]. If xn coincides with some of the µl, then the membership of xn can be distributed arbitrarily among those µl with µl = xn. Fixing the memberships {rnk}n,k, the optimal means are given by\nµk =\n∑N n=1 r m nkwnxn\n∑N n=1 r m nkwn\n. (2)\nA major downside of the FM algorithm is that there are no guarantees on the quality of computed solutions.\nObservation 1 The FM algorithm converges to a (local) minimum or a saddle point that can be arbitrarily poor compared to an optimal solution. Such points can be reached by the FM algorithm even if it is initialized with points from the given point set\nA proof of this observation can be found in Section 7.8."
    }, {
      "heading" : "1.2 Related Work",
      "text" : "Although the fuzzy K-means problem appears in a wide range of practical applications, so far there has been no complexity classification. To the best of our knowledge, there are no hardness results for the fuzzy K-means problem. It is not even known whether it lies in NP. The same holds for other soft clustering problems, such as the maximum-likelihood estimation problem for Gaussian mixture models [6] or the soft-clustering problem [7].\nTwo problems that are closely related to the fuzzy K-means problem are the K-means and the K-median problem. The complexity of the K-means problem is well-studied. For fixed K and D, there is a polynomial time algorithm solving the problem optimally [8]. The K-means problem is NP-complete, even if K or D is fixed to 2 [9] [10]. Furthermore, assuming P 6=NP, there is no PTAS for the K-means problem for arbitrary K and D [11]. However, there are several approximation algorithms known, such as a polynomial-time constant-factor approximation algorithm [12] and a (1 + ǫ)-approximation algorithm with runtime polynomial in N and D [13]. The K-median problem is a variant of the K-means problem that uses the Euclidean instead of the squared Euclidean distance. Just as the K-means problem, the K-median problem is NP-hard, even for D = 2 [14]. However, it is known that the optimal solutions to the K-median problem have an inherently different structure than the solutions to the K-means problem. Even in the plane, optimal solutions of the 1-median problem are in general not expressable by radicals over Q [15].\nMany practical applications make use of the fuzzy K-means (FM) algorithm, which does not yield any approximation guarantees. However, [2] and [5] proved convergence of the FM algorithm to a local minimum or a saddle point of the objective function. Among others, [16] and [17] address the problem of determining and distinguishing whether the algorithm has reached a local minimum or a saddle point. Furthermore, it is known that the algorithm converges locally, i.e. started sufficiently close to a minimizer, the iteration sequence converges to that particular minimizer [18]. However, to the best of our knowledge, there are no theoretical results on approximation algorithms for the fuzzy K-means problem."
    }, {
      "heading" : "1.3 Overview",
      "text" : "The following technical part of the paper is divided in three parts. In Section 2 we give an overview on our results. In Section 2.1 we formally state our result that the fuzzy K-means problem is not solvable by radicals. In Section 2.2 we present our results on approximation algorithms. In Section 3 give an overview on our algorithmic techniques. In Section 4 we outline the analysis of our approximation algorithms. The interested reader can find fully detailed proofs in Section 6."
    }, {
      "heading" : "2 Our Contribution",
      "text" : "We initiate the complexity theoretical and algorithmic study of the fuzzy K-means problem."
    }, {
      "heading" : "2.1 Complexity",
      "text" : "We say that a fuzzy K-means solution is not solvable by radicals if neither means nor memberships can be expressed in terms of (+,−, ·, /, q√ ) over the domain of the input.\nTheorem 1. The fuzzy K-means problem for m = 2, K = 2, D ≥ 1, X ⊂ N and |X | ≥ 6 is in general not optimally solvable by radicals over Q. That is, neither the coordinates of the mean vectors nor the membership values can be expressed in terms of (+,−, ·, /, q√ ).\nThis result is an application of the technique used by Bajaj [15] who proved the same result for the K-median problem. Notably our result already holds for m = 2 and in one-dimensional space. For instance, we show that an optimal solution of the fuzzy 2-means problem (with m = 2) for the set X = {−3,−2,−1, 1, 2, 3} is not solvable by radicals over Q. In contrast, the K-means and K-median problem can both even be solved efficiently for D = 1. As for m = 2, it is noteworthy that in this case the first-order optimality conditions for means and memberships (cf. Equations (1) and (2)) lead to rationals in the input domain, respectively. A consequence of the inexpressibility by radicals is that no algorithm can solve the fuzzy K-means problem optimally if it only uses arithmetic operations and root extraction to obtain the zeroes of an algebraic equation. A more detailed discussion of the implications of unsolvability by radicals can be found in [15]."
    }, {
      "heading" : "2.2 Approximation Algorithms",
      "text" : "We present the first (1 + ǫ)-approximation algorithms for the fuzzy K-means problem.\nA PTAS For Fixed K and m We present the first PTAS for the fuzzy K-means problem, assuming a constant number of clusters K and a constant fuzzifier m. That is, for any given ǫ ∈ [0, 1], our algorithm computes an (1+ ǫ)-approximation to the fuzzy K-means problem in time polynomial in the number of points N and dimension D.\nTheorem 2. There is a deterministic algorithm that, given X = {(xn, wn)}n∈[N ] ⊂ RD × Q≥0, K ∈ N, m ∈ N, and ǫ ∈ (0, 1], computes a solution (C,R) such that\nφ (m) X (C,R) ≤ (1 + ǫ)φOPT(X,K,m) .\nThe algorithms’ runtime is bounded by D · NO ( K2 log(K)· 1 ǫ ( m log(mǫ )+log ( wmax wmin )))\n, where wmax = maxn∈[N ]wn and wmin = minn∈[N ]wn.\nThe main idea behind our result is to exploit the existence of a hard clusters that exhibit characteristics similar to those of a fuzzy clusters. By combining this result with a sampling technique which is well known from the K-means problem and applying exhaustive search, we obtain the algorithm.\nA Fast Deterministic (1 + ǫ)-Approximation Algorithm By using a completely different technique, we obtain a deterministic algorithm whose runtime almost linearly depends on N . On the negative side, we have to give up the linear dependence on the dimension D for this.\nTheorem 3. There is a deterministic algorithm that, given X = {xn}n∈[N ] ⊂ RD, K ∈ N, m ∈ N, and ǫ ∈ (0, 1], computes a solution (C,R) such that\nφ (m) X (C,R) ≤ (1 + ǫ)φOPT(X,K,m) .\nThe algorithms’ runtime is bounded by N (log(N)) K KO(K 2D log(1/ǫ)m).\nThe runtime of this algorithm is not comparable with the runtime of the algorithm from Theorem 2. However, comparing the terms N log(N)K and NO(K\n2 log(K)) with one another, we find that the runtime of the algorithm from Theorem 2 depends much stronger on K than the runtime of our algorithm from Theorem 3. For instance, assuming K2D = O (log(N)/log log(N)), the runtime of our algorithm from Theorem 3 is still polynomial in N , i.e. NO(log( 1 ǫ )), assuming that m is constant. In comparison, the runtime of our PTAS from Theorem 2 would then be exponential in N . Hence, Theorem 2 and Theorem 3 complement each other.\nThe idea behind our algorithm from Theorem 3 is the same as behind the coreset construction of [19] as it is used by [20]. That is, we construct a small set of good candidate means. After generating this candidate set, our algorithm simply tests all these candidates and chooses the best one.\nA Fast Randomized (1 + ǫ)-Approximation Algorithm Last, we show that there is a randomized algorithm with runtime linear in N and D. However, in return for this speedup, this algorithm has some requirement on the input sets. More precisely, our algorithm from Theorem 4 approximates the best ( α ∑N n=1 wn, K ) -balanced solution.\nDefinition 1 ((B,K)-balanced). Let X = {(xn, wn)}n∈[N ] ⊂ RD × R≥0. Given a solution with memberships R = {rnl}n∈[N ],l∈[L], we denote the weight of the lth fuzzy cluster by\nRl := N∑\nn=1\nrmnlwn . (3)\nWe say that the memberships R are (B,K)-balanced if\nL ≤ K and B ≤ min l∈[L] Rl .\nAn optimal (B,K)-balanced solution has smallest cost among all solutions with (B,K)-balanced membership values. An optimal (0,K)-balanced solution is an optimal solution to the fuzzy Kmeans problem.\nTheorem 4. There is a randomized algorithm that, given X = {(xn, wn)}n∈[N ] ⊂ RD × Q≥0, K ∈ N, m ∈ N, ǫ ∈ (0, 1], and α ∈ (0, 1], computes (C,R) such that with constant probability\nφ (m) X (C,R) ≤ (1 + ǫ)φ (m) X\n(\nCoptB,K , R opt B,K\n)\n,\nwhere (\nCoptB,K , R opt B,K\n)\nis an optimal (B,K)-balanced solution with\nB = α N∑\nn=1\nwn .\nThe algorithms’ runtime is bounded by N ·D · 2O(K2· 1ǫ log( 1αǫ )).\nWe can boost the probability of success to an arbitrary 1 − δ by simply repeating the algorithm log(1/δ) times. Observe that the running time basically coincides with the running time of an algorithm that applies the superset sampling technique to the K-means problem [21].\nThe restriction B = α∑Nn=1 wn can also be seen as a restriction on the deviation between the single cluster weights Rk and the average cluster weight Ravg = 1/K · ∑K k=1 Rk. More precisely, using the Cauchy-Schwarz inequality,\n1\nKm−1 ·\nN∑\nn=1\nwn ≤ K ·Ravg = N∑\nn=1\n( K∑\nk=1\nrmnk\n)\nwn ≤ N∑\nn=1\nwn .\nHence, if B ≥ α∑Nn=1 wn, then for all k ∈ [K] we have Rk/Ravg ∈ [αK,Km]. For example, by choosing α = 2−O(K) we allow a deviation of a factor 2−O(K) and obtain a runtime that is still linear in N and becomes exponential in K3.\nThe main idea behind this algorithm is the same as behind the PTAS described in Theorem 2. Knowing that for given fuzzy clusters there exist a hard clusters with similar characteristics, we apply a sampling technique known from the K-means problem. However, instead of combining this technique with exhaustive search, we directly apply the sampling technique to obtain our randomized algorithm from Theorem 4. In other words, the PTAS from Theorem 2 can be seen as a de-randomized version of this algorithm."
    }, {
      "heading" : "3 Our Main Techniques",
      "text" : "In this section, we describe the techniques that we use to prove Theorems 2, 4, and 3. To this end, we use the following notation.\nDefinition 2 (Induced Solution). Let X ⊂ RD × R. Membership values R induce the solution (C̃, R) where C̃ contains the corresponding optimal mean vectors (cf. Equation (2)). Mean vectors C induce the solution (C, R̃) where R̃ contains the corresponding optimal membership values (cf. Equation (1)). We denote the costs of the induced solutions by φ (m) X (R) and φ (m) X (C), respectively.\nObserve that for all means C = {µk}k∈[K] and memberships R = {rnk}n∈[N ],k∈[K] we have\nφ (m) X (C) ≤ φ (m) X (C,R) and φ (m) X (R) ≤ φ (m) X (C,R) . (4)\n3.1 Structure of the Fuzzy K-Means Problem\nThere are two aspects of the structure of the fuzzy K-means problem that we exploit extensively. First, there is a coarse but still useful relation between the K-means and the fuzzy K-means cost function. Recall that optimal solutions of the fuzzy K-means problem seem to have a substantially different structure than optimal solutions of the K-means problem (cf. Section 2.1). Nonetheless, the fuzzy K-means and K-means cost of solutions induced the same set of mean vectors differ by at most a factor of Km−1. We use this result when transfering the ideas behind the coreset construction of [19] in order to obtain a candidate set of means.\nDefinition 3 (K-means). For X = {(xn, wn)}n∈[N ] ⊂ RD × R≥0 and C = {µk}k∈[K] ⊂ RD we define kmX (C) := ∑N n=1 wn mink∈[K] ‖xn − µk‖ 2 2.\nLemma 1. Let X ⊂ RD × R≥0, m ∈ N, and C ⊂ RD with |C| = K. Then,\n1\nKm−1 kmX(C) ≤ φ(m)X (C) ≤ kmX(C) .\nProof. Obviously, φ (m) X (C) ≤ kmX(C). Let X = {(xn, wn)}n∈[N ]. Let {rnk}n,k be the optimal memberships induced by C = {µk}k∈[K]. Using the Cauchy-Schwarz inequality, 1Km−1 · kmX(C) ≤ ∑N\nn=1\n( ∑K\nk=1 r m nk\n)\nwn\n( mink∈[K] ‖xn − µk‖22 ) ≤ φ(m)X (C).\nSecond, we can ignore fuzzy clusters with too small a weight. For each optimal fuzzy K-means solution, there exists a fuzzy L-means solution with L ≤ K clusters such that each cluster has a certain minimum weight B while the cost are only at most a factor (1 + ǫ) worse than the cost of the optimal solution. Recall that we denote such clusterings as (B,K)-balanced (cf. Definition 1). More precisely, B only depends on ǫ, m, K, and the smallest weight of a point in X . This result becomes important when we apply sampling techniques. When sampling points from X , we can only expect to sample points from a certain cluster if this cluster is large enough.\nLemma 2. Let X = {(xn, wn)}n∈[N ] ⊂ RD × R, m ∈ N, K ∈ N, and ǫ ∈ [0, 1]. There exist (B,K)-balanced membership values R such that\nφ (m) X (R) ≤ (1 + ǫ)φOPT(X,K,m)\nwhere\nB = ( ǫ\n4mK2\n)m\n· min n∈[N ] wn .\nProof. Consider an arbitrary but fixed solution {µk}k∈[K]. Let {rnk}n,k be the optimal membership values induced by {µk}k∈[K]. Assume that for some l ∈ [K] we have Rl = ∑N n=1 r m nlwn ≤ ( ǫ 4mK2\n)m · minn∈[N ]wn. Thus, we have rnl ≤ ǫ4mK2 for all n ∈ [N ].\nConsider an arbitrary n ∈ [N ]. Since ∑Kk=1 rnk = 1 and rnl ≤ 1K , there exists some k(n) ∈ [K] with k(n) 6= l such that rnk(n) ≥ 1K . Since rnl ≤ ǫ4mK2 , we have rnl ≤ ǫ4mK rnk(n). Hence,\n(rnk(n) + rnl) m ≤\n(\n1 + ǫ\n4mK\n)m rmnk(n) ≤ ( 1 + ǫ\n2K\n)\n· rmnk(n) . (5)\nDue to the optimality of the rnl and rnk(n) (cf. Equation (1)) and since rnl ≤ rnk(n), we have ∥ ∥xn − µk(n) ∥ ∥ 2 2 ≤ ‖xn − µl‖22 . (6)\nHence,\nφ (m) X ({µk}k∈[K] \\ {µl})\n≤ N∑\nn=1\n∑\nk∈[K] k 6=l,k(n)\nrmnkwn ‖xn − µk‖22 + N∑\nn=1\n(rnk(n) + rnl) mwn ∥ ∥xn − µk(n) ∥ ∥ 2\n2\n≤ N∑\nn=1\n∑\nk∈[K] k 6=l,k(n)\nrmnkwn ‖xn − µk‖22 + N∑\nn=1\n(\n1 + ǫ\n2K\n)\nrmnk(n)wn ∥ ∥xn − µk(n) ∥ ∥ 2 2 (by Eq. (5))\n≤ ( 1 + ǫ\n2K\n)\nφ (m) X ({µk}k) . (by Eq. (6))\nNow consider an optimal solution {µk}k∈[K]. Let B ⊂ {µk}k∈[K] be the set containing the means µl where Rl ≤ ( ǫ\n4mK2 )m ·minn∈[N ]wn. Note that B ≤ K − 1. Let L = K − |B|. Then, by the above there exists a set of membership values r̃L = {r̃nl}n∈[N ],l∈[L] such that\nφ (m) X ({µk}k∈[K] \\B, r̃L) ≤\n(\n1 + ǫ\n2K\n)K\nφOPT(X,K,m) ≤ (1 + ǫ)φOPT(X,K,m)\nand ∑N\nn=1(r̃nl) mwn ≥\n( ǫ\n4mK2 )m · minn∈[N ]wn for all l ∈ [L]. Finally, observe that φ(m)X (r̃L) ≤ φ (m) X ({µk}k∈[K] \\B, r̃L)."
    }, {
      "heading" : "3.2 Sampling Techniques",
      "text" : "Our algorithms from Theorems 2 and 4 both use sampling techniques. First, we show that there exist hard clusters suitably imitating soft clusters. Second, we show how to construct a candidate set that contains good approximations of the means of these (unknown) hard clusters.\nRelating Fuzzy to Hard Clusters A fundamental result is that, given fuzzy clusters with not too small a weight, there always exist hard clusters that exhibit characteristics similar to those of the fuzzy clusters.\nDefinition 4 (Cost of a Fuzzy Cluster, Hard Clusters). Let X = {(xn, wn)}n∈[N ] ⊂ RD×R and K ∈ N. Given memberships {rnk}n∈[N ],k∈[K] and induced means {µk}k∈[K], we let\nφ (m) X,k({rnk}n) :=\nN∑\nn=1\nrmnkwn ‖xn − µk‖22 .\nfor all k ∈ [K]. For all hard clusters C ⊂ X, C 6= ∅, we define\nw(C) := ∑\n(wn,xn)∈C wn, µ(C) :=\n∑\n(wn,xn)∈C wn · xn w(C)\nand\nkm(C) := ∑\n(wn,xn)∈C wn ‖xn − µ(C)‖22 .\nTheorem 5 (Existence of Similar Hard Clusters). Let X = {(xn, wn)}n∈[n] ⊂ RD×R≥0 and ǫ ∈ (0, 1]. Let {rnk}n,k be memberships values, and let {µk}k be the corresponding optimal mean vectors.\nIf mink∈[K] Rk ≥ 16Kwmax/ǫ, where wmax = maxn∈[N ]wn, then there exist pairwise disjoint sets C1, . . . , CK ⊆ X such that for all k ∈ [K]\nw(Ck) ≥ 1\n2 Rk , (7)\n‖µ(Ck)− µk‖22 ≤ ǫ\n2Rk · φ(m)X,k ({rnk}n) and (8)\nkm(Ck) ≤ 4K · φ(m)X,k ({rnk}n) . (9)\nProof (Idea of the Proof in Section 7). To prove this theorem, we apply the probabilistic method. Consider a random process that samples a hard assignment for each (xn, wn) ∈ X independently at random by assigning (xn, wn) ∈ X to the kth cluster with probability rmnk. This assignment can be considered a binary random variable znk ∈ {0, 1} with expected value E[znk] = rmnk. We compare the resulting hard clusters Ck = {(xn, wn) ∈ X | znk = 1} with the fuzzy clusters CFk = {(xn, wn · rmnk) ∈ X | xn ∈ X}. With positive probability, the weights, means, and costs of the constructed hard clusters satisfy the given properties.\nUnfortunately, to the best of our knowledge, the hard clusters Ck do not exhibit any structure, e.g. are not necessarily convex and do not necessarily cover X . In the next section, we describe how the superset sampling technique [8] [13] can be used approximate the means µ(Ck) well. It is not clear, how other techniques which do not solely rely on sampling can be applied. For instance, we presume that the sample and prune technique from [21] and the K-means++ algorithm [22] require that the convex hulls of clusters do not overlap. The hard clusters Ck whose existence we can prove do not necessarily have this property.\nSuperset Sampling From Theorem 5, we know that there exist hard clusters similar to given fuzzy clusters. Hence, means that are sufficiently close to the means of the hard clusters induce a solution that is also close to the solution given by the fuzzy clusters. The superset sampling technique introduced by [8] [13] can be used to find such means. More precisely, we can construct a candidate set containing good approximations of the means of unknown hard clusters if these hard clusters do not have too small a weight compared to the weight of the give point set.\nTheorem 6. There is a randomized algorithm that, given X = {(xn, wn)}n∈[N ] ⊂ RD × Q≥0, K ∈ N, ǫ ∈ (0, 1], and α ∈ (0, 1], constructs a set T ⊂ (RD)K in time\nO ( D · ( N + 2 K ǫ ·log( 1αǫ)·log(log(K)) ))\nsuch that for an arbitrary but fixed set {Ck}k∈[K] of (unknown) sets Ck ⊆ X, with constant probability, there exists a (µ̃k)k∈[K] ∈ T such that for all k ∈ [K] where\nw(Ck) ≥ α · N∑\nn=1\nwn\nwe have\n‖µ̃k − µ(Ck)‖22 ≤ ǫ\nw(Ck) km(Ck) .\nWe apply this result in two different ways: Firstly, we apply the superset sampling technique directly to obtain a randomized approximation algorithm. That is, we generate the candidate set T and determine the candidate with the smallest fuzzy K-means costs. Secondly, we use exhaustive search to obtain a deterministic approximation algorithm. That is, we generate all candidates that the algorithm from Theorem 6 might possibly generate and choose the best of these candidates. Note that the latter approach does not require that the weights of the fuzzy clusters make up a certain fraction of the weight of the point set."
    }, {
      "heading" : "4 Proof Sketches",
      "text" : ""
    }, {
      "heading" : "4.1 Relating Fuzzy to Hard Clusters (Theorems 2 and 4)",
      "text" : "The following proposition is the basis for the proofs of Theorems 2 and 4.\nProposition 1. There is a randomized algorithm that, given X = {(xn, wn)}n∈[N ] ⊂ RD × Q≥0, K ∈ N , m ∈ N, ǫ ∈ (0, 1], and α ∈ (0, 1], computes mean vectors C ⊂ RD, |C| = K, such that with probability at least 1/2 we have\nφ (m) X (C) ≤ (1 + ǫ)φ (m) X\n(\nRoptB,K\n)\n,\nwhere the memberships RoptB,K induce an optimal (B,K)-balanced solution with\nB = max { α · N∑\nn=1\nwn, 16Kwmax\nǫ\n}\nand wmax = max n∈[N ] wn .\nThe algorithms’ runtime is bounded by ND · 2O(K2/ǫ·log(1/(αǫ))). There is a deterministic version of this algorithm that, given X = {(xn, wn)}n∈[N ] ⊂ RD×Q≥0, K ∈ N , m ∈ N, and ǫ ∈ (0, 1], computes mean vectors which induce an (1+ ǫ)-approximation to an optimal (B, K)-balanced solution where B = 16Kwmaxǫ . The runtime of this algorithm is bounded by D ·NO(K2/ǫ).\nProof. First, we prove that there is a randomized algorithm as described in the theorem. Fix an optimal (B,K)-balanced solution with memberships {rnl}n∈[N ],l∈[L]. Let {µl}l∈[L] be the optimal mean vectors induced by the fixed {rnl}n∈[N ],l∈[L].\nBy Theorem 5, we know that there exist hard clusters {Cl}l∈[L] ⊆ X similar to the fuzzy clusters given by the {rnl}n∈[N ],l∈[L]. Due to Equation (7) and since Rl = ∑N n=1 r m nlwn ≥ α · ∑N n=1 wn, we have w(Cl) ≥ Rl/2 ≥ α/2 · ∑N n=1 wn.\nNow, apply Theorem 6. Consider the set T that is generated as in Theorem 6 with ǫ set to ǫ/(16K) and α set to α/2. With probability at least 1/2, the set T contains a candidate (µ̃k)k∈[K] such that for all l ∈ [L] we have\n‖µ̃l − µ(Cl)‖22 ≤ ǫ\n16Kw(Cl) km(Cl) . (10)\nWe can upper bound the cost of the means {µ̃k}k∈[K] by\nφ (m) X ({µ̃k}k∈[K]) ≤ φ (m) X ({µ̃l}l∈[L]) (since ({µ̃l}l∈[L] ⊆ {µ̃k}k∈[K])\n≤ φ(m)X ({µ̃l}l∈[L], {rnl}n∈[N ],l∈[L]) ≤ φ(m)X ({rnl}n∈[N ],l∈[L]) + L∑\nl=1\nRl ‖µl − µ̃l‖22\n≤ φ(m)X ({rnl}n∈[N ],l∈[L]) + 2 L∑\nl=1\nRl\n(\n‖µl − µ(Cl)‖22 + ‖µ(Cl)− µ̃l‖ 2 2\n)\n,\nwhere the second to last inequality is well-known (a proof can be found in Section 6.1 (Lem. 3)) and where the last inequality is due to the fact that ∀a, b ∈ R : (a+ b)2 ≤ 2a2 + 2b2.\nDue to Equation (8), we obtain\nL∑\nl=1\nRl ‖µl − µ(Cl)‖22 ≤ L∑\nl=1\nRl · ǫ\n2Rl · φ(m)X,l ({rnk}n) =\nǫ 2 · φ(m)X ( {rnl}n∈[N ],l∈[L] ) .\nFurthermore, we have\nL∑\nl=1\nRl ‖µ(Cl)− µ̃l‖22 ≤ L∑\nl=1\nRl · ǫ\n16Kw(Cl) · km(Cl) (by Equation (10))\n≤ ǫ 2 · φ(m)X ({rnl}n∈[N ],l∈[L]) . (by Equations (7), (9))\nTherefore, with probability 1/2, there exists a candidate tuple (µ̃k)k∈[K] ∈ T that induces the desired approximate solution of the given optimal (B,K)-balanced solution {rnl}n∈[N ],l∈[L]. Hence, the candidate with the smallest fuzzy K-means cost among all the possible candidates satisfies the approximation bound as well. This concludes the proof of the existence of the randomized algorithm.\nTo obtain a deterministic algorithm, we use exhaustive search. Using the same argument as above, but setting α to minl∈[L] Rl/ ∑N n=1 wn, one can obtain the desired approximation with positive probability. The set T contains tuples of means of multi-sets, which are subsets of X with size 32K/ǫ (cf. Section 7.4). Hence, by testing all combinations of means of all possible multisets, which are subsets of X with size 32K/ǫ, we obtain a candidate set containing the desired approximation. Note that there are at most N32K/ǫ such subsets. Again, the candidate with the smallest cost yields the desired approximation as well.\nProof (Proof of Theorem 2). Let {rnl}n∈[N ],l∈[L] be the memberships from Lemma 2 with ǫ replaced by ǫ/4. Then, we have φ\n(m) X ({rnl}n∈[N ],l∈[L]) ≤ (1 + ǫ/4)φOPT(X,K,m).\nLet X ′ be the point set containing c = ⌈ 2 · 16m+1mmK2m+1wmax/(wminǫm+1) ⌉ copies of each\npoint. From Lemma 2 we know that for all l ∈ [L] we have ∑Nn=1 rmnlwn ≥ ( ǫ/(16mK2) )m · wmin. Hence, for all l ∈ [L] we have ∑\nxn∈X′ r m nlwn = c\n∑\nxn∈X r m nlwn ≥ c\n( ǫ/(16mK2) )m wmin ≥\n32Kwmax/ǫ. Thus, we can apply Proposition 1 (with X replaced by X ′, ǫ replaced by ǫ/2, and {rnl}n∈[N ],l∈[L] replaced by a set containing c copies of the memberships in {rnl}n∈[N ],l∈[L]) and obtain means {µ̃k}k∈[K]. Observe that for all C ⊂ RD we have φ(m)X′ (C) = c · φ (m) X (C). Thus, φ (m) X ({µ̃k}k∈[K]) ≤ (1 + ǫ/2)φ (m) X ({rnl}n∈[N ],l∈[L]) ≤ (1 + ǫ)φOPT(X,K,m).\nSince we apply the algorithm from Lemma 2 to a set containing c copies of the points in X ,\nits runtime is bounded by D · (c · N)O ( K2 ǫ ) . Finally, note that (c · N)O ( K2 ǫ ) ⊆ NO ( log(c)·K2 ǫ )\n⊆ N O (( m log(K)+m log(m)+m log( 1ǫ )+log ( wmax wmin )) ·K2 ǫ ) ⊆ NO ( K2 log(K)· 1 ǫ ( m log(mǫ )+log ( wmax wmin ))) .\nProof (Proof of Theorem 4). Construct a point set X ′ that contains c = ⌈16K/(αǫ)⌉ copies of each point in X . Fix an arbitrary k ∈ [K]. Since B ≥ α∑Nn=1 wn, we have ∑ xn∈X r m nkwn ≥ α ∑\nxn∈X wn. By definition of X ′, ∑\nxn∈X′ r m nkwn =\n∑ xn∈X c · wnrmnk ≥ c · α ∑ xn∈X wn = α ∑\nxn∈X′ wn. Also note that c · α∑xn∈X wn ≥ c · α · wmax ≥ 16Kwmax/ǫ. Hence, we can apply Proposition 1 to X ′\nto find {µ̃k}k approximating the {µk}k induced by (copies of) the memberships in {rnk}n,k, with constant probability. Observe that for all C ⊂ RD, φ(m)X′ (C) = c · φ (m) X (C). Hence, φ (m) X ({µ̃k}k) ≤ (1 + ǫ)φOPT(X,K,m). Since we apply the algorithm from Lemma 2 to a set containing c copies of the points in X , it requires runtime (c·N)·D·2O(K2/ǫ·log(1/(αǫ))). Finally, note that c = 2O(log(c)) ⊆ 2O(log(K)+log(1/(αǫ))) ⊆ 2O(K 2/ǫ·log(1/(αǫ))), assuming that αǫ ≤ 1/2."
    }, {
      "heading" : "4.2 Candidate Set Search for Mean Vectors (Proof of Theorem 3)",
      "text" : "Using ideas behind the coreset construction of [19], we can construct a candidate set of mean vectors. The algorithm that creates and tests all these candidates and finally chooses the best candidates satisfies the properties from Theorem 3.\nTheorem 7 (Candidate Set). Let X ⊂ RD, K ∈ N, and ǫ ∈ (0, 1]. There exists a set G ⊂ RD with size\n|G| = O ( KmD+1ǫ−Dm log ( mK\nǫ\n)\nlog(N)\n)\nthat contains {µk}k∈[K] ⊂ G with\nφ (m) X ({µk}k∈[K]) ≤ (1 + ǫ)φOPT(X,K,m) .\nThe set G can be computed in time O(N(log(N))Kǫ−2K2D +NKD |G|). Proof (Sketch of Proof in Section 7.5). The idea behind the coreset construction of [19] can be used to construct a candidate set of mean vectors. Part of the construction is a constant factor approximation of the K-means problem. To this end, we use the deterministic algorithm presented in [23], which requires time O ( N(log(N))Kǫ−2K 2D ) ."
    }, {
      "heading" : "5 Future Work & Open Problems",
      "text" : "A goal of further research is to examine whether Theorem 5 can be transferred to other soft clustering problems. In particular, we hope to obtain a constant factor approximation algorithm for the maximum likelihood estimation problem for mixtures of Gaussian distributions, e.g. by using the results from [24].\nIt is an open question whether we are able to classify hardness of approximation of fuzzy Kmeans. We conjecture that, just as the classicalK-means problem, if P 6=NP, then there is no PTAS for the fuzzy K-means problem for arbitrary K and D."
    }, {
      "heading" : "6 Full Proofs",
      "text" : ""
    }, {
      "heading" : "6.1 Preliminaries",
      "text" : "In this section we introduce some notation and lemmata that are used throughout the rest of this appendix.\nHard Clusters and K-Means Costs The following definition restates some of the notation already presented in Definition 4.\nDefinition 5 (Hard Clusters). For weighted point sets C ⊂ RD × R≥0 , we let\nw(C) := ∑\n(xn,wn)∈C wn ,\nµ(C) :=\n∑\n(xn,wn)∈C wnxn\nw(C) and\nkm(C) := ∑\n(xn,wn)∈C wn ‖xn − µ(C)‖22 .\nThe following lemma is well known (e.g. used in the proof of Theorem 2 in [8]).\nLemma 3. Let C ⊂ RD × R be a weighted point set and µ ∈ RD. Then, ∑\n(xn,wn)∈C wn ‖xn − µ‖22 = km(C) + w(C) ‖µ− µ(C)‖ 2 2 .\nProof.\n∑\n(xn,wn)∈C wn ‖xn − µ‖22 = ∑\n(xn,wn)∈C wn ‖xn − µ(C) + µ(C)− µ‖22\n= ∑\n(xn,wn)∈C wn 〈xn − µ(C) + µ(C) − µ, xn − µ(C) + µ(C) − µ〉 (scalar product)\n= ∑\n(xn,wn)∈C wn (〈xn − µ(C), xn − µ(C)〉 + 2 〈xn − µ(C), µ(C) − µ〉+ 〈µ(C)− µ, µ(C)− µ〉)\n(bilinearity and symmetry)\n= ∑\n(xn,wn)∈C wn\n(\n‖xn − µ(C)‖22 + 2 〈xn − µ(C), µ(C) − µ〉+ ‖µ(C)− µ‖ 2 2\n)\n= ∑\n(xn,wn)∈C wn ‖xn − µ(C)‖22 +\n∑\n(xn,wn)∈C wn 〈xn − µ(C), µ(C) − µ〉+ w(C) ‖µ(C)− µ‖22\nwhere due to the bilinarity of the scalar product and by the definition of µ(C)\n∑\n(xn,wn)∈C wn 〈xn − µ(C), µ(C) − µ〉 =\n〈 ∑\n(xn,wn)∈C wn(xn − µ(C)), µ(C) − µ\n〉\n=\n〈\n ∑\n(xn,wn)∈C wnxn\n − w(C) · µ(C), µ(C) − µ 〉\n= 〈0, µ(C)− µ〉 = 0\nwhich yields the claim.\nFurthermore, the cluster costs km(C) can be expressed via pairwise distances.\nLemma 4. Let C ⊂ RD × R be a weighted point set. Then,\nkm(C) = 1\n2 ∑\n(xn,wn)∈C wn\n∑\n(xn,wn)∈C\n∑\n(xl,wl)∈C wnwl ‖xn − xl‖22 .\nDefinition 6 (K-Means Costs (unweighted)). For unweighted point sets X ⊂ RD, x ∈ RD, and finite sets M ⊂ RD we let\nd(x,M) := min m∈M\n‖x−m‖2 ,\nkmX(M) := ∑\nx∈X d(x,M)2 =\n∑ x∈X min m∈M ‖x−m‖22 and\nkmX,K := min M⊂RD\n|M|=K\nkmX(M) .\nDefinition 7 (Induced Partition (unweighted)). We say {Ck}Kk=1 is a partition of X induced by C ⊂ RD if Ck ⊆ {xn ∈ X | ∀l 6= k : ‖xn − µk‖2 ≤ ‖xn − µl‖2} and X = ∪̇ K k=1Ck.\nSome Useful Technical Lemmas Besides, we make extensive use of the following simple technical lemmata.\nLemma 5. For all a, b, c ∈ RD we have\n‖c− a‖22 − ‖c− b‖ 2 2 ≤ ‖a− b‖ 2 2 + 2 ‖a− b‖2 ‖c− b‖2\nProof.\n‖a− c‖22 − ‖b− c‖ 2 2 ≤ ∣ ∣ ∣‖a− c‖22 − ‖b− c‖ 2 2 ∣ ∣ ∣\n= ∣ ∣ ∣‖a− b+ b− c‖22 − ‖b− c‖ 2 2 ∣ ∣ ∣ = ∣ ∣ ∣‖a− b‖22 + 2 〈a− b, b− c〉 ∣ ∣ ∣\n≤ ‖a− b‖22 + 2 |〈a− b, b− c〉| ≤ ‖a− b‖22 + 2 ‖a− b‖2 ‖b− c‖2 (Cauchy-Schwarz)\nLemma 6. Let ǫ ∈ [0, 1], c > 1, and m ∈ N. Then, for all i ∈ [m] it holds (\n1 + ǫ\n2mc\n)i\n≤ 1 + i · ǫ mc .\nLemma 7. For all a, b ∈ R we have\n1. 2ab ≤ a2 + b2, 2. (a+ b)2 ≤ 2(a2 + b2) and 3. (a+ b + c)2 ≤ 3(a2 + b2 + c2)."
    }, {
      "heading" : "7 Stochastic Fuzzy Clustering (Proof of Theorem 5)",
      "text" : "In this section, we first describe a random process that, given some fuzzy K-means clusters, creates K hard clusters. We define different quantities that describe the different clusters and derive probabilistic bounds on the similarity between them with respect to these quantities."
    }, {
      "heading" : "7.1 Setting and Random Process",
      "text" : "In the following we consider arbitrary but fixed memberships {rnk}n,k. These membership values induce fuzzy clusters. We say that the kth fuzzy cluster has weight Rk, mean µk, and cost φ (m) X,k({rnk}n), where the µk are the optimal means with respect to the given memberships. Recall that by Equation (2), Equation (3), and Definition 4 we have\nRk =\nN∑\nn=1\nrmnkwn ,\nµk =\n∑N n=1 r m nkwnxn\n∑N n=1 r m nkwn\n, and\nφ (m) X,k({rnk}n) =\nN∑\nn=1\nrmnkwn ‖xn − µk‖22 . (11)\nWe consider the following random process that aims to imitate the fuzzy clustering. Given the fixed memberships {rnk}n,k, the process samples an assignment for each (xn, wn) ∈ X independently at random. Formally, we describe these assignments by random variables (znk) ∈ {0, 1}K with\n∑K k=1 znk ∈ {0, 1}. They are sampled according to the following distribution:\n1. With probability rmnk, the process assigns xn to the k th cluster. That is, Pr(znk = 1) = r m nk. 2. The process does not assign xn to any cluster at all with probability 1 − ∑K k=1 r m nk. That is,\nPr(∀k ∈ [K] : znk = 0) = 1− ∑K k=1 r m nk.\nThis process constructs hard clusters {Ck}k∈[K] with Ck = {(xn, wn) ∈ X |znk = 1} ⊆ X that do not necessarily cover X . Using Definition 5, we can conclude that\nw(Ck) =\nN∑\nn=1\nznkwn ,\nµ(Ck) = ∑N n=1 znkwnxn w(Ck) , and\nkm(Ck) =\nN∑\nn=1\nznkwn ‖xn − µ(Ck)‖22 .\nNote that these quantities are random variables defined by the random process. All of them depend on the binary random variables znk."
    }, {
      "heading" : "7.2 Proximity",
      "text" : "By definition, the binary random variables znk have the property\nE [znk] = Pr (znk = 1) = r m nk .\nIn the following, we use Chebyshev’s and Markov’s inequality to give concentration bounds on the difference between weights, means, and costs of the fuzzy clusters and the hard clusters constructed by the random process, respectively. One might suspect that Chernoff bounds yield better results. Unfortunately, these bounds do not directly measure the differences between the means and costs in terms of the fuzzy K-means costs, respectively. Hence, it is not clear how Chernoff bounds can be applied here.\nLet λ, ν ∈ R>1 be constants. Lemma 8 (Weights). For all k ∈ [K] we have\nPr (|w(Ck)−Rk| ≥ ληk) ≤ 1\nλ2 ,\nwhere\nηk =\n√ √ √ √ N∑\nn=1\nrmnk(1− rmnk)w2n . (12)\nProof. Since w(Ck) = ∑N n=1 znkwn, we have E [w(Ck)] = ∑N n=1 E [znk]wn = ∑N n=1 r m nkwn = Rk. Furthermore, since {znk}n is a set of independent random variables, we have\nVar (w(Ck)) =\nN∑\nn=1\nVar (znk)w 2 n =\nN∑\nn=1\nrmnk (1− rmnk)w2n = η2k .\nThe claim is a direct consequence of Chebyshev’s inequality.\nNote that numerator and denominator of\n‖µ(Ck)− µk‖22 =\n∥ ∥ ∥ ∑N n=1 znkwn(xn − µk) ∥ ∥ ∥ 2\n2\nw(Ck)2 (13)\nare both random variables depending on the same random variables znk. Lemma 8 already gives a bound on the denominator w(Ck). Next, we bound the numerator.\nLemma 9 (Means).\nPr (∥ ∥ ∥ ∑N n=1znkwn(xn − µk) ∥ ∥ ∥ 2\n2 ≥ ντk\n)\n≤ 1 ν ,\nwhere\nτk = N∑\nn=1\nrmnk (1− rmnk)w2n ‖xn − µk‖22 . (14)\nProof. Let Mk = ∥ ∥ ∥ ∑N n=1 znkwn(xn − µk) ∥ ∥ ∥ 2\n2 . Observe that\nMk =\n〈 N∑\nn=1\nznkwn(xn − µk), N∑\nn=1\nznkwn(xn − µk) 〉\n=\nN∑\nn=1\nN∑\no=1\nznkzokwnwo 〈xn − µk, xo − µk〉 .\nBecause the expectation is linear, we obtain\nE[Mk] =\nN∑\nn=1\nN∑\no=1\nE [znkzok]wnwo 〈(xn − µk), (xo − µk)〉\n=\nN∑\nn=1\nE [ z2nk ] w2n ‖xn − µk‖22 +\n∑ o 6=n E [znkzok]wnwo 〈(xn − µk), (xo − µk)〉 .\nRecall that the znk are independent binary random variables, hence for all n, o ∈ [N ], n 6= o,\nE [ z2nk ] =Pr (znk = 1) = r m nk\nE [znkzok] =Pr (znkzok = 1) = r m nkr m ok .\nThus,\nE[Mk] =\nN∑\nn=1\nrmnkw 2 n ‖xn − µk‖22 +\n∑ o 6=n rmnkr m okwnwo 〈(xn − µk), (xo − µk)〉\n= N∑\nn=1\nrmnkw 2 n ‖xn − µk‖22 − r2mnk ‖xn − µk‖ 2 2 +\nN∑\no=1\nrmnkr m okwnwo 〈(xn − µk), (xo − µk)〉\n=\nN∑\nn=1\nrmnk(1 − rmnk)w2n ‖xn − µk‖22 + rmnkwn 〈 (xn − µk), N∑\no=1 rmokwo(xo − µk) ︸ ︷︷ ︸\n=0\n〉\n= N∑\nn=1\nrmnk(1 − rmnk)w2n ‖xn − µk‖22 = τk .\nApplying Markov’s inequality yields the claim.\nFinally, we can bound the cluster-wise cost as follows.\nLemma 10 (Cost). For all k ∈ [K] we have\nPr ( km(Ck) ≥ ν · φ(m)X,k({rnk}n) ) ≤ 1 ν\nProof. Observe that, by definition of µ(Ck) and Lemma 3,\nkm(Ck) =\nN∑\nn=1\nznkwn ‖xn − µ(Ck)‖22 ≤ N∑\nn=1\nznkwn ‖xn − µk‖22\nThe expectation of the upper bound evaluates to\nE\n[ N∑\nn=1\nznkwn ‖xn − µk‖22\n]\n=\nN∑\nn=1\nrmnkwn ‖xn − µk‖22 = φ (m) X,k({rnk}n) .\nApplying Markovs’s inequality yields the claim.\nNow, we can formally prove the existence of hard clusters imitating given fuzzy clusters.\nCorollary 1. Let X = {(xn, wn)}n∈[N ] ⊂ RD × R≥0 be a weighted point set. Let {rnk}n,k be the memberships of a fuzzy K-means solution for X with corresponding optimal means {µk}k. Then, there exist pairwise disjoint subsets {Ck}k∈[K] of X such that for all k ∈ [K]\n|w(Ck)−Rk| ≤ √ 4K · ηk (15)\n‖µ(Ck)− µk‖22 ≤ 4K\n(Rk − √ 4Kηk)2 · τk (16)\nkm(Ck) ≤ 4K · φ(m)X,k({rnk}n) , (17)\nwhere ηk = √ ∑N n=1 r m nk(1− rmnk)w2n and τk = ∑N n=1 r m nk (1− rmnk)w2n ‖xn − µk‖ 2 2.\nProof. Recall that the binary random variables znk indicate hard clusters {Ck}k∈[K] by means of Ck = {(xn, wn) ∈ X |znk = 1}. If we apply Lemma 8 through 10 with λ = √ 4K and ν = λ2, we can take the union bound and obtain that the inequalities stated in the lemmata hold simultaneously with probability strictly larger than 0. Finally, using Equation (13) yields the claim."
    }, {
      "heading" : "7.3 Proof of Theorem 5",
      "text" : "We apply Corollary 1 to the given membership values {rnk}n,k and the point set\nX̂ =\n{(\nxn, wn wmax\n) ∣ ∣ ∣ ∣ (xn, wn) ∈ X } .\nLet k ∈ [K]. Note that the cluster weight R̂k with respect to X̂ is given by\nR̂k =\nN∑\nn=1\nrmnk ( wn wmax ) = 1 wmax Rk .\nUsing mink∈[K] Rk ≥ 16Kwmax/ǫ, we can conclude\nǫ · R̂k ≥ 16K . (18)\nObserve that\nηk =\n√ √ √ √ N∑\nn=1\nrmnk(1− rmnk) (\nwn wmax\n)2\n≤\n√ √ √ √ N∑\nn=1\nrmnk wn wmax =\n√\nR̂k . (19)\nDue to Inequality (18), we have\n√ 4K ≤\n√ 16K\n2 <\n√\nǫ · R̂k 2 <\n√\nR̂k 2 . (20)\nUsing Inequality (20) and (19), we can conclude\n√ 4K · ηk <\nR̂k 2 . (21)\nHence, Inequality (15) of Corollary 1 yields Inequality (7) of Theorem 5. Note that the optimal mean vectors µk with respect to X and {rnk}n,k coincide with the corresponding optimal mean vectors with respect to X̂ and {rnk}n,k (cf. Equation (2)). Next, we have\nτk = N∑\nn=1\nrmnk (1− rmnk) (\nwn wmax\n)2\n‖xn − µk‖22 ≤ 1\nwmax φ (m) X,k({rnk}n) .\nUsing Inequality (21), we obtain\nR̂k − √ 4Kηk ≥\nR̂k 2 > 0 .\nDue to Inequality (20), we have\n4K ≤ ǫR̂k 4 .\nHence, 4K\n(R̂k − √ 4Kηk)2 ≤ ǫ R̂k = ǫ · wmax Rk .\nTherefore, Inequality (16) of Corollary 1 yields Inequality (8) of Theorem 5. Finally, recall that the optimal mean vectors for X and {rnk}n,k coincide with those for X̂ and {rnk}n,k. Thus, φ(m)X̂,k({rnk}n) = 1 wmax φ (m) X,k({rnk}n). Analogously, for all C ⊂ X and Ĉ ⊂ X̂ with {(\nx, wwmax\n)∣ ∣ ∣(x,w) ∈ C }\n= Ĉ we have km(Ĉ) = 1wmax km(C). Hence, Inequality (17) of Corollary 1\nyields Inequality (9) of Theorem 5. ⊓⊔"
    }, {
      "heading" : "7.4 Superset Sampling (Proof of Theorem 6)",
      "text" : "Recall that, in Section 7, we argued on the existence of hard clusters which approximate optimal fuzzy clusters well. In this section we consider the problem of finding a good approximation to the means of such unknown hard clusters.\nFirst, consider the problem of finding the mean of a single unknown cluster C. That is, given a set X ⊂ RD and some unknown subset C ⊂ X , we want to find a good approximation to µ(C). If we assume that C contains at least a constant fraction of the points of X , then this problem can be solved via the superset sampling technique [8]. The main idea behind this technique is that the mean of a small uniform sample of a set is, with high probability, already a good approximation to the mean of the whole set. Knowing that C contains a constant fraction of points from X , we can obtain a uniform sample of C by sampling a uniform multiset S from X and inspecting all subsets of S. Thereby, we obtain a set of candidate means, i.e. the means of all subsets of S, including (with certain probability) one candidate that is a good approximation to the mean of C. Formally, using [21], we directly obtain the following lemma.\nLemma 11 (Superset Sampling [21]). Let X ⊂ RD, α ∈ (0, 1], and ǫ ∈ (0, 1]. Let S ⊂ X be a uniform sample multiset of size at least 4/(αǫ).\nConsider an arbitrary but fixed (unknown) subset C ⊂ X with |C| ≥ α |X |. With probability at least 1/10 there exists a subset C′ ⊂ S with |C′| = ⌈2/ǫ⌉ satisfying\n‖µ(C) − µ(C′)‖22 ≤ ǫ |C| ∑\nx∈C ‖x− µ(C)‖22 .\nwhere for any finite set S ⊂ RD we set µ(S) := ∑ x∈S x\n|S| .\nAs a consequence, for weighted point sets X with weights in Q, we obtain a good approximation of the mean of X by sampling each (unweighted) point with probability proportional to its weight.\nCorollary 2 (Weighted Superset Sampling). Let X = {(xn, wn)}n∈[N ] ⊂ RD × Q≥0, α ∈ (0, 1], and ǫ ∈ (0, 1]. Let W = ∑Nn=1 wn. Let S ⊂ {(xn, 1)}n∈[N ] be a sample multiset of size at least 4/(αǫ), where each point xn ∈ X is sampled with probability wn/W .\nConsider an arbitrary but fixed (unknown) subset C ⊂ X with w(C) ≥ αW . With probability at least 1/10 there exists a subset C′ ⊂ S with |C′| = ⌈2/ǫ⌉ satisfying\n‖µ(C) − µ(C′)‖22 ≤ ǫ\nw(C) km(C) .\nProof. Let ω be a common denominator of all {wn}n∈[N ]. Let X̂ ⊂ RD be the multiset containing wn ·ω copies of each xn with (xn, wn) ∈ X , and let Ĉ ⊂ RD be the multiset containing wn ·ω copies of each xn with (xn, wn) ∈ C. Note that sampling a point uniformly at random from X̂ yields the same distribution on the points as sampling a point from X with probability wn/W . Furthermore, if C ⊂ X with w(C) ≥ αW , then ˆ|C| = w(C) ≥ αW = α ˆ|X |. Hence, applying the previous lemma directly yields the claim.\nGiven Corollary 2, we can finally prove Theorem 6.\nProof (Proof of Theorem 6).\nLet R = ⌈10 log(2K)⌉ and let W = ∑Nn=1 wn. For each r ∈ [R], sample an unweighted multiset Sr ⊂ X of size at least 4/(αǫ) by choosing a point xn ∈ X with probability wn/W . Define the candidate set T as\nT :=\n{\nµ(S′) ∣ ∣ ∣ ∣ S′ ⊂ Sr, |S′| = ⌈2/ǫ⌉, r ∈ [R] }K .\nNext, we prove that T and its construction have the desired properties.\nLet M = {µ(S′) | S′ ⊂ Sr, |S′| = ⌈2/ǫ⌉, r ∈ [R]} and fix an arbitrary k ∈ [K] with w(Ck) ≥ αW . By Lemma 2, there is, with probability p := 1− (9/10)R, a candidate µ̃k ∈ M satisfying\n‖µ(Ck)− µ̃k‖22 ≤ ǫ\nw(Ck) km(Ck) .\nSince R ≥ 10 ln(2K), we have that (9/10)R ≤ 1/(2K). Hence, p ≥ 1− 1/(2K). By taking the union bound, we obtain that, with probability at least 1/2, T = MK contains a tuple (µ̃k)k∈[K] where for each k ∈ [K] the vector µ̃k is close to µ(Ck) if w(Ck) ≥ αW . Finally, we analyze the time needed to construct T . We have to sample R multisets of size ⌈4/(αǫ)⌉ from X , which needs running time O(R · (1/(αǫ)) ·N). Each of the multisets contains at most (4/(αǫ))2/ǫ subsets of size ⌈2/ǫ⌉. We compute the the means of all these subsets, which needs time O(1/ǫ) per subset. Then, by combining all these means we obtain the candidate set of tuples T . Consequently, this set has size\n|T | ≤ ( R · ( 4\nαǫ\n)1/ǫ )K\n= 2 1 ǫ ·log( 4αǫ )·log(R) .\nHence, we can bound the running time of our construction by\nO (\nR · 1 ǫ\n(\nN + 2 K ǫ ·log( 1αǫ )·log(R)\n))\n."
    }, {
      "heading" : "7.5 Candidate Set of Means (Proof of Theorem 7)",
      "text" : "The following we use the coreset construction used by [20], [19], and [23] to obtain a candidate set that contains a set of means which induce an (1+ ǫ)-approximation to the fuzzy K-means problem.\nConstruction We are given X = {xn}n∈[N ] ⊂ RD and K ∈ N. Let A = {ak}k∈[K] ⊂ RD be an α-approximation of the K-means problem with respect to X , i.e.\nkmX(A) ≤ α · kmX,K . (22)\nFurthermore, let\nR :=\n√\nkmX(A)\nαN , (23)\nB(x, r) := { y ∈ RD | ‖x− y‖2 ≤ r } , (24)\nΦ :=\n⌈ 1\n2\n( log(αN) +m · log ( 64αmK2\nǫ\n))⌉\n, (25)\nU := K⋃\nk=1\nB(ak, 2ΦR) , (26)\nLk,j :=\n{\nB(ak, R), if j = 0 B(ak, 2jR) \\ B(ak, 2j−1R) if j ≥ 1 , (27)\nκ := αKm−1 , and (28)\nb := 1208 (29)\nfor all k ∈ [K], j ∈ {0, . . . , Φ}, x ∈ RD, and r ∈ R. Construct an axis-parallel grid with side length\nρj = 2jǫR\nbκ √ D\nto partition Lk,j into cells. Inside each Lk,j pick the center of the cell as its representative point. Denote by G be the set of all representative points.\nProof of Theorem 7 In Section 7.5, we show that there exists C̃ = {µ̃l}l∈[L] ⊆ U with L ∈ [K] and\nφ (m) X (C̃) ≤\n(\n1 + ǫ\n2\n)\nφOPT(X,K,m) .\nSince C̃ = {µ̃l}l∈[L] ⊆ U , there our construction from Section 7.5 defines a representative µ̃′l ∈ G for each µ̃l ∈ C̃.\nIn Section 7.5, we consider arbitrary sets C = U with representatives C′ ⊆ G. We show that K-means costs of C and C′ are similar. Given this result, in Section 7.5, we show that the fuzzy K-means costs of C and C′ are similar. More precisely, we show that if α ≤ 2, then\n∣ ∣ ∣φ\n(m) X (C)− φ (m) X (C\n′) ∣ ∣ ∣ ≤ ǫ\n4 φ (m) X (C) .\nIn particular, this holds for C̃ and its representatives C̃′ ⊂ G. From these results we can conclude that\nφ (m) X (C̃\n′) ≤ ( 1 + ǫ\n4\n)\nφ (m) X (C̃) ≤\n(\n1 + ǫ\n2\n)(\n1 + ǫ\n4\n)\nφOPT(X,K,m) ≤ (1 + ǫ)φOPT(X,K,m) .\nFinally, observe that for all M ⊂ RD it holds φ(m)X (C̃′∪M) ≤ φ (m) X (C̃ ′). Thus, by testing all subsets {µ′k}k∈[K] ⊂ G, we find a solution that is at least as good as C̃′ and hence a (1+ ǫ)-approximation.\nPreliminaries In the following proof, we make extensive use of the following lemmata, as well as the lemmata given in Section 6.1.\nCorollary 3. Let X ⊂ RD × R≥0 and C ⊂ RD with |C| = K. If kmX(C) ≤ γ kmX,K , then φ (m) X (C) ≤ (γ ·Km−1)φOPT(X,K,m). If φ (m) X (C) ≤ γφOPT(X,K,m), then kmX(C) ≤ (γ ·Km−1) kmX,K\nProof. Use Lemma 1.\nLemma 12. For all C ⊂ RD, |C| = K, we have\nN · R2 = N∑\nn=1\nd(xn, A) 2 = kmX(A) ≤ α · kmX,K ≤ κ · φOPT(X,K,m) ≤ κ · φ (m) X (C) .\nProof. Use Definition 6, Equation (23), Equation (22), and Corollary 3.\nLemma 13. For each µ ∈ U with representative µ′ ∈ G it holds\n‖µ− µ′‖2 ≤ 2ǫ bκ (d (µ̃, A) +R) ≤ 2ǫ bκ (‖x− µ̃‖2 + d(x,A) +R) .\nand\n‖µ− µ′‖22 ≤ 12ǫ2\nb2κ2\n( ‖x− µ̃‖22 + d(x,A)2 +R2 )\nfor all x ∈ RD and µ̃ ∈ {µ, µ′}.\nProof. By construction, some Lk,j contains µ and its representative µ ′. Moreover, µ and µ′ are contained in the same grid cell with side length ρj . Hence, ‖µ− µ′‖2 ≤ 2 jǫR bκ . By construction of Lk,j , we have min{d(µ,A), d(µ′, A)} ≥ 2j−1R for all xn ∈ Lk,j with j ≥ 1. For j = 0 we know that ‖µ− µ′‖2 ≤ 2 0ǫR bα = ǫR bκ . Applying Lemma 7 and observing that for all x, y ∈ RD and C ⊂ RD we have d(x,C) ≤ ‖x− y‖2 + d(y, C), yields the claim.\nExistence of an (1 + ǫ2 )-Approximation in U\nClaim. There exists C̃ = {µ̃k}k ⊂ U with\nφ (m) X (C̃) ≤\n(\n1 + ǫ\n2\n)\nφOPT(X,K,m) .\nIn the following, we prove Claim 7.5.\nClaim.\n⋃\nx∈X B(x, r) ⊆\nK⋃\nk=1\nB(ak, 2ΦR) = U\nwhere\nr =\n√ (\n1 + ǫ\n2\n)(8mK2\nǫ\n)m\nkmX(A) .\nProof. Towards a contradiction, assume that there exists an x ∈ X with B(x, r) * U . By definition of U , this implies that for all k ∈ [K] we have B(x, r) * B(ak, 2ΦR). Hence,\nd(x,A)\n≥ 2ΦR− r = 2Φ √ kmX(A)\nαN − √ ( 1 + ǫ 2 )(8mK2 ǫ )m kmX(A) (by Equation (23))\n= √\nkmX(A)\n(\n2Φ √ 1 αN − √ ( 1 + ǫ 2 )(8mK2 ǫ )m ) .\nObserve that by Equation (25), we have\nΦ = 1\n2\n( log(αN) +m · log ( 64αmK2\nǫ\n))\n= log (√ αN ) + log\n(√( 64αmK2\nǫ\n)m )\n≥ log (√ αN ) + log\n(√\n2 · 2α · 2 ( 8mK2\nǫ\n)m )\n= log ( √ αN ·\n(√\n2 · 2α · 2 ( 8mK2\nǫ\n)m ))\n≥ log ( √ αN · ( √ 2α+\n√\n2\n( 8mK2\nǫ\n)m ))\n(since √ a+ b ≤ 2 √ ab for all a, b ≥ 1)\n≥ log ( √ αN · ( √ 2α+ √ ( 1 + ǫ\n2\n)(8mK2\nǫ\n)m ))\n. (since ǫ < 1)\nHence, d(x,A) ≤ √ kmX(A) √ 2α .\nUsing Definition 6, we can conclude\nkmX(A) ≥ d(x,A)2 ≥ 2α kmX(A) ≥ 2αkmX,K > α kmX,K ,\nwhich contradicts Equation (22).\nClaim. Let C̃ = {µ̃k}k∈[K] ⊂ RD such that\nφ (m) X (C̃) ≤\n(\n1 + ǫ\n4K\n)ℓ\nφOPT(X,K,m) .\nLet {C̃k}k∈[K] be a partition of X induced by C̃. For all k ∈ [K] we have\n(µk /∈ U ∧ Ck = ∅) ⇒ φ(m)X (C̃ \\ {µk}) ≤ ( 1 + ǫ\n4K\n)ℓ+1\nφ (m) X (C̃) .\nProof. Let {rnk}n,k be the optimal responsibilities induced by C̃. Using Claim 7.5 and µk /∈ U , we obtain\n(\n1 + ǫ\n4K\n)ℓ\nφOPT(X,K,m) ≥ φ (m) X (C̃) ≥ φ (m) X,k(C̃) =\nN∑\nn=1\nrmnk ‖xn − µk‖22 ≥ ( N∑\nn=1\nrmnk\n)\nr2 ,\nwhere r is defined as in Claim 7.5. Hence,\nN∑\nn=1\nrmnk ≤ ( 1 + ǫ4K )ℓ φOPT(X,K,m)\nr2\n≤ ( 1 + ǫ4K )ℓ kmX(A)\nr2 (Lemma 1)\n=\n( 1 + ǫ4K )K kmX(A)\n( 1 + ǫ2 ) ( 8mK2 ǫ )m kmX(A)\n(Claim 7.5)\n≤ ( 1 + ǫ2 )\n( 1 + ǫ2 ) ( 8mK2 ǫ\n)m (Lemma 6)\n≤ ( ǫ\n8mK2\n)m\n.\nConsequently,\nrnk ≤ ǫ\n8mK2 . (30)\nSince Ck = ∅, for all n ∈ [N ] there exists an l(n) ∈ [K] with l(n) 6= k and\nrn,l(n) ≥ 1\nK .\nUsing Equation (30), we can conclude\nrnk ≤ ǫ 8mK2 ≤ ǫ 8mK rn,l(n) .\nUsing Lemma 6, we obtain ( rnk + rn,l(n) )m ≤ (( 1 + ǫ\n8Km\n)\nrn,l(n)\n)m ≤ ( 1 + ǫ\n4K\n)\nrmn,l(n) .\nHence, we have\nφ (m) X (C̃ \\ {µ̃k}) ≤\nN∑\nn=1\n∑\nl 6=l(n),k rmnk ‖xn − µ̃l‖22 +\n( rn,l(n) + rnk )m ∥ ∥xn − µ̃l(n) ∥ ∥ 2\n2\n≤ N∑\nn=1\n∑\nl 6=l(n),k rmnk ‖xn − µ̃l‖22 +\n(\n1 + ǫ\n4K\n)\nrmn,l(n) ∥ ∥xn − µ̃l(n) ∥ ∥ 2\n2\n≤ ( 1 + ǫ\n4K\n)\nφ (m) X (C̃)\n≤ ( 1 + ǫ\n4K\n)ℓ+1\nφOPT(X,K,m)\nwhich yields the claim.\nClaim. There exists L ∈ [K] and C̃ = {µ̃l}l∈[L] ⊂ RD satisfying the following properties: 1. φ\n(m) X (C̃) ≤ ( 1 + ǫ2 ) φOPT(X,K,m)\n2. Let {C̃l}l∈[L] be a partition of X induced by C. For all l ∈ [L] we have µl /∈ U ⇒ C̃l 6= ∅ .\nProof. Consider an arbitrary but fixed optimal fuzzy K-means solution O = {ok}Kk=1. There are at most K − 1 means in O that are not in U and where the corresponding clusters Ok are empty. By repeatedly applying Claim 7.5, we obtain a solution Õ with ∣ ∣ ∣Õ ∣ ∣ ∣ ≤ K satisfying the second\nproperty in the claim. Furthermore, we have φ (m) X (Õ) ≤ (1 + ǫ4K )KφOPT(X,K,m) ≤ ( 1 + ǫ2 ) φOPT(X,K,m), where the last inequality is due to Lemma 6. This yields the claim.\nProof (Proof of Claim 7.5). Consider the solution C̃ = {µ̃l}l∈[L] from Claim 7.5. Assume µ̃l /∈ U . Then, by the second property of Claim 7.5, the corresponding cluster C̃l is not empty. Hence,\nφ (m) X (C̃) ≥\n1\nKm−1 · km(C̃) (Lemma 1 and L ≥ K)\n≥ 1 Km−1 · ∑\nx∈C̃k\n‖xn − µ̃k‖22\n≥ 1 Km−1 · ∑\nx∈C̃k\nr2 (by Claim 7.5)\n≥ 1 Km−1 · r2 (Ck 6= ∅)\n≥ 1 Km−1 · ( 1 + ǫ 2\n)(2mK2\nǫ\n)m\nkmX(A)\n> ( 1 + ǫ\n2\n)\nφOPT(X,K,m) , (by Lemma 1)\nwhich is a contradiction to the first property of Claim 7.5. Hence, from the properties of Claim 7.5, we can conclude that C̃ ⊂ U .\nNotation For the following proofs fix an arbitrary solution\nC = {µk}k∈[K] ⊂ U ⊂ RD . Let {rnk}n,k be the optimal responsibilities induced by C. We denote the representative of µk ∈ C by µ′k ∈ G, and the set of all these representatives by\nC′ := {µ′1, . . . , µ′K | µ′k is representative of µk ∈ C for all k ∈ [K]} .\nCloseness with respect to the K-Means Problem\nClaim. kmX(C\n′) ≤ γ kmX(C) , where\nγ = 1 + 72ǫ\nbKm−1 .\nIn the following, we prove Claim 7.5 To this end, denote by ˙ ⋃ k∈[K]Ck = X and ˙⋃ k∈[K]C ′ k = X\nthe partitions (ties broken arbitrarily) induced by C and C′, respectively.\nClaim. If kmX(C) > kmX(C ′), then\n|kmX(C) − kmX(C′)| ≤ K∑\nk=1\n|C′k| ‖µk − µ′k‖ 2 2 + 2\n∑\nxn∈C′k\n‖µk − µ′k‖2 ‖xn − µ′k‖ 2 2 ,\notherwise\n|kmX(C) − kmX(C′)| ≤ K∑\nk=1\n|Ck| ‖µk − µ′k‖ 2 2 + 2\n∑\nxn∈Ck ‖µk − µ′k‖2 ‖xn − µk‖ 2 2 .\nProof. If kmX(C) ≥ kmX(C′), then\n|kmX(C) − kmX(C′)| = K∑\nk=1\n∑\nxn∈Ck ‖xn − µk‖22 −\n∑\nxn∈C′k\n‖xn − µ′k‖ 2 2\n≤ K∑\nk=1\n∑\nxn∈C′k\n‖xn − µk‖22 − ‖xn − µ′k‖ 2 2 ({Ck}k induced by C)\n≤ K∑\nk=1\n∑\nxn∈C′k\n‖µk − µ′k‖ 2 2 + 2 ‖µk − µ′k‖2 ‖xn − µ′k‖ 2 2 (Lemma 5)\nIf kmX(C) < kmX(C ′), then the term can be bounded analogously. This yields the claim.\nClaim. K∑\nk=1\n|Ck| ‖µk − µ′k‖ 2 2 ≤\n36ǫ2\nb2κ kmX(C)\nProof.\nK∑\nk=1\n|Ck| ‖µk − µ′k‖ 2 2 ≤\n12ǫ2 b2κ\nK∑\nk=1\n∑\nxn∈Ck\n( ‖xn − µk‖22 + d(xn, A)2 +R2 )\n(Lemma 13)\n= 12ǫ2\nb2κ2\n( K∑\nk=1\n∑\nxn∈Ck ‖xn − µk‖22 +\nN∑\nn=1\nd(xn, A) 2 +NR2\n)\n≤ 12ǫ 2 b2κ2 ( kmX(C) + kmX(A) +NR 2 ) ≤ 12ǫ 2\nb2κ2 (kmX(C) + 2α kmX,K) (Lemma 12)\n≤ 36ǫ 2\nb2κ kmX(C) (since α ≥ 1)\nClaim.\nK∑\nk=1\n|C′k| ‖µk − µ′k‖ 2 2 ≤\n12ǫ2 b2κ2 (kmX (C ′) + 2α · kmX(C))\nProof.\nK∑\nk=1\n|C′k| ‖µk − µ′k‖ 2 2 ≤\n12ǫ2 b2κ2\nK∑\nk=1\n∑\nxn∈C′k\n(\n‖xn − µ′k‖ 2 2 + d(xn, A)\n2 +R2 )\n(Lemma 13)\n= 12ǫ2\nb2κ2\n\n\nK∑\nk=1\n∑\nxn∈C′k\n‖xn − µ′k‖ 2 2 +\nN∑\nn=1\nd(xn, A) 2 +NR2\n\n\n≤ 12ǫ 2 b2κ2 ( kmX(C ′) + kmX(A) +NR 2 ) ≤ 12ǫ 2\nb2κ2 (kmX(C\n′) + 2α · kmX,K) (Lemma 12)\n≤ 12ǫ 2\nb2κ2 (kmX(C\n′) + 2α · kmX(C))\nClaim.\n2\nK∑\nk=1\n∑\nxn∈Ck ‖µk − µ′k‖2 ‖xn − µk‖2 ≤\n24ǫ\nbKm−1 kmX(C)\nProof.\n2 K∑\nk=1\n∑\nxn∈Ck\n‖µk − µ′k‖2 ‖xn − µk‖2\n≤ 2ǫ bκ\nK∑\nk=1\n∑\nxn∈Ck 2 (‖x− µk‖2 + d(xn, A) +R) ‖xn − µk‖2 (Lemma 13)\n≤ 6ǫ bκ\nK∑\nk=1\n∑\nxn∈Ck\n(\n‖x− µk‖22 + d(xn, A)2 +R2 + ‖xn − µk‖ 2 2\n)\n(Lemma 7)\n≤ 6ǫ bκ ( 2 kmX(C) + kmX(A) +NR 2 ) ≤ 6ǫ bκ (2 kmX(C) + 2α kmX,K) (Lemma 12) ≤ 24ǫ bKm−1 kmX(C) (since α ≥ 1)\nClaim.\n2\nK∑\nk=1\n∑\nxn∈C′k\n‖µk − µ′k‖2 ‖xn − µ′k‖2 ≤ 12ǫ\nbκ (kmX(C\n′) + α · kmX(C))\nProof.\n2\nK∑\nk=1\n∑\nxn∈C′k\n‖µk − µ′k‖2 ‖xn − µ′k‖2\n≤ 2ǫ bκ\nK∑\nk=1\n∑\nxn∈C′k\n2 (‖x− µ′k‖2 + d(xn, A) +R) ‖xn − µ′k‖2 (Lemma 13)\n≤ 6ǫ bκ\nK∑\nk=1\n∑\nxn∈C′k\n(\n‖x− µ′k‖ 2 2 + d(xn, A) 2 +R2 + ‖xn − µ′k‖ 2 2\n)\n(Lemma 7)\n≤ 6ǫ bκ ( 2 kmX(C ′) + kmX(A) +NR 2 ) ≤ 6ǫ bκ (2 kmX(C ′) + 2α kmX,K) (Lemma 12) ≤ 12ǫ bκ (kmX(C ′) + α kmX(C))\nProof (Proof of Claim 7.5). If kmX(C) > kmX(C ′), then by Claim 7.5, 7.5, and 7.5 we have\n0 ≤ kmX(C) − kmX(C′)\n≤ K∑\nk=1\n|C′k| ‖µk − µ′k‖ 2 2 + 2\n∑\nxn∈C′k\n‖µk − µ′k‖2 ‖xn − µ′k‖ 2 2\n≤ 12ǫ 2\nb2κ2 (kmX (C\n′) + 2α · kmX(C)) + 12ǫ\nbκ (kmX(C\n′) + 2 · kmX(C))\n≤ 24ǫ bκ (kmX (C ′) + 2α · kmX(C))\nHence,\n(\n1− 48αǫ bκ\n) kmX(C)− ( 1 + 24ǫ\nbκ\n)\nkmX(C ′) ≤ 0\n(\n1 + 24ǫ\nbκ\n) (kmX(C) − kmX(C′)) ≤ ( 24ǫ\nbκ +\n48αǫ\nbκ\n)\nkmX(C)\nkmX(C)− kmX(C′) ≤ ( 24ǫ\nbκ +\n48αǫ\nbκ\n)\n/\n(\n1 + 24ǫ\nbκ\n)\nkmX(C)\n≤ 72ǫ bKm−1 kmX(C) .\nIf km(C′) > km(C), then by Claim 7.5, 7.5 and 7.5 we obtain\n0 ≤ kmX(C) − kmX(C′)\n≤ K∑\nk=1\n|Ck| ‖µk − µ′k‖ 2 2 + 2\n∑\nxn∈Ck ‖µk − µ′k‖2 ‖xn − µk‖ 2 2\n≤ 36ǫ 2\nb2κ kmX(C) +\n24ǫ\nbKm−1 kmX(C)\n≤ 60ǫ bKm−1 kmX(C)\nCloseness with respect to the Fuzzy K-Means Problem\nClaim. If α ≤ 2, then ∣ ∣ ∣φ\n(m) X (C)− φ (m) X (C\n′) ∣ ∣ ∣ ≤ ǫ\n4 φ (m) X (C)\nIn the following we prove Claim 7.5. To this end, let {rnk}n,k and {r′nk}n,k be the optimal responsibilities with respect to C and C′, respectively. Then, let\nE := ∣ ∣ ∣φ\n(m) X (C)− φ (m) X (C\n′) ∣ ∣ ∣ = ∣ ∣ ∣ ∣ ∣ N∑\nn=1\nK∑\nk=1\nrmnk ‖xn − µk‖22 − (r′nk)m ‖xn − µ′k‖ 2 2 ∣ ∣ ∣ ∣ ∣ .\nClaim.\nE ≤ max { N∑\nn=1\nK∑\nk=1\nrmnk ‖µk − µ′k‖ 2 2 + 2\nN∑\nn=1\nK∑\nk=1\nrmnk ‖µk − µ′k‖2 ‖xn − µk‖2 ,\nN∑\nn=1\nK∑\nk=1\n(r′nk) m ‖µk − µ′k‖ 2 2 + 2\nN∑\nn=1\nK∑\nk=1\n(r′nk) m ‖µk − µ′k‖2 ‖xn − µ′k‖2\n}\n.\nProof. If the first term in E is larger than the second, then\nE = N∑\nn=1\nK∑\nk=1\nrmnk ‖xn − µk‖22 − (r′nk)m ‖xn − µ′k‖ 2 2\n≤ N∑\nn=1\nK∑\nk=1\n(r′nk) m ( ‖xn − µk‖22 − ‖xn − µ′k‖ 2 2 )\n(rnk optimal wrt. C)\n≤ N∑\nn=1\nK∑\nk=1\n(r′nk) m ( ‖µk − µ′k‖ 2 2 + 2 ‖µk − µ′k‖2 ‖xn − µ′k‖2 ) , (Lemma 5)\nAnalogously, if the second term in E is larger than the first, then\nE ≤ N∑\nn=1\nK∑\nk=1\nrmnk\n(\n‖µk − µ′k‖ 2 2 + 2 ‖µk − µ′k‖2 ‖xn − µk‖2\n)\n.\nThis yields the claim.\nClaim.\nφ (m) X (C ′) ≤ γKm−1 · φ(m)X (C)\nProof. Using Claim 7.5 and Lemma 1, we obtain\nφ (m) X (C ′) ≤ kmX(C′) ≤ γ kmX(C) ≤ γKm−1 · φ(m)X (C) .\nClaim.\nK∑\nk=1\nmax\n{ N∑\nn=1\nrmnk,\nN∑\nn=1\n(r′nk) m\n}\n‖µk − µ′k‖ 2 2 ≤\n36(γ + 2α)ǫ2\nb2α φ (m) X (C)\nProof. Observe that\nN∑\nn=1\nK∑\nk=1\nrmnk ‖µk − µ′k‖ 2 2 ≤\n12ǫ2 b2κ2\nN∑\nn=1\nK∑\nk=1\nrmnk\n(\n‖xn − µk‖22 + d (xn, A) 2 +R2\n)\n(Lemma 13)\n≤ 12ǫ 2\nb2κ2\n(\nφ (m) X (C) +\nN∑\nn=1\nd (xn, A) 2 +NR2\n)\n≤ 36ǫ 2\nb2κ φ (m) X (C) (Lemma 12)\nSimilarly,\nN∑\nn=1\nK∑\nk=1\n(r′nk) m ‖µk − µ′k‖ 2 2 ≤\n12ǫ2 b2κ2\nN∑\nn=1\nK∑\nk=1\n(r′nk) m ( ‖xn − µ′k‖ 2 2 + d (xn, A) 2 +R2 ) (Lemma 13)\n≤ 12ǫ 2\nb2κ2\n(\nφ (m) X (C\n′) + N∑\nn=1\nd (xn, A) 2 +NR2\n)\n≤ 12ǫ 2\nb2κ2\n(\nφ (m) X (C ′) + 2κφ(m)X (C) )\n(Lemma 12)\n≤ 12ǫ 2\nb2κ2\n(\nγKm−1φ(m)X (C) + 2κφ (m) X (C)\n)\n(Claim 7.5)\n≤ 12(γ + 2α)ǫ 2\nb2α2 φ (m) X (C) .\nClaim.\n2\nN∑\nn=1\nK∑\nk=1\nrmnk ‖µk − µ′k‖2 ‖xn − µk‖2 ≤ 24ǫ\nb φ (m) X (C)\nProof.\n2\nN∑\nn=1\nK∑\nk=1\nrmnk ‖µk − µ′k‖2 ‖xn − µk‖2\n≤ 2ǫ bκ\nN∑\nn=1\nK∑\nk=1\nrmnk2 (‖µk − xn‖2 + d(xn, A) +R) ‖xn − µk‖2 (Lemma 12)\n≤ 2ǫ bκ\n( N∑\nn=1\nK∑\nk=1\nrmnk (‖µk − xn‖2 + d(xn, A) +R) 2 +\nN∑\nn=1\nK∑\nk=1\nrmnk ‖xn − µk‖22\n)\n(Lemma 7)\n≤ 6ǫ bκ\n( N∑\nn=1\nK∑\nk=1\nrmnk\n( ‖µk − xn‖22 + d(xn, A)2 +R2 ) + φ (m) X (C)\n)\n(Lemma 7)\n≤ 6ǫ bκ\n(\nφ (m) X (C) +\nN∑\nn=1\nd(xn, A) 2 +NR2 + φ (m) X (C)\n)\n≤ 24ǫ b φ (m) X (C) (Lemma 12)\nClaim.\n2\nN∑\nn=1\nK∑\nk=1\n(r′nk) m ‖µk − µ′k‖2 ‖xn − µ′k‖2 ≤\n12(γ + α)ǫ\nbα φ (m) X (C)\nProof.\n2\nN∑\nn=1\nK∑\nk=1\n(r′nk) m ‖µk − µ′k‖2 ‖xn − µ′k‖2\n≤ 2ǫ bκ\nN∑\nn=1\nK∑\nk=1\n(r′nk) m2 (‖µ′k − xn‖2 + d(xn, A) +R) ‖xn − µ′k‖2 (Lemma 12)\n≤ 2ǫ bκ\n( N∑\nn=1\nK∑\nk=1\n(r′nk) m (‖µ′k − xn‖2 + d(xn, A) +R) 2 +\nN∑\nn=1\nK∑\nk=1\n(r′nk) ‖xn − µ′k‖ 2 2\n)\n(Lemma 7)\n≤ 6ǫ bκ\n( N∑\nn=1\nK∑\nk=1\n(r′nk) m ( ‖µ′k − xn‖ 2 2 + d(xn, A) 2 +R2 ) + φ (m) X (C ′)\n)\n(Lemma 7)\n≤ 6ǫ bκ\n(\nφ (m) X (C\n′) + N∑\nn=1\nd(xn, A) 2 +NR2 + φ (m) X (C ′)\n)\n≤ 6ǫ bκ\n(\n2φ (m) X (C ′) + 2κφ(m)X (C) )\n(Lemma 12)\n≤ 12(γ + α)ǫ bα φ (m) X (C) (Claim 7.5)\nProof (Proof of Claim 7.5).\nE ≤ max { N∑\nn=1\nK∑\nk=1\nrmnk ‖µk − µ′k‖ 2 2 + 2\nN∑\nn=1\nK∑\nk=1\nrmnk ‖µk − µ′k‖2 ‖xn − µk‖2 ,\nN∑\nn=1\nK∑\nk=1\n(r′nk) m ‖µk − µ′k‖ 2 2 + 2\nN∑\nn=1\nK∑\nk=1\n(r′nk) m ‖µk − µ′k‖2 ‖xn − µ′k‖2\n}\n≤ max { N∑\nn=1\nK∑\nk=1\nrmnk ‖µk − µ′k‖ 2 2 ,\nN∑\nn=1\nK∑\nk=1\n(r′nk) m ‖µk − µ′k‖ 2 2\n}\n+\nmax\n{\n2\nN∑\nn=1\nK∑\nk=1\nrmnk ‖µk − µ′k‖2 ‖xn − µk‖2 , 2 N∑\nn=1\nK∑\nk=1\n(r′nk) m ‖µk − µ′k‖2 ‖xn − µ′k‖2\n}\n≤ 36(γ + 2α)ǫ 2\nb2α φ (m) X (C) +\n24(γ + α)ǫ\nb φ (m) X (C) (By Claims 7.5, 7.5 and 7.5)\n≤ 60(γ + 2α)ǫ b φ (m) X (C)\nwhere γ = 1 + 72ǫbKm−1 (cf. Claim 7.5). Since b = 1208 and α ≤ 2, we have E ≤ ǫ4φ (m) X (C), which yields the claim.\nUpper Bound on the Size of G In the following, we upper bound |G| analogously to [20]. Recall from Section 7.5 that each Lk,j is partitioned into an axis-parallel grid with side length ρj = 2jǫR bκ √ D . Hence, the volume of each grid cell is\nVj =\n( 2jǫR\nbκ √ D\n)D\n.\nFurthermore, observe that the distance between a point x ∈ Lk,j and mean vector ak is at most 2jR+ ρj < 2\nj+1R. Hence, the grid cell that contains x is contained in B(ak, 2j+1R). Each of these balls has a volume of\nvol(B(ak, 2j+1R)) = πD/2(2j+1R)D\nΓ (D/2 + 1) .\nConsequently, the number of grid cells in each Lk,j is bounded by\nvol(B(ak, 2j+1R)) Vj =\n( πD/2(2j+1R)D\nΓ (D/2 + 1)\n)( bκ √ D\n2jǫR\n)D\n≤ ( π D/2(2j+1R)D\n(D/4e)D/2\n)( bκ √ D\n2jǫR\n)D\n=\n( 2bκ\nǫ\n)D\n(4eπ) D/2 ≤\n( 12bκ\nǫ\n)D\n.\nOverall, we obtain\n|G| ≤ Φ∑\nj=0\nK∑\nk=1\nvol(B(ak, 2j+1R)) Vj\n≤ K ( log(αN) +m log ( 64αmK2\nǫ\n))( 12bκ\nǫ\n)D\n= O ( KmD+1ǫ−Dm log (mK/ǫ) log(N) ) ."
    }, {
      "heading" : "7.6 Unsolvability by Radicals (Proof of Theorem 1)",
      "text" : "Consider the fuzzy 2-means instance with m = 2 and X = {−3,−2,−1, 1, 2, 3} ⊂ R. Let {µ∗1, µ∗2} ⊂ R be the means of an optimal solution.\nClaim. sgn(µ∗1) 6= sgn(µ∗2)\nProof. Assume w.l.o.g. that µ∗1, µ ∗ 2 ≤ 0. Let {rnk}n,k be the memberships induced by {µ∗1, µ∗2}. Using the Cauchy-Schwarz inequality, we conclude\nφOPT(X,2,2) = φ (2) X (µ ∗ 1, µ ∗ 2) ≥ r231(3− µ1)2 + r232(3− µ2)2 ≥\n1 2 32 > 4 .\nThis contradicts the fact that, due Lemma 1, we have φOPT(X,2,2) ≤ kmX({2,−2}) = ∑\nx∈X min{(x− 2)2, (x+ 2)2} = 4. This yields the claim.\nObservation 2 µ∗1 and µ ∗ 2 lie inside the convex hull of the point set X.\nFrom this observation and Claim 7.6, we can conclude that the optimal solution {µ∗1, µ∗2} satisfies the following equations:\n∂φ (2) X ({µ1, µ∗2})\n∂µ1 (µ∗1) = 0 ,\n∂φ (2) X ({µ∗1, µ2})\n∂µ2 (µ∗2) = 0\n3 ≥ µ∗1 > 0 , 0 > µ∗2 ≥ −3.\nOne can check that the only pair of real values satisfying all of the equations above are the two real roots of the polynomial\ng(x) = 3x12 + 84x10 + 490x8 − 292x6 − 8981x4 − 17640x2 − 11664.\nThe interested reader can reproduce this result using the CAS MapleTM1 and the worksheet provided in Section 7.7.\nNote, that the roots of the polynomial\nh(x) = 3x6 + 84x5 + 490x4 − 292x3 − 8981x2 − 17640x− 11664\nare the square roots of the roots of g. By using the following well-known results from algebra, we can show that the roots of h, and hence also the roots of g, are not solvable by radicals over Q.\n1 Maple is a trademark of Waterloo Maple Inc.\nDefinition 8. We call a prime p good for a polynomial f ∈ Q[x] if p does not divide the discriminant of f .\nLemma 14 ([15]). Let f ∈ Q[x] with deg(f) = n > 2 and deg(f) = 0 mod 2. If there are good primes p1, p2, p3 for f such that\n1. f mod p1 is an irreducible polynomial of degree n, 2. f mod p2 factors into a linear polynomial and an irreducible polynomial of degree n− 1, and 3. f mod p3 factors into a linear polynomial, an irreducible polynomial of degree 2 and an irre-\nducible polynomial of degree n− 3,\nthen Gal(f) ∼= Sn.\nLemma 15 ([25]). Let f ∈ Q[x]. If the equation f(x) = 0 is solvable by radicals over Q, then the Galois group of f is a solvable group.\nLemma 16 ([25]). The symmetric group Sn is not solvable for n ≥ 5 .\nCorollary 4. The equation h(x) = 0 is not solvable by radicals over Q.\nProof. Since the discriminant of h is D(h) = 231 ·37 ·52 ·73 ·76637866514129, we can conclude that 11, 17, and 89 are good primes for h. We factor h modulo these good primes\nh = 3 · (x6 + 6x5 + 2x4 + 9x3 + 2x2 + 5x+ 6) mod 11 h = 3 · (x5 + 3x4 + 9x3 + 12x2 + 10x+ 7) · (x+ 8) mod 17 h = 3 · (x3 + 17x2 + 50x+ 17) · (x2 + 9x+ 27) · (x + 2) mod 89.\nFrom Lemma 14 we obtain Gal(h) ∼= S6. Applying Lemmata 15 and 16 yields the claim.\n7.7 MapleTMWorksheet\nThe following worksheet was developed using MapleTM 13.0. It can be downloaded at https://www-old.cs.uni-paderb In the worksheet we use the following formulation of the fuzzy 2-means objective function with m = 2.\nObservation 3 For all {µk}k ⊂ RD and X = {xn}n∈[N ] ⊂ RD with xn 6= µk for all k ∈ [K] and n ∈ [N ], we have\nφ (m) X ({µk}k) =\nN∑\nn=1\nK∑\nk=1\n(\n‖xn − µk‖−22 ∑K\nl=1 ‖xn − µl‖ −2 2\n)2\n‖xn − µk‖22 = N∑\nn=1\n1 ∑K\nk=1 ‖xn − µk‖ −2 2\n,\nwhere the first equality is due to Equation 1."
    }, {
      "heading" : "7.8 Arbitrarily Poor Local Minima (Proof of Observation 1)",
      "text" : "It is known that the FM algorithm converges to a stationary point of the objective function that is either a saddlepoint or a (local) minimum [5]. We show that there are instances for which this point is arbitrarily poor compared to an optimal solution.\nClaim. Let m ∈ N, D ≥ 2, and K = 2. Choose an arbitrary c ∈ R. Then, there exists a point set Xa and initial point set I ⊂ Xa, |I| = 2, satisfying the following properties: If the FM algorithm is initialized with I, then in each round it computes a solution whose cost are at least c · φOPT(X,K,m).\nProof. Consider the unweighted instances\nXa := {(a, 1), (−a, 1), (−a,−1), (a,−1)} ⊂ R2,\nwhere a ∈ R with a > 1.\nClaim. Let {rnk}n,k be a solution to the fuzzy 2-means problem with respect to Xa. Then, for all xn ∈ Xa we have rmn1 + rmn2 ≥ ( 1 2 )m . Proof. Since rn1 + rn2 = 1, we know max{rn1, rn2} ≥ 1/2 and thus rmn1 + rmn2 ≥ ( 1 2 )m .\nClaim. An optimal fuzzy 2-means clustering of Xa costs at least 1\n2m−1 and at most 4.\nProof. Observe that the means of every optimal solution lie in the convex hull of the input points. Consider an arbitrary solution {µ1, µ2} inside the rectangle spanned by Xa. There are two points in Xa for which the distance to both means is at least 1. Using Claim 7.8, we conclude that the costs of any solution can be lower bounded by 2 ·\n( 1 2 )m · 1 = 12m−1 . Finally, observe that for µ1 = (−a, 0), µ2 = (a, 0) we have φOPT(Xa,2,m) < ‖x1 − µ2‖ 2 2+‖x2 − µ2‖ 2 2+\n‖x3 − µ2‖22 + ‖x4 − µ1‖ 2 2 = 4.\nHowever, the FM algorithm might compute arbitrarily poor solutions, even if it is initialized with points from the point set.\nClaim. If the FM algorithm is started on Xa with {(a, 1), (a,−1)} as initial centers, then it computes a solution that has at least cost a 2\n2m+1φ (OPT ) (Xa,2,m) .\nProof. Let x1 = (a, 1), x2 = (−a, 1), x3 = −x1, x4 = −x2, µ1 = (a, 1), and µ2 = (a,−1). First, we show that if the FM algorithm is initialized with means (µ̃1, µ̃2) that lie on a line parallel to the y-axis, then it computes means that also lie on a line parallel to the y-axis. Given (µ̃1, µ̃2), the algorithm computes memberships where r11 = r42, r21 = r32, r31 = r22 and r41 = r12. Hence,\n(µ̃1)x = rm11a− rm21a− rm31a+ rm41a\nrm11 + r m 21 + r m 31 + r m 41\n= rm42a− rm32a− rm22a+ rm12a\nrm42 + r m 32 + r m 22 + r m 12\n= (µ̃2)x .\nNext, we lower bound the cost of means {µ̃1, µ̃2} that lie on a line parallel to the y-axis. Observe that there are always at least 2 points in Xa that have distance at least a\n2 from both means. Without loss of generality, we can assume that these points are x2 and x3. Denote by {rnk}n,k the optimal responsibilities induced by {µ̃1, µ̃2}. Then,\nφ({µ̃1, µ̃2} = 4∑\nn=1\n2∑\nk=1\nrmnk ‖xn − µ̃k‖22 ≥ a2 2∑\nk=1\nrm2k + r m 3k ≥\na2\n2m−1 ≥ a\n2\n2m+1 φ (OPT ) (Xa,2,m) ,\nwhere the second last inequality follows from Claim 7.8 and the last inequality follows from Claim 7.8.\nApplying Claim 7.8 with a := ⌈2m√c⌉ yields the claim."
    } ],
    "references" : [ {
      "title" : "A Fuzzy Relative of the ISODATA Process and Its Use in Detecting Compact WellSeparated Clusters",
      "author" : [ "J.C. Dunn" ],
      "venue" : "Journal of Cybernetics 3(3)",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1973
    }, {
      "title" : "FCM: The fuzzy c-means clustering algorithm",
      "author" : [ "J. Bezdek", "R. Ehrlich", "W. Full" ],
      "venue" : "Computers & Geosciences 10(2)",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 1984
    }, {
      "title" : "A multiresolution image segmentation technique based on pyramidal segmentation and fuzzy clustering",
      "author" : [ "M. Rezaee", "P. van der Zwet", "B. Lelieveldt", "R. van der Geest", "J. Reiber" ],
      "venue" : "Image Processing, IEEE Transactions on 9(7)",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Fuzzy C-means method for clustering microarray data",
      "author" : [ "D. Dembélé", "P. Kastner" ],
      "venue" : "Bioinformatics 19(8)",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Convergence theory for fuzzy c-means: Counterexamples and repairs",
      "author" : [ "J. Bezdek", "R. Hathaway", "M. Sabin", "W. Tucker" ],
      "venue" : "Systems, Man and Cybernetics, IEEE Transactions on 17(5)",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 1987
    }, {
      "title" : "Pattern Recognition and Machine Learning (Information Science and Statistics)",
      "author" : [ "C. Bishop" ],
      "venue" : "SpringerVerlag New York, Inc., Secaucus, NJ, USA",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Information Theory, Inference, and Learning Algorithms",
      "author" : [ "D. Mackay" ],
      "venue" : null,
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2003
    }, {
      "title" : "Applications of Weighted Voronoi Diagrams and Randomization to Variance-based K-clustering: (Extended Abstract)",
      "author" : [ "M. Inaba", "N. Katoh", "H. Imai" ],
      "venue" : "Proceedings of the Tenth Annual Symposium on Computational Geometry. SCG ’94, New York, NY, USA, ACM",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "The hardness of k-means clustering",
      "author" : [ "S. Dasgupta" ],
      "venue" : "Technical report, Department of Computer Science and Engineering, University of California, San Diego",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "The Hardness of Approximation of Euclidean k-means",
      "author" : [ "P. Awasthi", "M. Charikar", "R. Krishnaswamy", "A. Sinop" ],
      "venue" : "31st Annual Symposium on Computational Geometry, SOCG’15.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "A local search approximation algorithm for k-means clustering",
      "author" : [ "T. Kanungo", "D.M. Mount", "N.S. Netanyahu", "C.D. Piatko", "R. Silverman", "A.Y. Wu" ],
      "venue" : "Proceedings of the Eighteenth Annual Symposium on Computational Geometry. SCG ’02, ACM",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "A simple linear time (1+ ε)-approximation algorithm for geometric k-means clustering in any dimensions",
      "author" : [ "A. Kumar", "Y. Sabharwal", "S. Sen" ],
      "venue" : "Proceedings-Annual Symposium on Foundations of Computer Science, IEEE",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "On the Complexity of Some Common Geometric Location Problems",
      "author" : [ "N. Megiddo", "K. Supowit" ],
      "venue" : "SIAM J. Comput. 13(1)",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 1984
    }, {
      "title" : "The Algebraic Degree of Geometric Optimization Problems",
      "author" : [ "C. Bajaj" ],
      "venue" : "Discrete Comput. Geom. 3(2)",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 1988
    }, {
      "title" : "A contribution to convergence theory of fuzzy c-means and derivatives",
      "author" : [ "F. Hoppner", "F. Klawonn" ],
      "venue" : "Fuzzy Systems, IEEE Transactions on 11(5)",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Optimality tests for fixed points of the fuzzy c-means algorithm",
      "author" : [ "T. Kim", "J. Bezdek", "R. Hathaway" ],
      "venue" : "Pattern Recognition 21(6)",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 1988
    }, {
      "title" : "Local convergence of the fuzzy c-means algorithms",
      "author" : [ "R. Hathaway", "J. Bezdek" ],
      "venue" : "Pattern Recognition 19(6)",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 1986
    }, {
      "title" : "Coresets for k-Means and k-Median Clustering and their Applications",
      "author" : [ "S. Har-peled", "S. Mazumdar" ],
      "venue" : "In Proc. 36th Annu. ACM Sympos. Theory Comput.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "On coresets for k-median and k-means clustering in metric and euclidean spaces and their applications",
      "author" : [ "K. Chen" ],
      "venue" : "SIAM J. Comput. 39(3)",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Clustering for Metric and Nonmetric Distance Measures",
      "author" : [ "M. Ackermann", "J. Blömer", "C. Sohler" ],
      "venue" : "ACM Trans. Algorithms 6(4)",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "K-means++: The advantages of careful seeding",
      "author" : [ "D. Arthur", "S. Vassilvitskii" ],
      "venue" : "Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms. SODA ’07, Society for Industrial and Applied Mathematics",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "On approximate geometric k -clustering",
      "author" : [ "J. Matousek" ],
      "venue" : "Discrete & Computational Geometry 24(1)",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "A theoretical and experimental comparison of the EM and SEM algorithm",
      "author" : [ "J. Blömer", "K. Bujna", "D. Kuntze" ],
      "venue" : "22nd International Conference on Pattern Recognition, ICPR 2014, Stockholm, Sweden, August 24-28, 2014.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Algebra",
      "author" : [ "T. Hungerford" ],
      "venue" : "Graduate Texts in Mathematics. Springer",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 1974
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "1 Fuzzy K-Means [1] was the first to present a fuzzy K-means objective function, which was later extended by [2].",
      "startOffset" : 16,
      "endOffset" : 19
    }, {
      "referenceID" : 1,
      "context" : "1 Fuzzy K-Means [1] was the first to present a fuzzy K-means objective function, which was later extended by [2].",
      "startOffset" : 109,
      "endOffset" : 112
    }, {
      "referenceID" : 2,
      "context" : "Today, fuzzy K-means has found a wide range of practical applications, for example in image segmentation [3] and biological data analysis [4].",
      "startOffset" : 105,
      "endOffset" : 108
    }, {
      "referenceID" : 3,
      "context" : "Today, fuzzy K-means has found a wide range of practical applications, for example in image segmentation [3] and biological data analysis [4].",
      "startOffset" : 138,
      "endOffset" : 141
    }, {
      "referenceID" : 0,
      "context" : "In a fuzzy clustering, each data point xn belongs to each cluster, represented by a μk, with a certain membership value rnk ∈ [0, 1].",
      "startOffset" : 126,
      "endOffset" : 132
    }, {
      "referenceID" : 0,
      "context" : "Given X = {(xn, wn)}n∈[N ] ⊂ R × R≥0, K ≥ 1 and m ≥ 2, find C = {μk}k∈[K] ⊂ R and R = {rnk}n∈[N ],k∈[K] ⊂ [0, 1] minimizing",
      "startOffset" : 106,
      "endOffset" : 112
    }, {
      "referenceID" : 1,
      "context" : "Our problem definition is a generalization of the original definition presented in [2] in that we consider weighted data sets.",
      "startOffset" : 83,
      "endOffset" : 86
    }, {
      "referenceID" : 4,
      "context" : "It is defined by the following first-order optimality conditions [5]: Fixing the means {μk}k∈[K], optimal memberships are given by",
      "startOffset" : 65,
      "endOffset" : 68
    }, {
      "referenceID" : 5,
      "context" : "The same holds for other soft clustering problems, such as the maximum-likelihood estimation problem for Gaussian mixture models [6] or the soft-clustering problem [7].",
      "startOffset" : 129,
      "endOffset" : 132
    }, {
      "referenceID" : 6,
      "context" : "The same holds for other soft clustering problems, such as the maximum-likelihood estimation problem for Gaussian mixture models [6] or the soft-clustering problem [7].",
      "startOffset" : 164,
      "endOffset" : 167
    }, {
      "referenceID" : 7,
      "context" : "For fixed K and D, there is a polynomial time algorithm solving the problem optimally [8].",
      "startOffset" : 86,
      "endOffset" : 89
    }, {
      "referenceID" : 8,
      "context" : "The K-means problem is NP-complete, even if K or D is fixed to 2 [9] [10].",
      "startOffset" : 65,
      "endOffset" : 68
    }, {
      "referenceID" : 9,
      "context" : "Furthermore, assuming P 6=NP, there is no PTAS for the K-means problem for arbitrary K and D [11].",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 10,
      "context" : "However, there are several approximation algorithms known, such as a polynomial-time constant-factor approximation algorithm [12] and a (1 + ǫ)-approximation algorithm with runtime polynomial in N and D [13].",
      "startOffset" : 125,
      "endOffset" : 129
    }, {
      "referenceID" : 11,
      "context" : "However, there are several approximation algorithms known, such as a polynomial-time constant-factor approximation algorithm [12] and a (1 + ǫ)-approximation algorithm with runtime polynomial in N and D [13].",
      "startOffset" : 203,
      "endOffset" : 207
    }, {
      "referenceID" : 12,
      "context" : "Just as the K-means problem, the K-median problem is NP-hard, even for D = 2 [14].",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 13,
      "context" : "Even in the plane, optimal solutions of the 1-median problem are in general not expressable by radicals over Q [15].",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 1,
      "context" : "However, [2] and [5] proved convergence of the FM algorithm to a local minimum or a saddle point of the objective function.",
      "startOffset" : 9,
      "endOffset" : 12
    }, {
      "referenceID" : 4,
      "context" : "However, [2] and [5] proved convergence of the FM algorithm to a local minimum or a saddle point of the objective function.",
      "startOffset" : 17,
      "endOffset" : 20
    }, {
      "referenceID" : 14,
      "context" : "Among others, [16] and [17] address the problem of determining and distinguishing whether the algorithm has reached a local minimum or a saddle point.",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 15,
      "context" : "Among others, [16] and [17] address the problem of determining and distinguishing whether the algorithm has reached a local minimum or a saddle point.",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 16,
      "context" : "started sufficiently close to a minimizer, the iteration sequence converges to that particular minimizer [18].",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 13,
      "context" : "This result is an application of the technique used by Bajaj [15] who proved the same result for the K-median problem.",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 13,
      "context" : "A more detailed discussion of the implications of unsolvability by radicals can be found in [15].",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 0,
      "context" : "That is, for any given ǫ ∈ [0, 1], our algorithm computes an (1+ ǫ)-approximation to the fuzzy K-means problem in time polynomial in the number of points N and dimension D.",
      "startOffset" : 27,
      "endOffset" : 33
    }, {
      "referenceID" : 17,
      "context" : "The idea behind our algorithm from Theorem 3 is the same as behind the coreset construction of [19] as it is used by [20].",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 18,
      "context" : "The idea behind our algorithm from Theorem 3 is the same as behind the coreset construction of [19] as it is used by [20].",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 19,
      "context" : "Observe that the running time basically coincides with the running time of an algorithm that applies the superset sampling technique to the K-means problem [21].",
      "startOffset" : 156,
      "endOffset" : 160
    }, {
      "referenceID" : 17,
      "context" : "We use this result when transfering the ideas behind the coreset construction of [19] in order to obtain a candidate set of means.",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 0,
      "context" : "Let X = {(xn, wn)}n∈[N ] ⊂ R × R, m ∈ N, K ∈ N, and ǫ ∈ [0, 1].",
      "startOffset" : 56,
      "endOffset" : 62
    }, {
      "referenceID" : 7,
      "context" : "In the next section, we describe how the superset sampling technique [8] [13] can be used approximate the means μ(Ck) well.",
      "startOffset" : 69,
      "endOffset" : 72
    }, {
      "referenceID" : 11,
      "context" : "In the next section, we describe how the superset sampling technique [8] [13] can be used approximate the means μ(Ck) well.",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 19,
      "context" : "For instance, we presume that the sample and prune technique from [21] and the K-means++ algorithm [22] require that the convex hulls of clusters do not overlap.",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 20,
      "context" : "For instance, we presume that the sample and prune technique from [21] and the K-means++ algorithm [22] require that the convex hulls of clusters do not overlap.",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 7,
      "context" : "The superset sampling technique introduced by [8] [13] can be used to find such means.",
      "startOffset" : 46,
      "endOffset" : 49
    }, {
      "referenceID" : 11,
      "context" : "The superset sampling technique introduced by [8] [13] can be used to find such means.",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 17,
      "context" : "2 Candidate Set Search for Mean Vectors (Proof of Theorem 3) Using ideas behind the coreset construction of [19], we can construct a candidate set of mean vectors.",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 17,
      "context" : "The idea behind the coreset construction of [19] can be used to construct a candidate set of mean vectors.",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 21,
      "context" : "To this end, we use the deterministic algorithm presented in [23], which requires time O ( N(log(N))Kǫ−2K D ) .",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 22,
      "context" : "by using the results from [24].",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 7,
      "context" : "used in the proof of Theorem 2 in [8]).",
      "startOffset" : 34,
      "endOffset" : 37
    }, {
      "referenceID" : 0,
      "context" : "Let ǫ ∈ [0, 1], c > 1, and m ∈ N.",
      "startOffset" : 8,
      "endOffset" : 14
    }, {
      "referenceID" : 7,
      "context" : "If we assume that C contains at least a constant fraction of the points of X , then this problem can be solved via the superset sampling technique [8].",
      "startOffset" : 147,
      "endOffset" : 150
    }, {
      "referenceID" : 19,
      "context" : "Formally, using [21], we directly obtain the following lemma.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 19,
      "context" : "Lemma 11 (Superset Sampling [21]).",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 18,
      "context" : "The following we use the coreset construction used by [20], [19], and [23] to obtain a candidate set that contains a set of means which induce an (1+ ǫ)-approximation to the fuzzy K-means problem.",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 17,
      "context" : "The following we use the coreset construction used by [20], [19], and [23] to obtain a candidate set that contains a set of means which induce an (1+ ǫ)-approximation to the fuzzy K-means problem.",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 21,
      "context" : "The following we use the coreset construction used by [20], [19], and [23] to obtain a candidate set that contains a set of means which induce an (1+ ǫ)-approximation to the fuzzy K-means problem.",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 18,
      "context" : "Upper Bound on the Size of G In the following, we upper bound |G| analogously to [20].",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 13,
      "context" : "Lemma 14 ([15]).",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 23,
      "context" : "Lemma 15 ([25]).",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 23,
      "context" : "Lemma 16 ([25]).",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 4,
      "context" : "8 Arbitrarily Poor Local Minima (Proof of Observation 1) It is known that the FM algorithm converges to a stationary point of the objective function that is either a saddlepoint or a (local) minimum [5].",
      "startOffset" : 199,
      "endOffset" : 202
    } ],
    "year" : 2015,
    "abstractText" : "The fuzzy K-means problem is a generalization of the classical K-means problem to soft clusterings, i.e. clusterings where each points belongs to each cluster to some degree. Although popular in practice, prior to this work the fuzzy K-means problem has not been studied from a complexity theoretic or algorithmic perspective. We show that optimal solutions for fuzzy K-means cannot, in general, be expressed by radicals over the input points. Surprisingly, this already holds for very simple inputs in one-dimensional space. Hence, one cannot expect to compute optimal solutions exactly. We give the first (1+ ǫ)-approximation algorithms for the fuzzy K-means problem. First, we present a deterministic approximation algorithm whose runtime is polynomial in N and linear in the dimension D of the input set, given that K is constant, i.e. a polynomial time approximation algorithm given a fixed K. We achieve this result by showing that for each soft clustering there exists a hard clustering with comparable properties. Second, by using techniques known from coreset constructions for the K-means problem, we develop a deterministic approximation algorithm that runs in time almost linear in N but exponential in the dimension D. We complement these results with a randomized algorithm which imposes some natural restrictions on the input set and whose runtime is comparable to some of the most efficient approximation algorithms for Kmeans, i.e. linear in the number of points and the dimension, but exponential in the number of clusters.",
    "creator" : "LaTeX with hyperref package"
  }
}