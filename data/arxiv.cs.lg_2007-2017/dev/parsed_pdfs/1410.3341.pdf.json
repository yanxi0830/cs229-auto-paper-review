{
  "name" : "1410.3341.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n41 0.\n33 41\nv1 [\ncs .L\nG ]\n9 O\nct 2\n01 4"
    }, {
      "heading" : "1 Introduction",
      "text" : "Many Internet applications, such as sponsored search and crowdsourcing, can be regarded as dynamic systems that involve multi-party interactions. Specifically, users arrive at the system at random with their particular needs; agents provide products or services that could potentially satisfy users’ needs; and the platform employs a mechanism to match agents with users. Afterwards, users may give feedback to the platform about their satisfactions; the platform extracts revenue and may provide agents with some signals as their performance indicator. Since both the information reported by the agents and the mechanism will affect the payoff of the agents, self-interested agents may strategically adjust their\nCopyright c© 2014, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nbehaviors (e.g., strategically report the information about their services or products) in response to the mechanism (or more accurately the signals they receive since the mechanism is invisible to them). Take sponsored search as an example. When a user submits a query to the search engine (the platform), the search engine runs an auction to determine a ranked list of ads based on the bid prices reported by the advertisers (the agents). If the user clicks on (gives feedback to) an ad, the search engine will charge the corresponding advertiser by a certain amount of money. After a few rounds of auctions, the search engine will provide the advertisers with some signals on the auction outcome, e.g., the average rank positions of their ads, the numbers of clicks, and the total payments. Based on such signals, the advertisers may adjust their bidding behaviors to be better off in the future.\nIt is clear that the mechanism plays a central role in the aforementioned dynamic system. It determines the satisfaction of the users, the payoffs of the agents, and the revenue of the platform. Therefore, how to optimize the mechanism becomes an important research topic. In recent years, a number of research works (Lahaie and Pennock 2007; Radlinski et al. 2008; Zhu et al. 2009a; Zhu et al. 2009b; Medina and Mohri 2014; He et al. 2014; Tian et al. 2014) have used machine learning to optimize the mechanism. These works could be categorized into three types.\n• Some researchers assume that the agents are fully rational and investigate the Nash (or dominantstrategy) equilibrium of the mechanism. For example, (Medina and Mohri 2014) proposes a machine learning framework to optimize the second-price auction in sponsored search in the single-slot setting. In this case, the dominant strategy for fully rational advertisers is to truthfully reveal their valuations through the bid prices and therefore their bidding behaviors have no dynamics.\n• Some researchers assume that the behaviors of the agents are i.i.d. and independent of the mechanism, and optimize the mechanisms based on historical behavior data. For example, (Zhu et al. 2009a) and (Zhu et al. 2009b) apply machine learning algorithms to optimize the first-price auction based on the advertisers’ historical bidding data.\n• Some other researchers believe that the behaviors of the agents are neither fully rational nor i.i.d., instead, they are dependent on the mechanism through a data-\ndriven Markov model. For example, (He et al. 2013) and (Tian et al. 2014) assume that the agents’ behavior change is Markovian, i.e., dependent on their historical behaviors and the received signals in previous time periods.\nPlease note that the assumption in the third type of works is more general, and can cover the other two types as its special cases. According to (Fudenberg 1998), Nash (and also dominant-strategy) equilibrium in many games can be achieved by best-response behaviors, with which an agent determines the next action by maximizing his/her payoff based on the current action profile and mechanism. It is clear that the best-response behaviors are Markovian. Furthermore, it is also clear that the i.i.d. behaviors are special cases of Markov behaviors, where the Markov transition probability is reduced to a fixed distribution independent of the signals and the previous behaviors.\nBased on the Markov assumption on agent behaviors, (He et al. 2014) propose a new framework for mechanism optimization, called game-theoretic machine learning (GTML). The GTML framework involves a bi-level empirical risk minimization (ERM): it first learns a Markov model to characterize how agents change their behaviors, and then optimizes the mechanism by simulating agents’ behavior changes in response to the mechanism based on the learned Markov model. The GTML framework has demonstrated promising empirical results, however, its generalization analysis is missing in the literature1. Actually this is a very challenging task because conventional machine learning assumes data is i.i.d. generated from an unknown but fixed distribution (Devroye 1996; Vidyasagar 2003), while in GTML, agents behavior data have time dependency and may dynamically change in response to the mechanism. As a result, conventional generalization analysis techniques could not be directly applied.\nIn this paper, we present a formal analysis on the generalization ability of GTML. Specifically, utilizing the stability property of the stationary distribution of Markov chain (Mitrophanov 2005), we decompose the generalization error for GTML into the behavior learning error and the mechanism learning error. The former relates to the process of learning a Markov behavior model from data, and the latter relates to the process of learning the optimal mechanism based on the learned behavior model. For the behavior learning error, we offer novel non-asymptotic error bounds for both parametric and non-parametric behavior learning methods: for parametric behavior learning method, we upper bound the behavior learning error by parameter learning error; for non-parametric behavior learning method, we derive a new upper bound for the gap between transition frequency and transition probability of a Markov chain. After that, we apply the Hoeffding inequality for Markov chains to both of the upper bounds, and obtain the error bound for both parametric and non-parametric behavior learning methods. For the mechanism learning error, we make use of a\n1 In (Tian et al. 2014), authors only studied the generalization ability for behavior learning. Furthermore, their definition for behavior learning error is different from ours, and cannot be applied to the generalization analysis for GTML.\nnew concept called nested covering number of the mechanism space. Specifically, we first partition the mechanism space into subspaces (i.e., a cover) according to the similarity between the stationary distributions of the data induced by mechanisms. In each subspace, the data distribution is similar and therefore one can substitute the data sample associated with each mechanism by a common sample without affecting the expected risk by much. Second, for each mechanism subspace, we derive a uniform convergence bound based on its covering number (Anthony and Bartlett 2009) by using the generalization analysis techniques developed for mixing sequences. In the end of this paper, we apply our generalization analysis of GTML to sponsored search, and give theoretical guarantee to GTML in this scenario.\nTo the best of our knowledge, this is the first work that performs formal generalization analysis on GTML, and we believe the methodologies we use have their general implications to the theoretical analysis of other complicated machine learning problems as well."
    }, {
      "heading" : "2 GTML Framework",
      "text" : "In this section, we briefly introduce the game-theoretic machine learning (GTML) framework. For ease of reference, we summarize related notations in Table 1."
    }, {
      "heading" : "2.1 Mechanisms in Internet Applications",
      "text" : "Internet applications such as sponsored search and crowdsourcing can be regarded as dynamic systems involving interactions between multiple parties, e.g., users, agents, and platform. For example, in sponsored search, the search engine (platform) ranks and shows ads to users and charges the advertisers (agents) if their ads are clicked by users, based on the relevance degrees of the ads and the bid prices reported by the advertisers. Similar multi-party relationship can also be found in crowdsourcing, where the platform corresponds to Mechanical Turk, the agents corresponds to employers. While we can assume the behaviors of the users to be i.i.d., the behaviors of the agents are usually not. This is because agents usually have clear utilities in their minds, and they may change behaviors in order to maximize their utilities given the understandings on the mechanism used by the platform. As a result, the agents’ behaviors might be dependent on the mechanism.\nMathematically, we denote the space of mechanisms as A, and assume it to be bounded with distance dA. We denote the space of user need/feedback, the space of agent behaviors, and the space of the signals, as U , B, and H respectively. We assume B and H are both finite, with size |B| and |H|, since the behaviors and signals are usually discrete and bounded. For a mechanism a ∈ A, at the t-th time period, agents’ behavior profile is bat ∈ B, and a user ut ∈ U arrives at the system. The platform matches the agents to the user and charges them, according to mechanism a. After that, the platform will provide some signals ht ∈ H (e.g., the number of clicks on the ads ) to the agents as an indication of their performances. Since ht may be affected by agents’ behavior profile bat , mechanism a, and user data ut, we denote ht = sig(a, b a t , ut), where sig : A × B × U → H is a\nfunction generating the signals for agents. After observing ht, agents will change their behavior to bat+1 to be better off in the future."
    }, {
      "heading" : "2.2 Markov Agent Behavior Model",
      "text" : "In order to describe how agents change their behaviors, the authors of (He et al. 2013) and (Tian et al. 2014) proposed a Markov behavior model. The key assumption made by the Markov model is that any agent only has a limited memory, and his/her behavior change only depends on his/her previous behaviors and signals in a finite number of time periods. To ease the discussion and without loss of too much generality, they assume the behavior model to be first-order Markovian. Formally, given the signal ht, the distribution of agents’ next behavior profile can be written as follows,\nP (bat+1|b a t , ..., b a 1 ;ut, ..., u1)\n=P (bat+1|b a t , ..., b a 1 ;ht, ..., h1) =P (bat+1|b a t , ht) := Mht(b a t , b a t+1),\nwhere Mh is the transition probability matrix of the behavior profile, given the signals h ∈ H.\nAs mentioned in the introduction, the Markov behavior model is very general and can cover other types of behavior models studied in the literature, such as the best-response behaviors and the i.i.d. behaviors."
    }, {
      "heading" : "2.3 Bi-Level Empirical Risk Minimization",
      "text" : "In (He et al. 2014), bi-level empirical risk minimization (ERM) algorithm is proposed to solve the GTML problem. The first-level ERM corresponds to behavior learning, i.e., learning the Markov behavior model (the transition probability matrixes Mh(·, ·), h ∈ H) from training data containing signals and corresponding behavior changes. The second-level ERM corresponds to mechanism learning, i.e., learning the mechanism with the minimum empirical risk defined with both the behavior model learned at the first level and the training data containing users’ needs/feedback.\nFor behavior learning, suppose we have T1 samples of historical behaviors and signals {bt, ht} T1 t=1. The goal is to learn the transition matrix Mh(·, ·) from these data. In (He et al. 2014) and (Tian et al. 2014), both parametric and non-parametric approaches were adopted for behavior learning. With the parametric approach, one assumes the transition probability to take a certain mathematical form, e.g., Mh(b, b\n′) ∝ exp(−(b′ − 〈w, (b, h, 1)〉)2), where 〈·, ·〉 denotes the inner product of two vectors and parameter w is learned by maximum likelihood estimation. With the non-parametric approach, one directly estimates each entry Mh(b, b\n′) by counting the frequencies of the event (bt = b, bt+1 = b\n′) out of the event (bt = b) given signal h. No matter which approach is used, we denote the learned behavior model as M̂T1 for ease of reference.\nFor mechanism learning, suppose we have T2 samples of user data {ut} T2 t=1 and a Markov behavior model M̂T1 , learned as above. The goal is to learn an optimal mechanism to minimize the empirical risk (e.g., minus empirical revenue/social welfare) on the user data, denoted as L(a, b, u) where L : A×B×U → [−K, 0]. For this purpose, for arbitrary mechanism a ∈ A, one generates T2 samples of behavior data {bat } T2 t=1 in a sequential manner using the Markov\nmodel M̂T1 and T2 samples of user data. With the T2 samples of behavior data and user data, the empirical risk of mechanism a can be computed. To improve the computational efficiency of mechanism learning, in (He et al. 2014), the authors introduce a technique called δ-sample sharing. Specifically, given δ > 0, in the optimization process, if the distance between a new mechanism a and another mechanism a′ whose behavior data is already generated is smaller than δ (i.e., dA(a, a′) ≤ δ), then one will not generate behavior data for a any more, but instead reuse the behavior data previously generated for mechanism a′. Therefore, we denote the sample for mechanism a as {bs(a,δ)t } T2 t=1, where s(a, δ) is equal to a itself or another mechanism satisfying dA(a, s(a, δ)) ≤ δ. Consequently, the empirical risk of mechanism a is defined as below,\nRT2(a, M̂T1 , δ) = 1\nT2\nT2 ∑\nt=1\nL(a, b s(a,δ) t , ut).\nBy minimizing RT2(a, M̂T1 , δ), one can obtain an empirically optimal mechanism:\nâT2 = argmin a∈A RT2(a, M̂T1 , δ).\nWhile GTML and the bi-level ERM algorithm have demonstrated their practical success (He et al. 2013), their theoretical properties are not yet well understood. In particular, given that GTML is more complicated than conventional machine learning (in GTML the behavior data are timedependent and mechanism-dependent), conventional generalization analysis techniques cannot be directly applied and new methodologies need to be proposed."
    }, {
      "heading" : "3 Generalization Analysis for GTML",
      "text" : "In this section, we first give a formal definition to the generalization error of the bi-level ERM algorithm for GTML, and then discuss how to derive a meaningful upper bound for this generalization error. Finally, we apply our generalization analysis of GTML to sponsored search, and show the GTML in this scenario has good generalization ability.\nAccording to (Tian et al. 2014), for a behavior model M (such as the true Markov behavior model M∗ and the model M̂T1 obtained by the behavior learning algorithm), under some mild conditions (e.g., is irreducible and aperiodic), the process (bat , ut) is a uniformly ergodic Markov chain for arbitrary mechanism a. Then given mechanism a and behavior model M , there exists a stationary distribution for (bat , ut), which we denote as π(a,M). For simplicity, we assume the process {(bat , ut) : t ≥ 1} is stationary\n2. We define the risk for each mechanism a ∈ A as the expected loss with respect to the stationary distribution of this mechanism under the true behavior model M∗, i.e.,\nR(a,M∗) = Eπ(a,M∗)L(a, b a, u).\nThe optimal mechanism minimizing this risk is denoted as a∗, i.e.,\na∗ = argmin a∈A R(a,M∗).\nWe consider the gap between the risk of the mechanism âT2 learned by the bi-level ERM algorithm and the risk of the\n2Our results can similarly holds without this assumption.\noptimal mechanism a∗, i.e., R(âT2 ,M ∗)−R(a∗,M∗). We call this gap the generalization error for the bi-level ERM algorithm, or simply the generalization error for GTML.\nTo ease the analysis, we utilize the stability property of the stationary distribution of uniformly ergodic Markov Chain and decompose the generalization error for GTML into two parts, as shown in the following Theorem. Due to space restrictions, we leave all proofs in this paper to supplemental materials. Theorem 3.1. The generalization error of the bi-level ERM algorithm for GTML R(âT2 ,M\n∗) − R(a∗,M∗) can be bounded as:\nR(âT2 ,M ∗)−R(a∗,M∗) ≤ 2KC(M∗)||M∗ − M̂T1 ||∞\n+2 sup a∈A\n|R(a, M̂T1)−RT2(a, M̂T1 , δ)|, (1)\nwhere K is an upper bound for loss L, and C(M∗) is a non-negative constant depending on M∗.\nFor ease of reference, we call the first term ||M∗ − M̂T1 ||∞ in the right-hand side of inequality (1) behavior learning error and the second term supa∈A |R(a, M̂T1) − RT2(a, M̂T1 , δ)| mechanism learning error. We will derive upper bounds for both errors in the following subsections."
    }, {
      "heading" : "3.1 Error Bound for Behavior Learning",
      "text" : "In this subsection we derive error bounds for both parametric and non-parametric behavior learning methods. Since the behavior space and signal space are both finite, it is shown in (Tian et al. 2014) that {bt+1, bt, ht} forms a timehomogeneous Markov chain. Furthermore, under regular conditions, the Markov chain is uniformly ergodic, i.e., there exists N0, such that the elements in the N0-step transition probability matrix of {bt+1, bt, ht} are all positive. For ease of reference, we denote the minimum element in this matrix as δ0. Since the mechanism a0 is fixed in the process of behavior learning , we omit all the super scripts a0 in b a0 t if without confusion. Please note that, in (Tian et al. 2014), although authors studied the generalization analysis for behavior learning, their definition on behavior learning error is different from ours and cannot be applied in the generalization analysis for GTML. To be specific, they measure the behavior learning error by the expected behavior prediction loss of the learned behavior model with respect to the stationary distribution under the true behavior model, while we measure behavior learning error in a stricter way by the infinity distance between the learned model and the true model.\nParametric Behavior Learning With the parametric approach (He et al. 2014), the transition probability is proportional to a truncated Gaussian function, i.e., Mh(b′, b) ∝ exp(−(b′ − 〈ω, (b, h, 1)〉)2) where ω is bounded . The parameter is obtained by maximizing the likelihood. We first bound the behavior learning error by the gap between the learned parameter and the parameter in the true model, utilizing the property of maximum likelihood method; then we apply the Hoeffding inequality for uniformly ergodic Markov chain (Glynn and Ormoneit 2002) and finally obtain the error bound for parametric behavior learning method as shown in the following theorem.\nTheorem 3.2. For any ǫ > 0, we have, for T1 > (2C1N0)/(|B|2|H|δ0ǫ),\nP (||M̂T1 −M ∗||∞ ≥ ǫ)\n≤2 exp ( −\n(\nT1ǫ|B| 2|H|δ0 − 2C1N0\n)2\n2T1N20C 2 1\n)\n= O(exp(−T1))\nwhere δ0, N0, C1 are positive constants.\nNon-parametric Behavior Learning In the nonparametric behavior learning, we estimate the transition probability MHj (Bi, Bk) by the conditional frequency of the event bt+1 = Bk given that bt = Bi and ht = Hj , i.e., M̂Hj (Bi, Bk) = ∑T1 t=1 1{bt+1=Bk,bt=Bi,ht=Hj} ∑T1\nt=1 1{bt=Bi,ht=Hj}\n. The\ndifficulty in analyzing the error of the above estimation comes from the sum of random variables in the denominator of the conditional frequency. To tackle the challenge, we first derive an upper bound for the gap between conditional transition frequency and conditional transition probability, which does not involve such a sum of random variables, then apply the Hoeffding inequality for uniformly ergodic Markov chain (Glynn and Ormoneit 2002) to this upper bound. In this way, we manage to obtain a behavior learning error bound, as shown in the following theorem.\nTheorem 3.3. For any ǫ > 0, we have for T1 > ( 2N0(|B|+ 1) ) / ( |B||H|δ0C2ǫ ) ,\nP (||M̂T1 −M ∗||∞ ≥ ǫ)\n≤2|H||B|2(|B|+ 1) exp ( −\n( C2T1δ0|B||H|ǫ − 2N0(|B|+ 1) )2\n2T1N20 (|B|+ 1) 2\n)\n=O(exp(−T1))\nwhere δ0, N0, C2 are positive constants."
    }, {
      "heading" : "3.2 Error Bound for Mechanism Learning",
      "text" : "In this section, we bound the mechanism learning error by using a new concept called nested covering number for the mechanism space. We first give its definition, and then prove a uniform convergence bound for mechanism learning on its basis.\nNested Covering Number of Mechanism Space The nested cover contains two layers of covers: the first-layer cover is defined for the entire mechanism space based on the distance between stationary distributions induced by the mechanisms. The second-layer cover is defined for each partition (subspace) obtained in the first layer based on the distance between the losses of the mechanisms projected onto finite common data samples.\nFirst, we construct the first-layer cover for the mechanism space A. In mechanism learning, the learned Markov behavior model M̂T1 is used to generate the behavior data for different mechanisms. For simplicity, we denote the stationary distribution of the generated data as π(a, M̂T1) (or πa for simplification) and the set of stationary distributions for A as π(A). We define the (induced) total variance distance on A as the total variance distance on π(A), i.e., for ∀a, a′ ∈ A, dTV (a, a′) = dTV (πa, πa′). For ∀ǫ > 0,\nthe smallest ǫ-cover of A w.r.t. the total variance distance is coverǫTV = {g ǫ 1, · · · , g ǫ i , · · · }, where g ǫ i ∈ A. That is, A ⊆ ∪iB(gǫi , ǫ), where B(g ǫ i , ǫ) is the ǫ-balls of g ǫ i with respect to the (induced) total variance distance. We define the first-layer covering number as the cardinality of coverǫTV , denoted as NTV (ǫ,A). Based on coverǫTV , we can obtain a partition for A, denoted as {Aǫi}, where A ǫ i is an ǫ-partition of A. When the mapping from mechanism to its stationary distribution is α uniformly Lipschitz continuous, then NTV (ǫ,A) < ∞. Because for ∀δ > 0, a and s(a, δ) belong to the same δα-partition of A. So, consideringA is bounded, we have NTV (δα,A) ≤ NdA(δ,A) < ∞.\nSecond, we consider the loss functions for each mechanism subspace L ◦ Aǫi := {L ◦ a : B × U → [−K, 0]|L ◦ a(bat , ut) = L(a, b a t , ut), a ∈ A ǫ i}, and define its covering number w.r.t. the T2 common samples {Xǫi,t} T2 t=1, where Xǫi,t = {b gǫi t , ut} and {b gǫi t } T2 t=1 are generated by mechanism gǫi . Again, we define the second-layer cover as the smallest ǫ′-cover of L ◦ Aǫi |{Xǫ\ni,t } T2 t=1\nunder the l1 distance, i.e.,\ncoverǫ ′\n1 (L ◦ A ǫ i |{Xǫ\ni,t } T2 t=1\n), and define the second-layer cov-\nering number N1(ǫ′, L◦Aǫi , T2) as its maximum cardinality with respect to the sample {Xǫi,t} T2 t=1.\nIn summary, the nested covering numbers for a mechanism space are defined as follows:\nDefinition 3.4. Suppose A is a mechanism space, we define its nested covering numbers as { NTV (ǫ,A), {N1(ǫ′, L◦ Aǫi , T2)} } .\nUniform Convergence Bound for Mechanism Learning In this subsection, we derive a uniform convergence bound for the ERM algorithm for mechanism learning. We first relate the uniform convergence bound for the entire mechanism space to that for the subspaces constructed according to the first-layer cover. Then considering that uniformly ergodic Markov chains are β-mixing (Doob 1990), we make use of the independent block technique for mixing sequences (Yu 1994) to transform the original problem based on dependent samples to that based on independent blocks. Finally, we apply the symmetrization technique and Hoeffding inequality to obtain the desired bound.\nTheorem 3.5. Suppose that the mapping from A to πA is α uniformly Lipschitz continuous, and the β-mixing rate of Markov chain {(bat , ut) : t ≥ 1} (denoted as β(a,m)) is algebraical(i.e., β(a,m) ≤ β0m−γ , where β0, γ ≥ 0,m ∈ Z.). For any ǫ > 0, we have\nP (sup a∈A |RT2 (a, M̂T1 , δ)−R(a, M̂T1 )| ≥ ǫ)\n≤NdA(δ,A) max 1≤i≤NdA (δ,A)\n(\n16N1((ǫ−Kαδ)/16, L ◦ A δ i , T2)\nexp ( − (ǫ− δαK)2 128K2 ⌈ T\ns 1+s\n2 2 ⌉ )\n+ β0⌈T s−γ 1+s 2 ⌉ )\n= O\n(\nNdA(δ,A) ( N1(ǫ, L ◦ A, T2)e −T\ns 1+s 2 + T s−γ 1+s 2 ) ) ,\nwhere ⌈⌉ denotes ceiling function, δ ∈ (0, ǫ/(Kα)) and s ∈ (0, γ).\nRemark 1: For space restrictions, we only present the bound with specific mixing rate, which is simpler and easier to understand. Without the assumption on the mixing rate, we can also obtain a similar bound, which can be found in Theorem C.1 in the supplemental materials.\nRemark 2: Although we have to leave the proofs to the supplementary materials due to space restrictions, we would like to point out one particular discovery from our proofs. While the δ-sample sharing technique was originally proposed to improve efficiency, according to our proof it plays an important role in generalization ability . Then a question is whether this technique is necessary for generalization ability. Our answer is yes if A is infinite. Let us consider a special case in which U = {u} and πa ≡ π, ∀a ∈ A, i.e., the behavior model does not rely on the signals. If δ-sample sharing is not used, for finite T ,\nP (sup a∈A\n| 1\nT\nT ∑\nt=1\nL(a, bat , ut)− EL(a, b a t , ut)| ≥ ǫ)\n=1− ∏\na∈A\n( 1− P (| 1\nT\nT ∑\nt=1\nL(a, bat , ut)− EL(a, b a t , ut)|)\n)\n= 1.\nThis implies that mechanism learning without δ-sample sharing does not have generalization ability.\nRemark 3: An assumption made in our analysis is that the map from A to πA is uniformly Lipschitz continuous. However, sometimes this assumption might not hold. In this case, we propose a modification to the original δ-sample sharing technique. The modification comes from the observation that the first-layer cover is constructed based on the total variance distance between stationary distributions of mechanisms. Therefore, in order to ensure a meaningful cover, we could let two mechanisms share the same data sample if the estimates of their induced stationary distributions (instead of their parameters) are similar. Please refer to the supplementary materials for details of this modification and a proof showing how it can bypass the discontinuity challenge. Note that the modified δ-sample sharing technique no longer has efficiency advantage since it involves the generation of behavior data for every mechanism examined during the training process, however, it ensures the generalization ability of the mechanism learning algorithm, which is desirable from the theoretic perspective."
    }, {
      "heading" : "3.3 The Total Error Bound",
      "text" : "By combining Theorem 3.1, Theorem 3.3, Theorem 3.2 and Theorem 3.5, we obtain the total error bound for GTML as shown in the following theorem. Theorem 3.6. With the same assumptions in Theorem 3.5, for bi-level ERM algorithm in GTML, for any ǫ > 0, we have the following generalization error bound 3:\nP (R(âT2 ,M ∗)−R(a∗,M∗) ≥ ǫ)\n≤O(e−T1) +O\n(\nNdA(δ,A) ( N1(ǫ, L ◦ A, T2)e −T\ns 1+s 2 + T s−γ 1+s\n2\n)\n)\n,\nwhere s ∈ (0, γ), and δ ∈ (0, ǫ/Kα).\n3Please refer to Theorem C.3 for the total error bound without the assumption on the mixing rate.\nFrom the above theorem, we have following observations: 1) The error bound will converge to zero when the scales of agent behavior data T1 and user data T2 approach infinity. 2) The convergence rate w.r.t. T1 is faster than that w.r.t. T2, indicating that one needs more user data than agent behavior data for training. 3) The mechanism space impacts the generalization ability through both its first layer covering number (which is finite) and second layer covering number."
    }, {
      "heading" : "3.4 Application to Sponsored Search Auctions",
      "text" : "In this section, we apply our generalization analysis for GTML to sponsored search auctions. In sponsored search, GSP auctions with a query-dependent reserve price are widely used (Edelman, Ostrovsky, and Schwarz 2005; Easley and Kleinberg 2010; Medina and Mohri 2014).\nWhen a reserve price r ∈ R+ is used, the GSP auction runs in the following manners. First, the search engine ranks the ads according to their bid prices (here we follow the common practice to absorb the click-through rate of an ad into its bid price to ease the notations), and will show to the users those ads whose bid prices are higher than the reserve price. If the ad on the i-th position (denoted as adi) is clicked by a user, the search engine will charge the corresponding advertiser by the maximum of the bid price of adi+1 and the reserve price r. For sake of simplicity and without loss of generality, we will only consider two ad slots. Let the binary vector c = {c1, c2} indicate whether ad1 and ad2 are clicked by users. Then the user data include two components, i.e., u = (q, c), where q ∈ Q is the query issued by the user and c records user’s click feedback. Denote the bid profile of the shown ads as (b(1),q , b(2),q) (for simplicity we sometimes omit q in the notation). We consider a query-dependent reserve price, i.e., the auction family is A = {a : Q → R+}. For a mechanism a, the revenue of the search engine can be represented as:\nRev(a, b, u) = a(q)c11{b(2)≤a(q)≤b(1)}\n+(b(2)c1 + a(q)c2)1{b(3)≤a(q)≤b(2)} + (b (2)c1 + b (3)c2)1{a(q)≤b(3)},\nand the loss is L(a, b, u) = −Rev(a, b, u). Since the first layer covering number is always finite and\nindependent of the user data size (i.e., T2), we just need to bound the second-layer covering number for GSP auctions space with reserve price, which is shown as below. Theorem 3.7. For GSP auctions with reserve price, the second layer covering number can be bounded by the pseudodimension (P-dim) of the reserve price function class. To be specific, we have:\nN1(ǫ ′, L◦Aǫi , T2) ≤ (eT2K/ǫ ′)16|B|P-dim(A),∀T2 > 4|B|P-dim(A).\nCombine Theorem 3.6 and Theorem 3.7, we get a total error bound for GTML applied to GSP auctions with reserve price in the following theorem, which first gives generalization guarantees for GTML in sponsored search. Corollary 3.8. With the same assumptions in Theorem 3.5, for any ǫ > 0, for GTML applied to GSP auctions with reserve price, we have the following generalization error bound: P (R(âT2 ,M ∗)−R(a∗,M∗) ≥ ǫ)\n≤O(e−T1) +O\n(\nNdA(δ,A) ( T 16|B|P-dim(A) 2 e\n−T s\n1+s 2 + T\ns−γ 1+s\n2\n)\n)\n,\nwhere s ∈ (0, γ), and δ ∈ (0, ǫ/Kα)."
    }, {
      "heading" : "4 Conclusion and Future Work",
      "text" : "In this paper, we have given a formal generalization analysis to the game-theoretic machine learning (GTML) framework, which involves a bi-level ERM learning process (i.e., mechanism learning and behavior learning). The challenges of generalization analysis for GTML lies in the dependency between the behavior data and the mechanism. To tackle the challenge, we first bound the error of behavior learning by leveraging the Hoeffding inequality for Markov Chains, and then introduce a new notion called nested covering number and bound the errors of mechanism learning on its basis. Our theoretical analysis not only enriches the understanding on machine learning algorithms in complicated dynamic systems with multi-party interactions, but also provides some practical algorithmic guidance to mechanism design for these systems. As for future work, we would also like to extend the idea of δ-sample sharing and apply it to improve the mechanisms in other real-world applications, such as mobile apps and social networks.\nNotation Meaning U ,B,H Spaces of user need/feedback, agent\nbehaviors, and signals A, dA mechanism space and the distance on it ut, b a t , ht at the t-th time period, under mech-\nanism a, the users need/feedback, agents’ behavior, and the signal\nMh(·, ·) transition probability matrix of agents behavior under signal h\nM̂T1 ,M ∗ the learned behavior model and the true\nbehavior model âT2 , a\n∗ the learned mechanism and the optimal mechanism\nL(a, b, u) loss function s(a, δ) a mechanism that is equal to a\nor another mechanism satisfying dA(a, s(a, δ)) ≤ δ\nπ(a,M) stationary distribution of the process (bat , ut) with behavior model M RT2 (a, M̂T1 , δ) empirical risk of mechanism a with behavior model M̂T1 by δ-sample sharing technique\nR(a,M∗) expected risk of mechanism a with the true behavior model M∗ dTV (a, a ′) (induced) total variance distance on\nmechanism space A NdA(δ,A),NTV (ǫ,A) covering number of mechanism space\nA under distance dA and (induced) total variance distance dTV\nAǫi the ǫ-partition of mechanism space A according to its first layer cover L ◦ Aǫi the loss function class in each partition N1(ǫ, L ◦ A ǫ i , T2) covering number for the function class\nL ◦ Aǫi under l1 distance β(a,m) beta mixing rate of Markov chain\n{(bat , ut) : t ≥ 1}\nTable 1: Notations"
    } ],
    "references" : [ {
      "title" : "P",
      "author" : [ "M. Anthony", "Bartlett" ],
      "venue" : "L.",
      "citeRegEx" : "Anthony and Bartlett 2009",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "and Kleinberg",
      "author" : [ "D. Easley" ],
      "venue" : "J.",
      "citeRegEx" : "Easley and Kleinberg 2010",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Internet advertising and the generalized second price auction: Selling billions of dollars worth of keywords",
      "author" : [ "Ostrovsky Edelman", "B. Schwarz 2005] Edelman", "M. Ostrovsky", "M. Schwarz" ],
      "venue" : null,
      "citeRegEx" : "Edelman et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Edelman et al\\.",
      "year" : 2005
    }, {
      "title" : "and Ormoneit",
      "author" : [ "P.W. Glynn" ],
      "venue" : "D.",
      "citeRegEx" : "Glynn and Ormoneit 2002",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "A game-theoretic machine learning approach for revenue maximization in sponsored search",
      "author" : [ "He" ],
      "venue" : "In Proceedings of the Twenty-Third International Joint Conference on Artificial Intelligence",
      "citeRegEx" : "He,? \\Q2013\\E",
      "shortCiteRegEx" : "He",
      "year" : 2013
    }, {
      "title" : "A game-theoretic machine learning approach for revenue maximization in sponsored search. CoRR abs/1406.0728",
      "author" : [ "He" ],
      "venue" : null,
      "citeRegEx" : "He,? \\Q2014\\E",
      "shortCiteRegEx" : "He",
      "year" : 2014
    }, {
      "title" : "D",
      "author" : [ "S. Lahaie", "Pennock" ],
      "venue" : "M.",
      "citeRegEx" : "Lahaie and Pennock 2007",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "and Mohri",
      "author" : [ "A.M. Medina" ],
      "venue" : "M.",
      "citeRegEx" : "Medina and Mohri 2014",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "A",
      "author" : [ "Mitrophanov" ],
      "venue" : "Y.",
      "citeRegEx" : "Mitrophanov 2005",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Optimizing relevance and revenue in ad search: a query substitution approach",
      "author" : [ "Radlinski" ],
      "venue" : "In Proceedings of the 31st annual international ACM SIGIR conference on Research and development",
      "citeRegEx" : "Radlinski,? \\Q2008\\E",
      "shortCiteRegEx" : "Radlinski",
      "year" : 2008
    }, {
      "title" : "Agent behavior prediction and its generalization analysis",
      "author" : [ "Tian" ],
      "venue" : "In Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence",
      "citeRegEx" : "Tian,? \\Q2014\\E",
      "shortCiteRegEx" : "Tian",
      "year" : 2014
    }, {
      "title" : "Rates of convergence for empirical processes of stationary mixing sequences. The Annals of Probability 94–116",
      "author" : [ "B. Yu" ],
      "venue" : null,
      "citeRegEx" : "Yu,? \\Q1994\\E",
      "shortCiteRegEx" : "Yu",
      "year" : 1994
    }, {
      "title" : "Revenue optimization with relevance constraint in sponsored search",
      "author" : [ "Zhu" ],
      "venue" : "In Proceedings of the Third International Workshop on Data Mining and Audience Intelligence for Advertising,",
      "citeRegEx" : "Zhu,? \\Q2009\\E",
      "shortCiteRegEx" : "Zhu",
      "year" : 2009
    }, {
      "title" : "Optimizing search engine revenue in sponsored search",
      "author" : [ "Zhu" ],
      "venue" : "In Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval,",
      "citeRegEx" : "Zhu,? \\Q2009\\E",
      "shortCiteRegEx" : "Zhu",
      "year" : 2009
    } ],
    "referenceMentions" : [ ],
    "year" : 2014,
    "abstractText" : "For Internet applications like sponsored search, cautions need to be taken when using machine learning to optimize their mechanisms (e.g., auction) since selfinterested agents in these applications may change their behaviors (and thus the data distribution) in response to the mechanisms. To tackle this problem, a framework called game-theoretic machine learning (GTML) was recently proposed, which first learns a Markov behavior model to characterize agents behaviors, and then learns the optimal mechanism by simulating agents’ behavior changes in response to the mechanism. While GTML has demonstrated practical success, its generalization analysis is challenging because the behavior data are non-i.i.d. and dependent on the mechanism. To address this challenge, first, we decompose the generalization error for GTML into the behavior learning error and the mechanism learning error; second, for the behavior learning error, we obtain novel non-asymptotic error bounds for both parametric and non-parametric behavior learning methods; third, for the mechanism learning error, we derive a uniform convergence bound based on a new concept called nested covering number of the mechanism space and the generalization analysis techniques developed for mixing sequences. To the best of our knowledge, this is the first work on the generalization analysis of GTML, and we believe it has general implications to the theoretical analysis of other complicated machine learning problems.",
    "creator" : "LaTeX with hyperref package"
  }
}