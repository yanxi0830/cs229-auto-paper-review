{
  "name" : "1404.4171.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Dropout Training for Support Vector Machines",
    "authors" : [ "Ning Chen", "Jun Zhu", "Jianfei Chen", "Bo Zhang" ],
    "emails" : [ "{ningchen@mail,", "dcszj@mail,", "chenjf10@mails,", "dcszb@mail}.tsinghua.edu.cn" ],
    "sections" : [ {
      "heading" : "Introduction",
      "text" : "Artificial feature noising augments the finite training data with an infinite number of corrupted versions, by corrupting the given training examples with a fixed noise distribution. Among the many noising schemes, dropout training (Hinton et al. 2012) is an effective way to control over-fitting by randomly omitting subsets of features at each iteration of a training procedure. By formulating the feature noising methods as minimizing the expectation of some loss functions under the corrupting distributions, recent work has provided theoretical understandings of such schemes from the perspective of adaptive regularization (Wager, Wang, and Liang 2013); and has shown promising empirical results in various applications, including document classification (van der Maaten et al. 2013; Wager, Wang, and Liang 2013), named entity recognition (Wang et al. 2013), and image classification (Wang and Manning 2013).\nRegarding the loss functions, though much work has been done on the quadratic loss, logistic loss, or the log-loss\nCopyright c© 2014, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\ninduced from a generalized linear model (GLM) (van der Maaten et al. 2013; Wager, Wang, and Liang 2013; Wang et al. 2013), little work has been done on the margin-based hinge loss underlying the very successful support vector machines (SVMs) (Vapnik 1995). One technical challenge is that the non-smoothness of the hinge loss makes it hard to compute or even approximate its expectation under a given corrupting distribution. Existing methods are not directly applicable, therefore calling for new solutions. This paper attempts to address this challenge and fill up the gap by extending dropout training as well as other feature noising schemes to support vector machines.\nPrevious efforts on learning SVMs with feature noising have been devoted to either explicit corruption or an adversarial worst-case analysis. For example, virtual support vector machines (Burges and Scholkopf 1997) explicitly augment the training data, which are usually support vectors from previous learning iterations for computational efficiency, with additional examples that are corrupted through some invariant transformation models. A standard SVM is then learned on the corrupted data. Though simple and effective, such an approach lacks elegance and the computational cost of processing the additional corrupted examples could be prohibitive for many applications. The other work (Globerson and Roweis 2006; Dekel and Shamir 2008; Teo et al. 2008) adopts an adversarial worst-case analysis to improve the robustness of SVMs against feature deletion in testing data. Though rigorous in theory, a worst-case scenario is unlikely to be encountered in practice. Moreover, the worst-case analysis usually results in solving a complex and computationally demanding problem.\nIn this paper, we show that it is efficient to train linear SVM predictors on an infinite amount of corrupted copies of the training data by marginalizing out the corruption distributions, an average-case analysis. We concentrate on dropout training, but the results are directly applicable to other noising models, such as Gaussian, Poisson and Laplace (van der Maaten et al. 2013). For all these noising schemes, the resulting expected hinge loss can be upperbounded by a variational objective by introducing auxiliary variables, which follow a generalized inverse Gaussian distribution. We then develop an iteratively re-weighted least square (IRLS) algorithm to minimize the variational bounds. At each iteration, our algorithm minimizes the ex-\nar X\niv :1\n40 4.\n41 71\nv1 [\ncs .L\nG ]\n1 6\nA pr\n2 01\n4\npectation of a re-weighted quadratic loss under the given corrupting distribution, where the re-weights are computed in a simple closed form. We further apply the similar ideas to develop a new IRLS algorithm for the dropout training of logistic regression, which extends the well-known IRLS algorithm for standard logistic regression (Hastie, Tibshirani, and Friedman 2009). Our IRLS algorithms shed light on the connection and difference between the hinge loss and logistic loss in the context of dropout training, complementing to the previous analysis (Rosasco et al. 2004; Globerson et al. 2007) in the supervised learning settings. Finally, empirical results on classification and a challenging “nightmare at test time” scenario (Globerson and Roweis 2006) demonstrate the effectiveness of our approaches, in comparison with various strong competitors."
    }, {
      "heading" : "Preliminaries",
      "text" : "We setup the problem in question and review the learning with marginalized corrupted features."
    }, {
      "heading" : "Regularized loss minimization",
      "text" : "Consider the binary classification, where each training example is a pair (x, y) with x ∈ RD being an input feature vector and y ∈ {+1,−1} being a binary label. Given a set of training data D = {(xn, yn)}Nn=1, supervised learning aims to find a function f ∈ F that maps each input to a label. To find the optimal candidate, it commonly solves a regularized loss minimization problem\nmin f∈F\nΩ(f) + 2c · R(D; f), (1)\nwhereR(D; f) is the risk of applying f to the training data; Ω(f) is a regularization term to control over-fitting; and c is a non-negative regularization parameter.\nFor linear models, the function f is simply parameterized as f(x;w, b) = w>x+ b, where w is the weight vector and b is an offset. We will denote θ := {w, b} for clarity. Then, the regularization can be any Euclidean norms1, e.g., the `2-norm, Ω(w) = ‖w‖22, or the `1-norm, Ω(w) = ‖w‖1. For the loss functions, the most relevant measure is the training error, ∑N n=1 δ(f(xn;θ) 6= yn), which however is not easy to optimize. A convex surrogate loss is used instead, which normally upper bounds the training error. Two popular examples are the hinge loss and logistic loss2:\nRh(D;θ) = N∑\nn=1\nmax (0, `− ynf(xn;θ)) ,\nRl(D;θ) = N∑\nn=1\n(− log p(yn|xn,θ)) ,\nwhere `(> 0) is the cost of making a wrong prediction, and p(yn|xn,θ) := 1/(1 + exp(−ynf(xn;θ))) is the logistic likelihood. Other losses include the quadratic loss,\n∑N n=1(f(xn;θ) − yn)2, and the exponential loss,∑N\nn=1 exp(−ynf(xn;θ)), whose feature noising analyses are relatively simpler (van der Maaten et al. 2013).\n1It is a common practice to not regularize the offset. 2The natural logarithm is not an upper bound of the training\nerror. We can simply change the base without affecting learning.\nLearning with marginalized corruption Let x̃ be the corrupted version of the input features x. Consider the commonly used independent corrupting model:\np(x̃|x) = D∏ d=1 p(x̃d|xd; ηd),\nwhere each individual distribution is a member of the exponential family, with the natural parameter ηd. Another common assumption is that the corrupting distribution is unbiased, that is, Ep[x̃|x] = x, where we use Ep[·] := Ep(x̃|x)[·] to denote the expectation taken over the corrupting distribution p(x̃|x). Such examples include the unbiased blankout (or dropout) noise, Gaussian noise, Laplace noise, and Poisson noise (Vincent et al. 2008; van der Maaten et al. 2013).\nFor the explicit corruption in (Burges and Scholkopf 1997), each example (xn, yn) is corrupted M times from the corrupting model p(x̃n|xn), resulting in the corrupted examples (x̃nm, yn), m ∈ [M ]. This procedure generates a new corrupted data set D̃ with a larger size of NM . The generated dataset can be trained by minimizing the average loss function over M corrupted data points:\nL(D̃;θ) = N∑\nn=1\n1\nM M∑ m=1 R(x̃nm, yn;θ), (2)\nwhere R(x, y;θ) is the loss function of the model incurred on the training example (x, y). As L(D̃;θ) scales linearly with the number of corrupted observations, this approach may suffer from high computational costs.\nDropout training adopts the strategy of implicit corruption, which learns the model with marginalized corrupted features by minimizing the expectation of a loss function under the corrupting distribution\nL(D;θ) = N∑\nn=1\nEp[R(x̃n, yn;θ)]. (3)\nThe objective can be seen as a limit case of (2) when M → ∞, by the law of large numbers. Such an expectation scheme has been widely adopted in previous work (Wager, Wang, and Liang 2013; van der Maaten et al. 2013; Wang et al. 2013; Wang and Manning 2013).\nThe choice of the loss function R in (3) can make a significant difference, in terms of computation cost and prediction accuracy. Previous work on feature noising has covered the quadratic loss, exponential loss, logistic loss, and the loss induced from generalized linear models (GLM). For the quadratic loss and exponential loss, the expectation in Eq. (3) can be computed analytically, thereby leading to simple gradient descent algorithms (van der Maaten et al. 2013). However, it does not have a closed form to compute the expectation for the logistic loss or the GLM loss. Previous analysis has resorted to approximation methods, such as using the second-order Taylor expansion (Wager, Wang, and Liang 2013) or an upper bound by applying Jensen’s inequality (van der Maaten et al. 2013), both of which lead to effective algorithms in practice. In contrast, little work has been done on the hinge loss, for which the expectation under corrupting distributions cannot be analytically computed either, therefore calling for new algorithms."
    }, {
      "heading" : "Learning SVMs with Corrupting Noise",
      "text" : "We now present a simple iteratively re-weighted least square (IRLS) algorithm to learn SVMs with the expected hinge loss under corrupting distributions. Our method consists of a variational upper bound of the expected loss and a simple algorithm that iteratively minimizes an expectation of a re-weighted quadratic loss. We also apply the similar ideas to develop a simple IRLS algorithm for minimizing the expected logistic loss, thereby allowing for a systematical comparison of the hinge loss with the logistic and quadratic losses in the context of feature noising."
    }, {
      "heading" : "A variational bound with data augmentation",
      "text" : "Let ζn := ` − yn(w>x̃n).3 Then, the expected hinge loss can be written as\nRh(D;θ) = N∑ n=1 Ep[max (0, ζn)], (4)\nSince we do not have a closed-form of the expectation of the max function, minimizing the expected loss (4) is intractable. Here, we derive a variational upper bound based on a data augmentation formulation of the expected hinge loss. Let φ(yn|x̃n,θ) = exp{−2cmax(0, ζn)} be the pseudo-likelihood of the response variable for sample n. Then we have\nRh(D;θ) = − 1\n2c ∑ n Ep[log φ(yn|x̃n,θ)]. (5)\nUsing the ideas of data augmentation (Polson and Scott 2011; Zhu et al. 2014), the pseudo-likelihood can be expressed as\nφ(yn|x̃n,θ) = ∫ ∞ 0 1√ 2πλn exp { − (λn + cζn) 2 2λn } dλn, (6)\nwhere λn, n ∈ [N ], is the augmented variable. Using (6) and Jensen’s inequality, we can derive a variational upper bound L of the expected hinge loss as\nL(θ, q(λ)) = N∑ n=1 { −H(λn) + 1 2 Eq[log λn] (7)\n+ Eq [ 1\n2λn Ep(λn + cζn)2\n]} + constant,\nwhere H(λn) is the entropy of the variational distribution q(λn); q(λ) := ∏ n q(λn) is joint distribution; and we have defined Eq[·] := Eq(λ)[·] to denote the expectation taken over a variational distribution q. Now, our variational optimization problem is\nmin θ,q(λ)∈P\n‖w‖22 + L(θ, q(λ)), (8)\nwhere P is the simplex space of normalized distributions. We should note that when there is no feature noise (i.e., x̃ = x), the bound is tight and we are learning the standard SVM classifier. Please see Appendix A for the derivation. We will empirically compare with SVM in experiments.\n3We treat the offset b implicitly by augmenting xn and x̃n with one dimension of deterministic 1. More details will be given in the algorithm."
    }, {
      "heading" : "Iteratively Re-weighted Least Square Algorithm",
      "text" : "In the upper bound, we note that when the variational distribution q(λ) is given, the term Ep[(λn+cζn)2] is an expectation of a quadratic loss, which can be analytically computed. We leverage such a nice property and develop a coordinate descent algorithm to solve problem (8). Our algorithm iteratively solves the following two steps, analogous to the common two-step procedure of a variational EM algorithm.\nFor q(λ) (i.e., E-step): infer the variational distribution q(λ). Specifically, optimize L over q(λ), we get:\nq(λn)∝ 1√ λn exp\n{ −1\n2\n( λn +\nc2Ep[ζ2n] λn )} ∼GIG ( λn; 1\n2 , 1, c2Ep[ζ2n]\n) , (9)\nwhere the second-order expectation is Ep[ζ2n] = w>(Ep[x̃n]Ep[x̃n]> + Vp[x̃n])w −2`ynw>Ep[x̃n] + `2; (10) and Vp[x̃n] is a D ×D diagonal matrix with the dth diagonal element being the variance of x̃nd, under the corrupting distribution p(x̃n|xn). We have denoted GIG(x; p, a, b) ∝ xp−1 exp(− 12 ( b x + ax)) as a generalized inverse Gaussian distribution. Thus, λ−1n follows an inverse Gaussian distribution\nq(λ−1n |x̃n,θ) ∼ IG ( λ−1n ;\n1 c √ E[ζ2n] , 1\n) (11)\nFor θ := w (i.e., M-step): removing irrelevant terms, this step involves minimizing the following objective:\nL[θ] = ‖w‖22 + N∑ n=1 Ep [ cζn + c2 2 γnζ 2 n ] , (12)\nwhere γn := Eq[λ−1n ]. We observe that this substep is equivalent to minimizing the expectation of a re-weighted quadratic loss, as summarized in Lemma 1, whose proof is deferred to Appendix B, for brevity. Lemma 1. Given q(λ), the M-step minimizes the reweighted quadratic loss (with the `2-norm regularizer):\n‖w‖22 + c2\n2 ∑ n γnEp [ (w>x̃n − yhn)2 ] , (13)\nwhere yhn = (` + 1 cγn )yn is the re-weighted label, and the re-weights are computed in closed-form:\nγn := Eq[λ−1n ] = 1 c √ Ep[ζ2n] . (14)\nFor low-dimensional data, we can solve for the closed form solutions by doing matrix inversion. Specifically, optimizing L[θ] over w, we get4:\nw =\n( 2\nc2 I + N∑ n=1 γn(Ep[x̃nx̃>n ]) )−1( N∑ n=1 γny h nEp[x̃n] ) ,\n4To consider offset, we simply augment x and x̃ with an additional unit of 1. The variance Vp[x̃n] is augmented accordingly. The identity matrix I is augmented by adding one zero row and one zero column.\nwhere Ep[x̃nx̃>n ] = Ep[x̃n]Ep[x̃n]> + Vp[x̃n]. However, if the data are in a high-dimensional space, e.g., text documents, the above matrix inversion will be computationally expensive. In such cases, we can use numerical methods, e.g., the quasi-Newton method.\nTo summarize, our algorithm iteratively minimizes the expectation of a simple re-weighted quadratic loss under the given corrupting distribution, where the re-weights γn are computed in an analytic form. Therefore, it is an extension of the classical iteratively re-weighted least square (IRLS) algorithm (Hastie, Tibshirani, and Friedman 2009) for dropout training. We also observe that if we fix γn at 1c and set ` = 0, we are minimizing the quadratic loss under the corrupting distribution, as studied in (van der Maaten et al. 2013). We will empirically show that our iterative algorithm for the expected hinge-loss will consistently improve over the standard quadratic loss by adaptively updating γn. Finally, as we assume that the corrupting distribution is unbiased, i.e., Ep[x̃|x] = x, we only need to compute the variance of the corrupting distribution, which is easy for all the existing exponential family distributions. An overview of the variance of the commonly used corrupting distributions can be found in (van der Maaten et al. 2013).\nAn IRLS algorithm for the logistic Loss We now extend the above ideas to develop a new IRLS algorithm for the logistic-loss, which also minimizes the expectation of a re-weighted quadratic loss under the corrupting distribution and computes the re-weights analytically.\nLet ωn := w>x̃n. Then the expected logistic loss under a corrupting distribution is\nRl(D;w) = − N∑ n=1 Ep [ log ( eynωn 1 + eynωn )] . (15)\nAgain since the expectation cannot be computed in closedform, we derive a variational bound as a surrogate. Specifically, let ψ(yn|x̃n,w) = pc(yn|x̃n,w) = e cynωn\n(1+eynωn )c be the pseudo-likelihood of the response variable for sample n. We have Rl(D;w) = − 1c ∑ n Ep[logψ(yn|x̃n,w)]. Using the recent work of data augmentation (Polson, Scott, and Windle 2012; Chen et al. 2013), the pseudo-likelihood can be expressed as\nψ(yn|x̃n,w) = 1\n2c eκnωn ∫ ∞ 0 e− λn(ynωn) 2 2 p(λn)dλn, (16)\nwhere κn := c2yn and λn is the augmented Polya-gamma variable, p(λn) ∼ PG(λn; c, 0). Using (16), we can derive the upper bound of the expected logistic loss: L′(w, q(λ)) = N∑ n=1 {1 2 Eq[λn]Ep[ω2n]−H(λn) (17)\n−Eq[log p(λn)]− c\n2 ynEp[ωn]\n} + constant,\nand get the variational optimization problem\nmin w,q(λ)∈P\n‖w‖22 + L′(w, q(λ)), (18)\nwhere q(λ) is the variational distribution We solve the variational problem with a coordinate descent algorithm as follows: For q(λ) (i.e., E-step): optimizingL′ over q(λ), we have:\nq(λn)∝ exp ( −1\n2 λnEp[ω2n]\n) p(λn|c, 0)\n∼PG ( λn; c, √ Ep[ω2n] ) (19)\na Polya-Gamma distribution (Polson, Scott, and Windle 2012), where Ep[ω2n] = w>(Ep[x̃n]Ep[x̃n]> + Vp[x̃n])w.\nFor w (i.e., M-step): removing irrelevant terms, this step minimizes the objective\nL′[w] = ‖w‖ 2 2 + N∑ n=1 1 2 Eq[λn]Ep[ω2n]− c 2 ynEp[ωn]. (20)\nWe then have the optimal solution5:\nw = ( I + 1\n2 N∑ n=1 Eq[λn]Ep[x̃nx̃>n ]\n)−1( c\n4 N∑ n=1 ynEp[x̃n]\n) .\nThis is actually equivalent to minimizing the expectation of a re-weighted quadratic loss, as in Lemma 2. The proof is similar to that of Lemma 1 and the expectation of a Polya-Gamma distribution follows (Polson, Scott, and Windle 2012). Lemma 2. Given q(λ), the M-step minimizes the reweighted quadratic loss (with the `2-norm regularizer)\n‖w‖22 + c\n2 ∑ n γlnEp[(w>x̃n − yln)2], (21)\nwhere yln = c 2γn yn is the re-weighted label, and γln = γn c with\nγn := Eq[λn] = c 2 √ Ep[ω2n] × e √ Ep[ω2n] − 1 1 + e √ Ep[ω2n] . (22) It can be observed that if we fix γn = c2 , the IRLS algorithm reduces to minimizing the expected quadratic loss under the corrupting distribution. This is similar as in the case with SVMs, where if we set ` = 0 and fix γn = 1c , the IRLS algorithm for SVMs essentially minimizes the expected quadratic loss under the corrupting distribution. Furthermore, by sharing a similar iterative structure, our IRLS algorithms shed light on the similarity and difference between the hinge loss and the logistic loss, as summarized in Table 1. Specifically, both losses can be minimized via iteratively minimizing the expectation of a re-weighted quadratic loss, while they differ in the update rules of the weights γn and the labels yn at each iteration.\n5The offset can be similarly incorporated as in the hinge loss."
    }, {
      "heading" : "Experiments",
      "text" : "We now present empirical results on both classification and the challenging “nightmare at test time” scenario (Globerson and Roweis 2006) to demonstrate the effectiveness of the dropout training algorithm for SVMs, denoted by DropoutSVM, and the new IRLS algorithm for the dropout training of the logistic loss, denoted by Dropout-Logistic. We consider the unbiased dropout (or blankout) noise model6, that is, p(x̃ = 0) = q and p(x̃ = 11−qx) = 1 − q, where q ∈ [0, 1) is a pre-specified corruption level. The variance of this model for each dimension d is Vp[x̃d] = q1−qx 2 d."
    }, {
      "heading" : "Binary classification",
      "text" : "We first evaluate Dropout-SVM and Dropout-Logistic on binary classification tasks. We use the public Amazon book review and kitchen review datasets (Blitzer, Dredze, and Pereira 2007), which consist of the text reviews about books and kitchen, respectively. In both datasets, each document is represented as a 20,000 dimensional bag-of-words feature. The binary classification task is to distinguish whether a review content is positive or negative. Following the previous settings, we choose 2,000 documents for training and approximately 4,000 for testing.\nWe compare our methods with the methods presented in (van der Maaten et al. 2013) that minimize the quadratic loss with marginalized corrupted features (MCF), denoted by MCF-Quadratic, and that minimize the expected logistic loss, denoted by MCF-Logistic. MCF-Logistic was shown to be the state-of-the-art method for dropout training on these datasets, outperforming a wide range of competitors, including the dropout training of the exponential loss and the various loss functions with a Poisson noise model. As we have discussed, both Dropout-SVM and Dropout-Logistic iteratively minimize the expectation of a re-weighted quadratic loss, with the re-weights updated in closed-form. We include MCF-Quadratic as a baseline to demonstrate the effectiveness of our methods on adaptively tuning the re-weights to get improved results. We implement both Dropout-SVM and Dropout-Logistic using C++, and solve the re-weighted least square problems using L-BFGS methods (Liu and Nocedal 1989), which are very efficient by exploring the sparsity of bag-of-words features when computing gradients7.\nFigure 1 shows classification errors, where the results of MCF-Logistic and MCF-Quadratic are cited from (van der Maaten et al. 2013). We can see that on both datasets, Dropout-SVM and Dropout-Logistic generally outperform MCF-Quadratic except when the dropout level is larger than 0.9. In the meanwhile, the proposed two models give comparable results with (a bit better than on the kitchen dataset) the state-of-art MCF-Logistic which means that dropout training on SVMs is an effective strategy for binary classifica-\n6Other noising models (e.g., Poisson) were shown to perform worse than the dropout model (van der Maaten et al. 2013). We have similar observations for Dropout-SVM and the new IRLS algorithm for logistic regression.\n7We don’t compare time with MCF methods, whose implementation are in Matlab (http://homepage.tudelft.nl/19j49/mcf/ Marginalized Corrupted Features.html), slower than ours.\ntion. Finally, by noting that Dropout-SVM reduces to the standard SVM when the corruption level q is zero, we can see that dropout training can significantly boost the classification performance for the simple linear SVMs."
    }, {
      "heading" : "Dropout-SVM vs. Explicit corruption",
      "text" : "Figure 2 shows the classification errors on the Amazonbooks dataset when a SVM classifier is trained using the explicit corruption strategy as in Eq. (2). We change the number of corrupted copies (i.e., M ) from 1 to 256. Following the previous setups (van der Maaten et al. 2013), for each value of M we choose the dropout model with q selected by cross-validation. The hyper-parameter of the SVM classifier is also chosen via cross-validation on the training data. We can observe a clear trend that the error decreases when the training set contains more corrupted versions of the original training data, i.e., M gets larger in Eq. (2). It also shows that the best performance is obtained when M approaches infinity, which is equivalent to our Dropout-SVM."
    }, {
      "heading" : "Multi-class classification",
      "text" : "We also evaluate our methods on multiclass classification tasks. We choose the CIFAR-10 image categorization dataset8. The CIFAR-10 dataset is the subset of the 80 million tiny images (Torralba, Fergus, and Freeman 2008). It consists of 10 classes of 32× 32 tiny images. We follow the\n8http://www.cs.toronto.edu/∼kriz/cifar.html\nexperimental setup of the previous work (Krizhevsky 2009; van der Maaten et al. 2013) and represent each image as a 8,192 dimensional feature descriptor. We use the same 50,000 images for training and 10,000 for testing. There are various approaches to applying the binary Dropout-SVM and Dropout-Logistic to multiclass classification, including “one-vs-all” and “one-vs-one” strategies. Here we choose “one-vs-all”, which has shown effectiveness in many applications (Rifkin and Klautau 2004). The hyper-parameters are selected via cross-validation on the training set.\nTable 2 presents the results, where the results of quadratic loss and logistic loss under the MCF learning setting9 are cited from (van der Maaten et al. 2013). We also report the results using Poisson noise. We can see that all the methods (except for the quadratic loss) can significantly boost the performance by adopting dropout training; meanwhile both Dropout-SVM and Dropout-Logistic are competitive, in fact achieving comparable performance as the state-ofthe-art method (i.e., MCF-Logistic) under the dropout training setting. Finally, the Poisson corruption model is slightly worse than the dropout noise, consistent with the previous observations (van der Maaten et al. 2013)."
    }, {
      "heading" : "Nightmare at test time",
      "text" : "Finally, we evaluate our methods under the “nightmare at test time” (Globerson and Roweis 2006) supervised learning scenario, where some input features that were present when building the classifiers may “die” or be deleted at testing time. In such a scenario, it is crucial to design algorithms that do not assign too much weight to any single feature during testing, no matter how informative it may seem at training. Previous work has conducted the worst-case analysis as well as the learning with marginalized corrupted features. We take this scenario to test the robustness of our dropout training algorithms for both SVM and logistic regression.\nWe follow the setup of (van der Maaten et al. 2013). Specifically, we choose the the MNIST dataset, which consists of 60,000 training and 10,000 testing handwritten digital images from 10 categories (i.e., 0, · · · , 9). The images are represented by 28×28 pixels which results in the feature dimension of 784. We train the models on the full training set, and evaluate the performance on different versions of test set in which a certain level of the features are randomly dropped out, i.e., set to zero. We compare the performance of our dropout learning algorithms with the state-of-art MCFpredictors that use the logistic loss and quadratic loss. These two models also show the state-of-art performance on the same task to the best of our knowledge. We also compare\n9The exponential loss was shown to be worse; thus omitted.\nwith FDROP (Globerson and Roweis 2006), which is a stateof-the-art algorithm for the “nightmare at test time” setting that minimizes the hinge loss under an adversarial worstcase analysis. During training, we choose the best models over different dropout levels via cross-validation. For both Dropout-SVM and Dropout-Logistic, we adopt the “one-vsall” strategy as above for the multiclass classification task.\nFigure 3 shows the classification errors of different methods as a function of the random deletion percentage of features at the testing time. Following previous settings, for each deletion percentage, we use a small validation set with the same deletion level to determine the regularization parameters and the dropout level q on the whole training data. From the results, we can see that the proposed DropoutSVM is consistently more robust than all the other competitors, including the two methods to minimize the expected logistic-loss, especially when the feature deletion percentage is high (e.g., > 50%). Comparing with the standard SVM (i.e., the method Hinge-L2) and the worst-case analysis of hinge loss (i.e., Hinge-FDROP), Dropout-SVM consistently boosts the performance when the deletion ratio is greater than 10%. As expected, Dropout-SVM also significantly outperforms the MCF method with a quadratic loss (i.e., MCF-Quadratic), which is a special case of DropoutSVM as shown in our theory. Finally, we also note that our iterative algorithm for the logistic-loss works slightly better than the previous algorithm (i.e., MCF-Logistic) when the deletion ratio is larger than 50%."
    }, {
      "heading" : "Conclusions",
      "text" : "We present dropout training for SVMs, with an iteratively re-weighted least square (IRLS) algorithm by using data augmentation techniques. Similar ideas are applied to develop a new IRLS algorithm for the dropout training of logistic regression. Our IRLS algorithms provide insights on the connection and difference among various losses in dropout learning settings. Empirical results on various tasks demonstrate the effectiveness of our approaches.\nFor future work, it is remained open whether the kernel trick can be incorporated in dropout learning. We are also interested in developing more efficient algorithms, e.g., online dropout learning, to deal with even larger datasets, and investigating whether Dropout-SVM can be incorporated into\na deep learning architecture or learning with latent structures (Zhu et al. 2014)."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work is supported by National Key Project for Basic Research of China (Grant Nos: 2013CB329403, 2012CB316301), National Natural Science Foundation of China (Nos: 61305066, 61322308, 61332007), Tsinghua Self-innovation Project (Grant Nos: 20121088071) and China Postdoctoral Science Foundation Grant (Grant Nos: 2013T60117, 2012M520281)."
    } ],
    "references" : [ {
      "title" : "Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification",
      "author" : [ "Dredze Blitzer", "J. Pereira 2007] Blitzer", "M. Dredze", "F. Pereira" ],
      "venue" : "In Association of Computational Linguistics",
      "citeRegEx" : "Blitzer et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Blitzer et al\\.",
      "year" : 2007
    }, {
      "title" : "Improving the accuracy and speed of support vector machiens",
      "author" : [ "Burges", "C. Scholkopf 1997] Burges", "B. Scholkopf" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Burges et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Burges et al\\.",
      "year" : 1997
    }, {
      "title" : "Generalized relational topic models with data augmentation",
      "author" : [ "Chen" ],
      "venue" : "In International Joint Conference on Artificial Intelligence",
      "citeRegEx" : "Chen,? \\Q2013\\E",
      "shortCiteRegEx" : "Chen",
      "year" : 2013
    }, {
      "title" : "Learning to classify with missing and corrpted features",
      "author" : [ "Dekel", "O. Shamir 2008] Dekel", "O. Shamir" ],
      "venue" : "In International Conference on Machine Learning",
      "citeRegEx" : "Dekel et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Dekel et al\\.",
      "year" : 2008
    }, {
      "title" : "Nightmare at test time: Robust learning by feature deletion",
      "author" : [ "Globerson", "A. Roweis 2006] Globerson", "S. Roweis" ],
      "venue" : "In International Conference on Machine Learning",
      "citeRegEx" : "Globerson et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Globerson et al\\.",
      "year" : 2006
    }, {
      "title" : "Exponentiated gradient algorithms for log-linear structured prediction",
      "author" : [ "Globerson" ],
      "venue" : null,
      "citeRegEx" : "Globerson,? \\Q2007\\E",
      "shortCiteRegEx" : "Globerson",
      "year" : 2007
    }, {
      "title" : "The elements of statistical learning: data mining",
      "author" : [ "Tibshirani Hastie", "T. Friedman 2009] Hastie", "R. Tibshirani", "J. Friedman" ],
      "venue" : null,
      "citeRegEx" : "Hastie et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Hastie et al\\.",
      "year" : 2009
    }, {
      "title" : "Improving neural networks by preventing co-adaptation of feature detectors. arXiv:1207.0580v1, preprint",
      "author" : [ "Hinton" ],
      "venue" : null,
      "citeRegEx" : "Hinton,? \\Q2012\\E",
      "shortCiteRegEx" : "Hinton",
      "year" : 2012
    }, {
      "title" : "On the limited memory BFGS method for large scale optimization",
      "author" : [ "Liu", "D.C. Nocedal 1989] Liu", "J. Nocedal" ],
      "venue" : "Mathematical Programming",
      "citeRegEx" : "Liu et al\\.,? \\Q1989\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 1989
    }, {
      "title" : "Data Augmentation for Support Vector Machines. Bayesian Analysis 6(1):1–24",
      "author" : [ "Polson", "N.G. Scott 2011] Polson", "S.L. Scott" ],
      "venue" : null,
      "citeRegEx" : "Polson et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Polson et al\\.",
      "year" : 2011
    }, {
      "title" : "Bayesian Inference for Logistic Models using Polya-Gamma Latent Variables. arXiv:1205.0310v1",
      "author" : [ "Scott Polson", "N.G. Windle 2012] Polson", "J.G. Scott", "J. Windle" ],
      "venue" : null,
      "citeRegEx" : "Polson et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Polson et al\\.",
      "year" : 2012
    }, {
      "title" : "In defense of one-vs-all classification",
      "author" : [ "Rifkin", "R. Klautau 2004] Rifkin", "A. Klautau" ],
      "venue" : "Journal of Machine Learning Research",
      "citeRegEx" : "Rifkin et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Rifkin et al\\.",
      "year" : 2004
    }, {
      "title" : "Are loss functions all the same? Neural Computation 16(5):1063–1076",
      "author" : [ "Rosasco" ],
      "venue" : null,
      "citeRegEx" : "Rosasco,? \\Q2004\\E",
      "shortCiteRegEx" : "Rosasco",
      "year" : 2004
    }, {
      "title" : "Convex learning with invariances",
      "author" : [ "Teo" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Teo,? \\Q2008\\E",
      "shortCiteRegEx" : "Teo",
      "year" : 2008
    }, {
      "title" : "A large dataset for nonparametric object and scene recognition",
      "author" : [ "Fergus Torralba", "A. Freeman 2008] Torralba", "R. Fergus", "W. Freeman" ],
      "venue" : "IEEE Transaction on Pattern Analysis and Machine Intelligence",
      "citeRegEx" : "Torralba et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Torralba et al\\.",
      "year" : 2008
    }, {
      "title" : "Learning with marginalized corrupted features",
      "author" : [ "van der Maaten" ],
      "venue" : "In International Conference on Machine Learning",
      "citeRegEx" : "Maaten,? \\Q2013\\E",
      "shortCiteRegEx" : "Maaten",
      "year" : 2013
    }, {
      "title" : "Extracting and composing robust features with denoising autoencoders",
      "author" : [ "Vincent" ],
      "venue" : "In International Conference on Machine Learning",
      "citeRegEx" : "Vincent,? \\Q2008\\E",
      "shortCiteRegEx" : "Vincent",
      "year" : 2008
    }, {
      "title" : "Dropout training as adaptive regularization",
      "author" : [ "Wang Wager", "S. Liang 2013] Wager", "S. Wang", "P. Liang" ],
      "venue" : "In Advances in Neural Information Processing",
      "citeRegEx" : "Wager et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Wager et al\\.",
      "year" : 2013
    }, {
      "title" : "Fast dropout training",
      "author" : [ "Wang", "S. Manning 2013] Wang", "C. Manning" ],
      "venue" : "In International Conference on Machine Learning",
      "citeRegEx" : "Wang et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2013
    }, {
      "title" : "Feature noising for log-linear structured prediction",
      "author" : [ "Wang" ],
      "venue" : "In Empirical Methods in Natural Language Processing",
      "citeRegEx" : "Wang,? \\Q2013\\E",
      "shortCiteRegEx" : "Wang",
      "year" : 2013
    }, {
      "title" : "Gibbs max-margin topic models with data augmentation",
      "author" : [ "Zhu" ],
      "venue" : "Journal of Machine Learning Research",
      "citeRegEx" : "Zhu,? \\Q2014\\E",
      "shortCiteRegEx" : "Zhu",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 18,
      "context" : "2013; Wager, Wang, and Liang 2013), named entity recognition (Wang et al. 2013), and image classification (Wang and Manning 2013).",
      "startOffset" : 61,
      "endOffset" : 79
    }, {
      "referenceID" : 18,
      "context" : "induced from a generalized linear model (GLM) (van der Maaten et al. 2013; Wager, Wang, and Liang 2013; Wang et al. 2013), little work has been done on the margin-based hinge loss underlying the very successful support vector machines (SVMs) (Vapnik 1995).",
      "startOffset" : 46,
      "endOffset" : 121
    }, {
      "referenceID" : 18,
      "context" : "Such an expectation scheme has been widely adopted in previous work (Wager, Wang, and Liang 2013; van der Maaten et al. 2013; Wang et al. 2013; Wang and Manning 2013).",
      "startOffset" : 68,
      "endOffset" : 166
    } ],
    "year" : 2014,
    "abstractText" : "Dropout and other feature noising schemes have shown promising results in controlling over-fitting by artificially corrupting the training data. Though extensive theoretical and empirical studies have been performed for generalized linear models, little work has been done for support vector machines (SVMs), one of the most successful approaches for supervised learning. This paper presents dropout training for linear SVMs. To deal with the intractable expectation of the non-smooth hinge loss under corrupting distributions, we develop an iteratively re-weighted least square (IRLS) algorithm by exploring data augmentation techniques. Our algorithm iteratively minimizes the expectation of a re-weighted least square problem, where the re-weights have closedform solutions. The similar ideas are applied to develop a new IRLS algorithm for the expected logistic loss under corrupting distributions. Our algorithms offer insights on the connection and difference between the hinge loss and logistic loss in dropout training. Empirical results on several real datasets demonstrate the effectiveness of dropout training on significantly boosting the classification accuracy of linear SVMs. Introduction Artificial feature noising augments the finite training data with an infinite number of corrupted versions, by corrupting the given training examples with a fixed noise distribution. Among the many noising schemes, dropout training (Hinton et al. 2012) is an effective way to control over-fitting by randomly omitting subsets of features at each iteration of a training procedure. By formulating the feature noising methods as minimizing the expectation of some loss functions under the corrupting distributions, recent work has provided theoretical understandings of such schemes from the perspective of adaptive regularization (Wager, Wang, and Liang 2013); and has shown promising empirical results in various applications, including document classification (van der Maaten et al. 2013; Wager, Wang, and Liang 2013), named entity recognition (Wang et al. 2013), and image classification (Wang and Manning 2013). Regarding the loss functions, though much work has been done on the quadratic loss, logistic loss, or the log-loss Copyright c © 2014, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. induced from a generalized linear model (GLM) (van der Maaten et al. 2013; Wager, Wang, and Liang 2013; Wang et al. 2013), little work has been done on the margin-based hinge loss underlying the very successful support vector machines (SVMs) (Vapnik 1995). One technical challenge is that the non-smoothness of the hinge loss makes it hard to compute or even approximate its expectation under a given corrupting distribution. Existing methods are not directly applicable, therefore calling for new solutions. This paper attempts to address this challenge and fill up the gap by extending dropout training as well as other feature noising schemes to support vector machines. Previous efforts on learning SVMs with feature noising have been devoted to either explicit corruption or an adversarial worst-case analysis. For example, virtual support vector machines (Burges and Scholkopf 1997) explicitly augment the training data, which are usually support vectors from previous learning iterations for computational efficiency, with additional examples that are corrupted through some invariant transformation models. A standard SVM is then learned on the corrupted data. Though simple and effective, such an approach lacks elegance and the computational cost of processing the additional corrupted examples could be prohibitive for many applications. The other work (Globerson and Roweis 2006; Dekel and Shamir 2008; Teo et al. 2008) adopts an adversarial worst-case analysis to improve the robustness of SVMs against feature deletion in testing data. Though rigorous in theory, a worst-case scenario is unlikely to be encountered in practice. Moreover, the worst-case analysis usually results in solving a complex and computationally demanding problem. In this paper, we show that it is efficient to train linear SVM predictors on an infinite amount of corrupted copies of the training data by marginalizing out the corruption distributions, an average-case analysis. We concentrate on dropout training, but the results are directly applicable to other noising models, such as Gaussian, Poisson and Laplace (van der Maaten et al. 2013). For all these noising schemes, the resulting expected hinge loss can be upperbounded by a variational objective by introducing auxiliary variables, which follow a generalized inverse Gaussian distribution. We then develop an iteratively re-weighted least square (IRLS) algorithm to minimize the variational bounds. At each iteration, our algorithm minimizes the exar X iv :1 40 4. 41 71 v1 [ cs .L G ] 1 6 A pr 2 01 4 pectation of a re-weighted quadratic loss under the given corrupting distribution, where the re-weights are computed in a simple closed form. We further apply the similar ideas to develop a new IRLS algorithm for the dropout training of logistic regression, which extends the well-known IRLS algorithm for standard logistic regression (Hastie, Tibshirani, and Friedman 2009). Our IRLS algorithms shed light on the connection and difference between the hinge loss and logistic loss in the context of dropout training, complementing to the previous analysis (Rosasco et al. 2004; Globerson et al. 2007) in the supervised learning settings. Finally, empirical results on classification and a challenging “nightmare at test time” scenario (Globerson and Roweis 2006) demonstrate the effectiveness of our approaches, in comparison with various strong competitors. Preliminaries We setup the problem in question and review the learning with marginalized corrupted features. Regularized loss minimization Consider the binary classification, where each training example is a pair (x, y) with x ∈ R being an input feature vector and y ∈ {+1,−1} being a binary label. Given a set of training data D = {(xn, yn)}n=1, supervised learning aims to find a function f ∈ F that maps each input to a label. To find the optimal candidate, it commonly solves a regularized loss minimization problem min f∈F Ω(f) + 2c · R(D; f), (1) whereR(D; f) is the risk of applying f to the training data; Ω(f) is a regularization term to control over-fitting; and c is a non-negative regularization parameter. For linear models, the function f is simply parameterized as f(x;w, b) = w>x+ b, where w is the weight vector and b is an offset. We will denote θ := {w, b} for clarity. Then, the regularization can be any Euclidean norms1, e.g., the `2-norm, Ω(w) = ‖w‖2, or the `1-norm, Ω(w) = ‖w‖1. For the loss functions, the most relevant measure is the training error, ∑N n=1 δ(f(xn;θ) 6= yn), which however is not easy to optimize. A convex surrogate loss is used instead, which normally upper bounds the training error. Two popular examples are the hinge loss and logistic loss2:",
    "creator" : "LaTeX with hyperref package"
  }
}