{
  "name" : "1703.11008.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Computing Nonvacuous Generalization Bounds for Deep (Stochastic) Neural Networks with Many More Parameters than Training Data",
    "authors" : [ "Gintare Karolina Dziugaite" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 3.\n11 00\n8v 1\n[ cs\n.L G\n] 3\nContents\n1. Introduction 1 2. Preliminaries 4 3. Optimizing the PAC-Bayes bound 6 4. Experiments 8 5. Results 9 6. Related work 10 7. Conclusions and Future work 13 Acknowledgments 13 References 14 A. Network symmetries 15 B. Approximating KL−1(q|c) 16"
    }, {
      "heading" : "1. Introduction",
      "text" : "By optimizing a PAC-Bayes bound, we show that it is possible to compute nonvacuous numerical bounds on the generalization error of deep stochastic neural networks with millions of parameters, despite the training data sets being one or more orders of magnitude smaller than the number of parameters.\n1\nTo our knowledge, these are the first explicit and nonvacuous numerical bounds computed in the deep learning regime where the number of network parameters eclipses the number of training examples. The bounds we compute are data dependent: indeed, even for relatively tiny neural networks, data independent bounds are vacuous (i.e., they bound the classification error by a number greater than 1). Evidently, we are operating far from the worst case.\nOur investigation was instigated by recent empirical work by Zhang, Bengio, Hardt, Recht, and Vinyals [Zha+17], who show that stochastic gradient descent (SGD), applied to deep learning architectures with millions of parameters, is:\n(1) able to achieve ≈ 0 training error on CIFAR10 and IMAGENET and still generalize (i.e., test error remains small, despite the potential for overfitting); (2) still able to achieve ≈ 0 training error even after the labels are randomized, and does so with only a small factor of additional computational time.\nTaken together, these two observations demonstrate that the network architecture has tremendous capacity to overfit and yet SGD does not abuse this capacity as it optimizes the surrogate loss, despite the lack of explicit regularization.\nIt is a major open problem to explain this phenomenon. A natural approach would be to show that, under natural conditions, SGD finds solutions that possess structural properties that we already know to be connected to generalization. However, in order to complete the logical connection, the associated learning bounds must be nonvacuous in the regime of model size / data size where we hope to explain the phenomenon.\nThis work establishes a potential candidate. Chernoff bounds on held-out data suggest our generalization bounds are loose: across a variety of network architectures, our PAC-Bayes bounds on the test error are in the range 16–22%, while Chernoff bounds on the test error based on held-out data are consistently around 3%. Despite the gap, theoreticians will likely be surprised that it is possible at all to obtain nonvacuous numerical bounds on generalization error for a model with such large capacity trained on so few training examples. While we cannot entirely explain the magnitude of generalization, we can demonstrate nontrivial generalization.\nOur approach was inspired by a line of work in physics by Baldassi, Ingrosso, Lucibello, Saglietti, and Zecchina [Bal+15] and the same authors with Borgs and Chayes [Bal+16]. Based on theoretical results for discrete optimization linking computational efficiency to the existence of nonisolated solutions, the authors propose a number of new algorithms for learning discrete neural networks by explicitly driving them towards nonisolated solutions. On the basis of Bayesian ideas, they posit that these solutions should have good generalization properties. In a recent collaboration with Chaudhari, Choromanska, Soatto, and LeCun [Cha+17], they extend these ideas to modern deep learning architectures with continuous parametrizations, obtaining impressive empirical results.\nIn this setting, nonisolated solutions correspond to “flat minima”. The existence and generalization properties of flat minima in the neural-network error surface is an old observation, going back at least to work by Hochreiter and Schmidhuber [HS97], who discuss sharp versus flat minima using the language of minimum description length (MDL; [Ris83]). In short, describing weights in sharp minima requires high precision in order to not incur nontrivial excess error, whereas flat minimum can be described with lower precision.\nHochreiter and Schmidhuber propose an algorithm to find flat minima by minimizing the training error while maximizing the log volume of a connected region of the parameter space that yields similar classifiers with similarly good training error.\nThere are very close connections—at both the level of analysis and algorithms—with the work of Chaudhari et al. [Cha+17] and close connections with the approach we take to compute nonvacuous generalization bounds by exploiting the local structure of the learned solution. (We discuss more related work in Section 6.)\nDespite the promising theoretical underpinnings, the generalizations theorems given by [Cha+17] have admittedly unrealistic assumptions, and thus fall short of demonstrating that small local-entropic loss or the structure of the flat minimum found during optimization explains the generalization performance that they observe.\nThe goal of this work is to identify structure in the solutions obtained by SGD that provably implies small generalization error. Computationally, it is much easier to demonstrate that a randomized classifier will generalize, and so our results actually pertain to the generalization error of a stochastic neural network, i.e., one whose weights/biases are drawn at random from some distribution on every forward evaluation of the network. In order to explain the observations of [Zha+17], the next step would be to study whether such structure necessarily arises from performing SGD under natural conditions. (We suspect one condition may be that the Bayes error rate is close to zero.) More ambitiously, perhaps the same structure can explain also the efficiency of SGD in practice.\n1.1. Approach. Our working hypothesis is that SGD finds good solutions only if they are surrounded by a relatively large volume of solutions that are nearly as good. This hypothesis suggests that PAC-Bayes bounds may be fruitful: if SGD finds a solution contained in a large volume of equally good solutions, then the expected error rate of a classifier drawn at random from this volume should match that of the SGD solution. The PAC-Bayes theorem [McA99] bounds the expected error rate of a classifier chosen from a distribution Q in terms of the Kullback– Liebler divergence from some a priori fixed distribution P , and so if the volume of equally good solutions is large, and not too far from the mass of P , we will obtain a nonvacuous bound.\nOur approach will be to use optimization to find a broad distribution Q over neural network parameters that minimizes the PAC-Bayes bound, in effect mapping out the volume of equally good solutions surrounding the SGD solution. This idea is actually a modern take on an old idea by Langford and Caruana [LC02], who apply PAC-Bayes bounds to small two-layer stochastic neural networks (with only 2 hidden units) that were trained on (relatively large, in comparison) data sets of several hundred labeled examples.\nThe basic idea can be traced back even further to work by Hinton and Camp [HC93], who propose an algorithm for controlling overfitting in neural networks via the minimum description length principle. In particular, they minimize the sum of the empirical squared error and the KL divergence between a prior and posterior distribution on the weights. Their algorithm is applied to networks with 100’s of inputs and 4 hidden units, trained on several hundred labeled examples. Hinton and Camp do not compute numerical generalization bounds to verify that MDL principles alone suffice to explain the observed generalization.\nOur algorithm more directly extends the work by Langford and Caruana, who propose to construct a distribution Q over neural networks by performing a sensitivity analysis on each parameter after training, searching for the largest deviation that does not increase the training error by more than, e.g., 1%. For Q, Langford and Caruana choose a multivariate normal distribution over the network parameters, centered at the parameters of the trained neural network. The covariance matrix is diagonal, with the variance of each parameter chosen to be the estimated sensitivity, scaled by a global constant. (The global scale is chosen so that the\ntraining error of Q is within, e.g., 1% of that of the original trained network.) Their prior P is also a multivariate Gaussian, but with zero mean and covariance given by some scalar multiple of the identity matrix. By employing a union bound, they allow themselves to choose the scalar multiple in a data-dependent fashion to optimize the PAC-Bayes bound.\nThe algorithm sketched by Langford and Caruana does not scale to modern neural networks for several reasons, but one dominates: in massively overparametrized networks, individual parameters often have negligible effect on the training classification error, and so it is not possible to estimate the relative sensitivity of large populations of neurons by studying the sensitivity of neurons in isolation.\nInstead, we use stochastic gradient descent to directly optimize the PAC-Bayes bound on the error rate of a stochastic neural network. At each step, we update the network weights and their variances by taking a step along an unbiased estimate of the gradient of (an upper bound on) the PAC-Bayes bound. In effect, the objective function is the sum of i) the empirical surrogate loss averaged over a random perturbation of the SGD solution, and ii) a generalization error bound that acts like a regularizer.\nHaving demonstrated that this simple approach can construct a witness to generalization, it is worthwhile asking whether these ideas can be extended the setting of local-entropic loss [Cha+17]. If we view the distribution that defines the localentropic loss as defining a stochastic neural network, can we use PAC-Bayes bounds to establish nonvacuous bounds on its generalization error?"
    }, {
      "heading" : "2. Preliminaries",
      "text" : "Much of the setup is identical to that of [LC02]: We are working in the batch supervised learning setting. Data points are elements x ∈ X ⊆ Rk with binary class labels y ∈ {−1, 1}. We will denote the training set of size m as Sm:\nSm = {(xi, yi)}i=1,...,m, where (xi, yi) ∈ (X × Y). (1) Let M be all probability measures on the data space Rk ×{−1, 1}. We will assume that the training examples are i.i.d. samples from some µ ∈ M.\nA parametric family of classifiers is a function H : Rd × Rk → {−1, 1}, where hw := H(w, ·) : Rk → {−1, 1} is the classifier indexed by the parameter w ∈ Rd. The hypotheses space induced by H is H = {hw : w ∈ Rd}. A randomized classifier is a distribution Q on Rd. Informally, we will speak of distributions on H when we mean distributions on the underlying parametrization.\nWe are interested in the 0–1 loss ℓ : R× {−1, 1} → {0, 1} ℓ(ŷ, y) = I(sign(ŷ) = y). (2)\nWe will also make use of the logistic loss ℓ̆ : R× {−1, 1} → R+\nℓ̆(ŷ, y) = 1\nlog(2) log\n( 1 + exp(−ŷy) ) , (3)\nwhich will serve as a convex surrogate (i.e., upper bound) to the 0–1 loss. We define the following notions of error:\n• ê(h, Sm) = 1\nm\nm ∑\ni=1\nℓ(h(xi), yi) empirical classification error of hypothesis h\nfor sample Sm;\n• ĕ(h, Sm) = 1\nm\nm ∑\ni=1\nℓ̆(h(xi), yi) empirical (surrogate) error of a hypothesis h\non the training data set Sm. We will use this for training purposes when we need our empirical loss to be differentiable;\n• eµ(h) = E Sm∼µm [ê(h, Sm)] expected error for hypothesis h under the data\ndistribution µ (we will often drop the subscript µ and just write e(h)); • ê(Q,Sm) = E\nw∼Q [ê(hw, Sm)] expected empirical error under the randomized\nclassifier Q on H; • e(Q) = E\nw∼Q [eµ(hw)] expected error for Q on H.\n2.1. KL divergence. Let Q,P be probability measures defined on a common measurable space H, such that Q is absolutely continuous with respect to P , and write dQ dP : H → R+ ∪ {∞} for some Radon–Nikodym derivative of Q with respect to P . Then the Kullback–Liebler divergence (or relative entropy) of P from Q is defined to be\nKL(Q||P ) := ∫ log dQ\ndP dQ. (4)\nWe will mostly be concerned with KL divergences where Q and P are probability measures on Euclidean space, Rd, absolutely continuous with respect to Lebesgue measure. Let q and p denote the respective densities. In this case, the definition of the KL divergence simplifies to\nKL(Q||P ) = ∫ log q(x)\np(x) q(x)dx. (5)\nOf particular interest to us is the KL divergence between multivariate Gaussian distributions in Rd. Let Nq = N (µq ,Σq) be a multivariate Gaussian with mean µq and covariance matrix Σq, let Np = N (µp,Σp), and assume Σq and Σp are positive definite. Then\nKL(Nq||Np) = 1\n2\n(\ntr ( Σ−1p Σq ) − k + (µp − µq)⊤ Σ−1p (µp − µq) + ln ( det Σp detΣq )) .\n(6)\nFor p, q ∈ [0, 1], we will abuse notation and define\nKL(q||p) := KL(B(q)||B(p)) = q log q p + (1− q) log 1− q 1− p, (7)\nwhere B(p) denotes the Bernoulli distribution on {0, 1} with mean p.\n2.2. Inverting KL bounds. In the following sections, we will encounter bounds of the form\nKL(q||p) ≤ c (8) for q, p ∈ [0, 1] and c ≥ 0. We will most often be interested in the quantity\nKL−1(q|c) := sup {p ∈ [0, 1] : KL(q||p) ≤ c}. (9)\nWe are not aware of a simple formula for KL−1(q|c), although numerical approximations are readily obtained via Newton’s method (Appendix B). For the purpose of gradient-based optimization, we can use the well-known inequality, 2(q − p)2 ≤ KL(q||p), to obtain a simple upper bound\nKL−1(q|c) ≤ q + √ c/2, (10)\nwhich holds whenever the right hand side is bounded by 1. This bound is quantitatively loose when q ≈ 0, because then KL−1(q|c) ≈ c for c ≪ 1, as compared with the upper bound of Θ( √ c). On the other hand, when c is large enough that q+ √\nc 2 > 1, the derivative of KL −1(q|c) is zero, whereas the upper bound provides a useful derivative.\n2.3. Bounds. We will employ three bounds to control the generalization error: the union bound, a sample convergence bound derived from the Chernoff bound, and the PAC-Bayes bound due to McAllester [McA99]. We state the union bound for completeness.\nTheorem 2.1 (union). Let E1, E2, . . . be events. Then P( ⋃ n En) ≤ ∑ n P(En).\nRecall that B(p) denotes the Bernoulli distribution on {0, 1} with mean p ∈ [0, 1]. The following bound is derived from the KL formulation of the Chernoff bound:\nTheorem 2.2 (sample convergence [LC02]). For every p, δ ∈ (0, 1) and n ∈ N,\nP x∼B(p)n\n(\nKL(n−1 ∑n i=1 xi||p) ≥ log 2δ n\n)\n≤ δ. (11)\nFinally, we present a variant of the PAC-Bayes bound due to Langford and Seeger [LS01]. (See also [Lan02].) The PAC-Bayes theorem was first established by McAllester [McA99].\nTheorem 2.3 (PAC-Bayes [McA99; LS01]). For every δ > 0, m ∈ N, distribution µ on Rk × {−1, 1}, and distribution P on H,\nP Sm∼µm\n( (∃Q) KL(ê(Q,Sm)||e(Q)) ≥ KL(Q||P ) + log m δ\nm− 1 ) ≤ δ. (12)\nThe PAC-Bayes bound leads to the following learning algorithm [McA99]:\n(1) Fix a probability δ > 0 and a distribution P on H. (2) Collect an i.i.d. dataset Sm of size m. (3) Compute the distribution Q on H that minimizes the error bound\nKL−1 ( ê(Q,S)\n∣ ∣ ∣ ∣ KL(Q||P ) + log m δ\nm− 1\n)\n. (13)\n(4) Return the randomized classifier corresponding to Q.\nIn all but the simplest scenarios, this learning algorithm is intractable. However, we can attempt to approximate it."
    }, {
      "heading" : "3. Optimizing the PAC-Bayes bound",
      "text" : "Let H be a parametric family of classifiers and write hw for H(w, ·). We will interpret hw as a neural network with (weight/bias) parameters w ∈ Rd, although the development below is more general.\nFix δ ∈ (0, 1) and some distribution P on Rd, and let Sm ∼ µm be m i.i.d. training examples. We are interested in minimizing the PAC-Bayes bound Eq. (13) with respect to Q.\nFor every w ∈ Rd and s ∈ Rd+, let Nw,s = N (w, diag(s)) denote the multivariate normal distribution with mean w and diagonal covariance diag(s). As our first simplifications, we replace the PAC-Bayes with the upper bound described by Eq. (10), replace the empirical loss with its convex surrogate, and restrict Q to the family of multivariate normal distributions with diagonal covariance structure, yielding the optimization problem\nmin w∈Rd,s∈Rd\n+\nĕ(Nw,s, Sm) + √ KL(Nw,s||P ) + log mδ 2(m− 1) . (14)\n3.1. The Prior. It remains to choose the prior P . Choosing P to be multivariate normal leads to a simple analytical formula for the KL divergence. Symmetry considerations would suggest that we choose P = N (0, λI) for some λ > 0, however there is no single good choice of λ. (We will also see that there are good reasons not to choose a zero mean, and so we will let w0 denote the mean to be chosen a priori.)\nIn order to deal with the problem of choosing λ, we will follow Langford and Caruana [LC02] and use a Structural Risk Minimization type argument to choose λ optimally from a discrete set, at the cost of a slight expansion to our generalization bound. In particular, we will take λ = c exp{−j/b} for some j ∈ N and fixed b, c ≥ 0. If the PAC-Bayes bound for each j ∈ N is designed to hold with probability at least 1− 6\nπ2j2 , then, by the union bound (Theorem 2.1), it will hold uniformly for\nall j ∈ N with probability at least 1 − δ, as desired.1 During optimization, we will want to avoid discrete optimization, and so we will treat λ as if it were a continuous variable. (We will then discretize λ when we evaluate the PAC-Bayes bound after the fact.) Solving for j, we have j = b log c\nλ , and so we will replace j with this\nterm during optimization. Taking into account the choice of P and the continuous approximation to the union bound, we have the following minimization problem:\nmin w∈Rd,s∈Rd\n+ ,λ∈(0,c)\nĕ(Nw,s, Sm) + √ BRE(w, s, λ; δ) (15)\nwhere\nBRE(w, s, λ; δ) = KL(Nw,s||N (w0, λI)) + 2 log(b log cλ) + log π 2m 6δ\n2(m− 1) (16)\nand, using Eq. (6), the KL term simplifies to\nKL(Nw,s||N(w0, λI)) = 1 2 ( 1 λ ‖s‖1 + 1 λ ‖w − w0‖22 + d logλ− 1d · log s− d). (17)\n3.2. Stochastic Gradient Descent. We cannot optimize this objective directly because we cannot compute ĕ(Nw,s, Sm) or its gradients efficiently. We can, however, employ a type of stochastic gradient descent. Therefore, the final simplification that we will make is to take gradient steps with respect to the unbiased estimator ĕ(hw′ , Sm), w\n′ ∼ Nw,s. We will use a new independent unbiased estimate at each iteration. Note that we compute the estimate of the gradient with respect to the entire data set, although one could also have used a random mini-batch of the training data at each step. Given the stochastic approximation to the error term, we can now employ any gradient-based optimization algorithm.\n3.3. Evaluating the final PAC-Bayes bound. While we treat λ as a continuous parameter during optimization, the union bound requires that λ be of the form λ = c exp{−j/b}, for some j ∈ N. We therefore round λ up or down, choosing that which delivers the best bound, as computed below.\nAccording to the PAC-Bayes and union bound, with probability 1− δ, uniformly over all w ∈ Rd, s ∈ Rd+, and λ of the form c exp{−j/b}, for j ∈ N, the error rate of the randomized classifier Q = Nw,s is bounded by\nKL−1(ê(Q,S)|BRE(w, s, λ; δ)). (18) We cannot compute this bound exactly because computing ê(Q,S) is intractable. However, we can obtain unbiased estimates and apply the sample convergence\n1A superior but intractable approach is to choose P as a scale mixture of multivariate normal distributions. If the scale mixture is chosen to “match” the prior we have defined here, then we would expect the bound to be tighter because nearby values of j cannot “share statistical strength”, while they would were P a mixture.\nbound (Theorem 2.2). In particular, given n i.i.d. samples w1, . . . , wn from Q, we produce the Monte Carlo approximation Q̂n = ∑n\ni=1 δwi , for which ê(Q̂n, Sm) is exactly computable, and obtain the bound\nê(Q,S) ≤ ên,δ′(Q,S) := KL−1(ê(Q̂n, Sm)|n−1 log 2/δ′), (19) which holds with probability 1− δ′. By another application of the union bound,\ne(Q) ≤ KL−1(ên,δ′(Q,S)|BRE(w, s, λ; δ)), (20) with probability 1− δ − δ′. We use this bound in our reported results."
    }, {
      "heading" : "4. Experiments",
      "text" : "We optimize PAC-Bayes bounds on the error rates of stochastic neural networks trained on a binary classification variant of MNIST. We train several different network architectures, varying both the depth and the width of the network. We obtain nonvacuous generalization bounds for networks with large VC dimension.\n4.1. Dataset. We use the MNIST handwritten digits data set [LCB10] as provided in Tensorflow [Aba+15], where the dataset is split into the training set (55000 images) and test set (10000 images). (We do not use the validation set.) Each MNIST image is black and white and 28-pixels square, resulting in a network input dimension of k = 784. MNIST is usually treated as a multiclass classification problem. In order to use standard PAC-Bayes bounds, we produce a binary classification problem by mapping numbers {0, . . . , 4} to label 1 and {5, . . . , 9} to label −1. In some experiments, we train on random labels, i.e., binary labels drawn independently and uniformly at random.\n4.2. Initial network training using SGD. All experiments are performed on fully connected feed-forward neural networks with 2–4 layers. We choose a standard initialization scheme for the weights and biases: Weights are initialized randomly from a normal distribution (with mean zero and standard deviation 0.04) that is truncated to [-0.08, 0.08]. Biases are initialized to a constant value of 0.1 for the first layer and 0 for the remaining layers.\nWe use REctified Linear Unit (RELU) activations at every hidden node. The last layer is linear. We minimize the logistic loss by Stochastic Gradient Descent (SGD) and Momentum: learning rate 0.01; momentum 0.9. SGD is run in mini-batches of size 100.\nOn our binary variant of MNIST, we train several neural network architectures of varying depth and width (see Table 1). In each case, we train for a total of 20 epochs. We also train a small network with 1 hidden layer of 600 nodes on random labels, in order to demonstrate the large capacity of the network. Obtaining ≈0 training error required 120 epochs. See the first two rows of Table 1 for the train/test error rates.\n4.3. PAC-Bayes bound optimization. As described in Section 3, we optimize w ∈ Rd, s ∈ Rd+, and λ ∈ (0, c) according to Eq. (15) using gradient descent, replacing the empirical surrogate error of the randomized classifier Q = Nw,s with an unbiased estimate produced from a single sample from Q at each iteration. Before optimization, we fix δ = 0.025, b = 100, and c = 0.1.\nTo ensure that the variables λ ∈ (0, c) and s ∈ Rd+ remain positive, we reparametrize and instead optimize variables representing 12 log(λ) and 1 2 log(s).\nWe run gradient descent with the RMSprop optimizer (decay 0.9). The learning rate is set to 0.001 for the first 150000 iterations. We then lower it to 0.0001 for the final 50000 iterations. For the random label experiment, we optimize the bound with a smaller learning rate 0.0001 for 500000 iterations.\nAlgorithm 1 is pseudo code for optimizing the PAC-Bayes bound. The code implements vanilla SGD, although it can be easily modified to use other optimizer, e.g., RMSprop, momentum.\n4.4. Initialization and Symmetry Breaking. In an ideal world, we would account for all the network symmetries when computing the KL divergence in the PAC-Bayes bound. (See Appendix A for a discussion.) Because it does not seem to be computational feasible to account for the symmetries, it makes sense to try to break the symmetries somehow. In fact, one consequence of randomly initializing a network is that some symmetries are broken. If we do not expect SGD to reverse these symmetries, then the initial weight configuration, which we will denote by w0, will be a better mean for the prior P than the origin. In fact, breaking the symmetries in this way lead to much better bounds than setting the means to zero.\nThe prior variance λ is initialized to a fixed valued of exp{−6}, and the sensitivities s to |w| with a = 1. For experiments with random labels, we take s to be |w|/10.\n4.5. Reported values. All reported error rates correspond to classification error. The train and test errors are evaluated on the network learned by SGD. In all experiments, SGD achieves perfect or near-perfect classification accuracy on the training data. We start from the SGD solution when optimizing the PAC-Bayes bound.\nThe reported SNN train and test error rates are upper bounds computed by an application of Theorem 2.2 as described in Section 3.3 with δ′ = 0.01 and n = 150000. These numbers are chosen in order to get the estimate within 0.001– 0.002. The reported test error of the SGD solution is the empirical mean. (In light of 10000 test data points and the observed error rates, upper bounds via Theorem 2.2 are only 0.005 higher.)\nThe PAC-Bayes bound is computed as described in Section 3.3. Each bound holds with probability 0.965 over the choice of the training set and the draws from the learned SNN Q. For the random label experiment, we report √\nBRE(w, s, λ; δ) as defined in Eq. (16), since the PAC-Bayes bound cannot be computed for values greater than 1.\nTo calculate the upper bound on the VC dimension of the network, we use an upper bound communicated to us by Bartlett [Bar17] which is itself in O(LW logW ), where L is the number of layers and W is the total number of tunable parameters."
    }, {
      "heading" : "5. Results",
      "text" : "See Table 1. All SGD trained networks achieve perfect or nearly perfect accuracy on the training data. On true labels, the SNN mean training error increases slightly as the weight distribution broadens to minimize the KL divergence. The SGD solution is close to mean of the SNN as measured with respect to the SNN covariance. For the random label experiment, the SNN mean training error rises above 10%. Ideally, it might have risen to nearly 50%, while driving down the KL term to near zero.\nThe empirical test error of the SGD classifiers does not change much across the different architectures, despite the potential for overfitting. This phenomenon is well known, though still remarkable. For the random label experiment, the empirical test classification error of 0.508 represents lack of generalization, as expected. The same two patterns hold for the SNN test error too, with slightly higher error rates.\nRemarkably, the PAC-Bayes bounds do not grow much despite the networks becoming several times larger, and all true label experiments have classification\nerror bounded by 0.23. Since larger networks possess many more symmetries, the true PAC-Bayes bounds for our learned stochastic neural network classifier might be substantially smaller. (See Appendix A for a discussion.) While these bounds are several times larger than the test error estimated on held out data (approximately, 0.03), they demonstrate nontrivial generalization. As expected mathematically, the PAC-Bayes bound on the classification error for random labels is 1.0, indicating the absence of generalization.\nThe VC dimension upper bounds indicate that data independent bounds will be vacuous by several orders of magnitude. Because the number of parameters exceeds the available training data, lower bounds imply that generalization cannot be explained in a data independent way.\n5.1. Parameter optimization. One question we were interested in was whether the weights obtained from optimizing the PAC-Bayes bound had changed much from the SGD solution wSGD that served as an initialization. To answer this question, we calculated the p-value of the SGD solution under the distribution of the stochastic neural network.\nLet QSNN denote the distribution obtained by optimizing the PAC-Bayes bound, write wSNN and ΣSNN for its mean and covariance, and let ‖w‖ΣSNN = wTΣ−1SNNw denote the induced norm. Using 10000 samples, we estimated\nP w∼QSNN\n( ‖w − wSNN‖ΣSNN < ‖wSGD − wSNN‖ΣSNN ) . (21)\nThe estimate was 0 for all true label experiments, i.e., wSGD is less extreme of a perturbation of wSNN than a typical perturbation. For the random label experiments, wSNN and wSGD differ significantly, which is consistent with the bound being optimized in the face of random labels."
    }, {
      "heading" : "6. Related work",
      "text" : "As we mention in the introduction, our approach scales the ideas in [HC93] and [LC02] to the modern deep learning regime where the networks have millions of parameters, but are trained on one or two orders of magnitude fewer training examples. The objective we optimize is an upper bound on the PAC-Bayes bound, which we know from the discussion in Section 2.2 will be very loose when the\nAlgorithm 1 PAC-Bayes bound optimization by SGD\nInput:\nw0 ∈ Rd ⊲ Network parameters (random init.) w ∈ Rd ⊲ Network parameters (SGD solution) Sm ⊲ Training examples δ ∈ (0, 1) ⊲ Confidence parameter b ∈ N, c ∈ (0, 1) ⊲ Precision and bound for λ τ ∈ (0, 1), T ⊲ Learning rate; # of iterations Output: Optimal w, s, λ ⊲ Weights, variances 1: procedure PAC-Bayes-SGD 2: ς ← abs(w) ⊲ where ς = log√s 3: ̺ ← −3 ⊲ where ̺ = log √ λ\n4: B(w, s, λ, w′) = ĕ(hw′ , Sm) + √ 1 2BRE(w, s, λ) 5: for t ← 1, T do ⊲ Run SGD for T iterations. 6: Sample w̃ ∼ N (w, diag(e2ς))\n⊲ Gradient step\n7:\n\n w ς ̺\n\n =\n\n w ς ̺\n\n− τ\n\n ∇wB(w, e2ς , e2̺, w̃) ∇ςB(w, e2ς , e2̺, w̃) ∇̺B(w, e2ς , e2̺, w̃)\n\n\n8: return w, e2ς , e2̺\nempirical classification error is approximately zero. Indeed, in that case, the PACBayes bound is approximately\nê(Nw,s, Sm) + KL(Nw,s||P ) + log mδ\n(m− 1) . (22)\nThe objective optimized by Hinton and Camp is of the same essential form as this one, except for the choice of squared error and different prior and posterior distributions. We explored using Eq. (22) as our objective with a surrogate loss, but it did not produce better results.\nIn the introduction we discuss the close connection of our work to several recent papers [Bal+15; Bal+16; Cha+17] that study “flat” or nonisolated minima on the account of their generalization and/or algorithmic properties.\nBased on theoretical results for k-SAT that efficient algorithms find nonisolated solutions, Baldassi et al. [Bal+16] model efficient neural network learning algorithms as minimizers of a replicated version of the empirical loss surface, which emphasizes nonisolated minima and deemphasizes isolated minima. They then propose several algorithms for learning discrete neural networks using these ideas.\nIn follow-up work with Chaudhari, Choromanska, Soatto, and LeCun [Cha+17], they translate these ideas into the setting of continuously parametrized neural networks. They introduce an algorithm, called Entropy-SGD, which seeks out large regions of dense local minima: it maximizes the depth and flatness of the energy landscape. Their objective integrates both the energy of nearby parameters and the weighted distance to the parameters. In particular, rather than directly minimizing an error surface w 7→ L(hw, Sm), they propose the following minimization problem over the so-called local-entropic loss:\nmin w∈Rd log E W∼Nw,1/γ\n[C(γ) exp{−L(hW , Sm)}], (23)\nwhere γ > 0 is a parameter and C(γ) a constant. In comparison, our algorithm can be interpreted as an optimization of the form\nmin w∈Rp,s∈Rp+ E W∼Nw,s [L(hW , Sm)] + R(w, s) (24)\nwhere R serves as a regularizer that accounts for the generalization error by, roughly speaking, trying to expand the axis-aligned ellipsoid {x ∈ Rd : (w − x)T diag(s)−1(w − x) = 1} and draw it closer to some point w0 near the origin. Comparing Eqs. (23) and (24) highlights similarities and differences. The localentropic loss is sensitive to the volume of the regions containing good solutions. While the first term in our objective function looks similar, it does not, on its own, account for the volume of regions. This role is played by the second term, which prefers large regions (but also ones near the initialization w0). In our formulation, the first term is the empirical error of a stochastic neural network, which is precisely the term whose generalization error we are trying to bound. Entropy-SGD was not designed for the purpose of finding good stochastic neural networks, although it seems possible that having small local-entropic loss would lead to generalization for neural networks whose parameters are drawn from the local Gibbs distribution. Another difference is that, in our formulation, the shape of the Gaussian perturbation is learned adaptively, and driven by the goal of minimizing generalization error. The shape of the Gaussian perturbation is not learned, although the region whose volume is being measured is determined by the error surface, and it seems likely that this volume will be larger than that spanned by a multivariate Gaussian chosen to lie entirely in a region with good loss.\nChaudhari et al. [Cha+17] give an informal characterization of the generalization properties of local-entropic loss in Bayesian terms by comparing the marginal likelihood of two Bayesian priors centered at a solution with small and large localentropic loss. Informally, a Bayesian prior centered on an isolated solution will lead to small marginal likelihood in contrast to one centered in a wide valley. They give a formal result relying on the uniform stability of SGD [HRS15] to show under some strong (and admittedly unrealistic) conditions that Entropy-SGD generalizes better than SGD. The key property is that the local-entropic loss surface is smoother than the original error surface.\nOther authors have found evidence of the importance of “flat” minima: Recent work by Keskar, Mudigere, Nocedal, Smelyanskiy, and Tang [Kes+17] finds that large-batch methods tend to converge to sharp / isolated minima and have worse generalization performance compared to mini-batch algorithms, which tend to converge to flat minima and have good generalization performance. The bulk of their paper is devoted to the problem of restoring good generalization behavior to batch algorithms.\nDinh, Pascanu, Bengio, and Bengio [Din+17] criticize various notions of “flatness” and argue that flatness, as defined, is not a necessary condition for generalization. They do this by demonstrating reparameterizations that relocate “flat” minima to yield sharp minima. However, one can logically explain the generalization properties of SGD solutions in terms of sufficient conditions, including the “flatness” of minima or volume of nearly equivalent solutions.\nFinally, our algorithm also bears resemblance to graduated optimization, an approach toward non-convex optimization attributed to Blake and Zisserman [BZ87] whereby a sequence of increasingly fine-grained versions of an optimization problem are solved in succession. (See [HLS16] and references therein.) In this context, Eq. (23) is the result of a local smoothing operation acting on the objective function w 7→ ℓ̆(hw, SM ). In graduate optimization, the effect of the local smoothing operation would be decreased over time, eventually disappearing. In our formulation,\nthe act of balancing the empirical loss and generalization error serve to drive the evolution of the local smoothing in an adaptive fashion. Moreover, in the limit, the local smoothing does not vanish in our algorithm, as the volume spanned by the perturbations relates to the generalization error. Our results suggest that SGD solutions live inside relatively large volumes, and so perhaps SGD can be understood in terms of graduated optimization."
    }, {
      "heading" : "7. Conclusions and Future work",
      "text" : "We obtain nonvacuous generalization bounds for deep neural networks with millions of parameters trained on 55000 MNIST examples. These bounds are obtained by optimizing an objective derived from the PAC-Bayes bound, starting from the solution produced by SGD. Despite the weights changing, the SGD solution remains well within the 1% ellipsoidal quantile, i.e., the volume spanned by the stochastic neural network contains the original SGD solution. (When labels are randomized, however, optimizing the PAC-Bayes bound causes the solution to shift considerably.)\nOur experiments look only at fully connected feed forward networks trained on a binary classification problem derived from MNIST. It would be interesting to see if the results extend to multiclass classification, to other data sets, and to other types of architectures, especially convolutional ones.\nIn our experiments, we optimize the PAC-Bayes bound starting from an SGD solution. One could instead train the network entirely by optimizing the PACBayes bound from a random initialization and study how the optima and bounds compare to those produced via SGD. There are other changes worth studying: highly dependent weights constrain the size of the axis-aligned ellipsoid representing the stochastic neural network. We can potentially recognize small populations of highly dependent weights, and optimize their covariance parameters, rather than enforcing independence in the posterior.\nIt is also interesting to consider replacing the posterior with a distribution that is more tuned to the loss surface. One promising avenue is to follow the lines of Chaudhari et al. [Cha+17] and consider (local) Gibbs distributions. If the solutions obtained by minimizing the local-entropic loss are flatter than those obtained by SGD, than we may be able to demonstrate quantitatively tighter bounds.\nFinally, there is the hard work of understanding the generalization properties of SGD. In light of our work, it may be useful to start by asking whether SGD finds solutions in flat minima. Such solutions could then be lifted to stochastic neural networks with good generalization properties. Going from stochastic networks back to deterministic ones may require additional structure, such as margin."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This research was carried out while the authors were visiting the Simons Institute for the Theory of Computing at UC Berkeley. The authors would like to thank Peter Bartlett, Shai Ben-David, Dylan Foster, Matus Telgarsky, and Ruth Urner for helpful discussions. GKD is supported by an EPSRC studentship. DMR is supported by an NSERC Discovery Grant, Connaught Award, and U.S. Air Force Office of Scientific Research grant #FA9550-15-1-0074."
    }, {
      "heading" : "A. Network symmetries",
      "text" : "Fix a neural network architecture H : Rd × Rk → {−1, 1} and write hw for H(w, ·). It has long been appreciated that distinct parametrizations w,w′ ∈ Rd can lead to the same functions hw = hw′ , and so the set H = {hw : w ∈ Rd} of classifiers defined by a neural network architecture is a quotient space of Rd.\nFor the purposes of understanding the generalization error of neural networks, we would ideally work directly with H. Let P,Q be a distributions on Rd, i.e., stochastic neural networks. Then P and Q induce distributions on H, which we will denote by P̄ and Q̄, respectively. For the purposes of the PAC-Bayes bound, it is the KL divergence KL(Q̄||P̄ ) that upper bounds the performance of the stochastic neural network Q. In general, KL(Q̄||P̄ ) ≤ KL(Q||P ), but it is difficult in practice to approximate the former because the quotient space is extremely complex.\nOne potential way to approach H is to account for symmetries in the parameterization. A network symmetry is a map σ : Rd → Rd such that, for all w ∈ Rd, we have hw = hσ(w). As an example of such a symmetry, in a fully connected network with identical activation functions at every unit, the function computed by the network is invariant to permuting the nodes with a hidden layer. Let S be any finite set of symmetries possessed by the architecture. For every distribution Q on Rd and network symmetry σ, we may define Qσ = Q ◦ σ−1 to be the distribution over networks obtained by first sampling network parameters from Q and then applying the map σ to obtain a network that computes the same function.\nDefine QS = 1|S| ∑ σ∈S Qσ. Informally, Q andQ S are identical when viewed as distributions on functions, yet QS spreads its mass evenly over equivalent parametrizations. In particular, for any data set S, we have ê(Q,S) = ê(QS, S). We call QS a symmetrized version of Q. The following lemma states that symmetrized versions always have smaller KL divergence with respect to distributions that are invariant to symmetrization: Before stating the lemma, recall that the differential entropy of an absolutely continuous distribution Q on Rd with density q is ∫\nq(x) log q(x)dx ∈ R ∪ {−∞,∞}.\nLemma A.1. Let S be a finite set of network symmetries, let P be an absolutely continuous distribution such that P = Pσ for all σ ∈ S, and define QS as above for some arbitrary absolutely continuous distribution Q on Rd with finite differential entropy. Then KL(QS||P ) = KL(Q||P )−KL(Q||QS) ≤ KL(Q||P ).\nThe above lemma can be generalized to distributions over (potentially infinite) sets of network symmetries.\nIt follows from this lemma that one can do no worse by accounting for symmetries using mixtures, provided that one is comparing to a distribution P that is invariant to those symmetries. In light of the PAC-Bayes theorem, this means that a generalization bound based upon a KL divergence that does not account for symmetries can likely be improved. However, for a finite set S of symmetries, it is easy to show that the improvement is bounded by log |S|, which suggests that, in order to obtain appreciable improvements in a numerical bound, one would need to account for an exponential number of symmetries. Unfortunately, exploiting this many symmetries seems intractable. It is hard to obtain useful lower bounds to KL(Q||QS), while upper bounds from Jensen’s inequality led us to negative (hence vacuous) lower bounds on KL(QS||P ).\nIn this work, we therefore take a different approach to dealing with symmetries. Neural networks are randomly initialized in order to break symmetries. Combined with the idea that the learned parameters will reflect these broken symmetries, we choose our prior P to be located at the random initialization, rather than at zero."
    }, {
      "heading" : "B. Approximating KL−1(q|c)",
      "text" : "There is no simple formula for KL−1(q|c), but we can approximate it via rootfinding techniques. For all q ∈ (0, 1) and c ≥ 0, define hq,c(p) = KL(q||p)− c. Then h′q,c(p) = 1−q 1−p − q p . Given a sufficiently good initial estimate p0 of a root of hq,c(·), we can obtain improved estimates of a root via Newton’s method:\npn+1 = N(pn; q, c) where N(p; q, c) = p− hq,c(c)\nh′q,c(p) . (25)\nThis suggests the following approximation to KL−1(q|c): (1) Let b̃ = q + √\nc 2 .\n(2) If b̃ ≥ 1, then return 1. (3) Otherwise, return Nk(b̃), for some small integer k > 0.\nOur reported results use five steps of Newton’s method."
    } ],
    "references" : [ {
      "title" : "TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems. Software available from tensorflow.org",
      "author" : [ "V. Vanhoucke", "V. Vasudevan", "F. Viégas", "O. Vinyals", "P. Warden", "M. Wattenberg", "M. Wicke", "Y. Yu", "X. Zheng" ],
      "venue" : null,
      "citeRegEx" : "Vanhoucke et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Vanhoucke et al\\.",
      "year" : 2015
    }, {
      "title" : "Subdominant Dense Clusters Allow for Simple Learning and High Computational Performance in Neural Networks with Discrete Synapses",
      "author" : [ "C. Baldassi", "A. Ingrosso", "C. Lucibello", "L. Saglietti", "R. Zecchina" ],
      "venue" : "Phys. Rev. Lett",
      "citeRegEx" : "Baldassi et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Baldassi et al\\.",
      "year" : 2015
    }, {
      "title" : "Unreasonable effectiveness of learning neural networks: From accessible states and robust ensembles to basic algorithmic schemes",
      "author" : [ "C. Baldassi", "C. Borgs", "J.T. Chayes", "A. Ingrosso", "C. Lucibello", "L. Saglietti", "R. Zecchina" ],
      "venue" : "Proceedings of the National Academy of Sciences",
      "citeRegEx" : "Baldassi et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Baldassi et al\\.",
      "year" : 2016
    }, {
      "title" : "The impact of the nonlinearity on the VC-dimension of a deep network",
      "author" : [ "P.L. Bartlett" ],
      "venue" : "Preprint.",
      "citeRegEx" : "Bar17",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Visual Reconstruction",
      "author" : [ "A. Blake", "A. Zisserman" ],
      "venue" : "Cambridge, MA, USA: MIT Press",
      "citeRegEx" : "BZ87",
      "shortCiteRegEx" : null,
      "year" : 1987
    }, {
      "title" : "Entropy-SGD: Biasing Gradient Descent Into Wide Valleys",
      "author" : [ "P. Chaudhari", "A. Choromanska", "S. Soatto", "Y. LeCun", "C. Baldassi", "C. Borgs", "J. Chayes", "L. Sagun", "R. Zecchina" ],
      "venue" : "In: International Conference on Learning Representations (ICLR). 2017. arXiv:",
      "citeRegEx" : "Cha+17",
      "shortCiteRegEx" : null,
      "year" : 1611
    }, {
      "title" : "2017",
      "author" : [ "L. Dinh", "R. Pascanu", "S. Bengio", "Y. Bengio. Sharp Minima Can Generalize For Deep Nets" ],
      "venue" : "arXiv:",
      "citeRegEx" : "Din+17",
      "shortCiteRegEx" : null,
      "year" : 1703
    }, {
      "title" : "Keeping the Neural Networks Simple by Minimizing the Description Length of the Weights",
      "author" : [ "G.E. Hinton", "D. van Camp" ],
      "venue" : "Proceedings of the Sixth Annual Conference on Computational Learning Theory. COLT ’93",
      "citeRegEx" : "Hinton and Camp.,? \\Q1993\\E",
      "shortCiteRegEx" : "Hinton and Camp.",
      "year" : 1993
    }, {
      "title" : "On Graduated Optimization for Stochastic Non-Convex Problems",
      "author" : [ "E. Hazan", "K.Y. Levy", "S. Shalev-Shwartz" ],
      "venue" : "Proceedings of the 33rd International Conference on Machine Learning (ICML)",
      "citeRegEx" : "Hazan et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Hazan et al\\.",
      "year" : 2016
    }, {
      "title" : "Train faster, generalize better: Stability of stochastic gradient descent",
      "author" : [ "M. Hardt", "B. Recht", "Y. Singer" ],
      "venue" : "CoRR abs/1509.01240",
      "citeRegEx" : "Hardt et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Hardt et al\\.",
      "year" : 2015
    }, {
      "title" : "Flat Minima",
      "author" : [ "S. Hochreiter", "J. Schmidhuber" ],
      "venue" : "Neural Comput",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? \\Q1997\\E",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima",
      "author" : [ "N.S. Keskar", "D. Mudigere", "J. Nocedal", "M. Smelyanskiy", "P.T.P. Tang" ],
      "venue" : "In: International Conference on Learning Representations (ICLR). 2017. arXiv:",
      "citeRegEx" : "Kes+17",
      "shortCiteRegEx" : null,
      "year" : 1609
    }, {
      "title" : "Quantitatively tight sample complexity bounds",
      "author" : [ "J. Langford" ],
      "venue" : "Carnegie Mellon University",
      "citeRegEx" : "Lan02",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Not) Bounding the True Error",
      "author" : [ "J. Langford", "R. Caruana" ],
      "venue" : "Advances in Neural Information Processing Systems 14",
      "citeRegEx" : "Langford and Caruana.,? \\Q2002\\E",
      "shortCiteRegEx" : "Langford and Caruana.",
      "year" : 2002
    }, {
      "title" : "MNIST handwritten digit database",
      "author" : [ "Y. LeCun", "C. Cortes", "C.J.C. Burges" ],
      "venue" : "http://yann.lecun.com/exdb/mnist/.",
      "citeRegEx" : "LCB10",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Bounds for Averaging Classifiers",
      "author" : [ "J. Langford", "M. Seeger" ],
      "venue" : "Tech. rep. CMU-CS-01-102. Carnegie Mellon University",
      "citeRegEx" : "LS01",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "PAC-Bayesian Model Averaging",
      "author" : [ "D.A. McAllester" ],
      "venue" : "Proceedings of the Twelfth Annual Conference on Computational Learning Theory. COLT ’99. Santa Cruz, California, USA: ACM,",
      "citeRegEx" : "McAllester.,? \\Q1999\\E",
      "shortCiteRegEx" : "McAllester.",
      "year" : 1999
    }, {
      "title" : "A Universal Prior for Integers and Estimation by Minimum Description Length",
      "author" : [ "J. Rissanen" ],
      "venue" : "Ann. Statist",
      "citeRegEx" : "Rissanen.,? \\Q1983\\E",
      "shortCiteRegEx" : "Rissanen.",
      "year" : 1983
    }, {
      "title" : "Understanding deep learning requires rethinking generalization",
      "author" : [ "C. Zhang", "S. Bengio", "M. Hardt", "B. Recht", "O. Vinyals" ],
      "venue" : "In: International Conference on Representation Learning (ICLR). 2017. arXiv:",
      "citeRegEx" : "Zha+17",
      "shortCiteRegEx" : null,
      "year" : 1611
    } ],
    "referenceMentions" : [ {
      "referenceID" : 15,
      "context" : "Finally, we present a variant of the PAC-Bayes bound due to Langford and Seeger [LS01].",
      "startOffset" : 80,
      "endOffset" : 86
    }, {
      "referenceID" : 12,
      "context" : "(See also [Lan02].",
      "startOffset" : 10,
      "endOffset" : 17
    }, {
      "referenceID" : 15,
      "context" : "3 (PAC-Bayes [McA99; LS01]).",
      "startOffset" : 13,
      "endOffset" : 26
    }, {
      "referenceID" : 14,
      "context" : "We use the MNIST handwritten digits data set [LCB10] as provided in Tensorflow [Aba+15], where the dataset is split into the training set (55000 images) and test set (10000 images).",
      "startOffset" : 45,
      "endOffset" : 52
    }, {
      "referenceID" : 3,
      "context" : "To calculate the upper bound on the VC dimension of the network, we use an upper bound communicated to us by Bartlett [Bar17] which is itself in O(LW logW ), where L is the number of layers and W is the total number of tunable parameters.",
      "startOffset" : 118,
      "endOffset" : 125
    }, {
      "referenceID" : 4,
      "context" : "Finally, our algorithm also bears resemblance to graduated optimization, an approach toward non-convex optimization attributed to Blake and Zisserman [BZ87] whereby a sequence of increasingly fine-grained versions of an optimization problem are solved in succession.",
      "startOffset" : 150,
      "endOffset" : 156
    } ],
    "year" : 2017,
    "abstractText" : "One of the defining properties of deep learning is that models are chosen to have many more parameters than available training data. In light of this capacity for overfitting, it is remarkable that simple algorithms like SGD reliably return solutions with low test error. One roadblock to explaining these phenomena in terms of implicit regularization, structural properties of the solution, and/or easiness of the data is that many learning bounds are quantitatively vacuous in this “deep learning” regime. In order to explain generalization, we need nonvacuous bounds. We return to an idea by Langford and Caruana (2001), who used PAC-Bayes bounds to compute nonvacuous numerical bounds on generalization error for stochastic two-layer two-hiddenunit neural networks via a sensitivity analysis. By optimizing the PAC-Bayes bound directly, we are able to extend their approach and obtain nonvacuous generalization bounds for deep stochastic neural network classifiers with millions of parameters trained on only tens of thousands of examples. We connect our findings to recent and old work on flat minima and MDL-based explanations of generalization.",
    "creator" : "LaTeX with hyperref package"
  }
}