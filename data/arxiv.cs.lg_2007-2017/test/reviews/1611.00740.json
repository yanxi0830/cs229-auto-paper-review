{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Nov-2016", "title": "Why and When Can Deep -- but Not Shallow -- Networks Avoid the Curse of Dimensionality: a Review", "abstract": "The paper reviews an emerging body of theoretical results on deep learning including the conditions under which it can be exponentially better than shallow learning. Deep convolutional networks represent an important special case of these conditions, though weight sharing is not the main reason for their exponential advantage. Explanation of a few key theorems is provided together with new results, open problems and conjectures.", "histories": [["v1", "Wed, 2 Nov 2016 19:35:52 GMT  (1453kb,D)", "http://arxiv.org/abs/1611.00740v1", null], ["v2", "Tue, 22 Nov 2016 06:15:39 GMT  (1982kb,D)", "http://arxiv.org/abs/1611.00740v2", null], ["v3", "Tue, 29 Nov 2016 21:24:07 GMT  (1983kb,D)", "http://arxiv.org/abs/1611.00740v3", null], ["v4", "Wed, 25 Jan 2017 01:09:40 GMT  (1983kb,D)", "http://arxiv.org/abs/1611.00740v4", null], ["v5", "Sat, 4 Feb 2017 09:10:41 GMT  (2894kb,D)", "http://arxiv.org/abs/1611.00740v5", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["tomaso poggio", "hrushikesh mhaskar", "lorenzo rosasco", "brando miranda", "qianli liao"], "accepted": false, "id": "1611.00740"}
