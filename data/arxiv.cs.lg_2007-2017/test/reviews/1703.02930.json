{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Mar-2017", "title": "Nearly-tight VC-dimension bounds for piecewise linear neural networks", "abstract": "We prove new upper and lower bounds on the VC-dimension of deep neural networks with the ReLU activation function. These bounds are tight for almost the entire range of parameters. Letting $W$ be the number of weights and $L$ be the number of layers, we prove that the VC-dimension is $O(W L \\log(W))$ and $\\Omega( W L \\log(W/L) )$. This improves both the previously known upper bounds and lower bounds. In terms of the number $U$ of non-linear units, we prove a tight bound $\\Theta(W U)$ on the VC-dimension. All of these results generalize to arbitrary piecewise linear activation functions.", "histories": [["v1", "Wed, 8 Mar 2017 17:35:17 GMT  (75kb,D)", "http://arxiv.org/abs/1703.02930v1", "11 pages, 2 figures"], ["v2", "Sun, 4 Jun 2017 19:13:36 GMT  (89kb,D)", "http://arxiv.org/abs/1703.02930v2", "Accepted for presentation at Conference on Learning Theory (COLT) 2017. Minor corrections from first version, 12 pages, 2 figures"], ["v3", "Mon, 16 Oct 2017 01:29:59 GMT  (96kb,D)", "http://arxiv.org/abs/1703.02930v3", "Extended abstract appeared in COLT 2017; the upper bound was presented at the 2016 ACM Conference on Data Science. This version includes all the proofs and a refinement of the upper bound, Theorem 6. 16 pages, 2 figures"]], "COMMENTS": "11 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["nick harvey", "chris liaw", "abbas mehrabian"], "accepted": false, "id": "1703.02930"}
