{"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2017", "title": "On Calibration of Modern Neural Networks", "abstract": "Confidence calibration -- the problem of predicting probability estimates representative of the true correctness likelihood -- is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling -- a single-parameter variant of Platt Scaling -- is surprisingly effective at calibrating predictions.", "histories": [["v1", "Wed, 14 Jun 2017 17:33:50 GMT  (1825kb,D)", "http://arxiv.org/abs/1706.04599v1", "Accepted to ICML 2017"], ["v2", "Thu, 3 Aug 2017 13:29:46 GMT  (1271kb,D)", "http://arxiv.org/abs/1706.04599v2", "ICML 2017"]], "COMMENTS": "Accepted to ICML 2017", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["chuan guo", "geoff pleiss", "yu sun", "kilian q weinberger"], "accepted": true, "id": "1706.04599"}
