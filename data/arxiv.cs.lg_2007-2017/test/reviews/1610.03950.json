{"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Oct-2016", "title": "Compressing Neural Language Models by Sparse Word Representations", "abstract": "Neural networks are among the state-of-the-art techniques for language modeling. Existing neural language models typically map discrete words to distributed, dense vector representations. After information processing of the preceding context words by hidden layers, an output layer estimates the probability of the next word. Such approaches are time- and memory-intensive because of the large numbers of parameters for word embeddings and the output layer. In this paper, we propose to compress neural language models by sparse word representations. In the experiments, the number of parameters in our model increases very slowly with the growth of the vocabulary size, which is almost imperceptible. Moreover, our approach not only reduces the parameter space to a large extent, but also improves the performance in terms of the perplexity measure.", "histories": [["v1", "Thu, 13 Oct 2016 06:55:54 GMT  (2356kb,D)", "http://arxiv.org/abs/1610.03950v1", "ACL-16, pages 226--235"]], "COMMENTS": "ACL-16, pages 226--235", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["yunchuan chen", "lili mou", "yan xu", "ge li", "zhi jin"], "accepted": true, "id": "1610.03950"}
