{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Feb-2017", "title": "Exploring loss function topology with cyclical learning rates", "abstract": "We present observations and discussion of previously unreported phenomena discovered while training residual networks. The goal of this work is to better understand the nature of neural networks through the examination of these new empirical results. These behaviors were identified through the application of Cyclical Learning Rates (CLR) and linear network interpolation. Among these behaviors are counterintuitive increases and decreases in training loss and instances of rapid training. For example, we demonstrate how CLR can produce greater testing accuracy than traditional training despite using large learning rates. Files to replicate these results are available at", "histories": [["v1", "Tue, 14 Feb 2017 16:46:19 GMT  (471kb,D)", "http://arxiv.org/abs/1702.04283v1", "Submitted as an ICLR 2017 Workshop paper"]], "COMMENTS": "Submitted as an ICLR 2017 Workshop paper", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["leslie n smith", "nicholay topin"], "accepted": false, "id": "1702.04283"}
