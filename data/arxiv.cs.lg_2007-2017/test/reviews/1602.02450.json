{"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Feb-2016", "title": "Loss factorization, weakly supervised learning and label noise robustness", "abstract": "We prove that the empirical risk of most well-known loss functions factors into a linear term aggregating all labels with a term that is label free, and can further be expressed by sums of the loss. This holds true even for non-smooth, non-convex losses and in any RKHS. The first term is a (kernel) mean operator --the focal quantity of this work-- which we characterize as the sufficient statistic for the labels. The result tightens known generalization bounds and sheds new light on their interpretation.", "histories": [["v1", "Mon, 8 Feb 2016 01:50:43 GMT  (104kb)", "https://arxiv.org/abs/1602.02450v1", null], ["v2", "Tue, 9 Feb 2016 23:10:23 GMT  (107kb)", "http://arxiv.org/abs/1602.02450v2", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["giorgio patrini", "frank nielsen", "richard nock", "marcello carioni"], "accepted": true, "id": "1602.02450"}
