{"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Feb-2016", "title": "Learning to Generate with Memory", "abstract": "Memory units have been widely used to enrich the capabilities of deep networks on capturing long-term dependencies in reasoning and prediction tasks, but little investigation exists on deep generative models (DGMs) which are good at inferring high-level invariant representations from unlabeled data. This paper presents a deep generative model with a possibly large external memory and an attention mechanism to capture the local detail information that is often lost in the bottom-up abstraction process in representation learning. By adopting a smooth attention model, the whole network is trained end-to-end by optimizing a variational bound of data likelihood via auto-encoding variational Bayesian methods, where an asymmetric recognition network is learnt jointly to infer high-level invariant representations. The asymmetric architecture can reduce the competition between bottom-up invariant feature extraction and top-down generation of instance details. Our experiments on several datasets demonstrate that memory can significantly boost the performance of DGMs and even achieve state-of-the-art results on various tasks, including density estimation, image generation, and missing value imputation.", "histories": [["v1", "Wed, 24 Feb 2016 06:57:14 GMT  (722kb,D)", "http://arxiv.org/abs/1602.07416v1", null], ["v2", "Sat, 28 May 2016 03:41:27 GMT  (943kb,D)", "http://arxiv.org/abs/1602.07416v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["chongxuan li", "jun zhu", "bo zhang"], "accepted": true, "id": "1602.07416"}
