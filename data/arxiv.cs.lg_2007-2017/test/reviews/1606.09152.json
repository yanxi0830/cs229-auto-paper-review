{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Jun-2016", "title": "Actor-critic versus direct policy search: a comparison based on sample complexity", "abstract": "Sample efficiency is a critical property when optimizing policy parameters for the controller of a robot. In this paper, we evaluate two state-of-the-art policy optimization algorithms. One is a recent deep reinforcement learning method based on an actor-critic algorithm, Deep Deterministic Policy Gradient (DDPG), that has been shown to perform well on various control benchmarks. The other one is a direct policy search method, Covariance Matrix Adaptation Evolution Strategy (CMA-ES), a black-box optimization method that is widely used for robot learning. The algorithms are evaluated on a continuous version of the mountain car benchmark problem, so as to compare their sample complexity. From a preliminary analysis, we expect DDPG to be more sample efficient than CMA-ES, which is confirmed by our experimental results.", "histories": [["v1", "Wed, 29 Jun 2016 15:22:13 GMT  (601kb,D)", "https://arxiv.org/abs/1606.09152v1", null], ["v2", "Mon, 22 Aug 2016 11:07:23 GMT  (758kb,D)", "http://arxiv.org/abs/1606.09152v2", "Proceedings JFPDA (Journees Francaises Planification Decision Apprentissage)"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["arnaud de froissard de broissia", "olivier sigaud"], "accepted": false, "id": "1606.09152"}
