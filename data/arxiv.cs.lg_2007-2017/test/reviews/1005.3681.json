{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-May-2010", "title": "Learning Kernel-Based Halfspaces with the Zero-One Loss", "abstract": "We describe and analyze a new algorithm for agnostically learning kernel-based halfspaces with respect to the \\emph{zero-one} loss function. Unlike most previous formulations which rely on surrogate convex loss functions (e.g. hinge-loss in SVM and log-loss in logistic regression), we provide finite time/sample guarantees with respect to the more natural zero-one loss function. The proposed algorithm can learn kernel-based halfspaces in worst-case time $\\poly(\\exp(L\\log(L/\\epsilon)))$, for $\\emph{any}$ distribution, where $L$ is a Lipschitz constant (which can be thought of as the reciprocal of the margin), and the learned classifier is worse than the optimal halfspace by at most $\\epsilon$. We also prove a hardness result, showing that under a certain cryptographic assumption, no algorithm can learn kernel-based halfspaces in time polynomial in $L$.", "histories": [["v1", "Thu, 20 May 2010 12:39:56 GMT  (25kb)", "https://arxiv.org/abs/1005.3681v1", "This is a full version of the paper appearing in the 23rd International Conference on Learning Theory (COLT 2010)"], ["v2", "Sun, 1 Aug 2010 08:31:29 GMT  (25kb)", "http://arxiv.org/abs/1005.3681v2", "This is a full version of the paper appearing in the 23rd International Conference on Learning Theory (COLT 2010). Compared to the previous arXiv version, this version contains some small corrections in the proof of Lemma 3 and in appendix A"]], "COMMENTS": "This is a full version of the paper appearing in the 23rd International Conference on Learning Theory (COLT 2010)", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["shai shalev-shwartz", "ohad shamir", "karthik sridharan"], "accepted": false, "id": "1005.3681"}
