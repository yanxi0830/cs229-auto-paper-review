{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2016", "title": "Adversarial Perturbations Against Deep Neural Networks for Malware Classification", "abstract": "Deep neural networks have recently been shown to lack robustness against adversarially crafted inputs. These inputs are derived from regular inputs by minor yet carefully selected perturbations that deceive the neural network into desired misclassifications. Existing work in this emerging field was largely specific to the domain of image classification, since images with their high-entropy can be conveniently manipulated without changing the images' overall visual appearance. Yet, it remains unclear how such attacks translate to more security-sensitive applications such as malware detection - which may pose significant challenges in sample generation and arguably grave consequences for failure.", "histories": [["v1", "Tue, 14 Jun 2016 16:01:52 GMT  (198kb,D)", "http://arxiv.org/abs/1606.04435v1", "13 pages"], ["v2", "Thu, 16 Jun 2016 08:14:12 GMT  (198kb,D)", "http://arxiv.org/abs/1606.04435v2", "version update: correcting typos, incorporating external feedback"]], "COMMENTS": "13 pages", "reviews": [], "SUBJECTS": "cs.CR cs.LG cs.NE", "authors": ["kathrin grosse", "nicolas papernot", "praveen manoharan", "michael backes", "patrick mcdaniel"], "accepted": false, "id": "1606.04435"}
