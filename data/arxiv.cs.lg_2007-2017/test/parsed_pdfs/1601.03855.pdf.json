{
  "name" : "1601.03855.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "A Relative Exponential Weighing Algorithm for Adversarial Utility-based Dueling Bandits (extended version)",
    "authors" : [ "Pratik Gajane", "Tanguy Urvoy", "Fabrice Clérot" ],
    "emails" : [ "PRATIK.GAJANE@ORANGE.COM", "TANGUY.URVOY@ORANGE.COM", "FABRICE.CLEROT@ORANGE.COM" ],
    "sections" : [ {
      "heading" : null,
      "text" : "√ K ln(K)T ) for\nthis algorithm and a general lower bound of order Ω( √ KT ). At the end, we provide experimental results using real data from information retrieval applications."
    }, {
      "heading" : "1. Introduction",
      "text" : "The K-armed dueling bandit problem is a variation of the classical Multi-Armed Bandit (MAB) problem introduced by Yue and Joachims (2009) to formalize the exploration/exploitation dilemma in learning from preference feedback. In its utility-based formulation, at each time period, the environment sets a bounded value for each of K arms. Simultaneously the learner selects two arms and wagers that one of the arms will be better than the other. The learner only sees the outcome of the duel between the selected arms (i.e. the feedback indicates which of the selected arms has better value) and receives the average of the rewards of the selected arms. The goal of the learner is to maximize her cumulative reward. The difficulty of this problem stems from the fact that the learning algorithm has no way of directly observing the reward of its actions. This is a perfect example of partial monitoring problem as defined in Piccolboni and Schindelhauer (2001).\nProceedings of the 32nd International Conference on Machine Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).\nRelative feedback is naturally suited to many practical applications like user-perceived product preferences, where a relative perception: “A is better than B” is easier to obtain than its absolute counterpart: “A value is 42, B is worth 33”. Another important application of dueling bandits comes from information retrieval systems where users provide implicit feedback about the provided results. This implicit feedback is collected in various ways e.g. a click on a link, a tap, or any monitored action of the user. In all these ways however, this kind of feedback is often strongly biased by the model itself (the user cannot click on a link which was not proposed).\nTo remove this bias in search engines, Radlinski and Joachims (2007) propose to interleave the outputs of different ranking models: the model which scores a click wins the duel. The accuracy of this interleave filtering method was highlighted in several experimental studies (Radlinski and Joachims, 2007; Joachims et al., 2007; Chapelle et al., 2012).\nMain contribution\nThe main contribution of this article is an algorithm designed for the adversarial utility-based dueling bandit problem in contrast to most of the existing algorithms which assume a stochastic environment.\nOur algorithm, called Relative Exponential-weight algorithm for Exploration and Exploitation (REX3), is a non-trivial extension of the Exponential-weight algorithm for Exploration and Exploitation (EXP3) algorithm (Auer et al., 2002b) to the dueling bandit problem. We prove a finite time expected regret upper bound of order O( √ K ln(K)T ) and develop an argument initially proposed by Ailon et al. (2014) to exhibit a general lower bound of order Ω( √ KT ) for this problem.\nThese two bounds correspond to the original bounds of the classical EXP3 algorithm and the upper bound strictly improves from the Õ(K √ T ) obtained by existing generic\nar X\niv :1\n60 1.\n03 85\n5v 1\n[ cs\n.L G\n] 1\n5 Ja\nn 20\npartial monitoring algorithms.1\nOur experiments on information retrieval datasets show that the anytime version of REX3 is a highly competitive algorithm for dueling bandits, especially in the initial phases of the runs where it clearly outperforms the state of the art.\nOutline\nThis article is organized as follows: In section 2, we give a brief survey on dueling bandits with section 2.3 dedicated to the adversarial case. Most notations and formalizations are introduced in this section. Secondly, in section 3, we formally describe REX3 with its pseudo-code. Then, in section 4.1, we provide the upper bound on the expected regret of REX3. Furthermore, in section 4.2, we provide the lower bound on the regret of any algorithm attempting to solve the adversarial utility-based dueling bandit problem. Section 5 begins with an empirical study of the bound given in section 4.1. It then provides comparisons of REX3 with state-of-the art algorithms on information retrieval datasets. The conclusion is provided in section 6."
    }, {
      "heading" : "2. Previous Work and Notations",
      "text" : "The conventional MAB problem has been well studied in the stochastic setting as well as the (oblivious) adversarial setting (see Cesa-Bianchi and Lugosi, 2006; Bubeck and Cesa-Bianchi, 2012). These MAB algorithms are designed to optimize exploration and exploitation in order to control the cumulative regret which is the difference between the gain of a reference strategy and the actual gain of the algorithm."
    }, {
      "heading" : "2.1. Exponential-weight algorithm for Exploration and Exploitation",
      "text" : "Of particular interest are the Exponential-weight algorithm for Exploration and Exploitation (EXP3) and its variants presented by Auer et al. (2002b) for the adversarial bandit setting. For a fixed horizon T and K arms, the EXP3 algorithm provides an expected cumulative regret bound of order O( √ K ln(K)T ) against the best single-arm strategy. This algorithm is indeed adversarial because it does not require a stochastic assumption on the rewards. It is although not anytime because it requires the knowledge of the horizon T to run properly. A “doubling trick” solution is proposed by Auer et al. (2002b) to preserve the regret bound when T is unknown. It consists of running EXP3 in a carefully designed sequence of increasing epochs. Another elegant solution was later proposed by Seldin et al. (2012) for the same purpose.\n1The notation Õ(·) hides logarithmic factors."
    }, {
      "heading" : "2.2. Previous work on stochastic dueling bandits",
      "text" : "The dueling bandits problem is recent, although related to previous works on computing with noisy comparison (see for instance Karp and Kleinberg, 2007). This problem also falls under the framework of preference learning (Freund et al., 2003; Liu, 2009; Fürnkranz and Hüllermeier, 2010) which deals with learning of (predictive) preference models from observed (or extracted) preference information i.e. relative feedback which specifies which of the chosen alternatives is preferred. Most of the articles hitherto published on dueling bandits consider the problem under a stochastic assumption.\nYue and Joachims (2009) propose an algorithm called Dueling Bandit Gradient Descent (DBGD) to solve a version of the dueling bandits problem where context information is provided. They approach (contextual) dueling bandits as an on-line convex optimization problem.\nYue et al. (2012) propose an algorithm called Interleaved Flitering (IF). Their formulation is stochastic and matrixbased: for each pair (i, j) of arms, there is an unknown probability Pi,j for i to win against j. This preference matrix P of size K×K must satisfy the following symetry property:\n∀i, j ∈ {1, . . . ,K}, Pi,j + Pj,i = 1 (1)\nHence on the diagonal: Pi,i = 12 ∀i ∈ {1, . . . ,K}. Let i∗ be the “best arm” (as we will see later, this best arm coincides with the notion of Condorcet winner). Yue et al. (2012) define the regret incurred at the time instant t when arms a and b are pulled as:\nr′a,b = Pi∗,a + Pi∗,b − 1 2 ∈ (0, 1 2 ) (2)\nWe will call this regret a Condorcet regret.\nFor the IF algorithm to work, the P matrix is expected to satisfy several strong assumptions: strict linear ordering, stochastic transitivity, and stochastic triangular inequality. Under these three assumptions IF is guaranteed to suffer an expected cumulative regret of order O(K log T ). Yue and Joachims (2011) introduce Beat The Mean (BTM), an algorithm which proceeds by successive elimination of arms. This algorithm is less constrained than IF as it also applies to a relaxed setting where the preference matrix can slighlty violate the stochastic transitivity assumption. Its cumulative regret bound is of order O(γ7K log T ) where γ is here a known parameter.\nUrvoy et al. (2013) propose a generic algorithm called SAVAGE (for Sensitivity Analysis of VAriables for Generic Exploration) which does away with several assumptions made in the previous algorithms e.g. existence of inherent values of arms, existence of a linear order amongst arms. In\nthis general setting, the SAVAGE algorithm obtains a regret bound of order O(K2 log T ). The key notions they introduce for dueling bandits are the Copeland, Borda and Condorcet scores (Charon and Hudry, 2010). The Borda score of an arm i on a preference matrix P is ∑K j=1 Pi,j and its Copeland score is ∑K j=1JPi,j > 12K (We use J . . .K to denote the indicator function). If an arm has a Copeland score of K − 1, which means that it defeats all the other arms in the long run, it is called a Condorcet winner. The existence of a Condorcet winner is the minimum assumption required for the Condorcet regret as defined on equation (2) to be applicable. There exists however some datasets like MSLR30K (2012) where this Condorcet condition is not satisfied.\nZoghi et al. (2014a) extend the Upper Confidence Bound (UCB) algorithm (Auer et al., 2002a) and propose an algorithm called Relative Upper Confidence Bound (RUCB) provided that the preference matrix admits a Condorcet winner. They retrieve anO(K log T ) bound under this sole assumption. Unlike the previous algorithms, RUCB is an anytime dueling bandits algorithm since it does not require the time horizon T as input.\nAilon et al. (2014) propose three methods (DOUBLER, MULTISMB, and SPARRING) to reduce the stochastic utility-based dueling bandits problem to the conventional MAB problem. A stochastic dueling bandits problem is utility-based if the preference is the result of comparisons of the individual utility/reward of the arms. This is a strong restriction from the general – matrix-based – formulation of the problem.\nMore formally, there are K probability distributions v1, . . . , vK over [0, 1] associated respectively with arms 1, . . . ,K. Let µ1, . . . , µK be the respective means of v1, . . . , vK . When an arm a is pulled, its reward/utility xa is drawn from the corresponding distribution va2. Let i∗ ∈ arg maxµi be an optimal arm. The regret incurred at the time instant t when arms a and b are pulled is defined as:\nra,b(t) = 2xi∗ − xa − xb\n2 (3)\nWe will call this regret a bandit regret. With randomized tie-breaking we can rebuild the preference matrix:\nPi,j = P(xi > xj) + 1\n2 P(xi = xj)\nWhen all vi are Bernoulli laws, this reduces to:\nPi,j = µi − µj + 1\n2 (4)\n2Note that we frequently drop the time index when it is unnecessary or clear from context. For instance we simply write xa(t) or simply xa instead of xat(t).\nNote that if µi > µj for some arms i and j, then Pi,j > 12 . The best arm in the usual bandit sense hence coincides with the Condorcet winner (which turns out to be the Borda winner too) on the matrix formulation and the expected bandit regret is twice the Condorcet regret as defined in (2):\nEra,b = 2µi∗ − µa − µb\n2 = Pi∗,a + Pi∗,b − 1 = 2r′a,b\nSeveral other models and algorithms have been proposed since to handle stochastic dueling bandits. We can cite (Busa-Fekete et al., 2013; 2014; Zoghi et al., 2014b; 2015). See also (Busa-Fekete and Hüllermeier, 2014) for an extensive survey of this domain."
    }, {
      "heading" : "2.3. Adversarial dueling bandits",
      "text" : "The bibliography on stochastic dueling bandits is flourishing, but the results about adversarial dueling bandits remain quite scarce.\nA utility-based formulation of the problem is however proposed in Ailon et al. (2014, section 6). In this setting, as in classical adversarial MAB, the environment chooses beforehand an horizon T and a sequence of utility/reward vectors x(t) = (x1(t), . . . , xK(t)) ∈ [0, 1]K for t = 1, . . . , T . The learning algorithm aims at controling the bandit regret against the best single-arm strategy, as defined in (3), by choosing properly the pairs of arms (i, j) to be compared.\nTo tackle this problem, Ailon et al. (2014) suggest to apply the SPARRING reduction algorithm, although originally designed for stochastic settings, with an adversarial bandit algorithm like EXP3 as a black-box MAB. According to the authors, the SPARRING reduction preserves the O( √ KT lnK) upper bound of EXP3. This algorithm uses two separate MABs (one for each arm). As a consequence, when it gets a relative feedback about a duel (i, j), the left instantiation of EXP3 only updates its weight for arm i while the right instantiation only updates for j. The algorithm we propose improves from this solution by centralizing information for both arms on a single weight vector.\nAs mentionned earlier, the dueling bandits problem is a special instance of a partial monitoring game (Piccolboni and Schindelhauer, 2001; Cesa-Bianchi and Lugosi, 2009; Bartók et al., 2014). A partial monitoring game is defined by two matrices: a loss matrix L and a feedback matrix H. These two matrices are known by the learner. At each round, the learner chooses an action a while the environment simultaneously chooses an outcome (say x). The learner receives a feedback H(a,x) and suffers (in a blind manner) a loss L(a,x). It is straightforward to encode the classical MAB as a finite partial monitoring game: the actions are arms indices a ∈ {1, . . . ,K} while the environment outcomes\nare reward vectors x(t) = (x1(t), . . . , xK(t)) ∈ [0, 1]K . The loss and feedback matrices are respectivly defined by L(a,x) = −xa and H(a,x) = xa. For the utility-based dueling bandits problem, the learner’s actions are the duels (a, b) ∈ {1, . . . ,K}2 and the environment outcomes are reward vectors. If we constrain the rewards to be binary it turns out to be a finite partial monitoring game. The Loss matrix is defined by L ((a, b),x) = −(xa + xb)/2 and the feedback byH ((a, b),x) = ψ(xa − xb) where ψ is a nondecreasing transfer function such that ψ(0) = 0 (usually ψ(x) = Jx > 0K or ψ(x) = x). There are only four classes of finite partial monitoring games in term of time lower bounds, namely: trivial games with no regret at all, “easy” games with Θ̃ (√ T ) minimax regret, “hard” games with Θ̃ ( T 2/3 ) , and hopeless’ games with Ω (T ) regret.\nSeveral generic partial monitoring algorithms were recently proposed for both stochastic and adversarial settings (see Bartók et al., 2014, for details). If we except GLOBALEXP3 (Bartók, 2013) which tries to capture more finely the structure of the games, these algorithms only focus on the time bound and perform inefficiently when the number of actions grows.\nIn a dueling bandit the number of non-duplicate actions is actually K(K + 1)/2 and these algorithms, including GLOBALEXP3, provide at best a Õ ( K √ T )\nregret guarantee. The dedicated algorithm that we propose is using the preference feedback more efficiently."
    }, {
      "heading" : "3. Relative Exponential-weight Algorithm for Exploration and Exploitation",
      "text" : "The pseudo-code for the algorithm we propose is given in Algorithm 1. As previously stated on Section 2.3, this algorithm is designed to apply for the adversarial utility-based dueling bandits problem.\nIt is similar to the original EXP3 from step 1 to step 6 where it computes a distribution p(t) = (p1(t), . . . , pK(t)) which is a mixture of a normalized weighing of the arms wi/ ∑ i wi and a uniform distribution 1/K. As in EXP3, this uniform probability is introduced to ensure a minimum exploration of all arms.\nAt step 7, the algorithm draws two arms a and b independently according to p(t). At step 8, the algorithm gets ψ(xa − xb) as relative feedback . Note that, since arms are drawn with replacement, we may have a = b, in which case the algorithm will get no information. This event is indeed expected to become frequent when the p(t) distribution becomes peaked around the best arms. This necessity for a regret-minimizing dueling bandits algorithm to\nrenounce getting information when confident about its decision is a structural bias toward exploitation that is not encountered in classical bandits.\nStep 8 is the big difference from EXP3; because we only have access to the relative ψ(xa − xb) value, we have no mean to estimate the individual rewards xa or xb. There is however a solution to circumvent this problem: the best arm in expectation at time t is not only the one which maximizes the absolute reward. It is indeed the one which maximizes the regret of any fixed strategy π(t) against it:\narg max i xi(t) = arg max i\n( xi(t)− Ea∼π(t)xa ) .\nThis reference strategy could be a single-arm or uniform strategy but playing a suboptimal strategy to get a reference has a cost in terms of regret. One of our contributions is to show that the algorithm may use its own strategy as a reference.\nAt step 9, the condition a 6= b is only a slight improvement for matrix-based dueling bandits where the outcome of a duel of an arm against itself is randomized as in (4).\nAt steps 10 and 11, the weights of the played arms are updated. This update process is the core of our algorithm, it will be detailed in Section 4.\nStep 13 is only required for the anytime version of the algorithm. It will be explained in section 5.2.\nAlgorithm 1 REX3: Exp3 with relative feedback 1: Parameters: γ ∈ (0, 1] 2: Initialization: wi(1) = 1 for i = 1, . . . ,K. 3: for t = 1, 2, . . . do 4: for i = 1, . . . ,K do 5: Set pi(t)← (1− γ) wi(t)∑K\nj=1 wj(t) + γK\n6: end for 7: Pull two arms a and b chosen independently according to the distribution (p1(t), . . . , pK(t)). 8: Receive relative feedback ψ(xa − xb) ∈ [−1,+1] 9: if a 6= b then\n10: Set wa(t+ 1)← wa(t) · e γ K ψ(xa−xb) 2pa 11: Set wb(t+ 1)← wb(t) · e− γ K ψ(xa−xb) 2pb 12: end if 13: Update γ (for anytime version) 14: end for"
    }, {
      "heading" : "4. Analysis",
      "text" : "For the analysis, we focus on the simple case where ψ is the identity. It provides a ternary win/tie/loss feedback if we assume binary rewards. The main difference between EXP3 and our algorithm is at steps 10 and 11 of Algorithm 1,\nwhere we update the weights according to the duel outcome: the winning arm is gratified while the loser is penalized. This ‘punitive’ approach of exponential weighing departs from EXP3 and other weighing algorithms which gratify the most rewarding arms while kindly ignoring the nonrewarding ones (Freund and Schapire, 1999; Cesa-Bianchi and Lugosi, 2006).\n4.1. Upper bound for REX3\nIn this section, we provide a finite-horizon non-stochastic upper bound on the expected regret against the best single action policy.\nThe steps 10-11 on Algorithm 1 are equivalent to operating for each arm i an update of the form:\nwi(t+ 1) = wi(t) · e γ K ĉi(t)\nwhere\nĉi(t) = Ji = aK ψ(xa − xb) 2pa + Ji = bKψ(xb − xa) 2pb (5)\nOne big difference with EXP3 is that ĉi(t) not an estimator of the reward xi(t). We instead have:\nLemma 1.\nE [ĉi(t)|(a1, b1), .., (at−1, bt−1)] = Ea∼p(t)ψ(xi(t)−xa(t))\nThe proof of the lemma is given in Appendix B.2.\nIf ψ is the identity then Eĉi(t) = xi(t) − Ea∼p(t)xa(t) in which case we estimate the expected instantaneous regret of the algorithm against arm i. If we rather take ψ(x) = Jx > 0K, then Eĉi(t) = Pa∼p(t) (xi(t) > xa(t)), i.e. the probability for the algorithm to select an arm defeated by i. Let Gmax = maxi ∑T t=1 xi(t) be the best single-arm gain,\nand let Galg = 12 ∑T t=1 xa(t) + xb(t) be the gain of the al-\ngorithm. Let EGunif = 1K ∑T t=1 ∑K i=1 xi(t) be the average value of the game (i.e. the expected gain of the uniform sampling strategy).\nTheorem 1. If the transfer function ψ is the identity and γ ∈ (0, 12 ), then,\nGmax − E(Galg) ≤ K\nγ ln(K) + γτ\nwhere τ = e · EGalg − (4−e) · EGunif .\nThe proof of this theorem is detailed on Appendix B.\nProvided that EGalg ≤ Gmax and EGunif ≥ Gmin, where Gmin = mini ∑T t=1 xi(t) is the gain of the worst single-arm strategy, we can simplify the bound into:\nCorollary 1.\nGmax − EGalg ≤ KlnK\nγ + γ (eGmax − (4−e)Gmin)\nAs in (Auer et al., 2002b, section 3), since Kγ ln(K) + γτ is convex, we can obtain the optimal γ on (0, 12 ):\nγ∗ = min { 1\n2 ,\n√ K ln(K)\nτ\n} (6)\nSubstituting γ in Corollary 1 with its optimal value from eq. (6) we obtain: Gmax − E(Galg) ≤ 2 √ K ln(K) [eGmax − (4−e)Gmin]\nHence: Corollary 2. When γ = min { 1 2 , √ K ln(K) τ } , the expected regret of REX3 (Algorithm 1) is O (√ K ln(K)T ) .\nThe upper bound of REX3 for adversarial utility-based dueling bandits is hence the same as the one of EXP3 for aversarial MABs. For a high-number of arms or a short term horizon, this bound is competitive against theO (K ln(T )) or O ( K2 ln(T ) ) existing bounds for stochastic dueling bandits."
    }, {
      "heading" : "4.2. Lower bound for dueling bandits algorithms",
      "text" : "To provide a lower bound on the regret of any dueling bandits algorithm, we use a reduction to the classical MAB problem suggested by Ailon et al. (2014).\nAlgorithm 2 Reduction to classical MAB 1: DBA.init() 2: Set t = 1 3: repeat 4: (at, bt+1)← DBA.decide() 5: xat ← CBE.get reward() 6: xbt+1 ← CBE.get reward() 7: DBA.update((at, bt+1), (xat − xbt+1)) 8: t = t+ 2 9: until t ≥ T\nAlgorithm 2 gives an explicit formulation of this reduction by using a generic dueling bandits algorithm (DBA) as a black-box having the following public sub-routines: init(), decide() and update(). The classical bandit environment (CBE) provides get reward() which returns the reward of the input arm. The expected classicalbandit gain of Algorithm 2 will be twice the expected gain of the black-box dueling bandits it uses.\nIt is important to note that this reduction only works for stochastic settings where the expected reward of each arm remains the same across time instants. According to Theorem 5.1 given by Auer et al. (2002b, section 5), for K ≥ 2, the expected regret in the classical bandit setting is Ω( √ KT ) (assuming T is large enough i.e. T ≥ √ KT ). Since this result is obtained with a stationary stochastic distribution, by extension, the expected regret in the dueling bandits setting cannot be less than Ω( √ KT ).\nTheorem 2. For any number of actions K ≥ 2 and large enough time horizon T (i.e. T ≥ √ KT ), there exists a distribution over assignments of rewards such that the expected cumulative regret of any utility-based dueling bandits algorithm cannot be less than Ω( √ KT )."
    }, {
      "heading" : "5. Experiments",
      "text" : "To evaluate REX3 and other dueling bandits algorithms, we have applied them to the online comparison of rankers for search engines by interleaved filtering (Radlinski and Joachims, 2007). A search-engine ranker is a function that orders a collection of documents according to their relevancy to a given user search query. By interleaving the output of two rankers and tracking on which ranker’s output the user did click, we are able to get an unbiased feedback about the relative quality of these two rankers. Given K rankers, the problem of finding the best ranker is indeed a K-armed dueling bandits.\nIn order to obtain reproducible and comparable results, we adopted the stochastic matrix-based experiment setup already employed by Yue and Joachims (2011); Zoghi et al. (2014a;b; 2015) with both the cumulative Condorcet regret as defined by Yue et al. (2012) and the accuracy i.e. the best arm hit-rate over the runs: 1N ∑ n J(a, b) = (i∗, i∗)K.\nThis experimental setup uses real search engines’ logs to build empirical preference matrices. The duel outcomes are then simulated on these matrices. We used several preference matrices issued from namely: ARXIV dataset (Yue and Joachims, 2011), LETOR NP2004 dataset (Liu et al., 2007), and MSLR30K dataset. The last dataset distinguishes three kinds of queries: informational, navigational and perfecthit navigational (MSLR30K, 2012). These matrices are courtesy of Zoghi et al. (2014b)’s authors."
    }, {
      "heading" : "5.1. Empirical validation of Corollary 1",
      "text" : "We have used LETOR NP2004 and MSLR30K datasets (resricted to 64 rankers) to compare the average Condorcet regret of 100 runs of REX3 with T = 105 to the corresponding halved3 theoretical bounds from Corollary 1 for various values of γ. The results of this experiment are suma-\n3 As mentioned at the end of section 2.2, the utility-based bandit regret is indeed twice the Condorcet regret as defined in (2).\nrized in Figure 1. We plotted two theoretical curves: one with a conservative Gmax = T/2, and a riskier one with Gmax = T/4. This experiment illustrates the dual impact of the γ parameter on the exploration/exploitation tradeoff: a low value reduces both the exploration and the reactivity of the algorithm to unexpected feedbacks and a high value tends to uniformize exploration while increasing reactivity. It also shows that the theoretical optimal γ∗ we obtain with Equation (6) is a good guess even with a conservative upper-bound for Gmax."
    }, {
      "heading" : "5.2. Interleave filtering simulations",
      "text" : "For our experiments we have considered the following state of the art algorithms: BTM (Yue and Joachims, 2011) with γ = 1.1 and δ = 1/T (explore-then-exploit setting), Condorcet-SAVAGE (Urvoy et al., 2013) with δ = 1/T , RUCB (Zoghi et al., 2014a) with α = 0.51, and SPARRING coupled with EXP3 (Ailon et al., 2014). We also took the uniform sampling strategy RANDOM as a baseline. We considered three versions of REX3: two non-anytime versions where the optimal γ∗ is computed beforehand according to (6) with Gmax set respectively to T/2 and T/10 and one anytime version where γ∗ is recomputed at each time step according to (6) (see Seldin et al., 2012, for details about this form of “doubling trick”).\nA point which makes the comparison difficult is that some algorithms are anytime while others require the horizon as input. For anytime algorithms, namely RANDOM, RUCB and REX3 with adaptive γ, we displayed the average over 100 runs of the progressive accumulation of regret while for\nnon-anytime algorithms, namely BTM, CSAVAGE, SPARRING and other versions of REX3, we displayed the average over 50 runs of the final cumulative regret for several fixed and known horizons. This protocol is slightly favorable to non-anytime algorithms which benefit from more information. However, for elimination algorithms like BTM and CSAVAGE the difference between the anytime regret and the non-anytime regret is small. For adversarial algorithms like SPARRING and REX3 the “doubling trick” can be applied to make them anytime: the adaptive γ version of REX3 is an example of such a fixed-to-anytime transformation.\nThe results of these experiments are summarized in Figure 2, and 3. Furthermore, similar experiments are given as extended material.\nAs expected, the adversarial-setting algorithms SPARRING and REX3 follow an O( √ T ) regret curve while the stochastic-setting algorithms follow an O(lnT ) curve. Among the adversarial-setting algorithms, REX3 is shown to outperform SPARRING on all datasets. In the long run, adversarial-setting algorithms continue exploring and cannot compete in terms of regret against stochastic-setting algorithms, but the accuracy curves show that the cost of this exploration is very small. Moreover, for small horizons or high number of rankers, REX3 is extremely competitive against other algorithms.\nThis difference is clearly illustrated on the left-hand side of Figure 3 where we show the evolution of the expected cumulative regret at a fixed time horizon (T = 105) according to the number of arms. To obtain this plot we averaged the regret over 50 runs. For each K and each run we sampled uniformly K dimensions of the original 136×136 MSLR30K navigational preference matrix."
    }, {
      "heading" : "6. Conclusion",
      "text" : "We proposed REX3, an exponential weighing algorithm for adversarial utility-based dueling bandits. We provided both an upper and a lower bound for its expected cumulative regret. These two bounds match the original bounds of the classical EXP3 algorithm. A thorough empirical study on several information retrieval datasets has confirmed the validity of these theoretical results. It also showed that REX3 and especially its anytime version with adaptive γ are competitive solutions for dueling bandits, even when compared to stochastic-setting algorithms in a stochastic environment."
    }, {
      "heading" : "A. Proof Sketch for Theorem 1",
      "text" : "The general structure of the proof is similar to the one of (Auer et al., 2002b, section 3), but, as explained before, the ĉi(t) estimator we use differs from the one of EXP3\nbecause it gives an instantaneous regret estimate instead of an absolute reward estimate. As such, it may reach negative values and the wi(t) weights may decrease with time. We only give here a sketch of proof, stressing on the differences from (Auer et al., 2002b). An unfamiliar reader may refer to the extended version for step-by-step details.\nProof. LetWt = w1(t)+w2(t)+ · · ·+wK(t). As in EXP3 proof we consider:\nWt+1 Wt =\nK∑\ni=1\npi(t)− γ/K 1− γ e (γ/K)ĉi(t)\nThe inequality ex ≤ 1+x+(e−2)x2 is tight for x ∈ [0, 1] but it remains valid for negative values, hence:\nWt+1 Wt ≤ 1− γ 2/K 1− γ   1 K K∑\ni=1\nĉi(t)\n︸ ︷︷ ︸ =−M1\n \n+ (e− 2)γ2/K\n1− γ\n  1 K K∑\ni=1\npi(t)ĉi(t) 2\n︸ ︷︷ ︸ =M2\n \nAs in EXP3 we take the logarithm and sum over t. We get for any j:\nT∑\nt=1\nγ K ĉj(t)− ln(K) ≤\nγ2/K 1− γ M1 + (e− 2)γ2/K 1− γ M2\nBy taking the expectation over the algorithm’s randomization, we obtain for any j:\nT∑\nt=1\nγ K E∼pĉj(t)︸ ︷︷ ︸\n(8)\n− ln(K) ≤\nγ2/K 1− γ T∑\ni=t E∼pM1︸ ︷︷ ︸ (9)\n+ (e− 2)γ2/K 1− γ T∑\ni=t E∼pM2︸ ︷︷ ︸ (10) (7)\nFrom Lemma 1 we directly get the expected regret against j on the left side of the inequality:\nE∼pĉj(t) = xj − E∼p(xa) (8)\nBy averaging (8) over the arms, we obtain:\nE∼p(t)M1 = − 1\nK\nK∑\ni=1\nE∼pĉi(t) = E(xa)− 1\nK\nK∑\ni=1\nxi\n(9)\nThe following result is detailled in the extended version:\nE∼p(t)M2 ≤ 1\n2 E(xa) +\n1\n2K\nK∑\ni=1\nxi (10)\nFrom Lemma 1, (9), (10), and by definition of Gmax, EGalg, and EGunif , the inequality (7) rewrites into:\nGmax − EGalg − K lnK γ ≤ γ 1− γ (EGalg − EGunif )\n+ (e−2)γ 2(1− γ) (EGalg + EGunif )\nAssuming γ ≤ 12 we finally obtain:\nGmax − EGalg ≤ K lnK\nγ + γ (eEGalg − (4−e)EGunif )"
    }, {
      "heading" : "Acknowledgments",
      "text" : "We would like to thank the reviewers for their constructive comments and Masrour Zoghi who kindly sent us his experiment datasets."
    }, {
      "heading" : "B. Detailed Proof of Theorem 1",
      "text" : "For better readabilty, we simply write a, b instead of at, bt when referring of the arms chosen by the algorithm. We also frequently drop the time indices for p.(t) and x.(t). See Table 1 for an exhaustive list of notations.\nProof. Let Wt = w1(t) + w2(t) + · · ·+ wK(t)\nWt+1 Wt =\nK∑\ni=1\nwi(t+ 1)\nWt =\nK∑\ni=1\nwi(t)\nWt e(γ/K)ĉi(t)\nBy substituting the values of wi(t)Wt , we get:\nWt+1 Wt =\nK∑\ni=1\npi(t)− γ/K 1− γ e (γ/K)ĉi(t)\nUsing the inequality ex ≤ 1 + x+ (e− 2)x2 for x ≤ 1, we get:\nWt+1 Wt\n≤ K∑\ni=1\npi(t)− γ/K 1− γ (1 + (γ/K)ĉi(t)) +\nK∑\ni=1\npi(t)− γ/K 1− γ ( (e− 2)(γ2/K2)ĉi(t)2 )\n≤ K∑\ni=1\npi(t)− γ/K 1− γ ︸ ︷︷ ︸ =1 + γ/K 1− γ\n  K∑\ni=1\npi(t)ĉi(t)\n︸ ︷︷ ︸ =0 see (12)\n− γ K\nK∑\ni=1\nĉi(t)   + (e− 2)(γ2/K2) 1− γ K∑\ni=1\npi(t)ĉi(t) 2\n≤ 1 + γ/K 1− γ\n( 0− γ\nK\nK∑\ni=1\nĉi(t)\n) +\n(e− 2)(γ2/K2) 1− γ\nK∑\ni=1\npi(t)ĉi(t) 2\n≤ 1− γ 2/K\n1− γ   1 K K∑\ni=1\nĉi(t)\n︸ ︷︷ ︸ =−M1\n  + (e− 2)(γ2/K) 1− γ 1 K K∑\ni=1\npi(t)ĉi(t) 2\n︸ ︷︷ ︸ =M2\n(11)\nK∑\ni=1\npi(t)ĉi(t) =\nK∑\ni=1\npi(t)\n( Ji = aKxa − xb\n2pa\n) + K∑\ni=1\npi(t)\n( Ji = bKxb − xa\n2pb\n)\n= xa − xb 2 + xb − xa 2 = 0 (12)\nFrom (11) and (12), we obtain:\nWt+1 Wt ≤ 1 + γ 2/K 1− γ M1 + (e− 2)(γ2/K) 1− γ M2\nTaking logarithms and using the inequality 1 + x ≤ ex\nln Wt+1 Wt ≤ γ 2/K 1− γ M1 + (e− 2)(γ2/K) 1− γ M2\nSumming over t, we get:\nln WT+1 W1 ≤ γ 2/K 1− γ M1 + (e− 2)(γ2/K) 1− γ M2 (13)\nFor any arm j we have: T∑\nt=1\nγ K ĉj(t)− ln(K) ≤ ln WT+1 W1\n(14)\nThe proof of the above inequality is given in appendix B.1. By combining (13) and (14) , we get:\nT∑\nt=1\nγ K ĉj(t)− ln(K) ≤\nγ2/K 1− γ M1 + (e− 2)γ2/K 1− γ M2 (15)\nTaking the expectation over the algorithm’s randomization, we obtain:\nT∑\nt=1\nγ K E∼pĉj(t)︸ ︷︷ ︸\nLemma 1\n− ln(K) ≤ γ 2/K 1− γ T∑\ni=t E∼pM1︸ ︷︷ ︸ see (17)\n+ (e− 2)(γ2/K) 1− γ T∑\ni=t E∼pM2︸ ︷︷ ︸ see (18)\nFrom Lemma 1 which proof is detailed in appendix B.2, we have:\nE∼pĉj(t) = xj − E∼p(xa) (16)\nBy averaging (16) over the arms, we obtain:\nE∼p(t)M1 = E∼p ( − 1 K K∑\ni=1\nĉi(t)\n) = − 1\nK\nK∑\ni=1\nE∼pĉi(t) = E(xa)− 1\nK\nK∑\ni=1\nxi (17)\nThe following result is detailled in appendix B.3:\nE∼p(t)M2 = E∼p(t)\n( (pa + pb)(xa − xb)2\n4Kpapb\n) = 1\n2 E(x2a)− E(xa)\n1\nK\nK∑\ni=1\nxi + 1\n2K\nK∑\ni=1\nx2i\n≤ 1 2 E(xa)− E(xa) 1 K\nK∑\ni=1\nxi + 1\n2K\nK∑\ni=1\nxi as ∀i, xi ∈ [0, 1] (18)\nFrom Lemma 1, (17), and (18), we get for any j:\nγ\nK\n( T∑\nt=1\nxj − T∑\nt=1\nE(xa)\n) − ln(K) ≤ γ 2/K\n1− γ T∑\ni=t\n( E(xa)− 1\nK\nK∑\ni=1\nxi\n)\n+ (e−2)γ2/K 2(1− γ) T∑\ni=t\n( E(x2a)− 2E(xa) 1\nK\nK∑\ni=1\nxi + 1\nK\nK∑\ni=1\nx2i\n) (19)\nBy definition, Gmax = maxj ∑T t=1 xj , EGalg = ∑T t=1E∼p(t)(xa), and EGunif = ∑T t=1 1 K ∑K i=1 xi. We can hence rewrite Equation (19) into:\nGmax − EGalg − K lnK γ ≤ γ 1− γ (EGalg − EGunif ) + (e−2)γ 2(1− γ)\n( EGalg + EGunif − 2 T∑\nt=1\nK∑\ni=1\nxi K E(xa)\n) (20)\nLet ε be such that ∀i, t ε ≤ xi(t) then:\nGmax − EGalg ≤ K lnK\nγ +\neγ 2(1− γ)EGalg − (4−e+ (e−2)ε)γ 2(1− γ) EGunif (21)\nAssuming γ ≤ 12 :\nGmax − EGalg ≤ K lnK\nγ + γ [eEGalg − (4−e+ (e−2)ε)EGunif ] (22)\nB.1. Proof of eq. (14) For any j we have: WT+1 = ∑K i=1 wi(T + 1) ≥ wj(T + 1).\nHence:\nWT+1 ≥ wj(T )e(γ/K)(ĉj(T )) = wj(T − 1)e(γ/K)(ĉj(T−1))e(γ/K)(ĉj(T )) = wj(1) T∏\nt=1\ne(γ/K)(ĉj(t)), and\nlnWT+1 ≥ lnwj(1) + T∑\nt=1\nγ K ĉj(t)\nSince wj(1) = 1 for any j, it turns out that: ∑T t=1 γ K ĉj(t)− ln(K) ≤ ln( WT+1 W1 )\nB.2. Proof of Lemma 1\nĉi(t) = Ji = atK ψ(xat − xbt)\n2pat(t) + Ji = btK ψ(xbt − xat) 2pbt(t)\nE(a,b)∼p(t)ĉi(t) = K∑\nj=1\nK∑\nk=1\npj(t)pk(t)\n( Ji = jKψ(xj − xk)\n2pj + Ji = kKψ(xk − xj) 2pk\n)\n=\nK∑\nj=1\nK∑\nk=1\npjpkJi = jK ψ(xj − xk)\n2pj +\nK∑\nj=1\nK∑\nk=1\npjpkJi = kK ψ(xk − xj)\n2pk\n= 1\n2\nK∑\nk=1\npkψ(xi − xk) + 1\n2\nK∑\nj=1\npjψ(xi − xj)\n= Ea∼pψ(xi − xa)\nIf ψ(x) = x it simplifies into:\nE(a,b)∼p(t)ĉi(t) = xi − Ea∼p(t)xa\nB.3. Proof of eq. (10) and (18)\nM2 = 1\nK\nK∑\ni=1\npi(t)ĉi(t) 2\n= 1\nK\nK∑\ni=1\npi\n( Ji = aKxa − xb\n2pa + Ji = bKxb − xa 2pb\n)2\n= 1\nK\nK∑\ni=1\n piJi = aK\n(xa − xb)2 4p2a + pi(t)Ji = bK (xb − xa)2 4p2b + 2piJi = aKJi = bK xa − xb 2pa xb − xa 2pb︸ ︷︷ ︸\n=0\n \n=\nK∑\ni=1\npiJi = aK (xa − xb)2\n4Kp2a +\nK∑\ni=1\npiJi = bK (xb − xa)2\n4Kp2b = (xa − xb)2 4Kpa + (xb − xa)2 4Kpb\n= (pa + pb)(xa − xb)2\n4Kpapb\nE(a,b)∼p(t)M2 = E(a,b)∼p(t)\n( (pa + pb)(xa − xb)2\n4Kpapb\n)\n=\nK∑\ni=1\nK∑\nj=1\npipj (pi + pj)(xi − xj)2\n4Kpipj =\n1\n4K\nK∑\ni=1\nK∑\nj=1\n(pi + pj)(xi − xj)2\n= 1\n4K\nK∑\ni=1\nK∑\nj=1\n(pi + pj)(x 2 i − 2xixj + x2j )\n= 1\n4K\n  K∑\ni=1\nK∑\nj=1\npix 2 i − 2\nK∑\ni=1\nK∑\nj=1\npixixj +\nK∑\ni=1\nK∑\nj=1\npix 2 j +\nK∑\ni=1\nK∑\nj=1\npjx 2 i − 2\nK∑\ni=1\nK∑\nj=1\npjxixj +\nK∑\ni=1\nK∑\nj=1\npjx 2 j\n \n= 1\n4K\n( 2KE(x2a)− 4E(xa) K∑\ni=1\nxi + 2\nK∑\ni=1\nx2i\n)\n= 1\n2\n( Ea∼p(t)(x 2 a)− 2Ea∼p(t)(xa) 1\nK\nK∑\ni=1\nxi + 1\nK\nK∑\ni=1\nx2i\n)\nFor any x ∈ [0, 1], x2 < x, hence:\nE(a,b)∼p(t)M2 ≤ 1\n2\n( Ea∼p(t)(xa)− 2Ea∼p(t)(xa) 1\nK\nK∑\ni=1\nxi + 1\nK\nK∑\ni=1\nxi\n)"
    }, {
      "heading" : "C. Further Experiments",
      "text" : "We give here the simulation results that could not fit on the core article: Figure 4 gives results for smaller number of rankers on NP2004 dataset and Figure 5 complete the experiments on MSLR30K dataset. On Figure 6 we added an experiment we made with Sparring coupled with UCB. We also considered two artificial matrices: SAVAGE and BVS. The 30×30 SAVAGE matrix, defined by Pi,j = 12 + j/(2K) for i < j as described in (Urvoy et al., 2013). The 20×20 BVS matrix is defined by: P1,j = 0.51 for any j > 1 and Pi,j = 1 for any 1 < i < j. Its Condorcet winner has a low Borda score (9.69 against 18.49 for the Borda winner) which makes it difficult for algorithms to find the real Condorcet winner. These experiments results are sumarized in Figure 7. The preference matrices we used and their properties are summarized on Table 2.\nWe conclude these experiments by a non-stationnary utility-based dueling bandit simulation where the expected reward gap ∆(t) between the best arm and the others is set in order to decieve stochastic algorithms (see Figure 8).\n∆(T )\n) ∼ O (√ KT · log(T ) ) . To ease reading we provide the same plot with logarithmic scale on the\nleft and linear scale on the right."
    } ],
    "references" : [ {
      "title" : "Reducing dueling bandits to cardinal bandits",
      "author" : [ "N. Ailon", "Z.S. Karnin", "T. Joachims" ],
      "venue" : "In ICML 2014,",
      "citeRegEx" : "Ailon et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Ailon et al\\.",
      "year" : 2014
    }, {
      "title" : "Finitetime analysis of the multiarmed bandit problem",
      "author" : [ "P. Auer", "N. Cesa-Bianchi", "P. Fischer" ],
      "venue" : null,
      "citeRegEx" : "Auer et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Auer et al\\.",
      "year" : 2002
    }, {
      "title" : "The nonstochastic multiarmed bandit problem",
      "author" : [ "P. Auer", "N. Cesa-Bianchi", "Y. Freund", "R.E. Schapire" ],
      "venue" : "SIAM Journal on Computing,",
      "citeRegEx" : "Auer et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Auer et al\\.",
      "year" : 2002
    }, {
      "title" : "A near-optimal algorithm for finite partial-monitoring games against adversarial opponents",
      "author" : [ "G. Bartók" ],
      "venue" : "In Proc. COLT",
      "citeRegEx" : "Bartók,? \\Q2013\\E",
      "shortCiteRegEx" : "Bartók",
      "year" : 2013
    }, {
      "title" : "Partial monitoring - classification, regret bounds, and algorithms",
      "author" : [ "G. Bartók", "D.P. Foster", "D. Pál", "A. Rakhlin", "C. Szepesvári" ],
      "venue" : null,
      "citeRegEx" : "Bartók et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bartók et al\\.",
      "year" : 2014
    }, {
      "title" : "Regret analysis of stochastic and nonstochastic multi-armed bandit problems",
      "author" : [ "S. Bubeck", "N. Cesa-Bianchi" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "Bubeck and Cesa.Bianchi,? \\Q2012\\E",
      "shortCiteRegEx" : "Bubeck and Cesa.Bianchi",
      "year" : 2012
    }, {
      "title" : "A survey of preference-based online learning with bandit algorithms",
      "author" : [ "R. Busa-Fekete", "E. Hüllermeier" ],
      "venue" : "ALT",
      "citeRegEx" : "Busa.Fekete and Hüllermeier,? \\Q2014\\E",
      "shortCiteRegEx" : "Busa.Fekete and Hüllermeier",
      "year" : 2014
    }, {
      "title" : "Preference-based rank elicitation using statistical models: The case of mallows",
      "author" : [ "R. Busa-Fekete", "E. Hüllermeier", "B. Szörényi" ],
      "venue" : "ICML",
      "citeRegEx" : "Busa.Fekete et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Busa.Fekete et al\\.",
      "year" : 2014
    }, {
      "title" : "Top-k selection based on adaptive sampling of noisy preferences",
      "author" : [ "R. Busa-Fekete", "B. Szörényi", "W. Cheng", "P. Weng", "E. Hüllermeier" ],
      "venue" : "In ICML 2013,",
      "citeRegEx" : "Busa.Fekete et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Busa.Fekete et al\\.",
      "year" : 2013
    }, {
      "title" : "Prediction, Learning, and Games",
      "author" : [ "N. Cesa-Bianchi", "G. Lugosi" ],
      "venue" : null,
      "citeRegEx" : "Cesa.Bianchi and Lugosi,? \\Q2006\\E",
      "shortCiteRegEx" : "Cesa.Bianchi and Lugosi",
      "year" : 2006
    }, {
      "title" : "Combinatorial bandits",
      "author" : [ "N. Cesa-Bianchi", "G. Lugosi" ],
      "venue" : "COLT",
      "citeRegEx" : "Cesa.Bianchi and Lugosi,? \\Q2009\\E",
      "shortCiteRegEx" : "Cesa.Bianchi and Lugosi",
      "year" : 2009
    }, {
      "title" : "Large-scale validation and analysis of interleaved search evaluation",
      "author" : [ "O. Chapelle", "T. Joachims", "F. Radlinski", "Y. Yue" ],
      "venue" : "ACM Trans. Inf. Syst.,",
      "citeRegEx" : "Chapelle et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Chapelle et al\\.",
      "year" : 2012
    }, {
      "title" : "An updated survey on the linear ordering problem for weighted or unweighted tournaments",
      "author" : [ "I. Charon", "O. Hudry" ],
      "venue" : "Annals OR,",
      "citeRegEx" : "Charon and Hudry,? \\Q2010\\E",
      "shortCiteRegEx" : "Charon and Hudry",
      "year" : 2010
    }, {
      "title" : "An efficient boosting algorithm for combining preferences",
      "author" : [ "Y. Freund", "R. Iyer", "R.E. Schapire", "Y. Singer" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "Freund et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Freund et al\\.",
      "year" : 2003
    }, {
      "title" : "Adaptive game playing using multiplicative weights",
      "author" : [ "Y. Freund", "R.E. Schapire" ],
      "venue" : "Games and Economic Behavior,",
      "citeRegEx" : "Freund and Schapire,? \\Q1999\\E",
      "shortCiteRegEx" : "Freund and Schapire",
      "year" : 1999
    }, {
      "title" : "Evaluating the accuracy of implicit feedback from clicks and query reformulations in web search",
      "author" : [ "T. Joachims", "L. Granka", "B. Pan", "H. Hembrooke", "F. Radlinski", "G. Gay" ],
      "venue" : "ACM Trans. Inf. Syst.,",
      "citeRegEx" : "Joachims et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Joachims et al\\.",
      "year" : 2007
    }, {
      "title" : "Noisy binary search and its applications",
      "author" : [ "R.M. Karp", "R. Kleinberg" ],
      "venue" : "SODA",
      "citeRegEx" : "Karp and Kleinberg,? \\Q2007\\E",
      "shortCiteRegEx" : "Karp and Kleinberg",
      "year" : 2007
    }, {
      "title" : "Learning to rank for information retrieval",
      "author" : [ "Liu", "T.-Y" ],
      "venue" : "Found. Trends Inf. Retr.,",
      "citeRegEx" : "Liu and T..Y.,? \\Q2009\\E",
      "shortCiteRegEx" : "Liu and T..Y.",
      "year" : 2009
    }, {
      "title" : "LETOR: Benchmark dataset for research on learning to rank for information retrieval",
      "author" : [ "Liu", "T.-Y", "J. Xu", "T. Qin", "W. Xiong", "H. Li" ],
      "venue" : null,
      "citeRegEx" : "Liu et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2007
    }, {
      "title" : "Discrete prediction games with arbitrary feedback and loss",
      "author" : [ "A. Piccolboni", "C. Schindelhauer" ],
      "venue" : "In COLT/EuroCOLT,",
      "citeRegEx" : "Piccolboni and Schindelhauer,? \\Q2001\\E",
      "shortCiteRegEx" : "Piccolboni and Schindelhauer",
      "year" : 2001
    }, {
      "title" : "Active exploration for learning rankings from clickthrough data",
      "author" : [ "F. Radlinski", "T. Joachims" ],
      "venue" : "KDD",
      "citeRegEx" : "Radlinski and Joachims,? \\Q2007\\E",
      "shortCiteRegEx" : "Radlinski and Joachims",
      "year" : 2007
    }, {
      "title" : "Evaluation and analysis of the performance of the exp3 Algorithm in stochastic environments",
      "author" : [ "Y. Seldin", "C. Szepesvári", "P. Auer", "Y. Abbasi-Yadkori" ],
      "venue" : "In EWRL, volume 24 of JMLR Proceedings,",
      "citeRegEx" : "Seldin et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Seldin et al\\.",
      "year" : 2012
    }, {
      "title" : "Generic exploration and K-armed voting bandits",
      "author" : [ "T. Urvoy", "F. Clerot", "R. Féraud", "S. Naamane" ],
      "venue" : "In ICML 2013,",
      "citeRegEx" : "Urvoy et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Urvoy et al\\.",
      "year" : 2013
    }, {
      "title" : "The k-armed dueling bandits problem",
      "author" : [ "Y. Yue", "J. Broder", "R. Kleinberg", "T. Joachims" ],
      "venue" : "J. Comput. Syst. Sci.,",
      "citeRegEx" : "Yue et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Yue et al\\.",
      "year" : 2012
    }, {
      "title" : "Interactively optimizing information retrieval systems as a dueling bandits problem",
      "author" : [ "Y. Yue", "T. Joachims" ],
      "venue" : "ICML",
      "citeRegEx" : "Yue and Joachims,? \\Q2009\\E",
      "shortCiteRegEx" : "Yue and Joachims",
      "year" : 2009
    }, {
      "title" : "Beat the mean bandit",
      "author" : [ "Y. Yue", "T. Joachims" ],
      "venue" : "ICML",
      "citeRegEx" : "Yue and Joachims,? \\Q2011\\E",
      "shortCiteRegEx" : "Yue and Joachims",
      "year" : 2011
    }, {
      "title" : "MergeRUCB: A method for large-scale online ranker evaluation",
      "author" : [ "M. Zoghi", "S. Whiteson", "M. de Rijke" ],
      "venue" : "WSDM",
      "citeRegEx" : "Zoghi et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Zoghi et al\\.",
      "year" : 2015
    }, {
      "title" : "Relative upper confidence bound for the karmed dueling bandit problem",
      "author" : [ "M. Zoghi", "S. Whiteson", "R. Munos", "M. de Rijke" ],
      "venue" : "In ICML 2014,",
      "citeRegEx" : "Zoghi et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Zoghi et al\\.",
      "year" : 2014
    }, {
      "title" : "Relative confidence sampling for efficient online ranker evaluation",
      "author" : [ "M. Zoghi", "S.A. Whiteson", "M. de Rijke", "R. Munos" ],
      "venue" : "WSDM",
      "citeRegEx" : "Zoghi et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Zoghi et al\\.",
      "year" : 2014
    }, {
      "title" : "Further Experiments We give here the simulation results that could not fit on the core article: Figure 4 gives results for smaller number of rankers on NP2004 dataset and Figure 5 complete the experiments on MSLR30K dataset. On Figure 6 we added an experiment we made with Sparring coupled with UCB",
      "author" : [ ],
      "venue" : "SAVAGE",
      "citeRegEx" : "C.,? \\Q2004\\E",
      "shortCiteRegEx" : "C.",
      "year" : 2004
    } ],
    "referenceMentions" : [ {
      "referenceID" : 20,
      "context" : "The accuracy of this interleave filtering method was highlighted in several experimental studies (Radlinski and Joachims, 2007; Joachims et al., 2007; Chapelle et al., 2012).",
      "startOffset" : 97,
      "endOffset" : 173
    }, {
      "referenceID" : 15,
      "context" : "The accuracy of this interleave filtering method was highlighted in several experimental studies (Radlinski and Joachims, 2007; Joachims et al., 2007; Chapelle et al., 2012).",
      "startOffset" : 97,
      "endOffset" : 173
    }, {
      "referenceID" : 11,
      "context" : "The accuracy of this interleave filtering method was highlighted in several experimental studies (Radlinski and Joachims, 2007; Joachims et al., 2007; Chapelle et al., 2012).",
      "startOffset" : 97,
      "endOffset" : 173
    }, {
      "referenceID" : 20,
      "context" : "Introduction The K-armed dueling bandit problem is a variation of the classical Multi-Armed Bandit (MAB) problem introduced by Yue and Joachims (2009) to formalize the exploration/exploitation dilemma in learning from preference feedback.",
      "startOffset" : 127,
      "endOffset" : 151
    }, {
      "referenceID" : 17,
      "context" : "This is a perfect example of partial monitoring problem as defined in Piccolboni and Schindelhauer (2001). Proceedings of the 32 International Conference on Machine Learning, Lille, France, 2015.",
      "startOffset" : 70,
      "endOffset" : 106
    }, {
      "referenceID" : 17,
      "context" : "This is a perfect example of partial monitoring problem as defined in Piccolboni and Schindelhauer (2001). Proceedings of the 32 International Conference on Machine Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s). Relative feedback is naturally suited to many practical applications like user-perceived product preferences, where a relative perception: “A is better than B” is easier to obtain than its absolute counterpart: “A value is 42, B is worth 33”. Another important application of dueling bandits comes from information retrieval systems where users provide implicit feedback about the provided results. This implicit feedback is collected in various ways e.g. a click on a link, a tap, or any monitored action of the user. In all these ways however, this kind of feedback is often strongly biased by the model itself (the user cannot click on a link which was not proposed). To remove this bias in search engines, Radlinski and Joachims (2007) propose to interleave the outputs of different ranking models: the model which scores a click wins the duel.",
      "startOffset" : 70,
      "endOffset" : 991
    }, {
      "referenceID" : 0,
      "context" : "We prove a finite time expected regret upper bound of order O( √ K ln(K)T ) and develop an argument initially proposed by Ailon et al. (2014) to exhibit a general lower bound of order Ω( √ KT ) for this problem.",
      "startOffset" : 122,
      "endOffset" : 142
    }, {
      "referenceID" : 5,
      "context" : "Previous Work and Notations The conventional MAB problem has been well studied in the stochastic setting as well as the (oblivious) adversarial setting (see Cesa-Bianchi and Lugosi, 2006; Bubeck and Cesa-Bianchi, 2012).",
      "startOffset" : 152,
      "endOffset" : 218
    }, {
      "referenceID" : 13,
      "context" : "This problem also falls under the framework of preference learning (Freund et al., 2003; Liu, 2009; Fürnkranz and Hüllermeier, 2010) which deals with learning of (predictive) preference models from observed (or extracted) preference information i.",
      "startOffset" : 67,
      "endOffset" : 132
    }, {
      "referenceID" : 1,
      "context" : "Exponential-weight algorithm for Exploration and Exploitation Of particular interest are the Exponential-weight algorithm for Exploration and Exploitation (EXP3) and its variants presented by Auer et al. (2002b) for the adversarial bandit setting.",
      "startOffset" : 192,
      "endOffset" : 212
    }, {
      "referenceID" : 1,
      "context" : "Exponential-weight algorithm for Exploration and Exploitation Of particular interest are the Exponential-weight algorithm for Exploration and Exploitation (EXP3) and its variants presented by Auer et al. (2002b) for the adversarial bandit setting. For a fixed horizon T and K arms, the EXP3 algorithm provides an expected cumulative regret bound of order O( √ K ln(K)T ) against the best single-arm strategy. This algorithm is indeed adversarial because it does not require a stochastic assumption on the rewards. It is although not anytime because it requires the knowledge of the horizon T to run properly. A “doubling trick” solution is proposed by Auer et al. (2002b) to preserve the regret bound when T is unknown.",
      "startOffset" : 192,
      "endOffset" : 672
    }, {
      "referenceID" : 1,
      "context" : "Exponential-weight algorithm for Exploration and Exploitation Of particular interest are the Exponential-weight algorithm for Exploration and Exploitation (EXP3) and its variants presented by Auer et al. (2002b) for the adversarial bandit setting. For a fixed horizon T and K arms, the EXP3 algorithm provides an expected cumulative regret bound of order O( √ K ln(K)T ) against the best single-arm strategy. This algorithm is indeed adversarial because it does not require a stochastic assumption on the rewards. It is although not anytime because it requires the knowledge of the horizon T to run properly. A “doubling trick” solution is proposed by Auer et al. (2002b) to preserve the regret bound when T is unknown. It consists of running EXP3 in a carefully designed sequence of increasing epochs. Another elegant solution was later proposed by Seldin et al. (2012) for the same purpose.",
      "startOffset" : 192,
      "endOffset" : 871
    }, {
      "referenceID" : 1,
      "context" : "Exponential-weight algorithm for Exploration and Exploitation Of particular interest are the Exponential-weight algorithm for Exploration and Exploitation (EXP3) and its variants presented by Auer et al. (2002b) for the adversarial bandit setting. For a fixed horizon T and K arms, the EXP3 algorithm provides an expected cumulative regret bound of order O( √ K ln(K)T ) against the best single-arm strategy. This algorithm is indeed adversarial because it does not require a stochastic assumption on the rewards. It is although not anytime because it requires the knowledge of the horizon T to run properly. A “doubling trick” solution is proposed by Auer et al. (2002b) to preserve the regret bound when T is unknown. It consists of running EXP3 in a carefully designed sequence of increasing epochs. Another elegant solution was later proposed by Seldin et al. (2012) for the same purpose. The notation Õ(·) hides logarithmic factors. 2.2. Previous work on stochastic dueling bandits The dueling bandits problem is recent, although related to previous works on computing with noisy comparison (see for instance Karp and Kleinberg, 2007). This problem also falls under the framework of preference learning (Freund et al., 2003; Liu, 2009; Fürnkranz and Hüllermeier, 2010) which deals with learning of (predictive) preference models from observed (or extracted) preference information i.e. relative feedback which specifies which of the chosen alternatives is preferred. Most of the articles hitherto published on dueling bandits consider the problem under a stochastic assumption. Yue and Joachims (2009) propose an algorithm called Dueling Bandit Gradient Descent (DBGD) to solve a version of the dueling bandits problem where context information is provided.",
      "startOffset" : 192,
      "endOffset" : 1607
    }, {
      "referenceID" : 1,
      "context" : "Exponential-weight algorithm for Exploration and Exploitation Of particular interest are the Exponential-weight algorithm for Exploration and Exploitation (EXP3) and its variants presented by Auer et al. (2002b) for the adversarial bandit setting. For a fixed horizon T and K arms, the EXP3 algorithm provides an expected cumulative regret bound of order O( √ K ln(K)T ) against the best single-arm strategy. This algorithm is indeed adversarial because it does not require a stochastic assumption on the rewards. It is although not anytime because it requires the knowledge of the horizon T to run properly. A “doubling trick” solution is proposed by Auer et al. (2002b) to preserve the regret bound when T is unknown. It consists of running EXP3 in a carefully designed sequence of increasing epochs. Another elegant solution was later proposed by Seldin et al. (2012) for the same purpose. The notation Õ(·) hides logarithmic factors. 2.2. Previous work on stochastic dueling bandits The dueling bandits problem is recent, although related to previous works on computing with noisy comparison (see for instance Karp and Kleinberg, 2007). This problem also falls under the framework of preference learning (Freund et al., 2003; Liu, 2009; Fürnkranz and Hüllermeier, 2010) which deals with learning of (predictive) preference models from observed (or extracted) preference information i.e. relative feedback which specifies which of the chosen alternatives is preferred. Most of the articles hitherto published on dueling bandits consider the problem under a stochastic assumption. Yue and Joachims (2009) propose an algorithm called Dueling Bandit Gradient Descent (DBGD) to solve a version of the dueling bandits problem where context information is provided. They approach (contextual) dueling bandits as an on-line convex optimization problem. Yue et al. (2012) propose an algorithm called Interleaved Flitering (IF).",
      "startOffset" : 192,
      "endOffset" : 1867
    }, {
      "referenceID" : 1,
      "context" : "Exponential-weight algorithm for Exploration and Exploitation Of particular interest are the Exponential-weight algorithm for Exploration and Exploitation (EXP3) and its variants presented by Auer et al. (2002b) for the adversarial bandit setting. For a fixed horizon T and K arms, the EXP3 algorithm provides an expected cumulative regret bound of order O( √ K ln(K)T ) against the best single-arm strategy. This algorithm is indeed adversarial because it does not require a stochastic assumption on the rewards. It is although not anytime because it requires the knowledge of the horizon T to run properly. A “doubling trick” solution is proposed by Auer et al. (2002b) to preserve the regret bound when T is unknown. It consists of running EXP3 in a carefully designed sequence of increasing epochs. Another elegant solution was later proposed by Seldin et al. (2012) for the same purpose. The notation Õ(·) hides logarithmic factors. 2.2. Previous work on stochastic dueling bandits The dueling bandits problem is recent, although related to previous works on computing with noisy comparison (see for instance Karp and Kleinberg, 2007). This problem also falls under the framework of preference learning (Freund et al., 2003; Liu, 2009; Fürnkranz and Hüllermeier, 2010) which deals with learning of (predictive) preference models from observed (or extracted) preference information i.e. relative feedback which specifies which of the chosen alternatives is preferred. Most of the articles hitherto published on dueling bandits consider the problem under a stochastic assumption. Yue and Joachims (2009) propose an algorithm called Dueling Bandit Gradient Descent (DBGD) to solve a version of the dueling bandits problem where context information is provided. They approach (contextual) dueling bandits as an on-line convex optimization problem. Yue et al. (2012) propose an algorithm called Interleaved Flitering (IF). Their formulation is stochastic and matrixbased: for each pair (i, j) of arms, there is an unknown probability Pi,j for i to win against j. This preference matrix P of size K×K must satisfy the following symetry property: ∀i, j ∈ {1, . . . ,K}, Pi,j + Pj,i = 1 (1) Hence on the diagonal: Pi,i = 12 ∀i ∈ {1, . . . ,K}. Let i∗ be the “best arm” (as we will see later, this best arm coincides with the notion of Condorcet winner). Yue et al. (2012) define the regret incurred at the time instant t when arms a and b are pulled as: r′ a,b = Pi∗,a + Pi∗,b − 1 2 ∈ (0, 1 2 ) (2)",
      "startOffset" : 192,
      "endOffset" : 2369
    }, {
      "referenceID" : 23,
      "context" : "Yue and Joachims (2011) introduce Beat The Mean (BTM), an algorithm which proceeds by successive elimination of arms.",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 22,
      "context" : "Urvoy et al. (2013) propose a generic algorithm called SAVAGE (for Sensitivity Analysis of VAriables for Generic Exploration) which does away with several assumptions made in the previous algorithms e.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 12,
      "context" : "The key notions they introduce for dueling bandits are the Copeland, Borda and Condorcet scores (Charon and Hudry, 2010).",
      "startOffset" : 96,
      "endOffset" : 120
    }, {
      "referenceID" : 9,
      "context" : "The key notions they introduce for dueling bandits are the Copeland, Borda and Condorcet scores (Charon and Hudry, 2010). The Borda score of an arm i on a preference matrix P is ∑K j=1 Pi,j and its Copeland score is ∑K j=1JPi,j > 1 2K (We use J . . .K to denote the indicator function). If an arm has a Copeland score of K − 1, which means that it defeats all the other arms in the long run, it is called a Condorcet winner. The existence of a Condorcet winner is the minimum assumption required for the Condorcet regret as defined on equation (2) to be applicable. There exists however some datasets like MSLR30K (2012) where this Condorcet condition is not satisfied.",
      "startOffset" : 97,
      "endOffset" : 621
    }, {
      "referenceID" : 9,
      "context" : "The key notions they introduce for dueling bandits are the Copeland, Borda and Condorcet scores (Charon and Hudry, 2010). The Borda score of an arm i on a preference matrix P is ∑K j=1 Pi,j and its Copeland score is ∑K j=1JPi,j > 1 2K (We use J . . .K to denote the indicator function). If an arm has a Copeland score of K − 1, which means that it defeats all the other arms in the long run, it is called a Condorcet winner. The existence of a Condorcet winner is the minimum assumption required for the Condorcet regret as defined on equation (2) to be applicable. There exists however some datasets like MSLR30K (2012) where this Condorcet condition is not satisfied. Zoghi et al. (2014a) extend the Upper Confidence Bound (UCB) algorithm (Auer et al.",
      "startOffset" : 97,
      "endOffset" : 691
    }, {
      "referenceID" : 0,
      "context" : "Ailon et al. (2014) propose three methods (DOUBLER, MULTISMB, and SPARRING) to reduce the stochastic utility-based dueling bandits problem to the conventional MAB problem.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 8,
      "context" : "We can cite (Busa-Fekete et al., 2013; 2014; Zoghi et al., 2014b; 2015).",
      "startOffset" : 12,
      "endOffset" : 71
    }, {
      "referenceID" : 6,
      "context" : "See also (Busa-Fekete and Hüllermeier, 2014) for an extensive survey of this domain.",
      "startOffset" : 9,
      "endOffset" : 44
    }, {
      "referenceID" : 19,
      "context" : "As mentionned earlier, the dueling bandits problem is a special instance of a partial monitoring game (Piccolboni and Schindelhauer, 2001; Cesa-Bianchi and Lugosi, 2009; Bartók et al., 2014).",
      "startOffset" : 102,
      "endOffset" : 190
    }, {
      "referenceID" : 10,
      "context" : "As mentionned earlier, the dueling bandits problem is a special instance of a partial monitoring game (Piccolboni and Schindelhauer, 2001; Cesa-Bianchi and Lugosi, 2009; Bartók et al., 2014).",
      "startOffset" : 102,
      "endOffset" : 190
    }, {
      "referenceID" : 4,
      "context" : "As mentionned earlier, the dueling bandits problem is a special instance of a partial monitoring game (Piccolboni and Schindelhauer, 2001; Cesa-Bianchi and Lugosi, 2009; Bartók et al., 2014).",
      "startOffset" : 102,
      "endOffset" : 190
    }, {
      "referenceID" : 0,
      "context" : "A utility-based formulation of the problem is however proposed in Ailon et al. (2014, section 6). In this setting, as in classical adversarial MAB, the environment chooses beforehand an horizon T and a sequence of utility/reward vectors x(t) = (x1(t), . . . , xK(t)) ∈ [0, 1] for t = 1, . . . , T . The learning algorithm aims at controling the bandit regret against the best single-arm strategy, as defined in (3), by choosing properly the pairs of arms (i, j) to be compared. To tackle this problem, Ailon et al. (2014) suggest to apply the SPARRING reduction algorithm, although originally designed for stochastic settings, with an adversarial bandit algorithm like EXP3 as a black-box MAB.",
      "startOffset" : 66,
      "endOffset" : 522
    }, {
      "referenceID" : 3,
      "context" : "If we except GLOBALEXP3 (Bartók, 2013) which tries to capture more finely the structure of the games, these algorithms only focus on the time bound and perform inefficiently when the number of actions grows.",
      "startOffset" : 24,
      "endOffset" : 38
    }, {
      "referenceID" : 14,
      "context" : "This ‘punitive’ approach of exponential weighing departs from EXP3 and other weighing algorithms which gratify the most rewarding arms while kindly ignoring the nonrewarding ones (Freund and Schapire, 1999; Cesa-Bianchi and Lugosi, 2006).",
      "startOffset" : 179,
      "endOffset" : 237
    }, {
      "referenceID" : 9,
      "context" : "This ‘punitive’ approach of exponential weighing departs from EXP3 and other weighing algorithms which gratify the most rewarding arms while kindly ignoring the nonrewarding ones (Freund and Schapire, 1999; Cesa-Bianchi and Lugosi, 2006).",
      "startOffset" : 179,
      "endOffset" : 237
    }, {
      "referenceID" : 0,
      "context" : "Lower bound for dueling bandits algorithms To provide a lower bound on the regret of any dueling bandits algorithm, we use a reduction to the classical MAB problem suggested by Ailon et al. (2014).",
      "startOffset" : 177,
      "endOffset" : 197
    }, {
      "referenceID" : 20,
      "context" : "Experiments To evaluate REX3 and other dueling bandits algorithms, we have applied them to the online comparison of rankers for search engines by interleaved filtering (Radlinski and Joachims, 2007).",
      "startOffset" : 168,
      "endOffset" : 198
    }, {
      "referenceID" : 25,
      "context" : "We used several preference matrices issued from namely: ARXIV dataset (Yue and Joachims, 2011), LETOR NP2004 dataset (Liu et al.",
      "startOffset" : 70,
      "endOffset" : 94
    }, {
      "referenceID" : 18,
      "context" : "We used several preference matrices issued from namely: ARXIV dataset (Yue and Joachims, 2011), LETOR NP2004 dataset (Liu et al., 2007), and MSLR30K dataset.",
      "startOffset" : 117,
      "endOffset" : 135
    }, {
      "referenceID" : 19,
      "context" : "Experiments To evaluate REX3 and other dueling bandits algorithms, we have applied them to the online comparison of rankers for search engines by interleaved filtering (Radlinski and Joachims, 2007). A search-engine ranker is a function that orders a collection of documents according to their relevancy to a given user search query. By interleaving the output of two rankers and tracking on which ranker’s output the user did click, we are able to get an unbiased feedback about the relative quality of these two rankers. Given K rankers, the problem of finding the best ranker is indeed a K-armed dueling bandits. In order to obtain reproducible and comparable results, we adopted the stochastic matrix-based experiment setup already employed by Yue and Joachims (2011); Zoghi et al.",
      "startOffset" : 169,
      "endOffset" : 772
    }, {
      "referenceID" : 19,
      "context" : "Experiments To evaluate REX3 and other dueling bandits algorithms, we have applied them to the online comparison of rankers for search engines by interleaved filtering (Radlinski and Joachims, 2007). A search-engine ranker is a function that orders a collection of documents according to their relevancy to a given user search query. By interleaving the output of two rankers and tracking on which ranker’s output the user did click, we are able to get an unbiased feedback about the relative quality of these two rankers. Given K rankers, the problem of finding the best ranker is indeed a K-armed dueling bandits. In order to obtain reproducible and comparable results, we adopted the stochastic matrix-based experiment setup already employed by Yue and Joachims (2011); Zoghi et al. (2014a;b; 2015) with both the cumulative Condorcet regret as defined by Yue et al. (2012) and the accuracy i.",
      "startOffset" : 169,
      "endOffset" : 876
    }, {
      "referenceID" : 18,
      "context" : "We used several preference matrices issued from namely: ARXIV dataset (Yue and Joachims, 2011), LETOR NP2004 dataset (Liu et al., 2007), and MSLR30K dataset. The last dataset distinguishes three kinds of queries: informational, navigational and perfecthit navigational (MSLR30K, 2012). These matrices are courtesy of Zoghi et al. (2014b)’s authors.",
      "startOffset" : 118,
      "endOffset" : 338
    }, {
      "referenceID" : 25,
      "context" : "Interleave filtering simulations For our experiments we have considered the following state of the art algorithms: BTM (Yue and Joachims, 2011) with γ = 1.",
      "startOffset" : 119,
      "endOffset" : 143
    }, {
      "referenceID" : 22,
      "context" : "1 and δ = 1/T (explore-then-exploit setting), Condorcet-SAVAGE (Urvoy et al., 2013) with δ = 1/T , RUCB (Zoghi et al.",
      "startOffset" : 63,
      "endOffset" : 83
    }, {
      "referenceID" : 0,
      "context" : "51, and SPARRING coupled with EXP3 (Ailon et al., 2014).",
      "startOffset" : 35,
      "endOffset" : 55
    } ],
    "year" : 2016,
    "abstractText" : "We study the K-armed dueling bandit problem which is a variation of the classical Multi-Armed Bandit (MAB) problem in which the learner receives only relative feedback about the selected pairs of arms. We propose an efficient algorithm called Relative Exponential-weight algorithm for Exploration and Exploitation (REX3) to handle the adversarial utility-based formulation of this problem. We prove a finite time expected regret upper bound of order O( √ K ln(K)T ) for this algorithm and a general lower bound of order Ω( √ KT ). At the end, we provide experimental results using real data from information retrieval applications.",
    "creator" : "LaTeX with hyperref package"
  }
}