{
  "name" : "1206.1529.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Sparse projections onto the simplex",
    "authors" : [ "Anastasios Kyrillidis", "Stephen Becker", "Volkan Cevher" ],
    "emails" : [ "volkan.cevher}@epfl.ch", "stephen.becker@upmc.fr" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Learning applications typically boil down to optimization of a loss function f(β) in order to obtain a (model or feature) vector β ∈ Rp. In this setting, we prefer sparse solutions even if there exist other solutions that might obtain better loss values. By sparse, we mean that β has at most s nonzero coefficients where s p. This choice is well-justified as sparsity-based learning not only shows great empirical success with improved interpretability, but also is backed up with rigorous theoretical analysis. For instance, sparsity provably avoids over-fitting for better generalization of learning algorithms, and provably circumvents the ill-posed nature of regression problems [1, 2].\nHowever, sparsity inherently introduces non-convexity into learning problems, which is undesirable according to conventional wisdom. For instance, to obtain a minimizer of f(β) with a specific sparsity s, we have to deal with a non-convex constraint: ‖β‖0 ≤ s, where ‖ · ‖0 counts the sparsity of β. In turn, the resulting minimization is typically NP-Hard (even if f(β) is convex) if we seek a global minimizer (cf., [3]). Surprisingly, it is possible to obtain sparse critical points of general loss functions with the projected gradient algorithm [4]. This algorithm iteratively uses the Euclidean projection onto the constraint ‖β‖0 ≤ s, which is efficiently obtained via hard-thresholding.\nLuckily, we can almost always achieve approximately sparse solutions via a simple convexification. For instance, we simply replace the ‖β‖0 ≤ s constraint with an `1-norm constraint as ‖β‖1 ≤ λ, where λ ∈ R+. The Euclidean projection onto the convex constraint ‖β‖1 ≤ λ can be efficiently obtained via soft-thresholding [5, 6], which automatically sparsifies solutions. Moreover, if f(β) is convex, then we can leverage decades of research in convex optimization, which not only provide computationally efficient algorithms to obtain accurate solutions, but also strong analytical tools to establish consistency and prediction efficiency of the solutions as λ varies [2].\nWhile the `1-norm is now a de facto standard in sparsity related problems, several important learning problems involve simplex constraints that mar its application in further sparsifying the solutions. Let us first recall the definitions\nar X\niv :1\n20 6.\n15 29\nv1 [\ncs .L\nG ]\n7 J\nof the two simplex variants. The standard simplex in Rp with parameter λ is given by ∆+λ = { β ∈ Rp : βi ≥\n0, ∑ i βi = λ } . The general simplex in Rp with parameter λ is given by ∆λ = { β ∈ Rp : ∑ i βi = λ } . It is clear that direct applications of the `1-norm (regularization, constraint, or otherwise) cannot achieve further sparsification beyond what the standard simplex constraint obtains. Unfortunately, many applications require further sparsification.\nAs a stylized example, consider the Markowitz portfolio selection, where we try to learn a design vector β that minimizes a return-adjusted empirical risk objective [7, 8]. Here, we immediately have simplex constraints, i.e., ∆+1 with no-short positions and ∆1 with short positions due to limited capital resources. Moreover, we typically need solutions sparser than the ones with the simplex constraint due to two reasons. The first reason is robustness: since empirical covariance and return estimates are rather noisy, sparser portfolios generalize better [7,8]. The second reason is cost: transactions fees increase with sparsity. Other example learning problems include sparse mixture/kernel density estimation [9, 10], and boosting/leveraging weak classifiers [11].\nContributions: Within this context, our contributions can be summarized as follows:\n1. We prove a new result that states greedy sparse selection followed by standard simplex projections provides efficient and optimal sparse Euclidean projections onto ∆+λ .\n2. We introduce a new projection algorithm for efficiently obtaining approximate sparse Euclidean projections onto ∆λ. This new scheme is based on a principled, iterative selection method that searches over relaxations of the general simplex constraint to obtain the best approximate projection. We establish the exact convergence of the scheme, and show that we can certify the optimality of our projections or obtain approximation guarantees online.\n3. We apply our projections in iterative optimization algorithms to real-life and synthetic examples. In Markowitz portfolio selection problems, we can reduce transaction costs while still improving average return. In mixture density learning, our results naturally reveal the underlying model order. Finally, in general simplex projection problems, our approach outperforms the state-of-the-art branch-and-bound approaches by three orders of magnitude in efficiency even in small size problems, while obtaining exactly the same solutions.\nTo the best of our knowledge, sparse Euclidean projections onto the simplex constraints have not been considered before. The closest works to ours are the papers [12, 13], where the authors recover a vector β in regression setting, where β is already sparse and within a convex norm-ball constraint. In this paper, we consider the problem of projecting an arbitrary given vector w onto convex-based and sparse constraints, jointly. While the results [12,13] only apply to linear regression problems with the restricted isometry property, our projectors can be used in general minimization problems.\nNotation: Given a set S ⊆ N = {1, . . . , p}, the complement Sc is defined with respect to N , and the cardinality is written |S|. The support set of w is defined as supp(w) = {i : wi 6= 0}. Given a vector w ∈ Rp, wS is either the projection (in Rp) of w onto S, i.e. (wS)Sc = 0, or a vector in R|S| depending on context. The all-ones column vector is 1, with dimensions apparent from the context. We define Σs as the set of all s-sparse subsets of N , and we sometimes write β ∈ Σs to mean supp(β) ∈ Σs."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "Problem statement: In this paper, we mainly study the following problem and its variant:\nProblem 1. Given a vector w ∈ Rp, find a Euclidean projection of w onto the intersection of s-sparse signals and the standard simplex ∆+λ :\nβ? ∈ arg min β:supp(β)∈Σs,β∈∆+λ ‖β − w‖22. (1)\nProblem 2. Same as Problem 1 but use the general simplex ∆λ instead of ∆+λ .\nRemark 1. Problems 1–2 are special instances of mixed-integer programming and are in general combinatorially hard. However, there exist well-built solvers relying on good branch-and-bound heuristics, such as CPLEX [14]. In the sequel, we prove that Problem 1 can be solved optimally. Therefore, we use CPLEX as a benchmark against our relaxed solution to Problem 2.\nProjections: We will write Pλ for the projector onto either the standard or general simplex (it will be clear from context which we mean). Without loss of generality, let w be sorted in descending order so that w1 is the largest element, and write [wi]+ to mean max(wi, 0).\nDefinition 1 (Greedy basis algorithm PΣs ). The greedy basis algorithm calculates the optimal projection PΣs onto Σs itself in O(pmax(s, log(p)))-time, by picking the s-largest entries of w. The optimality of this algorithm follows from the matroid structure of cardinality constraints [12,15]. This particular operation is also known as hard thresholding.\nDefinition 2 (Projection onto the standard simplex Pλ). The projector onto the standard simplex is given by\n(Pλ(w))i = [wi − τ ]+ where τ = τρ, τj := 1\nj ( j∑ i=1 wi − λ ) and ρ := max{j : wj > τj}. (2)\nThe projector onto the convex hull of the standard simplex (that is, require ∑ βi ≤ λ rather than ∑ βi = λ) is\nwritten P̄λ and is identical except we set replace τ with [τ ]+.\nDefinition 3 (Projection onto the general simplex Pλ). The projector onto the general simplex is [16]\n(Pλ(w))i = wi + τ where τ = 1\np (λ− p∑ i=1 wi). (3)\nA reformulation: The following observation is convenient in the sequel:\nRemark 2. The Problem 1 statement (1) can be equivalently transformed into the following nested minimization problem:\n{S?, β?S?} = arg min S:S∈Σs\n[ min\nβ:βS∈∆+λ , βSc=0\n‖(β − w)S‖22 + ‖(w)Sc‖22 ] . (4)\nwhere supp(β?) = S? and β? ∈ ∆+λ ."
    }, {
      "heading" : "3 Efficient sparse projections onto the simplex",
      "text" : "Let β? be a projection of w onto Σs ∩ Kλ, where Kλ is either ∆+λ or ∆λ. Let C(β) be a mapping that encodes the constraint Kλ. For instance, C(β) = ∑ i βi so C(β) = λ evaluates the simplex constraint.\nWe first make an elementary observation based on Remark 2. Given S? = supp(β?), we can find β? by projecting wS? onto Kλ within the s-dimensional space. Thus, the difficulty is finding S?.\nAlgorithm 1 suggests an obvious greedy approach. We select the set S? by naively projecting w onto Σs. Remarkably, this gives the correct support set for Problem 1, as we prove in §4.1. After finding the support, we project (restricted to the support) back to Kλ. We call this algorithm the greedy selector and simplex projector (GSSP).\nAlgorithm 1: Greedy selector and simplex projector (GSSP) 1: S? = supp(PΣs(w)) {Select support} 2: (β)S? = Pλ(wS?), (β)S?,c = 0 {Final projection}\nUnfortunately, the greedy approach fails for Problem 2. Here we propose Algorithm 2 which is non-obvious. The algorithm first projects w onto Kτ , which we call a relaxed version of Kλ since it is possible that τ 6= λ. Projecting once more onto Σs gives\nβ̂τ := PΣs(Pτ (w)).\nLet C(τ) := C(β̂τ ). If C(τ) = λ then β̂τ is feasible. In §4.2, we will prove that if τ is chosen such that C(τ) = λ, then remarkably supp(β̂τ ) = supp(β?) for some optimal solution β?. Finding this correct value of τ can be efficiently done with a bisection search, leading to an algorithm with O(ps log s) complexity. When there is no τ such that C(τ) = λ, the algorithm still provides an estimate which has a provable bound on its sub-optimality. Due to overwhelming numerical evidence, we also conjecture that supp(β̂τ ) is optimal in this case as well: in low-dimensional tests, the algorithm has always returned the exact global solution.\nAlgorithm 2: Relaxed selector and simplex projector (RSSP)\n1: t? = arg mint |λ− C(β̂t)| {One-dimensional optimization using relaxed constraints} 2: S? = supp(β̂t?) {Select support} 3: (β)S? = Pλ(wS?), (β)S?,c = 0 {Final projection}"
    }, {
      "heading" : "4 Main results",
      "text" : "Remark 3. When the symbol S is used as S = supp(β̄) where β̄ = PΣs(w̄) for any w̄, then if |S| < s, we enlarge S until it has s elements by taking the first s − |S| elements that are not already in S and defining β̄ = 0 on these elements. The lexicographic approach is used to break ties when there are more than one solution to Problem 1."
    }, {
      "heading" : "4.1 Projection onto the standard simplex with cardinality constraints",
      "text" : "Theorem 1. Algorithm 1 exactly solves Problem 1.\nProof. By Remark 2, we split the problem into the task of finding the support and then finding the values on the support. Given any support S, the unique corresponding estimator is β̂S = Pλ(wS).\nWe may conclude that β? satisfies (β?)S? = (Pλ(w))S? and (β?)(S?)c = 0, where\nS? ∈ arg min S:S∈Σs ‖(Pλ(w))S − w‖2 = arg max S:S∈Σs\n[ ‖(w)S‖22 − ‖(Pλ(w)− w)S‖22 ] = arg max S:S∈Σs ∑ i∈S ( w2i − ((Pλ(w))i − wi)2\n) = arg max S:S∈Σs F (S) (5)\nwith F (S) := ∑ i∈S ( w2i − ((Pλ(w))i − wi)2 ) . We now introduce a simple yet powerful lemma.\nLemma 1. Let β = Pλ(w) where βi = [wi − τ ]+. Then wi ≥ τ for all i ∈ S = supp(β). Furthermore, τ = 1|S| (∑ i∈S wi − λ ) .\nProof. Directly from the definition of τj in (2). The intuition is quite simple: the “threshold” τ should be smaller than the smallest entry in the selected support. Otherwise, we unnecessarily shrink the coefficients that are larger without introducing any new support to the solution.\nLet S be any support set, and let |S| = l ≤ s. If S is not chosen greedily, then there exists some wk /∈ S and some wj ∈ S such that wk > wj . Define Ŝ = (S \\ {j}) ∪ {k}. We will show that F (Ŝ) ≥ F (S), and therefore it is never\nharmful to keep adding the largest elements of w, and hence the greedy set is at least as good as all other possible sets. For convenience, define r = ∑ i∈S wi and r̂ = ∑ i∈Ŝ wi. First, we note that\nF (S) = ∑ i∈S (w2i − τ2) = (∑ i∈S w2i ) − 1 l (r − λ)2\nwhich follows from Lemma 1. Now,\nF (Ŝ)− F (S) = (w2k − w2j )− 1\nl\n( (r̂ − λ)2 − (r − λ)2 ) = (wk − wj)(wk + wj)− 1\nl (r̂ − r)(r̂ + r − 2λ)\n≥ (wk − wj) [(wk − (r̂ − λ)/l) + (wj − (r − λ)/l)] ≥ 0\nsince wk − wj > 0 and wk ≥ (r̂ − λ)/l and wj ≥ (r − λ)/l by Lemma 1.\nRemark 4. The same algorithm works for projecting onto the convex hull of the standard simplex, since forcing τ ≥ 0 does not change the proof."
    }, {
      "heading" : "4.2 Projection onto the general simplex with cardinality constraints",
      "text" : "We are now in position to specialize Algorithm 2 to project onto ∆λ ∩ Σs.\nTheorem 2. For all λ except on a set Λ of finite measure, Algorithm 2 computes the projection of w onto ∆λ ∩ Σs exactly. If λ ∈ Λ, the algorithm returns a solution with online optimality guarantees.\nEmpirically, the algorithm is exact even when λ ∈ Λ, but we defer this proof for later work. The rest of this subsection proves the theorem.\nConsider the nested form of the problem, as in (4). Given a support set S, the inner minimization in (4) has an analytical solution as a function of β using (3):\nβS = wS + τ(S) · 1S , (6)\nwith τ(S) := (λ−1 T SwS) |S| ∈ R. We sometimes write τ when the set S is clear from context. Replacing (6) in (4), we obtain the following subset selection problem:\nS? ∈ arg min S:|S|≤s\n[ ‖τ · 1S‖22 + ‖(w)Sc‖22 ] = arg max S:|S|≤s [ ‖w‖22 − ( ‖τ · 1S‖22 + ‖(w)Sc‖22 )] = arg max S:|S|≤s ∑ i∈S ( w2i − τ2 ) . (7)\nWrite F (S) = ∑ i∈S ( w2i − τ2 ) , and thus we seek S ∈ Σs that maximizes F (S).\nWrite βτ = w + τ , so that βτ = PKλτ (w) for some λτ , depending on τ . Then, let β̂τ = PΣs(βτ ) be the s-sparse projection of βτ , and define C(τ) = 1T β̂τ . Algorithm 2 suggests that finding the minimizer of |C(τ)− λ| will define the correct support. We start with a lemma about C(τ).\nLemma 2. The map C(τ) is monotonically increasing in τ , and is piecewise linear apart from s “turning points” τk, k = 1, . . . , s, where it is multi-valued.\nProof. Fix τ , and let σ(j) be a permutation such that |wσ(1) + τ | ≥ |wσ(2) + τ | ≥ . . . ≥ |wσ(n) + τ |. Let S = {σ(1), . . . , σ(s)}, so PS(βτ ) is a valid projection of βτ onto Σs. Thus a value for C(τ) is C(τ) = ∑ i∈S(wi + τ). The set S need not be unique; if wσ(s) = wσ(s+1), then there there are other valid S but it will result in the same value\nof C(τ), and if wσ(s) 6= wσ(s+1) but |wσ(s) + τ | = |wσ(s+1) + τ | then there is another valid S that gives a different value of C(τ). This latter case only happens at s-many points which we call “turning points.”\nNow examine C(τ+∆τ) for any ∆τ > 0. If there is no turning point τk ∈ [τ, τ+∆τ ], then C(τ+∆τ)−C(τ) = s∆τ . If there is a turning point, assume ∆τ is sufficiently small so that only one turning point, τk, is in [τ, τ + ∆τ ]. Let S be the index set corresponding to τ ; then the index set for τ + ∆τ is the same, except it has an additional index w(new) and it lacks some index w(old). Then C(τ + ∆τ) − C(τ) = s∆τ + w(new) − w(old). Since w(new) has been added, this means |w(new) + τ + ∆τ | > |w(new) + τ | and thus w(new) > 0. Likewise, w(old) < 0, and hence C(τ + ∆τ)− C(τ) > 0, so C is a monotonically increasing function.\nThe s turning points are easily found by sorting w (so assume w1 ≥ w2 ≥ . . .) and finding the points τ such that |wk + τ | = |wp−s+k + τ | for k = 1, . . . , s, i.e. τk = (w2(p−s+k) − w 2 k)/(2(wk − wp−s+k)).\nNow, we approach the problem from the combinatorial side. Let Ŝ be the unique support defined by any τ̂(Ŝ) that is not a turning point (for simplicity, we exclude the possibility that w has identical entries). The support set does not change as τ̂(Ŝ) changes except when τ̂(Ŝ) crosses a turning point. If Ŝ is fixed, then consider projecting wŜ onto ∆λτ̂(Ŝ) where λτ̂(Ŝ) = sτ̂(Ŝ) + 1 T Ŝ wŜ . Here, we use Remark 3, where |Ŝ| = s. The nonzero entries of β over Ŝ are given by (6) for\nτ̂(Ŝ) = λτ̂(Ŝ) − 1 T Ŝ wŜ\ns .\nThe algorithm proceeds by picking τ ∈ (τk, τk+1), with the convention that τ0 = −∞ and τs+1 = +∞, which defines a support Sτ and hence λτ := C(τ). We search for arg minτ |λ − λτ |. Because C(τ) is monotonic, we can compute bounds on λτ using the bisection method. Let τl and τu be values of τ such that λl = λτl < λ < λτu = λu.\nUsing (7), the optimal s-sparse set can be found by maximizing F (S). The final step in our proof shows that λτ ' λ implies F (Sτ ) ' F (S?).\nLemma 3. If C(τ) := C(β̂τ ) = λ, then β̂τ = β?.\nProof. Since β̂τ is defined as a projection of w + τ , we have\nβ̂τ = arg min β∈Σs\n‖β − (w + τ1)‖22\n= arg min β∈Σs,C(β)=λτ\n‖β − (w + τ1)‖22 since C(β̂τ ) = λτ\n= arg min β∈Σs,C(β)=λτ ‖β − w‖22 + ‖τ1‖2 − 2τ1T (β − w)︸ ︷︷ ︸ c .\nBecause of the constraint C(β) = λτ , then 1Tβ, and hence c, is a constant over the feasible set, and so this is the same as Problem 2 using λτ . Thus if λτ = λ, we have solved the original problem.\nLemma 4. Let Ŝ = supp(β̂t?) be the solution index set of t? = arg minτ |λ − λτ |, and λ̂ = λτ . Define Ŝ = G(λ, λ̂)/F (Ŝ) and G(λ, λ̂) = λ\n2−λ̂2 s + 2(λ̂−λ) s ∑ i∈S? wi. Then, we have the following approximation guarantee\nF (Ŝ) ≥ ( 1− Ŝ ) F (S?). (8)\nProof. We observe the following:\nF (Ŝ, w) = ∑ i∈Ŝ [w]2i − 1 s\n( λ̂−\n∑ i∈Sl [w]i\n)2\n≥ ∑ i∈S? [w]2i − 1 s\n( λ̂−\n∑ i∈S? [w]i\n)2\n= ∑ i∈S? [w]2i − 1 s\n( λ̂+ λ− λ−\n∑ i∈S? [w]i\n)2\n= F (S?, w)− 2λ(λ̂− λ) s + 2(λ̂− λ) s ∑ i∈S? [w]i − (λ̂− λ)2 s\n= F (S?, w) + λ 2 − λ̂2\ns + 2(λ̂− λ) s ∑ i∈S? [w]i,\nwhere we simply use the fact that given λ̂, the support Ŝ obtains the maximum objective.\nWe obtain Ŝ values to be much smaller than 1 after the termination of RSSP (e.g., typical values for the simplex projection experiments in Section 5.3 vary between 10−4 to 10−7). Given this epsilon value for RSSP, it is possible to obtain an online approximation guarantee for iterative optimization algorithms that can use RSSP (cf., [12]).\nRemark 5. The time complexity of the algorithm is O(ps log2(s)) because we need only test τ that define unique support, and there are s + 1 of these. Because C is monotonic, we can test just log2(s + 1) supports by using the bisection method. For each support, we compute a projection onto Σs which takes O(ps) complexity.\nRemark 6. We conjecture that it is possible to relax the constraint from ∑ βi = λ (which is non-convex) to ∑ βi ≤ λ (which is convex), but we leave a proof for future work."
    }, {
      "heading" : "5 Experiments",
      "text" : ""
    }, {
      "heading" : "5.1 Portfolio optimization",
      "text" : "Given a sample covariance matrix Σ and mean µ, the Markowitz mean-variance (MV) framework selects a portfolio β? = Rp (without short positions) via β? ∈ arg minβ∈∆+1 [ βTΣβ − αµTβ ] , where ∆+1 encodes the normalized capital constraint, and α trades off risk and return [7, 8]. The solution β? ∈ ∆+1 indicates the fractional investments over the p available assets. In mathematical terms, this leads to the following quadratic optimization problem:1\nβ? = arg min β∈∆+1\n[ βTΣβ − αµTβ ] , (9)\nfor a given regularization parameter α ≥ 0. In practice, the preferences of the investor may lead to further constraints in the optimization problem. Additional fees for asset trading (transaction costs) and costs of monitoring and portfolio re-weighting naturally lead to cardinality constraints in the optimization procedure. This additional flavor leads to mixed integer quadratic programming formulation which is difficult to solve by standard optimization techniques. Numerous approaches have been proposed in the literature to solve this problem [18, 19] and references therein. Most of the investigations on cardinality\n1There is an extensive list of works in the literature where the mean-variance efficient portfolios are compared with global minimum variance portfolios. The primary reason is that estimating the expected return from past data is a difficult task where large outliers lead to imprecise estimations. We refer the reader to [7, 17] for an excellent discussion.\nconstrained porfolio optimization focus on finding local solutions using greedy techniques, simulated annealing and evolution methods, genetic algorithms, and branch and bound ideas ( [18, 20, 21] and references therein).\nHere, we are interested in the MV optimization with the added twist that the solution satisfies β? ∈ Σs:\nβ? ∈ arg min β∈∆+1 ∩Σs\n[ βTΣβ − αµTβ ] , (10)\nfor a given level of sparsity s. Out-of-sample performance: We use a publicly available dataset compiled by Farma and French2. In this dataset, we monitor 49 diverse industry assets and consider both monthly and daily recordings. Procedure: We evaluate the out-of-sample performances of the estimated portfolios over various time periods. For instance, during each year from 1971 to 2011, we estimate expected monthly returns of the stocks and their covariance values using the available data from the preceding 5 years. Finally, we evaluate the estimated portfolio β? by computing the monthly returns and risks each year keeping β? fixed. The supplementary material has daily return values as well as a synthetic experiment to also illustrate Pareto trade-offs achieved by sparsity assumptions.\nWe compare the following approaches: (i) the constrained quadratic optimization as described in (10) without the cardinality constraint Σs using quadprog in MATLAB [22], (ii) the cardinality-constrained projected gradient descent algorithm that solves (10) using GSSP for s = {4, 10} and, (iii) the naive 1/p-strategy where we use the same weight over the portfolio, i.e. βi = 1/p for all i.\nResults: We provide some typical return evaluations with α = 1 in Table 1 (as α varies, the results qualitatively remain the same). Our approach with GSSP performs quite well, especially for smaller active portfolio sizes as constrained by s.3 We observe that as s decreases, the expected return µ̂ as well as the standard deviation σ̂ of the returns increase. Surprisingly, the GSSP solutions exhibit competitive Sharpe ratios µ̂/σ̂, which measures the risk adjusted return, as compared to the MV portfolio, and with much lower transactions costs. Overall, the quadratic programming approach has a median sparsity level of 14 and a mean sparsity level of 14.78. The 1/p baseline strategy has the worst returns and worst Sharpe ratios for most years.\nInterestingly, the naive baseline strategy does well in recession years like ’81 to ’86 and ’06 to ’11. In these years, presumably the model (Σ, µ) is less accurate, and hence the quadratic programming solution does much worse than the naive strategy. The sparsity-constrained solution does better than the quadratic programming solution, suggesting that sparsity helps buffer against inaccurate models.\nEfficient frontier with cardinality constraints: To provide further numerical evidence, we generate random expected returns and covariance quantities of p = 100 assets. In Figure 2, we depict the unconstrained Pareto frontier\n2 http://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html 3Moreover, our approach is extremely efficient as explored in supplementary material.\nby solving the optimization problem (9). Without any cardinality constraints, the MV framework suggests dense portfolio solutions for low risk investments (additional selections lower the risk) while sparser solutions can be obtained for riskier investments. In practice, dense portfolios are difficult to administrate and have higher transactions costs. To this end, we propose sparser portfolio strategies using a projected gradient descent solver for (10) where we use GSSP. The corresponding frontiers are depicted in Figure 2 for various s.\nAdditional comparisons: We perform a comparative study between the commercial toolbox CPLEX Studio V12.3 [14] using YALMIP [23] and our positive simplex, cardinality constrained, projected gradient method. The table in Figure 2 depicts a summary of the results. Even within this relatively small-scale configuration, our projected gradient descent algorithm admits fast performance (due to the first-order gradient statistics) and almost equivalent estimation performance compared to the state-of-the-art CPLEX algorithm."
    }, {
      "heading" : "5.2 Sparse kernel density estimation",
      "text" : "Let x(1), x(2), . . . , x(n) ∈ Rp be a n-size corpus of p-dimensional samples, drawn from an unknown probability density function (pdf) f(x). Given this dataset, we are interested in estimating f(x). We focus on kernel estimation techniques that employ the Integrated Squared Error (ISE) criterion, ISE = E‖f̂(x) − f(x)‖22, to design a weight vector β? ∈ ∆+1 such that f̂(x) := ∑n i=1 β ? i κσ(x, x (i)) and f̂(x) minimizes the ISE. The resulting problem can be\nwritten as follows [10, 24]\nβ? ∈ arg min β∈∆+1\n[ βTΣβ − cTβ ] , (11)\nwhere κσ(x, y) is a Gaussian kernel, Σ ∈ Rn×n with Σij = κ√2σ(x(i), x(j)) and c ∈ Rn×1 with\nci = 1 n− 1 ∑ j 6=i κσ(x (i), x(j)), ∀i, j (12)\nWhile the term −cTβ induces sparsity in the solution for βi ≥ 0,∀i, in several cases β? is not sparse enough even if f(x) is a linear combination of a few components; even sparser weight solutions are required for instance to avoid overfitting or obtain interpretable results. In this context, we extend (11) to include cardinality constraints, i.e. β? ∈ ∆+1 ∩ Σs.\nSparse estimation of Gaussian mixtures: We consider the following one-dimensional mixture of 5 Gaussians: f(x) = 15 ∑5 i=1 κσi(µi, x) where σi = (7/9)\ni and µi = 14(σi−1). A sample of 1000 points is drawn from f(x). We compare the density estimation performance of: (i) the Parzen window method [25], (ii) the quadratic programming formulation in (11), and (iii) our cardinality-constrained version of (11) using GSSP. While f(x) is constructed by kernels with various widths, we assume a constant width during the kernel estimation. In practice, the width is not known a priori but can be found using cross-validation techniques, i.e., leave-one-out technique; for simplicity, we assume kernels with width σ = 1.\nFigure 3(a) depicts the true pdf and the estimated densities using the Parzen window method and the quadratic programming approach. Moreover, the figure also include a scaled plot of 1/σi, indicating the height of the individual Gaussian mixtures. By default, the Parzen window method estimation interpolates 1000 Gaussian kernels with centers\naround the sampled points to compute the estimate f̂(x); unfortunately, neither the quadratic programming approach (as Figure 3(b) illustrates) nor the Parzen window estimator results are easily interpretable while both approaches provide a good approximation of the true pdf.\nIn stark contrast, using our cardinality-constrained approach, we can significantly enhance the interpretability. This is because the sparsity-constrained approach approximately reveals the number of Gaussian components. To see this, we first show the coefficient profile of the proposed approach for s = 5 in Figure 3(b) (middle-bottom). Moreover, Figure 3(c) shows the estimated pdf for s = 5 along with the positions of weight coefficients obtained by our sparsity enforcing approach. Note that most of the weights obtained concentrate around the true means, providing some prior information about the ingredients of f(x)—this happens with rather high frequency in the experiments we conducted. Figure 4 illustrates further estimated pdf’s using our approach for various s. From the plot, it is also clear that if s > 5, the resulting solutions are still approximately 5-sparse as the over estimated coefficients exhibit extremely small values."
    }, {
      "heading" : "5.3 Sparse projections onto the general simplex",
      "text" : "While the general simplex constraints also have applications with Markowitz portfolio selection, it is better to use synthetic data to numerically benchmark the quality and efficiency of our projection onto the general simplex ∆λ. Here, we generate random vector realizations w ∈ Rp with p = {102, 104} and compare the projection performance of the following approaches over various s and λ: (i) the GSSP on Σs∩∆λ, (ii) the proximal alternating minimization projection (PAMP) [26], (iii) the commercial mixed integer quadratic programming toolbox CPLEX Studio V12.3 [14], and (iv) the RSSP on Σs ∩ ∆λ. The PAMP approach is based on a general proximal scheme that tries to solve a coupled problem and requires turning parameters. For a fair comparison, we tuned PAMP parameters for best performance; we note that deviations from this fine tuning leads to performance degradation.\nTable 2 depicts the efficiency of our approaches where both GSSP and RSSP efficiently decrease the distance to the original vector w compared to state-of-the-art algorithms with total computational costs that scale well as p increases. Note that while RSSP requires more computation, its solution quality is consistently (and strictly, in our experiments) better, where the difference can be quite large. Some illustrative instance is depicted in Figure 5. In Figure 5(b), let β?i , i ∈ I = {GSSP,RSSP,Prox.Alter.} be the solutions obtained by the algorithms in comparison and let ei,j denote the distance ‖β?i −w‖2, i ∈ I at the j-th MC iteration. Then, we use the performance profile notion [27] where ρ(α) is the probability for a solver that the performance ratio ei,jmin{ei,j :i∈I} is within a factor α ∈ R of the best possible ratio. Note that RSSP performs exactly the same as CPLEX within numerical precision for small size problems while being three orders of magnitude faster. The PAMP approach is quite competitive but it requires some level of tuning."
    }, {
      "heading" : "6 Conclusions",
      "text" : "While non-convexity in learning algorithms is undesirable according to conventional wisdom, avoiding it might be difficult in many problems. In particular, sparse learning problems with simplex constraints disallow the application of the `1-relaxation for sparsification of the solution. In this setting, we show how to efficiently obtain sparse projections onto positive and general simplex and sparsity constraints. To this end, we provide an exact sparse projector for the positive simplex constraints and a relaxation based approach for approximate sparse projections onto the general simplex constraints. Interestingly, the latter approach can feature online optimality guarantees. We also empirically demonstrate that our projectors provide substantial benefits in generalization and interpretability in sparse portfolio selection and density estimation applications."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work was supported in part by the European Commission under Grant MIRG-268398, ERC Future Proof, SNF 200021 132548, and DARPA KeCoM program #11-DARPA-1055. VC also would like to acknowledge Rice University for his Faculty Fellowship, and SB would like to acknowledge the Fondation Sciences Mathématiques de Paris for his fellowship."
    } ],
    "references" : [ {
      "title" : "A desicion-theoretic generalization of on-line learning and an application to boosting",
      "author" : [ "Y. Freund", "R. Schapire" ],
      "venue" : "Computational learning theory, pages 23–37. Springer",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Statistics for High-Dimensional Data: Methods, Theory and Applications",
      "author" : [ "P. Bühlmann", "S. Van De Geer" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2011
    }, {
      "title" : "Sparse approximate solutions to linear systems",
      "author" : [ "B.K. Natarajan" ],
      "venue" : "SIAM J. Comput., 24(2):227–234",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Convergence of descent methods for semi-algebraic and tame problems: proximal algorithms",
      "author" : [ "H. Attouch", "J. Bolte", "B.F. Svaiter" ],
      "venue" : "forward–backward splitting, and regularized Gauss-Seidel methods. Mathematical Programming, pages 1–39",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "An O(n) algorithm for quadratic knapsack problems",
      "author" : [ "P. Brucker" ],
      "venue" : "Operations Research Letters, 3(3):163–166",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 1984
    }, {
      "title" : "Efficient projections onto the `1-ball for learning in high dimensions",
      "author" : [ "J. Duchi", "S. Shalev-Shwartz", "Y. Singer", "T. Chandra" ],
      "venue" : "ICML",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "A generalized approach to portfolio optimization: Improving performance by constraining portfolio norms",
      "author" : [ "V. DeMiguel", "L. Garlappi", "F.J. Nogales", "R. Uppal" ],
      "venue" : "Management Science, 55(5):798–812",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Sparse and stable Markowitz portfolios",
      "author" : [ "J. Brodie", "I. Daubechies", "C. De Mol", "D. Giannone", "I. Loris" ],
      "venue" : "Proceedings of the National Academy of Sciences, 106(30):12267–12272",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Multiscale poisson intensity and density estimation",
      "author" : [ "R.M. Willett", "R.D. Nowak" ],
      "venue" : "Information Theory, IEEE Transactions on, 53(9):3171–3187",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "SPADES and mixture models",
      "author" : [ "F. Bunea", "A.B. Tsybakov", "M.H. Wegkamp", "A. Barbu" ],
      "venue" : "The Annals of Statistics, 38(4):2525–2558",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Robust ensemble learning",
      "author" : [ "G. Ratsch", "B. Scholkopf", "A.J. Smola", "S. Mika", "T. Onoda", "K.R. Muller" ],
      "venue" : "Advances in Neural Information Processing Systems, pages 207–220",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Combinatorial selection and least absolute shrinkage via the CLASH algorithm",
      "author" : [ "A. Kyrillidis", "V. Cevher" ],
      "venue" : "EPFL Technical Report",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Hard thresholding with norm constraints",
      "author" : [ "A. Kyrillidis G. Puy", "V. Cevher" ],
      "venue" : "In International Conference on Acoustics, Speech, and Signal Processing",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2012
    }, {
      "title" : "Integer and combinatorial optimization",
      "author" : [ "G.L. Nemhauser", "L.A. Wolsey" ],
      "venue" : "volume 18. Wiley New York",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 1988
    }, {
      "title" : "Convex Optimization",
      "author" : [ "S.P. Boyd", "L. Vandenberghe" ],
      "venue" : "Cambridge University Press",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Risk reduction in large portfolios: Why imposing the wrong constraints helps",
      "author" : [ "R. Jagannathan", "T. Ma" ],
      "venue" : "The Journal of Finance, 58(4):1651–1684",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Algorithm for cardinality-constrained quadratic optimization",
      "author" : [ "D. Bertsimas", "R. Shioda" ],
      "venue" : "Computational Optimization and Applications, 43(1):1–22",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Portfolio selection problems in practice: a comparison between linear and quadratic optimization models",
      "author" : [ "F. Cesarone", "A. Scozzari", "F. Tardella" ],
      "venue" : "Arxiv preprint arXiv:1105.3594",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Heuristics for cardinality constrained portfolio optimisation",
      "author" : [ "T.J. Chang", "N. Meade", "J.E. Beasley", "Y.M. Sharaiha" ],
      "venue" : "Computers and Operations Research, 27(13):1271–1302",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Hybrid search for cardinality constrained portfolio optimization",
      "author" : [ "M.A. Gomez", "C.X. Flores", "M.A. Osorio" ],
      "venue" : "Proceedings of the 8th annual conference on Genetic and evolutionary computation, pages 1865–1866. ACM",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "YALMIP: A toolbox for modeling and optimization in MATLAB",
      "author" : [ "J. Lofberg" ],
      "venue" : "Computer Aided Control Systems Design, 2004 IEEE International Symposium on, pages 284–289. IEEE",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Least squares mixture decomposition estimation",
      "author" : [ "D. Kim" ],
      "venue" : "PhD thesis",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "On estimation of a probability density function and mode",
      "author" : [ "E. Parzen" ],
      "venue" : "The annals of mathematical statistics, 33(3):1065–1076",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 1962
    }, {
      "title" : "Alternating minimization and projection methods for nonconvex problems",
      "author" : [ "H. Attouch", "J. Bolte", "P. Redont", "A. Soubeyran" ],
      "venue" : "Arxiv preprint arXiv:0801.1780",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Benchmarking optimization software with performance profiles",
      "author" : [ "E.D. Dolan", "J.J. Moré" ],
      "venue" : "Mathematical Programming, 91(2):201–213",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2002
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "For instance, sparsity provably avoids over-fitting for better generalization of learning algorithms, and provably circumvents the ill-posed nature of regression problems [1, 2].",
      "startOffset" : 171,
      "endOffset" : 177
    }, {
      "referenceID" : 1,
      "context" : "For instance, sparsity provably avoids over-fitting for better generalization of learning algorithms, and provably circumvents the ill-posed nature of regression problems [1, 2].",
      "startOffset" : 171,
      "endOffset" : 177
    }, {
      "referenceID" : 2,
      "context" : ", [3]).",
      "startOffset" : 2,
      "endOffset" : 5
    }, {
      "referenceID" : 3,
      "context" : "Surprisingly, it is possible to obtain sparse critical points of general loss functions with the projected gradient algorithm [4].",
      "startOffset" : 126,
      "endOffset" : 129
    }, {
      "referenceID" : 4,
      "context" : "The Euclidean projection onto the convex constraint ‖β‖1 ≤ λ can be efficiently obtained via soft-thresholding [5, 6], which automatically sparsifies solutions.",
      "startOffset" : 111,
      "endOffset" : 117
    }, {
      "referenceID" : 5,
      "context" : "The Euclidean projection onto the convex constraint ‖β‖1 ≤ λ can be efficiently obtained via soft-thresholding [5, 6], which automatically sparsifies solutions.",
      "startOffset" : 111,
      "endOffset" : 117
    }, {
      "referenceID" : 1,
      "context" : "Moreover, if f(β) is convex, then we can leverage decades of research in convex optimization, which not only provide computationally efficient algorithms to obtain accurate solutions, but also strong analytical tools to establish consistency and prediction efficiency of the solutions as λ varies [2].",
      "startOffset" : 297,
      "endOffset" : 300
    }, {
      "referenceID" : 6,
      "context" : "As a stylized example, consider the Markowitz portfolio selection, where we try to learn a design vector β that minimizes a return-adjusted empirical risk objective [7, 8].",
      "startOffset" : 165,
      "endOffset" : 171
    }, {
      "referenceID" : 7,
      "context" : "As a stylized example, consider the Markowitz portfolio selection, where we try to learn a design vector β that minimizes a return-adjusted empirical risk objective [7, 8].",
      "startOffset" : 165,
      "endOffset" : 171
    }, {
      "referenceID" : 6,
      "context" : "The first reason is robustness: since empirical covariance and return estimates are rather noisy, sparser portfolios generalize better [7,8].",
      "startOffset" : 135,
      "endOffset" : 140
    }, {
      "referenceID" : 7,
      "context" : "The first reason is robustness: since empirical covariance and return estimates are rather noisy, sparser portfolios generalize better [7,8].",
      "startOffset" : 135,
      "endOffset" : 140
    }, {
      "referenceID" : 8,
      "context" : "Other example learning problems include sparse mixture/kernel density estimation [9, 10], and boosting/leveraging weak classifiers [11].",
      "startOffset" : 81,
      "endOffset" : 88
    }, {
      "referenceID" : 9,
      "context" : "Other example learning problems include sparse mixture/kernel density estimation [9, 10], and boosting/leveraging weak classifiers [11].",
      "startOffset" : 81,
      "endOffset" : 88
    }, {
      "referenceID" : 10,
      "context" : "Other example learning problems include sparse mixture/kernel density estimation [9, 10], and boosting/leveraging weak classifiers [11].",
      "startOffset" : 131,
      "endOffset" : 135
    }, {
      "referenceID" : 11,
      "context" : "The closest works to ours are the papers [12, 13], where the authors recover a vector β in regression setting, where β is already sparse and within a convex norm-ball constraint.",
      "startOffset" : 41,
      "endOffset" : 49
    }, {
      "referenceID" : 12,
      "context" : "The closest works to ours are the papers [12, 13], where the authors recover a vector β in regression setting, where β is already sparse and within a convex norm-ball constraint.",
      "startOffset" : 41,
      "endOffset" : 49
    }, {
      "referenceID" : 11,
      "context" : "While the results [12,13] only apply to linear regression problems with the restricted isometry property, our projectors can be used in general minimization problems.",
      "startOffset" : 18,
      "endOffset" : 25
    }, {
      "referenceID" : 12,
      "context" : "While the results [12,13] only apply to linear regression problems with the restricted isometry property, our projectors can be used in general minimization problems.",
      "startOffset" : 18,
      "endOffset" : 25
    }, {
      "referenceID" : 11,
      "context" : "The optimality of this algorithm follows from the matroid structure of cardinality constraints [12,15].",
      "startOffset" : 95,
      "endOffset" : 102
    }, {
      "referenceID" : 13,
      "context" : "The optimality of this algorithm follows from the matroid structure of cardinality constraints [12,15].",
      "startOffset" : 95,
      "endOffset" : 102
    }, {
      "referenceID" : 14,
      "context" : "The projector onto the general simplex is [16]",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 11,
      "context" : ", [12]).",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 6,
      "context" : "1 Portfolio optimization Given a sample covariance matrix Σ and mean μ, the Markowitz mean-variance (MV) framework selects a portfolio β = R (without short positions) via β ∈ arg β∈∆1 [ βΣβ − αμβ ] , where ∆1 encodes the normalized capital constraint, and α trades off risk and return [7, 8].",
      "startOffset" : 285,
      "endOffset" : 291
    }, {
      "referenceID" : 7,
      "context" : "1 Portfolio optimization Given a sample covariance matrix Σ and mean μ, the Markowitz mean-variance (MV) framework selects a portfolio β = R (without short positions) via β ∈ arg β∈∆1 [ βΣβ − αμβ ] , where ∆1 encodes the normalized capital constraint, and α trades off risk and return [7, 8].",
      "startOffset" : 285,
      "endOffset" : 291
    }, {
      "referenceID" : 16,
      "context" : "Numerous approaches have been proposed in the literature to solve this problem [18, 19] and references therein.",
      "startOffset" : 79,
      "endOffset" : 87
    }, {
      "referenceID" : 17,
      "context" : "Numerous approaches have been proposed in the literature to solve this problem [18, 19] and references therein.",
      "startOffset" : 79,
      "endOffset" : 87
    }, {
      "referenceID" : 6,
      "context" : "We refer the reader to [7, 17] for an excellent discussion.",
      "startOffset" : 23,
      "endOffset" : 30
    }, {
      "referenceID" : 15,
      "context" : "We refer the reader to [7, 17] for an excellent discussion.",
      "startOffset" : 23,
      "endOffset" : 30
    }, {
      "referenceID" : 16,
      "context" : "constrained porfolio optimization focus on finding local solutions using greedy techniques, simulated annealing and evolution methods, genetic algorithms, and branch and bound ideas ( [18, 20, 21] and references therein).",
      "startOffset" : 184,
      "endOffset" : 196
    }, {
      "referenceID" : 18,
      "context" : "constrained porfolio optimization focus on finding local solutions using greedy techniques, simulated annealing and evolution methods, genetic algorithms, and branch and bound ideas ( [18, 20, 21] and references therein).",
      "startOffset" : 184,
      "endOffset" : 196
    }, {
      "referenceID" : 19,
      "context" : "constrained porfolio optimization focus on finding local solutions using greedy techniques, simulated annealing and evolution methods, genetic algorithms, and branch and bound ideas ( [18, 20, 21] and references therein).",
      "startOffset" : 184,
      "endOffset" : 196
    }, {
      "referenceID" : 7,
      "context" : "Red solid curve denotes the quadratic programming solution as obtained by (9) and blue squares represent a variation of `1-norm regularized solver in [8].",
      "startOffset" : 150,
      "endOffset" : 153
    }, {
      "referenceID" : 20,
      "context" : "3 [14] using YALMIP [23] and our positive simplex, cardinality constrained, projected gradient method.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 9,
      "context" : "written as follows [10, 24]",
      "startOffset" : 19,
      "endOffset" : 27
    }, {
      "referenceID" : 21,
      "context" : "written as follows [10, 24]",
      "startOffset" : 19,
      "endOffset" : 27
    }, {
      "referenceID" : 22,
      "context" : "We compare the density estimation performance of: (i) the Parzen window method [25], (ii) the quadratic programming formulation in (11), and (iii) our cardinality-constrained version of (11) using GSSP.",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 23,
      "context" : "Here, we generate random vector realizations w ∈ R with p = {10, 10} and compare the projection performance of the following approaches over various s and λ: (i) the GSSP on Σs∩∆λ, (ii) the proximal alternating minimization projection (PAMP) [26], (iii) the commercial mixed integer quadratic programming toolbox CPLEX Studio V12.",
      "startOffset" : 242,
      "endOffset" : 246
    }, {
      "referenceID" : 24,
      "context" : "Then, we use the performance profile notion [27] where ρ(α) is the probability for a solver that the performance ratio ei,j min{ei,j :i∈I} is within a factor α ∈ R of the best possible ratio.",
      "startOffset" : 44,
      "endOffset" : 48
    } ],
    "year" : 2017,
    "abstractText" : "The past decade has seen the rise of `1-relaxation methods to promote sparsity for better interpretability and generalization of learning results. However, there are several important learning applications, such as Markowitz portolio selection and sparse mixture density estimation, that feature simplex constraints, which disallow the application of the standard `1-penalty. In this setting, we show how to efficiently obtain sparse projections onto the positive and general simplex with sparsity constraints. We provide an exact sparse projector for the positive simplex constraints, and derive a novel approach with online optimality and approximation guarantees for sparse projections onto the general simplex constraints. Even for small sized problems, this new approach is three orders of magnitude faster than the alternative, state-of-the-art branch-and-bound based CPLEX solver with no sacrifice in solution quality. We also empirically demonstrate that our projectors provide substantial benefits in portfolio selection and density estimation.",
    "creator" : "LaTeX with hyperref package"
  }
}