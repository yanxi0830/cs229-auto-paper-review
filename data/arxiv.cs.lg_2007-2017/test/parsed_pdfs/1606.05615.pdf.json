{
  "name" : "1606.05615.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Guaranteed Non-convex Optimization: Submodular Maximization over Continuous Domains∗",
    "authors" : [ "Andrew An Bian", "Baharan Mirzasoleiman", "Joachim M. Buhmann" ],
    "emails" : [ "bian.andrewa@gmail.com", "baharanm@inf.ethz.ch", "jbuhmann@inf.ethz.ch", "krausea@ethz.ch" ],
    "sections" : [ {
      "heading" : null,
      "text" : "∗Appears in the 20th International Conference on Artificial Intelligence and Statistics (AISTATS) 2017, Fort Lauderdale, Florida, USA.\nar X\niv :1\n60 6.\n05 61\n5v 4\n[ cs\n.L G"
    }, {
      "heading" : "1. Introduction",
      "text" : "Non-convex optimization delineates the new frontier in machine learning, arising in numerous learning tasks from training deep neural networks to latent variable models (Anandkumar et al., 2014). Understanding, which classes of objectives can be tractably optimized remains a central challenge. In this paper, we investigate a class of generally non-convex and non-concave functions–submodular continuous functions, and derive algorithms for approximately optimizing them with strong approximation guarantees.\nSubmodularity is a structural property usually associated with set functions, with important implications for optimization. Optimizing submodular set functions has found numerous applications in machine learning, including variable selection (Krause and Guestrin, 2005), dictionary learning (Krause and Cevher, 2010; Das and Kempe, 2011), sparsity inducing regularizers (Bach, 2010), summarization (Lin and Bilmes, 2011; Mirzasoleiman et al., 2013) and variational inference (Djolonga and Krause, 2014). Submodular set functions can be efficiently minimized (Iwata et al., 2001), and there are strong guarantees for approximate maximization (Nemhauser et al., 1978; Krause and Golovin, 2012).\nEven though submodularity is most widely considered in the discrete realm, the notion can be generalized to arbitrary lattices (Fujishige, 2005). Recently, Bach (2015) showed how results from submodular set function minimization can be lifted to the continuous domain. In this paper, we further pursue this line of investigation, and demonstrate that results from submodular set function maximization can be generalized as well. Note that the underlying concepts associated with submodular function minimization and maximization are quite distinct, and both require different algorithmic treatment and analysis techniques.\nAs motivation for our inquiry, we firstly give a thorough characterization of the class of submodular and DR-submodular2 functions in Section 3. We propose the weak DR property and prove that it is equivalent to submodularity for general functions. This resolves the question whether there exists a diminishing-return-style characterization that is equivalent to submodularity for all set, integer-lattice and continuous functions. We then present two guaranteed algorithms for maximizing submodular continuous functions in Sections 4 and 5, respectively. The first approach, based on the Frank-Wolfe algorithm (Frank and Wolfe, 1956) and the continuous greedy algorithm (Vondrák, 2008), applies to monotone DR-submodular functions. It provides a (1 − 1/e) approximation guarantee under general down-closed convex constraints. We also provide a second, coordinate-ascent-style algorithm, which applies to arbitrary submodular continuous function maximization under box constraints, and provides a 1/3 approximation guarantee. This algorithm is based on the double greedy algorithm (Buchbinder et al., 2012) from submodular set function maximization. In Section 6 we illustrate how submodular continuous maximization captures various important applications, ranging from sensor energy management, to influence and revenue maximization, to facility location, and non-convex/non-concave quadratic programming. Lastly, we experimentally demonstrate the effectiveness of our algorithms on several problem instances in Section 7."
    }, {
      "heading" : "2. Background and related work",
      "text" : "Notions of submodularity. Submodularity is often viewed as a discrete analogue of convexity, and provides computationally effective structure so that many discrete problems with this property are efficiently solvable or approximable. Of particular interest is a (1 − 1/e)-approximation for\n2. A DR-submodular function is a function with the diminishing returns property, which will be formally defined in Section 3.\nmaximizing a monotone submodular set function subject to a cardinality, a matroid, or a knapsack constraint (Nemhauser et al., 1978; Vondrák, 2008; Sviridenko, 2004). Another result relevant to this work is unconstrained maximization of non-monotone submodular set functions, for which Buchbinder et al. (2012) propose the deterministic double greedy algorithm with 1/3 approximation guarantee, and the randomized double greedy algorithm which achieves the tight 1/2 approximation guarantee.\nAlthough most commonly associated with set functions, in many practical scenarios, it is natural to consider generalizations of submodular set functions. Golovin and Krause (2011) introduce the notion of adaptive submodularity to generalize submodular set functions to adaptive policies. Kolmogorov (2011) studies tree-submodular functions and presents a polynomial algorithm for minimizing them. For distributive lattices, it is well-known that the combinatorial polynomial-time algorithms for minimizing a submodular set function can be adopted to minimize a submodular function over a bounded integer lattice (Fujishige, 2005). Recently, maximizing a submodular function over integer lattices has attracted considerable attention. In particular, Soma et al. (2014) develop a (1− 1/e)-approximation algorithm for maximizing a monotone DR-submodular integerlattice function under a knapsack constraint. For non-monotone submodular functions over the bounded integer lattice, Gottschalk and Peis (2015) provide a 1/3-approximation. Approximation algorithms for maximizing bisubmodular functions and k-submodular functions have also been proposed by Singh et al. (2012); Ward and Zivny (2014).\nWolsey (1982) considers maximizing a special class of submodular continuous functions subject to one knapsack constraint, in the context of solving location problems. That class of functions are additionally required to be monotone, piecewise linear and concave. Călinescu et al. (2007) and Vondrák (2008) discuss a subclass of submodular continuous functions, which is termed smooth submodular functions3, to describe the multilinear extension of a submodular set function. They propose the continuous greedy algorithm, which has a (1− 1/e) approximation guarantee on maximizing a smooth submodular functions under a down-monotone polytope constraint. Recently, Bach (2015) considers the minimization of a submodular continuous function, and proves that efficient techniques from convex optimization may be used for minimization. Very recently, Ene and Nguyen (2016) provide a reduction from a integer-lattice DR-submodular function maximization problem to a submodular set function maximization problem, which suggests a way to optimize submodular continuous functions over simple continuous constriants: Discretize the continuous function and constraint to be an integer-lattice instance, and then optimize it using the reduction. However, for monotone DR-submodular functions maximization, this method can not handle the general continuous constraints discussed in this work, i.e., arbitrary down-closed convex sets. And for general submodular function maximization, this method cannot be applied, since the reduction needs the additional diminishing returns property. Therefore we focus on continuous methods in this work.\nNon-convex optimization. Optimizing non-convex continuous functions has received renewed interest in the last decades. Recently, tensor methods have been used in various non-convex problems, e.g., learning latent variable models (Anandkumar et al., 2014) and training neural networks (Janzamin et al., 2015). A fundamental problem in non-convex optimization is to reach a stationary point assuming the smoothness of the objective (Sra, 2012; Li and Lin, 2015; Reddi et al., 2016; Allen-Zhu and Hazan, 2016). With extra assumptions, certain global convergence results can be obtained. For example, for functions with Lipschitz continuous Hessians, the regularized Newton scheme of Nesterov and Polyak (2006) achieves global convergence results for functions with\n3. A function f : [0, 1]n → R is smooth submodular if it has second partial derivatives everywhere and all entries of its Hessian matrix are non-positive.\nan additional star-convexity property or with an additional gradient-dominance property (Polyak, 1963). Hazan et al. (2015) introduce the family of σ-nice functions and propose a graduated optimization-based algorithm, that provably converges to a global optimum for this family of (generally) non-convex functions. However, it is typically difficult to verify whether these assumptions hold in real-world problems.\nTo the best of our knowledge, this work is the first to address the general problem of monotone and non-monotone submodular maximization over continuous domains. It is also the first to propose a sufficient and necessary diminishing-return-style characterization of submodularity for general functions. We propose efficient algorithms with strong approximation guarantees. We further show that continuous submodularity is a common property of many well-known objectives and finds various real-world applications.\nNotation. Throughout this work we assume E = {e1, e2, · · · , en} is the ground set of n elements, and χi ∈ Rn is the characteristic vector for element ei. We use boldface letters x ∈ RE and x ∈ Rn interchanglebly to indicate a n-dimensional vector, where xi is the i-th entry of x. We use a boldface captial letter A ∈ Rm×n to denote a matrix. For two vectors x,y ∈ RE , x ≤ y means xi ≤ yi for every element i in E. Finally, x|xi←k is the operation of setting the i-th element of x to k, while keeping all other elements unchanged."
    }, {
      "heading" : "3. Characterizations of submodular continuous functions",
      "text" : "Submodular continuous functions are defined on subsets of Rn: X = ∏n i=1Xi, where each Xi is a compact subset of R (Topkis, 1978; Bach, 2015). A function f : X → R is submodular iff for all (x,y) ∈ X × X ,\nf(x) + f(y) ≥ f(x ∨ y) + f(x ∧ y), (submodularity) (1)\nwhere ∧ and ∨ are the coordinate-wise minimum and maximum operations, respectively. Specifically, Xi could be a finite set, such as {0, 1} (in which case f(·) is called set function), or {0, · · · , ki − 1} (called integer-lattice function), where the notion of continuity is vacuous; Xi can also be an interval, which is referred to as a continuous domain. In this work, we consider the interval by default, but it is worth noting that the properties introduced in this section can be applied to Xi being a general compact subset of R.\nWhen twice-differentiable, f(·) is submodular iff all off-diagonal entries of its Hessian are nonpositive4 (Bach, 2015),\n∀x ∈ X , ∂ 2f(x)\n∂xi∂xj ≤ 0, ∀i 6= j. (2)\nSubmodular\nConcave Convex\nDR-submodular\nFigure 1: Concavity, convexity, submodularity and DRsubmodularity.\nThe class of submodular continuous functions contains a subset of both convex and concave functions, and shares some useful properties with them (illustrated in Figure 1). Examples include submodular and convex functions of the form φij(xi − xj) for φij convex; submodular and concave functions of the form x 7→ g( ∑n i=1 λixi) for g concave and λi non-negative (see Section 6 for example applications). Lastly, indefinite quadratic functions of the form f(x) = 12x\n>Hx + h>x with all off-diagonal entries of H non-positive are examples of submodular but non-convex/nonconcave functions. Continuous submodularity is preserved under\n4. Notice that an equilavent definition of (1) is that ∀x ∈ X , ∀i 6= j and ai, aj ≥ 0 s.t. xi + ai ∈ Xi, xj + aj ∈ Xj , it holds f(x+ aiχi) + f(x+ ajχj) ≥ f(x) + f(x+ aiχi + ajχj). With ai and aj approaching zero, one get (2).\nvarious operations, e.g., the sum of two submodular continuous functions is submodular, a submodular continuous function multiplied by a positive scalar is still submodular. Interestingly, characterizations of submodular continuous functions are in correspondence to those of convex functions, which are summarized in Table 1.\nIn the remainder of this section, we introduce useful properties of submodular continuous functions. First of all, we generalize the DR property (which was introduced when studying set and integer-lattice functions) to general functions defined over X . It will soon be clear that the DR property defines a subclass of submodular functions.\nDefinition 1 (DR property and DR-submodular functions). A function f(·) defined over X satisfies the diminishing returns (DR) property if ∀a ≤ b ∈ X , ∀i ∈ E, ∀k ∈ R+ s.t. (kχi+a) and (kχi+b) are still in X , it holds,\nf(kχi + a)− f(a) ≥ f(kχi + b)− f(b).\nThis function f(·) is called a DR-submodular5 function.\nOne immediate observation is that for a differentiable DR-submodular continuous function f(·), we have that ∀a ≤ b ∈ X , ∇f(a) ≥ ∇f(b), i.e., the gradient ∇f(·) is an antitone mapping from Rn to Rn. Recently, the DR property is explored by Eghbali and Fazel (2016) to achieve the worst-case competitive ratio for an online concave maximization problem. The DR property is also closely related to a sufficient condition on a concave function g(·) (Bilmes and Bai, 2017, Section 5.2), to ensure submodularity of the corresponding set function generated by giving g(·) boolean input vectors.\nIt is well known that for set functions, the DR property is equivalent to submodularity, while for integer-lattice functions, submodularity does not in general imply the DR property (Soma et al., 2014; Soma and Yoshida, 2015a,b). However, it was unclear whether there exists a diminishingreturn-style characterization that is equivalent to submodularity of integer-lattice functions. In this work we give a positive answer to this open problem by proposing the weak diminishing returns (weak DR) property for general functions defined over X , and prove that weak DR gives a sufficient and necessary condition for a general function to be submodular.\nDefinition 2 (weak DR property). A function f(·) defined over X has the weak diminishing returns property (weak DR) if ∀a ≤ b ∈ X , ∀i ∈ E s.t. ai = bi,∀k ∈ R+ s.t. (kχi + a) and (kχi + b) are still in X , it holds,\nf(kχi + a)− f(a) ≥ f(kχi + b)− f(b). (3)\nThe following proposition shows that for all set functions, as well as integer-lattice and continuous functions, submodularity is equivalent to the weak DR property.\n5. Note that DR property implies submodularity and thus the name “DR-submodular” contains redundant information about submodularity of a function, but we keep this terminology to be consistent with previous literature on submodular integer-lattice functions.\nProposition 1 (submodularity) ⇔ (weak DR). A function f(·) defined over X is submodular iff it satisfies the weak DR property.\nAll of the proofs can be found in Appendix A. Given Proposition 1, one can treat weak DR as the first order condition of submodularity: Notice that for a differentiable continuous function f(·) with the weak DR property, we have that ∀a ≤ b ∈ X , ∀i ∈ E s.t. ai = bi, it holds ∇if(a) ≥ ∇if(b), i.e., ∇f(·) is a weak antitone mapping. Now we show that the DR property is stronger than the weak DR property, and the class of DR-submodular functions is a proper subset of that of submodular functions, as indicated by Figure 1.\nProposition 2 (submodular/weak DR) + (coordinate-wise concave) ⇔ (DR). A function f(·) defined over X satisfies the DR property iff f(·) is submodular and coordinate-wise concave, where the coordinate-wise concave property is defined as: ∀x ∈ X , ∀i ∈ E, ∀k, l ∈ R+ s.t. (kχi + x), (lχi + x), ((k + l)χi + x) are still in X , it holds,\nf(kχi + x)− f(x) ≥ f((k + l)χi + x)− f(lχi + x),\nor equivalently (if twice differentiable) ∂ 2f(x) ∂x2i ≤ 0, ∀i ∈ E.\nProposition 2 shows that a twice differentiable function f(·) is DR-submodular iff ∀x ∈ X , ∂ 2f(x) ∂xi∂xj\n≤ 0,∀i, j ∈ E, which does not necessarily imply the concavity of f(·). Given Proposition 2, we also have the characterizations of DR-submodular continuous functions, which are summarized in Table 2."
    }, {
      "heading" : "4. Maximizing monotone DR-submodular continuous functions",
      "text" : "In this section, we present an algorithm for maximizing a monotone DR-submodular continuous function subject to a general down-closed convex constraint, i.e., maxx∈Pu f(x). A down-closed convex set Pu is a convex set P associated with a lower bound u ∈ P, such that 1) ∀y ∈ P, u ≤ y; and 2) ∀y ∈ P, x ∈ Rn, u ≤ x ≤ y implies x ∈ P. Without loss of generality, we assume P lies in the postitive orthant and has the lower bound 0, since otherwise we can always define a new set P ′ = {x | x = y−u,y ∈ P} in the positive orthant, and a corresponding monotone DR-submdular function f ′(x) := f(x+ u).\nMaximizing a monotone DR-submodular function over a down-closed convex constraint has many real-world applications, e.g., influence maximization with continuous assignments and sensor energy management. In particular, for influence maximization (see Section 6), the constraint is a down-closed polytope in the positive orthant: P = {x ∈ Rn | 0 ≤ x ≤ ū,Ax ≤ b, ū ∈ Rn+,A ∈ Rm×n+ , b ∈ Rm+}. We start with the following hardness result:\nAlgorithm 1: Frank-Wolfe variant for monotone DR-submodular function maximization\nInput: maxx∈P f(x), P is a down-closed convex set in the positive orthant with lower bound 0, prespecified stepsize γ ∈ (0, 1]\n1 x0 ← 0, t← 0, k ← 0; //k : iteration index 2 while t < 1 do 3 find vk s.t. 〈vk,∇f(xk)〉 ≥ αmaxv∈P〈v,∇f(xk)〉 − 12δL; //L > 0 is the Lipschitz parameter, α ∈ (0, 1] is the mulplicative error level, δ ∈ [0, δ̄] is the additive error level 4 find stepsize γk ∈ (0, 1], e.g., γk ← γ; set γk ← min{γk, 1− t}; 5 xk+1 ← xk + γkvk, t← t+ γk, k ← k + 1; 6 Return xK ; //assuming there are K iterations in total\nProposition 3. The problem of maximizing a monotone DR-submodular continuous function subject to a general down-closed polytope constraint is NP-hard. For any > 0, it cannot be approximated in polynomial time within a ratio of (1 − 1/e + ) (up to low-order terms), unless RP = NP.\nDue to the NP-hardness of converging to the global optimum, in the following by “convergence” we mean converging to a point near the global optimum. The algorithm is a generalization of the continuous greedy algorithm of Vondrák (2008) for maximizing a smooth submodular function, and related to the convex Frank-Wolfe algorithm (Frank and Wolfe, 1956; Jaggi, 2013) for minimizing a convex function. We summarize the Frank-Wolfe variant in Algorithm 1. In each iteration k, the algorithm uses the linearization of f(·) as a surrogate, and moves in the direction of the maximizer of this surrogate function, i.e. vk = arg maxv∈P〈v,∇f(xk)〉. Intuitively, we search for the direction in which we can maximize the improvement in the function value and still remain feasible. Finding such a direction requires maximizing a linear objective at each iteration. Meanwhile, it eliminates the need for projecting back to the feasible set in each iteration, which is an essential step for methods such as projected gradient ascent. The algorithm uses stepsize γk to update the solution in each iteration, which can be simply a prespecified value γ. Note that the Frank-Wolfe variant can tolerate both multiplicative error α and additive error δ when solving the subproblem (Step 3 of Algorithm 1). Setting α = 1 and δ = 0, we recover the error-free case.\nNotice that the Frank-Wolfe variant in Algorithm 1 is different from the convex Frank-Wolfe algorithm mainly in the update direction being used: For Algorithm 1, the update direction (in Step 5) is vk, while for convex Frank-Wolfe it is vk − xk, i.e., xk+1 ← xk + γk(vk − xk). The reason of this difference will soon be clear by exploring the property of DR-submodular functions. Specifically, DR-submodular functions are non-convex/non-concave in general, however, there is certain connection between DR-submodularity and concavity.\nProposition 4. A DR-submodular continuous function f(·) is concave along any non-negative direction, and any non-positive direction.\nProposition 4 implies that the univariate auxiliary function gx,v(ξ) := f(x+ξv), ξ ∈ R+,v ∈ RE+ is concave. As a result, the Frank-Wolfe variant can follow a concave direction at each step, which is the main reason it uses vk as the update direction (notice that vk is a non-negative direction).\nTo derive the approximation guarantee, we need assumptions on the non-linearity of f(·) over the domain P, which closely corresponds to a Lipschitz assumption on the derivative of gx,v(·). For\na gx,v(·) with L-Lipschitz continuous derivative in [0, 1] (L > 0), we have,\n−L 2 ξ2 ≤ gx,v(ξ)− gx,v(0)− ξ∇gx,v(0) = f(x+ ξv)− f(x)− 〈ξv,∇f(x)〉,∀ξ ∈ [0, 1]. (4)\nTo prove the approximation guarantee, we first derive the following lemma.\nLemma 1. The output solution xK ∈ P. Assuming x∗ to be the optimal solution, one has,\n〈vk,∇f(xk)〉 ≥ α[f(x∗)− f(xk)]− 1 2 δL, ∀k = 0, · · · ,K − 1. (5)\nTheorem 1 (Approximation guarantee). For error levels α ∈ (0, 1], δ ∈ [0, δ̄], with K iterations, Algorithm 1 outputs xK ∈ P s.t.,\nf(xK) ≥ (1− e−α)f(x∗)− L 2 ∑K−1 k=0 γ2k − Lδ 2 + e−αf(0). (6)\nTheorem 1 gives the approximation guarantee for arbitrary chosen stepsize γk. By observing that ∑K−1 k=0 γk = 1 and ∑K−1 k=0 γ 2 k ≥ K−1 (see the proof in Appendix B.5), with constant stepsize, we obtain the following “tightest” approximation bound,\nCorollary 1. For a fixed number of iterations K, and constant stepsize γk = γ = K −1, Algorithm"
    }, {
      "heading" : "1 provides the following approximation guarantee:",
      "text" : "f(xK) ≥ (1− e−α)f(x∗)− L 2K − Lδ 2 + e−αf(0).\nCorollary 1 implies that with a constant stepsize γ, 1) when γ → 0 (K →∞), Algorithm 1 will output the solution with the worst-case guarantee (1− 1/e)f(x∗) in the error-free case if f(0) = 0; and 2) The Frank-Wolfe variant has a sub-linear convergence rate for monotone DR-submodular maximization over a down-closed convex constraint. Time complexity. It can be seen that when using a constant stepsize, Algorithm 1 needs O(1 ) iterations to get -close to the worst-case guarantee (1− e−1)f(x∗) in the error-free case. When P is a polytope in the positive orthant, one iteration of Algorithm 1 costs approximately the same as solving a positive LP, for which a nearly-linear time solver exists (Allen-Zhu and Orecchia, 2015)."
    }, {
      "heading" : "5. Maximizing non-monotone submodular continuous functions",
      "text" : "The problem of maximizing a general non-monotone submodular continuous function under box constraints6, i.e., maxx∈[u,ū]⊆X f(x), has various real-world applications, including revenue maximization with continuous assignments, multi-resolution summarization, etc, as discussed in Section 6. The following proposition shows the NP-hardness of the problem.\nProposition 5. The problem of maximizing a generally non-monotone submodular continuous function subject to box constraints is NP-hard. Furthermore, there is no (1/2 + )-approximation ∀ > 0, unless RP = NP.\nWe now describe our algorithm for maximizing a non-monotone submodular continuous function subject to box constraints. It provides a 1/3-approximation, is inspired by the double greedy algorithm of Buchbinder et al. (2012) and Gottschalk and Peis (2015), and can be viewed as a procedure performing coordinate-ascent on two solutions.\nAlgorithm 2: DoubleGreedy algorithm for maximizing non-monotone submodular continuous functions\nInput: maxx∈[u,ū] f(x), f is generally non-monotone, f(u) + f(ū) ≥ 0 1 x0 ← u, y0 ← ū; 2 for k = 1→ n do 3 find ûa s.t. f(x\nk−1|xk−1ek ←ûa) ≥ maxua∈[uek ,ūek ] f(x k−1|xk−1ek ←ua)− δ,\nδa ← f(xk−1|xk−1ek ←ûa)− f(x k−1); //δ ∈ [0, δ̄] is the additive error level\n4 find ûb s.t. f(y k−1|yk−1ek ←ûb) ≥ maxub∈[uek ,ūek ] f(y k−1|yk−1ek ←ub)− δ, δb ← f(yk−1|yk−1ek ←ûb)− f(y k−1); 5 If δa ≥ δb: xk ← (xk−1|xk−1ek ←ûa), y k ← (yk−1|yk−1ek ←ûa) ; 6 Else: yk ← (yk−1|yk−1ek ←ûb), x k ← (xk−1|xk−1ek ←ûb); 7 Return xn (or yn); //note that xn = yn\nWe view the process as two particles starting from x0 = u and y0 = ū, and following a certain “flow” toward each other. The pseudo-code is given in Algorithm 2. We proceed in n rounds that correspond to some arbitrary order of the coordinates. At iteration k, we consider solving a one-dimensional (1-D) subproblem over coordinate ek for each particle, and moving the particles based on the calculated local gains toward each other. Formally, for a given coordinate ek, we solve a 1-D subproblem to find the value of the first solution x along coordinate ek that maximizes f , i.e., ûa = arg maxua f(x k−1|xk−1ek ←ua) − f(x k−1), and calculate its marginal gain δa. We then solve another 1-D subproblem to find the value of the second solution y along coordinate ek that maximizes f , i.e., ûb = arg maxub f(y k−1|yk−1ek ←ub) − f(y k−1), and calculate the second marginal gain δb. We decide by comparing the two marginal gains. If changing xek to be ûa has a larger local benefit, we change both xek and yek to be ûa. Otherwise, we change both of them to be ûb. After n iterations the particles should meet at point xn = yn, which is the final solution. Note that Algorithm 2 can tolerate additive error δ in solving each 1-D subproblem (Steps 3, 4).\nWe would like to emphasize that the assumptions required by DoubleGreedy are submodularity of f , f(u) + f(ū) ≥ 0 and the (approximate) solvability of the 1-D subproblem. For proving the approximation guarantee, the idea is to bound the loss in the objective value from the assumed optimal objective value between every two consecutive steps, which is then used to bound the maximum loss after n iterations.\nTheorem 2. Assuming the optimal solution to be x∗, the output of Algorithm 2 has function value no less than 13f(x ∗)− 4n3 δ, where δ ∈ [0, δ̄] is the additive error level for each 1-D subproblem.\nTime complexity. It can be seen that the time complexity of Algorithm 2 is O(n ∗ cost 1D), where cost 1D is the cost of solving the 1-D subproblem. Solving a 1-D subproblem is usually very cheap. For non-convex/non-concave quadratic programming it has a closed form solution."
    }, {
      "heading" : "6. Examples of submodular continuous objective functions",
      "text" : "In this part, we discuss several concrete problem instances with their corresponding submodular continuous objective functions.\n6. It is also called “unconstrained” maximization in the combinatorial optimization community, since the domain X itself is also a box. Note that the box can be in the negative orthant here.\nExtensions of submodular set functions. The multilinear extension (Călinescu et al., 2007) and softmax extension (Gillenwater et al., 2012) are special cases of DR-submodular functions, that are extensively used for submodular set function maximization. The Lovász extension (Lovász, 1983) used for submodular set function minimization is both submodular and convex (see Appendix A in Bach (2015)).\nNon-convex/non-concave quadratic programming (NQP). NQP problem of the form f(x) = 1 2x >Hx+h>x+c under linear constraints naturally arises in many applications, including scheduling (Skutella, 2001), inventory theory, and free boundary problems. A special class of NQP is the submodular NQP (the minimization of which was studied in Kim and Kojima (2003)), in which all off-diagonal entries of H are required to be non-positive. In this work, we mainly use submodular NQPs as synthetic functions for both monotone DR-submodular maximization and non-monotone submodular maximization.\nOptimal budget allocation with continuous assignments. Optimal budget allocation is a special case of the influence maximization problem. It can be modeled as a bipartite graph (S, T ;W ), where S and T are collections of advertising channels and customers, respectively. The edge weight, pst ∈ W , represents the influence probability of channel s to customer t. The goal is to distribute the budget (e.g., time for a TV advertisement, or space of an inline ad) among the source nodes, and to maximize the expected influence on the potential customers (Soma et al., 2014; Hatano et al., 2015). The total influence of customer t from all channels can be modeled by a proper monotone DR-submodular function It(x), e.g., It(x) = 1− ∏ (s,t)∈W (1− pst) xs where x ∈ RS+ is the budget assignment among the advertising channels. For a set of k advertisers, let xi ∈ RS+ to be the budget assignment for advertiser i, and x := [x1, · · · ,xk] denote the assignments for all the advertisers. The overall objective is,\ng(x) = ∑k\ni=1 αif(x\ni) with f(xi) := ∑\nt∈T It(x\ni), 0 ≤ xi ≤ ūi,∀i = 1, · · · , k (7)\nwhich is monotone DR-submodular. A concrete application is for search marketing advertiser bidding, in which vendors bid for the right to appear alongside the results of different search keywords. Here, xis is the volume of advertising space allocated to the advertiser i to show his ad alongside query keyword s. The search engine company needs to distribute the budget (advertising space) to all vendors to maximize their influence on the customers, while respecting various constraints. For example, each vendor has a specified budget limit for advertising, and the ad space associated with each search keyword can not be too large. All such constraints can be formulated as a down-closed polytope P, hence the Frank-Wolfe variant can be used to find an approximate solution for the problem maxx∈P g(x). Note that one can flexibly add regularizers in designing It(x\ni) as long as it remains monotone DR-submodular. For example, adding separable regularizers of the form∑\ns φ(x i s) does not change the off-diagonal entries of the Hessian, and hence maintains submodu-\nlarity. Alternatively, bounding the second-order derivative of φ(xis) ensures DR-submodularity.\nRevenue maximization with continuous assignments. In viral marketing, sellers choose a small subset of buyers to give them some product for free, to trigger a cascade of further adoptions through “word-of-mouth” effects, in order to maximize the total revenue (Hartline et al., 2008). For some products (e.g., software), the seller usually gives away the product in the form of a trial, to be used for free for a limited time period. In this task, except for deciding whether to choose a user or not, the sellers also need to decide how much the free assignment should be, in which the assignments should be modeled as continuous variables. We call this problem revenue maximization with continuous assignments. Assume there are q products and n buyers/users, let xi ∈ Rn+ to be\nthe assignments of product i to the n users, let x := [x1, · · · ,xq] denote the assignments for the q products. The revenue can be modelled as g(x) = ∑q i=1 f(x i) with\nf(xi) := αi ∑\ns:xis=0 Rs(x\ni) + βi ∑\nt:xit 6=0 φ(xit)+ γi ∑ t:xit 6=0 R̄t(x i), 0 ≤ xi ≤ ūi, (8)\nwhere xit is the assignment of product i to user t for free, e.g., the amount of free trial time or the amount of the product itself. Rs(x\ni) models revenue gain from user s who did not receive the free assignment. It can be some non-negative, non-decreasing submodular function. φ(xit) models revenue gain from user t who received the free assignment, since the more one user tries the product, the more likely he/she will buy it after the trial period. R̄t(x\ni) models the revenue loss from user t (in the free trial time period the seller cannot get profits), which can be some non-positive, non-increasing submodular function. With β=γ=0, we recover the classical model of Hartline et al. (2008). For products with continuous assignments, usually the cost of the product does not increase with its amount, e.g., the product as a software, so we only have the box constraint on each assignment. The objective in Equation 8 is generally non-concave/non-convex, and nonmonotone submodular (see Appendix D for more details), thus can be approximately maximized by the proposed DoubleGreedy algorithm.\nLemma 2. If Rs(x i) is non-decreasing submodular and R̄t(x i) is non-increasing submodular, then f(xi) in Equation 8 is submodular.\nSensor energy management. For cost-sensitive outbreak detection in sensor networks (Leskovec et al., 2007), one needs to place sensors in a subset of locations selected from all the possible locations E, to quickly detect a set of contamination events V , while respecting the cost constraints of the sensors. For each location e ∈ E and each event v ∈ V , a value t(e, v) is provided as the time it takes for the placed sensor in e to detect event v. Soma and Yoshida (2015a) considered the sensors with discrete energy levels. It is also natural to model the energy levels of sensors to be a continuous variable x ∈ RE+. For a sensor with energy level xe, the success probability it detects the event is 1−(1−p)xe , which models that by spending one unit of energy one has an extra chance of detecting the event with probability p. In this model, beyond deciding whether to place a sensor or not, one also needs to decide the optimal energy levels. Let t∞ = maxe∈E,v∈V t(e, v), let ev be the first sensor that detects event v (ev is a random variable). One can define the objective as the expected detection time that could be saved,\nf(x) := Ev∈V Eev [t∞ − t(ev, v)], (9)\nwhich is a monotone DR-submodular function. Maximizing f(x) w.r.t. the cost constraints pursues the goal of finding the optimal energy levels of the sensors, to maximize the expected detection time that could be saved.\nMulti-resolution summarization. Suppose we have a collection of items, e.g., images E = {e1, · · · , en}. Our goal is to extract a representative summary, where representativeness is defined w.r.t. a submodular set function F : 2E → R. However, instead of returning a single set, our goal is to obtain summaries at multiple levels of detail or resolution. One way to achieve this goal is to assign each item ei a nonnegative score xi. Given a user-tunable threshold τ , the resulting summary Sτ = {ei|xi ≥ τ} is the set of items with scores exceeding τ . Thus, instead of solving the discrete problem of selecting a fixed set S, we pursue the goal to optimize over the scores, e.g., to use the following submodular continuous function,\nf(x) = ∑\ni∈E ∑ j∈E φ(xj)si,j − ∑ i∈E ∑ j∈E xixjsi,j , (10)\nwhere si,j ≥ 0 is the similarity between items i, j, and φ(·) is a non-decreasing concave function.\nFacility location. The classical discrete facility location problem can be naturally generalized to the continuous case where the scale of a facility is determined by a continuous value in interval [0, ū]. For a set of facilities E, let x ∈ RE+ be the scale of all facilities. The goal is to decide how large each facility should be in order to optimally serve a set T of customers. For a facility s of scale xs, let pst(xs) be the value of service it can provide to customer t ∈ T , where pst(xs) is a normalized monotone function (pst(0) = 0). Assuming each customer chooses the facility with highest value, the total service provided to all customers is f(x) = ∑ t∈T maxs∈E pst(xs). It can be shown that f is monotone submodular.\nOther applications. Many discrete submodular problems can be naturally generalized to the continuous setting with submodular continuous objectives. The maximum coverage problem and the problem of text summarization with submodular objectives are among the examples (Lin and Bilmes, 2010). We defer further details to Appendix E."
    }, {
      "heading" : "7. Experimental results",
      "text" : "We compare the performance of our proposed algorithms, the Frank-Wolfe variant and DoubleGreedy, with the following baselines: a) Random: uniformly sample ks solutions from the constraint set using the hit-and-run sampler (Kroese et al., 2013), and select the best one. For the constraint set as a very high-dimensional polytope, this approach is computationally very expensive. To accelerate sampling from a high-dimensional polytope, we also use b) RandomCube: randomly sample ks solutions from the hypercube, and decrease their elements until they are inside the polytope. In addition we consider c) ProjGrad: projected gradient ascent with an empirically tuned step size; and d) SingleGreedy: for non-monotone submodular functions maximization over a box constraint, we greedily increase each coordinate, as long as it remains feasible. This approach is similar to the coordinate ascent method. In all of the experiments, we use random order of coordinates for DoubleGreedy. We use constant step size for the Frank-Wolfe variant since it gives the tightest approximation guarantee (see Corollary 1). The performance of the methods are evaluated for the following tasks."
    }, {
      "heading" : "7.1 Results for monotone maximization",
      "text" : "Monotone DR-submodular NQP. We randomly generated monotone DR-submodular NQP functions of the form f(x) = 12x\n>Hx+h>x, where H ∈ Rn×n is a random matrix with uniformly distributed non-positive entries in [−100, 0], n = 100. We further generated a set of m = 50 linear constraints to construct the positive polytope P = {x ∈ Rn|Ax ≤ b, 0 ≤ x ≤ ū}, where A has uniformly distributed entries in [0, 1], b = 1, ū = 1. To make the gradient non-negative, we set h = −H>ū. We empirically tuned step size αp for ProjGrad and ran all algorithms for 50 iterations. Figure 2a shows the utility obtained by the Frank-Wolfe variant v.s. the iteration index for 4 function instances with different values of b. Figure 2b shows the average utility obtained by different algorithms with increasing values of b. The result is the average of 20 repeated experiments. For ProjGrad, we plotted the curves for three different values of αp. One can observe that the performance of ProjGrad fluctuates with different step sizes. With the best-tuned step size, ProjGrad performs close to the Frank-Wolfe variant.\nOptimal budget allocation. As our real-world experiments, we used the Yahoo! Search Marketing Advertiser Bidding Data7, which consists of 1,000 search keywords, 10,475 customers and 52,567 edges. We considered the frequency of (keyword, customer) pairs to estimate the influence\n7. https://webscope.sandbox.yahoo.com/catalog.php?datatype=a\nprobabilities, and used the average of the bidding prices to put a limit on the budget of each advertiser. Since the Random sampling was too slow, we compared with the RandomCube method. Figures 2c and 2d show the value of the utility function (influence) when varying the budget on volume of ads and on budget of advertisers, respectively. Again, we observe that the Frank-Wolfe variant outperforms the other baselines, and the performance of ProjGrad highly depends on the choice of the step size."
    }, {
      "heading" : "7.2 Results for non-monotone maximization",
      "text" : "Non-monotone submodular NQP. We randomly generated non-monotone submodular NQP functions of the form f(x) = 12x\n>Hx+h>x+c, where H ∈ Rn×n is a sparse matrix with uniformly distributed non-positive off-diagonal entries in [−10, 0], n = 1000. We considered a matrix for which\naround 50% of the eigenvalues are positive and the other 50% are negative. We set h = −0.2∗H>ū to make f(x) non-monotone. We then selected a value for c such that f(0)+f(ū) ≥ 0. ProjGrad was executed for n iterations, with empirically tuned step sizes. For the Random method we set ks = 1, 000. Figure 3a shows the utility of the two intermediate solutions maintained by DoubleGreedy. One can observe that they both increase in each iteration. Figure 3b shows the values of the utility function for varying upper bound ū. The result is the average over 20 repeated experiments. We can see that DoubleGreedy has strong approximation performance, while ProjGrad’s results depend on the choice of the step size. With carefully hand-tuned step size, its performance is comparable to DoubleGreedy.\nRevenue maximization. Without loss of generality, we considered maximizing the revenue from selling one product (corresponding to q = 1, see Appendix D for more details on this model). It\ncan be observed that the objective in Equation 8 is generally non-smooth and discontinuous at any point x which contains the element of 0. Since the subdifferential can be empty, we cannot use the subgradient-based method and could not compare with ProjGrad. We performed our experiments on the top 500 largest communities of the YouTube social network8 consisting of 39,841 nodes and 224,235 edges. The edge weights were assigned according to a uniform distribution U(0, 1). See Figure 3c, 3d for an illustration of revenue for varying upper bound (ū) and different combinations of the parameters (α, β, γ) in the model (Equation 8). For different values of the upper bound, DoubleGreedy outperforms the other baselines, while SingleGreedy maintaining only one intermediate solution obtained a lower utility than DoubleGreedy."
    }, {
      "heading" : "8. Conclusion",
      "text" : "In this paper, we characterized submodular continuous functions, and proposed two approximation algorithms to efficiently maximize them. In particular, for maximizing monotone DR-submodular continuous functions subject to general down-closed convex constraints, we proposed a (1 − 1/e)approximation algorithm, and for maximizing non-monotone submodular continuous functions subject to a box constraint, we proposed a 1/3-approximation algorithm. We demonstrate the effectiveness of our algorithms through a set of experiments on real-world applications, including budget allocation, revenue maximization, and non-convex/non-concave quadratic programming, and show that our proposed methods outperform the baselines in all the experiments. This work demonstrates that submodularity can ensure guaranteed optimization in the continuous setting for problems with (generally) non-convex/non-concave objectives."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The authors would like to thank Martin Jaggi for valuable discussions. This research was partially supported by ERC StG 307036 and the Max Planck ETH Center for Learning Systems.\n8. http://snap.stanford.edu/data/com-Youtube.html"
    }, {
      "heading" : "Appendix",
      "text" : ""
    }, {
      "heading" : "A. Proofs of properties of submodular continuous functions",
      "text" : "Since Xi is a compact subset of R, we denote its lower bound and upper bound to be ui and ūi, respectively in this section.\nA.1 Alternative formulation of the weak DR property\nFirst of all, we will prove that weak DR has the following alternative formulation, which will be used to prove Proposition 1.\nLemma 3 (Alternative formulation of weak DR). The weak DR property (Equation 3, denoted as Formulation I) has the following equilvalent formulation (Equation 11, denoted as Formulation II): ∀a ≤ b ∈ X , ∀i ∈ {i′|ai′ = bi′ = ui′}, ∀k′ ≥ l′ ≥ 0 s.t. (k′χi + a), (l′χi + a), (k′χi + b) and (l′χi + b) are still in X , the following inequality is satisfied,\nf(k′χi + a)− f(l′χi + a) ≥ f(k′χi + b)− f(l′χi + b) (Formulation II) (11)\nProof. Let D1 = {i|ai = bi = ui}, D2 = {i|ui < ai = bi < ūi}, and D3 = {i|ai = bi = ūi}. 1) Formulation II ⇒ Formulation I When i ∈ D1, set l′ = 0 in Formulation II one can get f(k′χi+a)−f(a) ≥ f(k′χi+b)−f(b). When i ∈ D2, ∀k ≥ 0, let l′ = ai − ui = bi − ui > 0, k′ = k + l′ = k + (ai − ui), and let ā = (a|ai←ui), b̄ = (b|bi←ui). It is easy to see that ā ≤ b̄, and āi = b̄i = ui. Then from Formulation II,\nf(k′χi + ā)− f(l′χi + ā) = f(kχi + a)− f(a) ≥ f(k′χi + b̄)− f(l′χi + b̄) = f(kχi + b)− f(b).\nWhen i ∈ D3, Equation 3 holds trivially. The above three situations proves the Formulation I. 2) Formulation II ⇐ Formulation I ∀a ≤ b, ∀i ∈ D1, one has ai = bi = ui. ∀k′ ≥ l′ ≥ 0, let â = l′χi + a, b̂ = l′χi + b, let k = k′ − l′ ≥ 0, it can be verified that â ≤ b̂ and âi = b̂i, from Formulation I,\nf(kχi + â)− f(â) = f(k′χi + a)− f(l′χi + a) ≥f(kχi + b̂)− f(b̂) = f(k′χi + b)− f(l′χi + b)\nwhich proves Formulation II."
    }, {
      "heading" : "A.2 Proof of Proposition 1",
      "text" : "Proof. 1) submodularity ⇒ weak DR: Let us prove the Formulation II (Equation 11) of weak DR, which is, ∀a ≤ b ∈ X , ∀i ∈ {i′|ai′ = bi′ = ui′},∀k′ ≥ l′ ≥ 0, the following inequality holds,\nf(k′χi + a)− f(l′χi + a) ≥ f(k′χi + b)− f(l′χi + b).\nAnd f is a submodular function iff ∀x,y ∈ X , f(x) + f(y) ≥ f(x ∨ y) + f(x ∧ y), so f(y) − f(x ∧ y) ≥ f(x ∨ y)− f(x).\nNow ∀a ≤ b ∈ X , one can set x = l′χi + b and y = k′χi + a. It can be easily verified that x∧y = l′χi +a and x∨y = k′χi + b. Substituting all the above equalities into f(y)− f(x∧y) ≥ f(x ∨ y)− f(x) one can get f(k′χi + a)− f(l′χi + a) ≥ f(k′χi + b)− f(l′χi + b).\n2) submodularity ⇐ weak DR: Let us use Formulation I (Equation 3) of weak DR to prove the submodularity property. ∀x,y ∈ X , let D := {e1, · · · , ed} to be the set of elements for which ye > xe, let kei := yei −xei . Now set a0 := x ∧ y, b0 := x and ai = (ai−1|ai−1ei ←yei) = keiχi + a i−1, bi = (bi−1|bi−1ei ←yei) = keiχi + b i−1, for i = 1, · · · , d. One can verify that ai ≤ bi, aiei′ = b i ei′\nfor all i′ ∈ D, i = 0, · · · , d, and that ad = y, bd = x ∨ y.\nApplying Equation 3 of the weak DR property for i = 1, · · · , d one can get\nf(ke1χe1 + a 0)− f(a0) ≥ f(ke1χe1 + b0)− f(b0) f(ke2χe2 + a 1)− f(a1) ≥ f(ke2χe2 + b1)− f(b1) · · · f(kedχed + a d−1)− f(ad−1) ≥ f(kedχed + b d−1)− f(bd−1).\nTaking a sum over all the above d inequalities, one can get\nf(kedχed + a d−1)− f(a0) ≥ f(kedχed + b d−1)− f(b0)⇔ f(y)− f(x ∧ y) ≥ f(x ∨ y)− f(x)⇔ f(x) + f(y) ≥ f(x ∨ y) + f(x ∧ y),\nwhich proves the submodularity."
    }, {
      "heading" : "A.3 Proof of Proposition 2",
      "text" : "Proof. 1) submodular + coordinate-wise concave ⇒ DR: From coordinate-wise concavity we have f(a + kχi) − f(a) ≥ f(a + (bi − ai + k)χi) − f(a + (bi − ai)χi). Therefore, to prove DR it suffices to show that\nf(a+ (bi − ai + k)χi)− f(a+ (bi − ai)χi) ≥ f(b+ kχi)− f(b). (12)\nLet x := b,y := (a + (bi − ai + k)χi), so x ∧ y = (a + (bi − ai)χi),x ∨ y = (b + kχi). From submodularity, one can see that inequality 12 holds.\n2) submodular + coordinate-wise concave ⇐ DR: From DR property, the weak DR (Equation 3) property is implied, which equivalently proves the submodularity property. To prove coordinate-wise concavity, one just need to set b := a+ lχi, then it reads f(a+kχi)− f(a) ≥ f(a+ (k + l)χi)− f(a+ lχi)."
    }, {
      "heading" : "B. Proofs for the monotone DR-submodular continuous functions maximization",
      "text" : ""
    }, {
      "heading" : "B.1 Proof of Proposition 3",
      "text" : "Proof. On a high level, the proof idea follows from the reduction from the problem of maximizing a monotone submodular set function subject to cardinality constraints.\nLet us denote Π1 as the problem of maximizing a monotone submodular set function subject to cardinality constraints, and Π2 as the problem of maximizing a monotone DR-submodular continuous function under general down-closed polytope constraints. Following Călinescu et al. (2011),\nthere exist an algorithm A for Π1 that consists of a polynomial time computation in addition to polynomial number of subroutine calls to an algorithm for Π2. For details on A see the following.\nFirst of all, the multilinear extension (Călinescu et al., 2007) of a monotone submodular set function is a monotone submodular continuous function, and it is coordinate-wise linear, thus falls into a special case of monotone DR-submodular continuous functions.\nSo the algorithm A can be: 1) Maximize the multilinear extension of the submodular set function over the matroid polytope associated with the cardinality constraint, which can be achieved by solving an instance of Π2. We call the solution obtained the fractional solution; 2) Round the fractional solution to a feasible integeral solution using polynomial time rounding technique in Ageev and Sviridenko (2004); Călinescu et al. (2007) (called the pipage rounding). Thus we prove the reduction from Π1 to Π2.\nOur reduction algorithm A implies the NP-hardness and inapproximability of problem Π2. For the NP-hardness, because Π1 is well known to be NP-hard (Călinescu et al., 2007; Feige, 1998), so Π2 is NP-hard as well. For the inapproximability: Assume there exists a polynomial algorithm B that can solve Π2 better than 1−1/e, then we can use B as the subroutine algorithm in the reduction, which implies that one can solve Π1 better than 1−1/e. Now we slightly adapt the proof of inapproximability on max-k-cover in Feige (1998), since max-k-cover is a special case of Π1. According to Theorem 5.3 in Feige (1998) and our reduction A , we have a reduction from approximating 3SAT-5 to problem Π2. Using the rest proof of Theorem 5.3 in Feige (1998), we reach the result that one cannot solve Π2 better than 1− 1/e, unless RP = NP."
    }, {
      "heading" : "B.2 Proof of Proposition 4",
      "text" : "Proof. Consider a function g(ξ) := f(x+ ξv∗), ξ ≥ 0,v∗ ≥ 0. dg(ξ)dξ = 〈v ∗,∇f(x+ ξv∗)〉.\ng(ξ) is concave ⇔\nd2g(ξ)\ndξ2 = (v∗)>∇2f(x+ ξv∗)v∗ = ∑ i 6=j v∗i v ∗ j∇2ijf + ∑ i (v∗i ) 2∇2iif ≤ 0\nThe non-positiveness of ∇2ijf is ensured by submodularity of f(·), and the non-positiveness of ∇2iif results from the coordinate-wise concavity of f(·).\nThe proof of concavity along any non-positive direction is similar, which is omitted here."
    }, {
      "heading" : "B.3 Proof of Lemma 1",
      "text" : "Proof. It is easy to see that xK is a convex linear combination of points in P, so xK ∈ P. Consider the point v∗ := (x∗∨x)−x = (x∗−x)∨0 ≥ 0. Because v∗ ≤ x∗ and P is down-closed, we get v∗ ∈ P. By monotonicity, f(x+ v∗) = f(x∗ ∨ x) ≥ f(x∗). Consider the function g(ξ) := f(x+ ξv∗), ξ ≥ 0. dg(ξ)dξ = 〈v\n∗,∇f(x+ ξv∗)〉. From Proposition 4, g(ξ) is concave, hence\ng(1)− g(0) = f(x+ v∗)− f(x) ≤ dg(ξ) dξ ∣∣∣ ξ=0 × 1 = 〈v∗,∇f(x)〉.\nThen one can get\n〈v,∇f(x)〉 (a) ≥ α〈v∗,∇f(x)〉 − 1 2 δL ≥ α(f(x+ v∗)− f(x))− 1 2 δL ≥ α(f(x∗)− f(x))− 1 2 δL\nwhere (a) is from the selection rule in Step 3 of Algorithm 1."
    }, {
      "heading" : "B.4 Proof of Theorem 1",
      "text" : "Proof. From the Lipschitz continuous derivative assumption of g(·) (Equation 4): f(xk+1)− f(xk) = f(xk + γkvk)− f(xk)\n= g(γk)− g(0) ≥ γk〈vk,∇f(xk)〉 − L\n2 γ2k (Lipschitz assumption in Equation 4)\n≥ γkα[f(x∗)− f(xk)]− 1\n2 γkδL−\nL 2 γ2k (Lemma 1)\nAfter rearrangement,\nf(xk+1)− f(x∗) ≥ (1− αγk)[f(xk)− f(x∗)]− 1\n2 γkδL−\nL 2 γ2k\nTherefore,\nf(xK)− f(x∗) ≥ K−1∏ k=0 (1− αγk)[f(0)− f(x∗)]− δL 2 K−1∑ k=0 γk − L 2 K−1∑ k=0 γ2k .\nOne can observe that ∑K−1\nk=0 γk = 1, and since 1− y ≤ e−y when y ≥ 0,\nf(x∗)− f(xK) ≤ [f(x∗)− f(0)]e−α ∑K−1 k=0 γk + δL\n2 + L 2 K−1∑ k=0 γ2k\n= [f(x∗)− f(0)]e−α + δL 2 + L 2 K−1∑ k=0 γ2k .\nAfter rearrangement, we get,\nf(xK) ≥ (1− 1/eα)f(x∗)− L 2 K−1∑ k=0 γ2k − Lδ 2 + e−αf(0)."
    }, {
      "heading" : "B.5 Proof of Corollary 1",
      "text" : "Proof. Fixing K, to reach the tightest bound in Equation 6 amounts to solving the following problem:\nmin K−1∑ k=0 γ2k\ns.t. K−1∑ k=0 γk = 1, γk ≥ 0.\nUsing Lagrangian method, let λ be the Lagrangian multiplier, then\nL(γ0, · · · , γK−1, λ) = K−1∑ k=0 γ2k + λ [ K−1∑ k=0 γk − 1 ] .\nIt can be easily verified that when γ0 = · · · = γK−1 = K−1, ∑K−1 k=0 γ 2 k reaches the minimum (which is K−1). Therefore we obtain the tightest worst-case bound in Corollary 1."
    }, {
      "heading" : "C. Proofs for the non-monotone submodular continuous functions",
      "text" : "maximization"
    }, {
      "heading" : "C.1 Proof of Proposition 5",
      "text" : "Proof. The main proof follows from the reduction from the problem of maximizing an unconstrained non-monotone submodular set function.\nLet us denote Π1 as the problem of maximizing an unconstrained non-monotone submodular set function, and Π2 as the problem of maximizing a box constrained non-monotone submodular continuous function. Following the Appendix A of Buchbinder et al. (2012), there exist an algorithm A for Π1 that consists of a polynomial time computation in addition to polynomial number of subroutine calls to an algorithm for Π2. For details see the following.\nGiven a submodular set function F : 2E → R+, its multilinear extension (Călinescu et al., 2007) is a function f : [0, 1]E → R+, whose value at a point x ∈ [0, 1]E is the expected value of F over a random subset R(x) ⊆ E, where R(x) contains each element e ∈ E independently with probability xe. Formally, f(x) := E[R(x)] = ∑ S⊆E F (S) ∏ e∈S xe ∏ e′ /∈S(1 − xe′). It can be easily seen that f(x) is a non-monotone submodular continuous function. Then the algorithm A can be: 1) Maximize the multilinear extension f(x) over the box constraint [0, 1]E , which can be achieved by solving an instance of Π2. Obtain the fractional solution x̂ ∈ [0, 1]n; 2) Return the random set R(x̂). According to the definition of multilinear extension, the expected value of F (R(x̂)) is f(x̂). Thus proving the reduction from Π1 to Π2.\nGiven the reduction, the hardness result follows from the hardness of unconstrained nonmonotone submodular set function maximization.\nThe inapproximability result comes from that of the unconstrained non-monotone submodular set function maximization in Feige et al. (2011) and Dobzinski and Vondrák (2012)."
    }, {
      "heading" : "C.2 Proof of Theorem 2",
      "text" : "To better illustrate the proof, we reformulate Algorithm 2 into its equivalent form in Algorithm 3, where we split the update into two steps: when δa ≥ δb, update x first while keeping y fixed and then update y first while keeping x fixed (xi ← (xi−1|xi−1ei ← ûa), y\ni ← yi−1; xi+1 ← xi, yi+1 ← (yi|yiei←ûa) ), when δa < δb, update y first. This iteration index change is only used to ease the analysis.\nTo prove the theorem, we first prove the following Lemmas. Lemma 4 is used to demonstrate that the objective value of each intermediate solution is non-\ndecreasing,\nLemma 4. ∀i = 1, 2, · · · , 2n, one has,\nf(xi) ≥ f(xi−1)− δ, f(yi) ≥ f(yi−1)− δ. (13)\nProof of Lemma 4. Let j := ei be the coordinate that is going to be changed. From submodularity,\nf(xi−1|xi−1j ←ūj) + f(y i−1|yi−1j ←uj) ≥ f(x i−1) + f(yi−1)\nSo one can verify that δa + δb ≥ −2δ. Let us consider the following two situations: 1) If δa ≥ δb, x is changed first. We can see that the Lemma holds for the first change (where xi−1 → xi,yi = yi−1). For the second change, we are left to prove f(yi+1) ≥ f(yi)− δ. From submodularity:\nf(yi−1|yi−1j ←ûa) + f(x i−1|xi−1j ←ūj) ≥ f(x i−1|xi−1j ←ûa) + f(y i−1) (14)\nAlgorithm 3: DoubleGreedy algorithm reformulation (for analysis only)\nInput: max f(x), x ∈ [u, ū], f is generally non-monotone, f(u) + f(ū) ≥ 0 1 x0 ← u, y0 ← ū; 2 for i = 1, 3, 5, · · · , 2n− 1 do 3 find ûa s.t. f(x\ni−1|xi−1ei ←ûa) ≥ maxua∈[uei ,ūei ] f(x i−1|xi−1ei ←ua)− δ,\nδa ← f(xi−1|xi−1ei ←ûa)− f(x i−1) ; //δ ∈ [0, δ̄] is the additive error level.\n4 find ûb s.t. f(y i−1|yi−1ei ←ûb) ≥ maxub∈[uei ,ūei ] f(y i−1|yi−1ei ←ub)− δ, δb ← f(yi−1|yi−1ei ←ûb)− f(y\ni−1) ; 5 if δa ≥ δb then 6 xi ← (xi−1|xi−1ei ←ûa), y\ni ← yi−1 ; 7 xi+1 ← xi, yi+1 ← (yi|yiei←ûa) ; 8 else 9 yi ← (yi−1|yi−1ei ←ûb), x\ni ← xi−1; 10 yi+1 ← yi, xi+1 ← (xi|xiei←ûb);\n11 Return x2n (or y2n) ; //note that x2n = y2n\nTherefore, f(yi+1)− f(yi) ≥ f(xi−1|xi−1j ←ûa)− f(xi−1|x i−1 j ←ūj) ≥ −δ, the last inequality comes from the selection rule of δa in the algorithm. 2) Otherwise, δa < δb, y is changed first. The Lemma holds for the first change (yi−1 → yi,xi = xi−1). For the second change, we are left to prove f(xi+1) ≥ f(xi)− δ. From submodularity,\nf(xi−1|xi−1j ←ûb) + f(y i−1|yi−1j ←uj) ≥ f(y i−1|yi−1j ←ûb) + f(x i−1), (15)\nSo f(xi+1) − f(xi) ≥ f(yi−1|yi−1j ←ûb) − f(yi−1|y i−1 j ←uj) ≥ −δ, the last inequality also comes from the selection rule of δb.\nLet OPT i := (x∗ ∨ xi) ∧ yi, it is easy to observe that OPT 0 = x∗ and OPT 2n = x2n = y2n.\nLemma 5. ∀i = 1, 2, · · · , 2n, it holds,\nf(OPT i−1)− f(OPT i) ≤ f(xi)− f(xi−1) + f(yi)− f(yi−1) + 2δ. (16)\nBefore proving Lemma 5, let us get some intuition about it. We can see that when changing i from 0 to 2n, the objective value changes from the optimal value f(x∗) to the value returned by the algorithm: f(x2n). Lemma 5 is then used to bound the objective loss from the assumed optimal objective in each iteration.\nProof. Let j := ei be the coordinate that will be changed. First of all, let us assume x is changed, y is kept unchanged (xi 6= xi−1,yi = yi−1), this could happen in four situations: 1.1) xij ≤ x∗j and δa ≥ δb; 1.2) xij ≤ x∗j and δa < δb; 2.1) xij > x∗j and δa ≥ δb; 2.2) xij > x∗j and δa < δb. Let us prove the four situations one by one.\nIf xij ≤ x∗j , the Lemma holds in the following two situations: 1.1) When δa ≥ δb, it happens in the first change: xij = ûa ≤ x∗j , so OPT i = OPT i−1; According to Lemma 4, δa+δb ≥ −2δ, so f(xi)−f(xi−1)+f(yi)−f(yi−1)+2δ ≥ 0, so the Lemma holds;\n1.2) When δa < δb, it happens in the second change: x i j = ûb ≤ x∗j , yij = y i−1 j = ûb, and since OPT i−1 = (x∗ ∨ xi−1) ∧ yi−1, so OPT i−1j = ûb and OPT ij = ûb, so one still has OPT i = OPT i−1. So it amouts to prove that δa + δb ≥ −2δ, which is true according to Lemma 4.\nElse if xij > x ∗ j , it holds that OPT i j = x i j , all other coordinates of OPT i−1 remain unchanged. The Lemma holds in the following two situations:\n2.1) When δa ≥ δb, it happens in the first change. One has OPT ij = xij = ûa, x i−1 j = uj , so\nOPT i−1j = x ∗ j . And x i j = ûa > x ∗ j , y i−1 j = ūj . From submodularity,\nf(OPT i) + f(yi−1|yi−1j ←x ∗ j ) ≥ f(OPT i−1) + f(yi−1|yi−1j ←ûa) (17)\nSuppose by virtue of contradiction that,\nf(OPT i−1)− f(OPT i) > f(xi)− f(xi−1) + 2δ (18)\nSumming Equation 17 and 18 we get:\n0 > f(xi)− f(xi−1) + δ + f(yi−1|yi−1j ←ûa)− f(y i−1|yi−1j ←x ∗ j ) + δ (19)\nBecause δa ≥ δb then from the selection rule of δb,\nδa = f(x i)− f(xi−1) ≥ δb ≥ f(yi−1|yi−1j ←c)− f(y i−1)− δ, ∀uj ≤ c ≤ ūj . (20)\nSetting c = x∗j and substitite (20) into (19), one can get,\n0 > f(yi−1|yi−1j ←ûa)− f(y i−1) + δ = f(yi+1)− f(yi) + δ, (21)\nwhich contradicts with Lemma 4. 2.2) When δa < δb, it happens in the second change. y i−1 j = ûb, x i j = ûb > x ∗ j , OPT i j = ûb, OPT i−1 j = x ∗ j . From submodularity,\nf(OPT i) + f(yi−1|yi−1j ←x ∗ j ) ≥ f(OPT i−1) + f(yi−1|yi−1j ←ûb) (22)\nSuppose by virtue of contradiction that,\nf(OPT i−1)− f(OPT i) > f(xi)− f(xi−1) + 2δ. (23)\nSumming Equations 22 and 23 we get:\n0 > f(xi)− f(xi−1) + δ + f(yi−1|yi−1j ←ûb)− f(y i−1|yi−1j ←x ∗ j ) + δ. (24)\nFrom Lemma 4 we have f(xi)− f(xi−1) + δ ≥ 0, so 0 > f(yi−1|yi−1j ←ûb)− f(yi−1|y i−1 j ←x∗j ) + δ, which contradicts with the selection rule of δb. The case when y is changed, x is kept unchanged is similar, the proof of which is omitted here.\nWith Lemma 5 at hand, one can prove Theorem 2: Taking a sum over i from 1 to 2n, one can get,\nf(OPT 0)− f(OPT 2n) ≤ f(x2n)− f(x0) + f(y2n)− f(y0) + 4nδ = f(x2n) + f(y2n)− (f(u) + f(ū)) + 4nδ ≤ f(x2n) + f(y2n) + 4nδ\nThen it is easy to see that f(x2n) = f(y2n) ≥ 13f(x ∗)− 4n3 δ."
    }, {
      "heading" : "D. Details of revenue maximization with continuous assignments",
      "text" : "D.1 More details about the model\nFrom the discussion in the main text, Rs(x i) should be some non-negative, non-decreasing, submodular function, we set Rs(x i) := √∑ t:xit 6=0 xitwst, where wst is the weight of edge connecting users s and t. The first part in R.H.S. of Equation 8 models the revenue from users who have not received free assignments, while the second and third parts model the revenue from users who have gotten the free assignments. We use wtt to denote the “self-activation rate” of user t: Given certain amount of free trail to user t, how probable is it that he/she will buy after the trial. The intuition of modeling the second part in R.H.S. of Equation 8 is: Given the users more free assignments, they are more likely to buy the product after using it. Therefore, we model the expected revenue in this part by φ(xit) = wttx i t; The intuition of modeling the third part in R.H.S. of Equation 8 is: Giving the users more free assignments, the revenue could decrease, since the users use the product for free for a longer period. As a simple example, the decrease in the revenue can be modeled as γ ∑\nt:xit 6=0 −xit."
    }, {
      "heading" : "D.2 Proof of Lemma 2",
      "text" : "Proof. First of all, we prove that g(x) := ∑\ns:xs=0 Rs(x) is a non-negative submodular function.\nIt is easy to see that g(x) is non-negative. To prove that g(x) is submodular, one just need,\ng(a) + g(b) ≥ g(a ∨ b) + g(a ∧ b), ∀a, b ∈ [0, ū]. (25)\nLet A := supp(a), B := supp(b), where supp(x) := {i|xi 6= 0} is the support of the vector x. First of all, because Rs(x) is non-decreasing, and b ≥ a ∧ b, a ≥ a ∧ b,∑\ns∈A\\B\nRs(b) + ∑\ns∈B\\A\nRs(a) ≥ ∑\ns∈A\\B\nRs(a ∧ b) + ∑\ns∈B\\A\nRs(a ∧ b). (26)\nBy submodularity of Rs(x), and summing over s ∈ E\\(A ∪B),∑ s∈E\\(A∪B) Rs(a) + ∑ s∈E\\(A∪B) Rs(b) ≥ ∑ s∈E\\(A∪B) Rs(a ∨ b) + ∑ s∈E\\(A∪B) Rs(a ∧ b). (27)\nSumming Equations 26 and 27 one can get∑ s∈E\\A Rs(a) + ∑ s∈E\\B Rs(b) ≥ ∑ s∈E\\(A∪B) Rs(a ∨ b) + ∑ s∈E\\(A∩B) Rs(a ∧ b)\nwhich is equivalent to Equation 25. Then we prove that h(x) := ∑ t:xt 6=0 R̄t(x) is submodular. Because R̄t(x) is non-increasing, and a ≤ a ∨ b, b ≤ a ∨ b,∑ t∈A\\B R̄t(a) + ∑ t∈B\\A R̄t(b) ≥ ∑ t∈A\\B R̄t(a ∨ b) + ∑ t∈B\\A R̄t(a ∨ b). (28)\nBy submodularity of R̄t(x), and summing over t ∈ A ∩B,∑ t∈A∩B R̄t(a) + ∑ t∈A∩B R̄t(b) ≥ ∑ t∈A∩B R̄t(a ∨ b) + ∑ t∈A∩B R̄t(a ∧ b). (29)\nSumming Equations 28, 29 we get,∑ t∈A R̄t(a) + ∑ t∈B R̄t(b) ≥ ∑ t∈A∪B R̄t(a ∨ b) + ∑ t∈A∩B R̄t(a ∧ b)\nwhich is equivalent to h(a)+h(b) ≥ h(a∨b)+h(a∧b), ∀a, b ∈ [0, ū], thus proving the submodularity of h(x).\nFinally, because f(x) is the sum of two submodular functions and one modular function, so it is submodular.\nD.3 Solving the 1-D subproblem when applying the DoubleGreedy algorithm\nSuppose we are varying xj ∈ [0, ūj ] to maximize f(x), notice that this 1-D subproblem is nonsmooth and discontinuous at point 0. First of all, let us leave xj = 0 out, one can see that f(x) is concave and smooth along χj when xj ∈ (0, ūj ],\n∂f(x)\n∂xj = α ∑ s 6=j:xs=0\nwsj 2 √∑ t:xt 6=0 xtwst − γ + βwjj\n∂2f(x)\n∂x2j = −1 4 α ∑ s 6=j:xs=0 w2sj(√∑ t:xt 6=0 xtwst\n)3 ≤ 0. Let f̄(z) be the univariate function when xj ∈ (0, ūj ], then we extend the domain of f̄(z) to be z ∈ [0, ūj ] as,\nf̄(z) = f̄(xj) := α ∑\ns 6=j:xs=0 Rs(x) + β ∑ t6=j:xt 6=0 φ(xt) + γ ∑ t6=j:xt 6=0 R̄t(x) + βφ(xj) + γR̄j(x).\nOne can see that f̄(z) is concave and smooth. Now to solve the 1-D subproblem, we can first of all solve the smooth concave 1-D maximization problem9: z∗ := arg maxz∈[0,ūj ]f̄(z), then compare f̄(z∗) with the function value at the discontinuous point 0: f(x|xj←0), and return the point with the larger function value."
    }, {
      "heading" : "E. More applications",
      "text" : "Maximum coverage. In the maximum coverage problem, there are n subsets C1, · · · , Cn from the ground set V . One subset Ci can be chosen with “confidence” level xi ∈ [0, 1], the set of covered elements when choosing subset Ci with confidence xi can be modeled with the following monotone normalized covering function: pi : R+ → 2V , i = 1, · · · , n. The target is to choose subsets from C1, · · · , Cn with confidence level to maximize the number of covered elements | ∪ni=1 pi(xi)|, at the same time respecting the budget constraint ∑ i cixi ≤ b (where ci is the cost of choosing subset Ci). This problem generalizes the classical maximum coverage problem. It is easy to see that the objective function is monotone submodular, and the constraint is a down-closed polytope.\nText summarization. Submodularity-based objective functions for text summarization perform well in practice (Lin and Bilmes, 2010). Let C to be the set of all concepts, and E to be the set of all sentences. As a typical example, the concept-based summarization aims to find a subset S of the sentences to maximize the total credit of concepts covered by S. Soma et al. (2014) discussed\n9. It can be efficienlty solved by various methods, e.g., the bisection method or Newton method.\nextending the submodular text summarization model to the one that incorporates “confidence” of a sentence, which has discrete value, and modeling the objective to be a monotone submodular lattice function. It is also natural to model the confidence level of sentence i to be a continuous value xi ∈ [0, 1]. Let us use pi(xi) to denote the set of covered concepts when selecting sentence i with confidence level xi, it can be a monotone covering function pi : R+ → 2C ,∀i ∈ E. Then the objective function of the extended model is f(x) = ∑ j∈∪ipi(xi) cj , where cj ∈ R+ is the credit of concept j. It can be verified that this objective is a monotone submodular continuous function."
    } ],
    "references" : [ {
      "title" : "sitions for learning latent variable models",
      "author" : [ "Francis Bach" ],
      "venue" : null,
      "citeRegEx" : "Bach.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bach.",
      "year" : 2014
    }, {
      "title" : "Deep submodular functions",
      "author" : [ "Jeffrey Bilmes", "Wenruo Bai" ],
      "venue" : "arXiv preprint arXiv:1701.08939,",
      "citeRegEx" : "2010",
      "shortCiteRegEx" : "2010",
      "year" : 2017
    }, {
      "title" : "approximation for unconstrained submodular maximization",
      "author" : [ "Gruia Călinescu", "Chandra Chekuri", "Martin Pál", "Jan Vondrák" ],
      "venue" : "In FOCS,",
      "citeRegEx" : "Călinescu et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Călinescu et al\\.",
      "year" : 2012
    }, {
      "title" : "Maximizing a monotone submodular",
      "author" : [ "Springer", "2007. Gruia Călinescu", "Chandra Chekuri", "Martin Pál", "Jan Vondrák" ],
      "venue" : null,
      "citeRegEx" : "Springer et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Springer et al\\.",
      "year" : 2007
    }, {
      "title" : "Designing smoothing functions for improved worst-case competitive ratio",
      "author" : [ "Reza Eghbali", "Maryam Fazel" ],
      "venue" : "forty-fourth annual ACM symposium on Theory of computing,",
      "citeRegEx" : "Eghbali and Fazel.,? \\Q2012\\E",
      "shortCiteRegEx" : "Eghbali and Fazel.",
      "year" : 2012
    }, {
      "title" : "A reduction for optimizing lattice submodular functions with diminishing",
      "author" : [ "Alina Ene", "Huy L Nguyen" ],
      "venue" : null,
      "citeRegEx" : "Ene and Nguyen.,? \\Q2016\\E",
      "shortCiteRegEx" : "Ene and Nguyen.",
      "year" : 2016
    }, {
      "title" : "Near-optimal map inference for determinantal point",
      "author" : [ "Jennifer Gillenwater", "Alex Kulesza", "Ben Taskar" ],
      "venue" : "Satoru Fujishige. Submodular functions and optimization,",
      "citeRegEx" : "Gillenwater et al\\.,? \\Q1956\\E",
      "shortCiteRegEx" : "Gillenwater et al\\.",
      "year" : 1956
    }, {
      "title" : "Adaptive submodularity: Theory and applications in active learning and stochastic optimization",
      "author" : [ "Daniel Golovin", "Andreas Krause" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "Golovin and Krause.,? \\Q2011\\E",
      "shortCiteRegEx" : "Golovin and Krause.",
      "year" : 2011
    }, {
      "title" : "Submodular function maximization on the bounded integer lattice",
      "author" : [ "Corinna Gottschalk", "Britta Peis" ],
      "venue" : "In Approximation and Online Algorithms,",
      "citeRegEx" : "Gottschalk and Peis.,? \\Q2015\\E",
      "shortCiteRegEx" : "Gottschalk and Peis.",
      "year" : 2015
    }, {
      "title" : "Optimal marketing strategies over social networks",
      "author" : [ "Jason Hartline", "Vahab Mirrokni", "Mukund Sundararajan" ],
      "venue" : "In Proceedings of the 17th international conference on World Wide Web,",
      "citeRegEx" : "Hartline et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Hartline et al\\.",
      "year" : 2008
    }, {
      "title" : "Lagrangian decomposition algorithm for allocating marketing channels",
      "author" : [ "Daisuke Hatano", "Takuro Fukunaga", "Takanori Maehara", "Ken-ichi Kawarabayashi" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "Hatano et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Hatano et al\\.",
      "year" : 2015
    }, {
      "title" : "On graduated optimization for stochastic non-convex problems",
      "author" : [ "Elad Hazan", "Kfir Y Levy", "Shai Shalev-Swartz" ],
      "venue" : "arXiv preprint arXiv:1503.03712,",
      "citeRegEx" : "Hazan et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Hazan et al\\.",
      "year" : 2015
    }, {
      "title" : "A combinatorial strongly polynomial algorithm for minimizing submodular functions",
      "author" : [ "Satoru Iwata", "Lisa Fleischer", "Satoru Fujishige" ],
      "venue" : "Journal of the ACM,",
      "citeRegEx" : "Iwata et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Iwata et al\\.",
      "year" : 2001
    }, {
      "title" : "Revisiting frank-wolfe: Projection-free sparse convex optimization",
      "author" : [ "Martin Jaggi" ],
      "venue" : "ICML",
      "citeRegEx" : "Jaggi.,? \\Q2013\\E",
      "shortCiteRegEx" : "Jaggi.",
      "year" : 2013
    }, {
      "title" : "Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods",
      "author" : [ "Majid Janzamin", "Hanie Sedghi", "Anima Anandkumar" ],
      "venue" : "CoRR abs/1506.08473,",
      "citeRegEx" : "Janzamin et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Janzamin et al\\.",
      "year" : 2015
    }, {
      "title" : "Exact solutions of some nonconvex quadratic optimization problems via sdp and socp relaxations",
      "author" : [ "Sunyoung Kim", "Masakazu Kojima" ],
      "venue" : "Computational Optimization and Applications,",
      "citeRegEx" : "Kim and Kojima.,? \\Q2003\\E",
      "shortCiteRegEx" : "Kim and Kojima.",
      "year" : 2003
    }, {
      "title" : "Submodularity on a tree: Unifying l-convex and bisubmodular functions",
      "author" : [ "Vladimir Kolmogorov" ],
      "venue" : "In Mathematical Foundations of Computer Science,",
      "citeRegEx" : "Kolmogorov.,? \\Q2011\\E",
      "shortCiteRegEx" : "Kolmogorov.",
      "year" : 2011
    }, {
      "title" : "Submodular dictionary selection for sparse representation",
      "author" : [ "Andreas Krause", "Volkan Cevher" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Krause and Cevher.,? \\Q2010\\E",
      "shortCiteRegEx" : "Krause and Cevher.",
      "year" : 2010
    }, {
      "title" : "Submodular function maximization",
      "author" : [ "Andreas Krause", "Daniel Golovin" ],
      "venue" : "Tractability: Practical Approaches to Hard Problems,",
      "citeRegEx" : "Krause and Golovin.,? \\Q2012\\E",
      "shortCiteRegEx" : "Krause and Golovin.",
      "year" : 2012
    }, {
      "title" : "Near-optimal nonmyopic value of information in graphical models",
      "author" : [ "Andreas Krause", "Carlos Guestrin" ],
      "venue" : "In UAI,",
      "citeRegEx" : "Krause and Guestrin.,? \\Q2005\\E",
      "shortCiteRegEx" : "Krause and Guestrin.",
      "year" : 2005
    }, {
      "title" : "Handbook of Monte Carlo Methods, volume 706",
      "author" : [ "Dirk P Kroese", "Thomas Taimre", "Zdravko I Botev" ],
      "venue" : null,
      "citeRegEx" : "Kroese et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Kroese et al\\.",
      "year" : 2013
    }, {
      "title" : "Cost-effective outbreak detection in networks",
      "author" : [ "Jure Leskovec", "Andreas Krause", "Carlos Guestrin", "Christos Faloutsos", "Jeanne VanBriesen", "Natalie Glance" ],
      "venue" : "In ACM SIGKDD international conference on Knowledge discovery and data mining,",
      "citeRegEx" : "Leskovec et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Leskovec et al\\.",
      "year" : 2007
    }, {
      "title" : "Accelerated proximal gradient methods for nonconvex programming",
      "author" : [ "Huan Li", "Zhouchen Lin" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Li and Lin.,? \\Q2015\\E",
      "shortCiteRegEx" : "Li and Lin.",
      "year" : 2015
    }, {
      "title" : "Multi-document summarization via budgeted maximization of submodular functions",
      "author" : [ "Hui Lin", "Jeff Bilmes" ],
      "venue" : "In Annual Conference of the North American Chapter of the Association for Computational Linguistics,",
      "citeRegEx" : "Lin and Bilmes.,? \\Q2010\\E",
      "shortCiteRegEx" : "Lin and Bilmes.",
      "year" : 2010
    }, {
      "title" : "A class of submodular functions for document summarization",
      "author" : [ "Hui Lin", "Jeff Bilmes" ],
      "venue" : "In HLT,",
      "citeRegEx" : "Lin and Bilmes.,? \\Q2011\\E",
      "shortCiteRegEx" : "Lin and Bilmes.",
      "year" : 2011
    }, {
      "title" : "Submodular functions and convexity",
      "author" : [ "László Lovász" ],
      "venue" : null,
      "citeRegEx" : "Lovász.,? \\Q1983\\E",
      "shortCiteRegEx" : "Lovász.",
      "year" : 1983
    }, {
      "title" : "Optimal budget allocation",
      "author" : [ "Tasuku Soma", "Naonori Kakimura", "Kazuhiro Inaba", "Ken-ichi Kawarabayashi" ],
      "venue" : null,
      "citeRegEx" : "Soma et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Soma et al\\.",
      "year" : 2015
    }, {
      "title" : "Călinescu et al. (2007) (called the pipage rounding)",
      "author" : [ "Ageev", "Sviridenko" ],
      "venue" : null,
      "citeRegEx" : "Ageev and Sviridenko,? \\Q2004\\E",
      "shortCiteRegEx" : "Ageev and Sviridenko",
      "year" : 2004
    }, {
      "title" : "As a typical example, the concept-based summarization aims to find a subset S of the sentences to maximize the total credit of concepts covered",
      "author" : [ "S. Soma" ],
      "venue" : null,
      "citeRegEx" : "Soma,? \\Q2014\\E",
      "shortCiteRegEx" : "Soma",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 19,
      "context" : "Optimizing submodular set functions has found numerous applications in machine learning, including variable selection (Krause and Guestrin, 2005), dictionary learning (Krause and Cevher, 2010; Das and Kempe, 2011), sparsity inducing regularizers (Bach, 2010), summarization (Lin and Bilmes, 2011; Mirzasoleiman et al.",
      "startOffset" : 118,
      "endOffset" : 145
    }, {
      "referenceID" : 17,
      "context" : "Optimizing submodular set functions has found numerous applications in machine learning, including variable selection (Krause and Guestrin, 2005), dictionary learning (Krause and Cevher, 2010; Das and Kempe, 2011), sparsity inducing regularizers (Bach, 2010), summarization (Lin and Bilmes, 2011; Mirzasoleiman et al.",
      "startOffset" : 167,
      "endOffset" : 213
    }, {
      "referenceID" : 24,
      "context" : "Optimizing submodular set functions has found numerous applications in machine learning, including variable selection (Krause and Guestrin, 2005), dictionary learning (Krause and Cevher, 2010; Das and Kempe, 2011), sparsity inducing regularizers (Bach, 2010), summarization (Lin and Bilmes, 2011; Mirzasoleiman et al., 2013) and variational inference (Djolonga and Krause, 2014).",
      "startOffset" : 274,
      "endOffset" : 324
    }, {
      "referenceID" : 12,
      "context" : "Submodular set functions can be efficiently minimized (Iwata et al., 2001), and there are strong guarantees for approximate maximization (Nemhauser et al.",
      "startOffset" : 54,
      "endOffset" : 74
    }, {
      "referenceID" : 18,
      "context" : ", 2001), and there are strong guarantees for approximate maximization (Nemhauser et al., 1978; Krause and Golovin, 2012).",
      "startOffset" : 70,
      "endOffset" : 120
    }, {
      "referenceID" : 0,
      "context" : "Optimizing submodular set functions has found numerous applications in machine learning, including variable selection (Krause and Guestrin, 2005), dictionary learning (Krause and Cevher, 2010; Das and Kempe, 2011), sparsity inducing regularizers (Bach, 2010), summarization (Lin and Bilmes, 2011; Mirzasoleiman et al., 2013) and variational inference (Djolonga and Krause, 2014). Submodular set functions can be efficiently minimized (Iwata et al., 2001), and there are strong guarantees for approximate maximization (Nemhauser et al., 1978; Krause and Golovin, 2012). Even though submodularity is most widely considered in the discrete realm, the notion can be generalized to arbitrary lattices (Fujishige, 2005). Recently, Bach (2015) showed how results from submodular set function minimization can be lifted to the continuous domain.",
      "startOffset" : 247,
      "endOffset" : 737
    }, {
      "referenceID" : 14,
      "context" : ", 2014) and training neural networks (Janzamin et al., 2015).",
      "startOffset" : 37,
      "endOffset" : 60
    }, {
      "referenceID" : 22,
      "context" : "A fundamental problem in non-convex optimization is to reach a stationary point assuming the smoothness of the objective (Sra, 2012; Li and Lin, 2015; Reddi et al., 2016; Allen-Zhu and Hazan, 2016).",
      "startOffset" : 121,
      "endOffset" : 197
    }, {
      "referenceID" : 4,
      "context" : "Golovin and Krause (2011) introduce the notion of adaptive submodularity to generalize submodular set functions to adaptive policies.",
      "startOffset" : 0,
      "endOffset" : 26
    }, {
      "referenceID" : 4,
      "context" : "Golovin and Krause (2011) introduce the notion of adaptive submodularity to generalize submodular set functions to adaptive policies. Kolmogorov (2011) studies tree-submodular functions and presents a polynomial algorithm for minimizing them.",
      "startOffset" : 0,
      "endOffset" : 152
    }, {
      "referenceID" : 4,
      "context" : "Golovin and Krause (2011) introduce the notion of adaptive submodularity to generalize submodular set functions to adaptive policies. Kolmogorov (2011) studies tree-submodular functions and presents a polynomial algorithm for minimizing them. For distributive lattices, it is well-known that the combinatorial polynomial-time algorithms for minimizing a submodular set function can be adopted to minimize a submodular function over a bounded integer lattice (Fujishige, 2005). Recently, maximizing a submodular function over integer lattices has attracted considerable attention. In particular, Soma et al. (2014) develop a (1− 1/e)-approximation algorithm for maximizing a monotone DR-submodular integerlattice function under a knapsack constraint.",
      "startOffset" : 0,
      "endOffset" : 614
    }, {
      "referenceID" : 4,
      "context" : "Golovin and Krause (2011) introduce the notion of adaptive submodularity to generalize submodular set functions to adaptive policies. Kolmogorov (2011) studies tree-submodular functions and presents a polynomial algorithm for minimizing them. For distributive lattices, it is well-known that the combinatorial polynomial-time algorithms for minimizing a submodular set function can be adopted to minimize a submodular function over a bounded integer lattice (Fujishige, 2005). Recently, maximizing a submodular function over integer lattices has attracted considerable attention. In particular, Soma et al. (2014) develop a (1− 1/e)-approximation algorithm for maximizing a monotone DR-submodular integerlattice function under a knapsack constraint. For non-monotone submodular functions over the bounded integer lattice, Gottschalk and Peis (2015) provide a 1/3-approximation.",
      "startOffset" : 0,
      "endOffset" : 849
    }, {
      "referenceID" : 4,
      "context" : "Golovin and Krause (2011) introduce the notion of adaptive submodularity to generalize submodular set functions to adaptive policies. Kolmogorov (2011) studies tree-submodular functions and presents a polynomial algorithm for minimizing them. For distributive lattices, it is well-known that the combinatorial polynomial-time algorithms for minimizing a submodular set function can be adopted to minimize a submodular function over a bounded integer lattice (Fujishige, 2005). Recently, maximizing a submodular function over integer lattices has attracted considerable attention. In particular, Soma et al. (2014) develop a (1− 1/e)-approximation algorithm for maximizing a monotone DR-submodular integerlattice function under a knapsack constraint. For non-monotone submodular functions over the bounded integer lattice, Gottschalk and Peis (2015) provide a 1/3-approximation. Approximation algorithms for maximizing bisubmodular functions and k-submodular functions have also been proposed by Singh et al. (2012); Ward and Zivny (2014).",
      "startOffset" : 0,
      "endOffset" : 1015
    }, {
      "referenceID" : 4,
      "context" : "Golovin and Krause (2011) introduce the notion of adaptive submodularity to generalize submodular set functions to adaptive policies. Kolmogorov (2011) studies tree-submodular functions and presents a polynomial algorithm for minimizing them. For distributive lattices, it is well-known that the combinatorial polynomial-time algorithms for minimizing a submodular set function can be adopted to minimize a submodular function over a bounded integer lattice (Fujishige, 2005). Recently, maximizing a submodular function over integer lattices has attracted considerable attention. In particular, Soma et al. (2014) develop a (1− 1/e)-approximation algorithm for maximizing a monotone DR-submodular integerlattice function under a knapsack constraint. For non-monotone submodular functions over the bounded integer lattice, Gottschalk and Peis (2015) provide a 1/3-approximation. Approximation algorithms for maximizing bisubmodular functions and k-submodular functions have also been proposed by Singh et al. (2012); Ward and Zivny (2014). Wolsey (1982) considers maximizing a special class of submodular continuous functions subject to one knapsack constraint, in the context of solving location problems.",
      "startOffset" : 0,
      "endOffset" : 1038
    }, {
      "referenceID" : 4,
      "context" : "Golovin and Krause (2011) introduce the notion of adaptive submodularity to generalize submodular set functions to adaptive policies. Kolmogorov (2011) studies tree-submodular functions and presents a polynomial algorithm for minimizing them. For distributive lattices, it is well-known that the combinatorial polynomial-time algorithms for minimizing a submodular set function can be adopted to minimize a submodular function over a bounded integer lattice (Fujishige, 2005). Recently, maximizing a submodular function over integer lattices has attracted considerable attention. In particular, Soma et al. (2014) develop a (1− 1/e)-approximation algorithm for maximizing a monotone DR-submodular integerlattice function under a knapsack constraint. For non-monotone submodular functions over the bounded integer lattice, Gottschalk and Peis (2015) provide a 1/3-approximation. Approximation algorithms for maximizing bisubmodular functions and k-submodular functions have also been proposed by Singh et al. (2012); Ward and Zivny (2014). Wolsey (1982) considers maximizing a special class of submodular continuous functions subject to one knapsack constraint, in the context of solving location problems.",
      "startOffset" : 0,
      "endOffset" : 1053
    }, {
      "referenceID" : 1,
      "context" : "Călinescu et al. (2007) and Vondrák (2008) discuss a subclass of submodular continuous functions, which is termed smooth submodular functions3, to describe the multilinear extension of a submodular set function.",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 1,
      "context" : "Călinescu et al. (2007) and Vondrák (2008) discuss a subclass of submodular continuous functions, which is termed smooth submodular functions3, to describe the multilinear extension of a submodular set function.",
      "startOffset" : 0,
      "endOffset" : 43
    }, {
      "referenceID" : 0,
      "context" : "Recently, Bach (2015) considers the minimization of a submodular continuous function, and proves that efficient techniques from convex optimization may be used for minimization.",
      "startOffset" : 10,
      "endOffset" : 22
    }, {
      "referenceID" : 0,
      "context" : "Recently, Bach (2015) considers the minimization of a submodular continuous function, and proves that efficient techniques from convex optimization may be used for minimization. Very recently, Ene and Nguyen (2016) provide a reduction from a integer-lattice DR-submodular function maximization problem to a submodular set function maximization problem, which suggests a way to optimize submodular continuous functions over simple continuous constriants: Discretize the continuous function and constraint to be an integer-lattice instance, and then optimize it using the reduction.",
      "startOffset" : 10,
      "endOffset" : 215
    }, {
      "referenceID" : 0,
      "context" : "Recently, Bach (2015) considers the minimization of a submodular continuous function, and proves that efficient techniques from convex optimization may be used for minimization. Very recently, Ene and Nguyen (2016) provide a reduction from a integer-lattice DR-submodular function maximization problem to a submodular set function maximization problem, which suggests a way to optimize submodular continuous functions over simple continuous constriants: Discretize the continuous function and constraint to be an integer-lattice instance, and then optimize it using the reduction. However, for monotone DR-submodular functions maximization, this method can not handle the general continuous constraints discussed in this work, i.e., arbitrary down-closed convex sets. And for general submodular function maximization, this method cannot be applied, since the reduction needs the additional diminishing returns property. Therefore we focus on continuous methods in this work. Non-convex optimization. Optimizing non-convex continuous functions has received renewed interest in the last decades. Recently, tensor methods have been used in various non-convex problems, e.g., learning latent variable models (Anandkumar et al., 2014) and training neural networks (Janzamin et al., 2015). A fundamental problem in non-convex optimization is to reach a stationary point assuming the smoothness of the objective (Sra, 2012; Li and Lin, 2015; Reddi et al., 2016; Allen-Zhu and Hazan, 2016). With extra assumptions, certain global convergence results can be obtained. For example, for functions with Lipschitz continuous Hessians, the regularized Newton scheme of Nesterov and Polyak (2006) achieves global convergence results for functions with 3.",
      "startOffset" : 10,
      "endOffset" : 1682
    }, {
      "referenceID" : 11,
      "context" : "Hazan et al. (2015) introduce the family of σ-nice functions and propose a graduated optimization-based algorithm, that provably converges to a global optimum for this family of (generally) non-convex functions.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 4,
      "context" : "Recently, the DR property is explored by Eghbali and Fazel (2016) to achieve the worst-case competitive ratio for an online concave maximization problem.",
      "startOffset" : 41,
      "endOffset" : 66
    }, {
      "referenceID" : 13,
      "context" : "The algorithm is a generalization of the continuous greedy algorithm of Vondrák (2008) for maximizing a smooth submodular function, and related to the convex Frank-Wolfe algorithm (Frank and Wolfe, 1956; Jaggi, 2013) for minimizing a convex function.",
      "startOffset" : 180,
      "endOffset" : 216
    }, {
      "referenceID" : 8,
      "context" : "(2012) and Gottschalk and Peis (2015), and can be viewed as a procedure performing coordinate-ascent on two solutions.",
      "startOffset" : 11,
      "endOffset" : 38
    }, {
      "referenceID" : 25,
      "context" : "The Lovász extension (Lovász, 1983) used for submodular set function minimization is both submodular and convex (see Appendix A in Bach (2015)).",
      "startOffset" : 21,
      "endOffset" : 35
    }, {
      "referenceID" : 10,
      "context" : ", time for a TV advertisement, or space of an inline ad) among the source nodes, and to maximize the expected influence on the potential customers (Soma et al., 2014; Hatano et al., 2015).",
      "startOffset" : 147,
      "endOffset" : 187
    }, {
      "referenceID" : 0,
      "context" : "The Lovász extension (Lovász, 1983) used for submodular set function minimization is both submodular and convex (see Appendix A in Bach (2015)).",
      "startOffset" : 131,
      "endOffset" : 143
    }, {
      "referenceID" : 0,
      "context" : "The Lovász extension (Lovász, 1983) used for submodular set function minimization is both submodular and convex (see Appendix A in Bach (2015)). Non-convex/non-concave quadratic programming (NQP). NQP problem of the form f(x) = 1 2x >Hx+h>x+c under linear constraints naturally arises in many applications, including scheduling (Skutella, 2001), inventory theory, and free boundary problems. A special class of NQP is the submodular NQP (the minimization of which was studied in Kim and Kojima (2003)), in which all off-diagonal entries of H are required to be non-positive.",
      "startOffset" : 131,
      "endOffset" : 501
    }, {
      "referenceID" : 9,
      "context" : "In viral marketing, sellers choose a small subset of buyers to give them some product for free, to trigger a cascade of further adoptions through “word-of-mouth” effects, in order to maximize the total revenue (Hartline et al., 2008).",
      "startOffset" : 210,
      "endOffset" : 233
    }, {
      "referenceID" : 9,
      "context" : "With β=γ=0, we recover the classical model of Hartline et al. (2008). For products with continuous assignments, usually the cost of the product does not increase with its amount, e.",
      "startOffset" : 46,
      "endOffset" : 69
    }, {
      "referenceID" : 21,
      "context" : "For cost-sensitive outbreak detection in sensor networks (Leskovec et al., 2007), one needs to place sensors in a subset of locations selected from all the possible locations E, to quickly detect a set of contamination events V , while respecting the cost constraints of the sensors.",
      "startOffset" : 57,
      "endOffset" : 80
    }, {
      "referenceID" : 21,
      "context" : "For cost-sensitive outbreak detection in sensor networks (Leskovec et al., 2007), one needs to place sensors in a subset of locations selected from all the possible locations E, to quickly detect a set of contamination events V , while respecting the cost constraints of the sensors. For each location e ∈ E and each event v ∈ V , a value t(e, v) is provided as the time it takes for the placed sensor in e to detect event v. Soma and Yoshida (2015a) considered the sensors with discrete energy levels.",
      "startOffset" : 58,
      "endOffset" : 451
    }, {
      "referenceID" : 23,
      "context" : "The maximum coverage problem and the problem of text summarization with submodular objectives are among the examples (Lin and Bilmes, 2010).",
      "startOffset" : 117,
      "endOffset" : 139
    }, {
      "referenceID" : 20,
      "context" : "Experimental results We compare the performance of our proposed algorithms, the Frank-Wolfe variant and DoubleGreedy, with the following baselines: a) Random: uniformly sample ks solutions from the constraint set using the hit-and-run sampler (Kroese et al., 2013), and select the best one.",
      "startOffset" : 243,
      "endOffset" : 264
    } ],
    "year" : 2017,
    "abstractText" : "Submodular continuous functions are a category of (generally) non-convex/non-concave functions with a wide spectrum of applications. We characterize these functions and demonstrate that they can be maximized efficiently with approximation guarantees. Specifically, i) We introduce the weak DR property that gives a unified characterization of submodularity for all set, integer-lattice and continuous functions; ii) for maximizing monotone DR-submodular continuous functions under general down-closed convex constraints, we propose a Frank-Wolfe variant with (1−1/e) approximation guarantee, and sub-linear convergence rate; iii) for maximizing general non-monotone submodular continuous functions subject to box constraints, we propose a DoubleGreedy algorithm with 1/3 approximation guarantee. Submodular continuous functions naturally find applications in various real-world settings, including influence and revenue maximization with continuous assignments, sensor energy management, multi-resolution data summarization, facility location, etc. Experimental results show that the proposed algorithms efficiently generate superior solutions compared to baseline algorithms. ∗Appears in the 20 International Conference on Artificial Intelligence and Statistics (AISTATS) 2017, Fort Lauderdale, Florida, USA. ar X iv :1 60 6. 05 61 5v 4 [ cs .L G ] 1 M ar 2 01 7",
    "creator" : "LaTeX with hyperref package"
  }
}