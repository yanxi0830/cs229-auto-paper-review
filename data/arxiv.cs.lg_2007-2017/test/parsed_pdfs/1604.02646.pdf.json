{
  "name" : "1604.02646.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Visualization Regularizers for Neural Network based Image Recognition",
    "authors" : [ "Biswajit Paria", "Anirban Santara" ],
    "emails" : [ "biswajitsc@iitkgp.ac.in", "santara@iitkgp.ac.in", "pabitra@cse.iitkgp.ernet.in" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Regularization is an important aspect of deep neural network training. It prevents over-fitting in the absence of sufficient data. Usually regularizers restrict the norms of the weight parameters. A commonly used regularizer the L2 norm regularizer, penalizes the sum of squares of the weight parameters. Other regularizers include soft-weight sharing, introduced by Nowlan and Hinton [1]. Erhan et. al. showed that layer-wise unsupervised pre-training acts as a regularizer for deep neural networks [2]. Dropout [3] is a more recent regularizer proposed for deep neural networks. There has also been recent works on adaptive dropout [4], which is an improvement over the original dropout. In this paper, we propose a novel regularizer which we call as the visualization regularizer (VR), based on the visual quality of features learned by the internal nodes.\nVision tasks benefit from features consisting of primitives recognized by mid-level vision systems. Deep neural networks have been known to learn hierarchical layers of feature representation [5, 6]. On observing the features learned by deep neural networks trained using back propagation, it is seen that in contrast to well defined mid-level features, the node features are often noisy. More meaningful features (smoother features for example) can be favorable to training the network. Our proposed regularizer imposes a constraint on the hidden nodes of a neural network to learn smoother features. Since, the definition of the regularizer depends on the visual property of smoothness, it is only pertinent to domains with a notion of spatial locality, such as images.\nWe show that the VR regularizer is a special case of Tikhonov regularization [7]. The Tikhonov matrix of the conventional L2 norm regularizer corresponds to an identity matrix multiplied by the regularization weight. Whereas, the Tikhonov matrix for the VR regularizer is more generalised but sparse.\nUsing our regularizer, we achieve better performance in terms of accuracy than the baselines when compared on benchmark datasets such as MNIST [8] and CIFAR-10 [9]. Moreover the computational complexity of the training algorithm is same as that of the unregularized training algorithm. Our experimental results show that the features features learned by the nodes are smoother than the features learned without the VR regularizer.\nar X\niv :1\n60 4.\n02 64\n6v 1\n[ cs\n.L G\n] 1\n0 A\npr 2\nThis paper is organized in five sections. Section II gives a brief introduction to the architecture of deep neural networks and the notation used. The notion of visualization of a node is formally defined in section III. The proposed VR regularizer and the training algorithm along with the complexity analysis are described in section IV. Section V establishes the relationship to Tikhonov regularization. Section VI presents all the experimental results and observations. Finally Section VII concludes with a summary of the achievements and scopes for future work."
    }, {
      "heading" : "2 Deep neural network architecture",
      "text" : "The investigations presented in this paper are based on a muti-class classification setting. The notation followed in this paper is as follows: x denotes the input, Wi and bi respectively denote the weights and biases corresponding of the layers, ui denotes the pre-activation of the layers, hi denotes the activation of layers and y denotes the output of the neural network. The subscripts i in the notation denotes the ith hidden layer. The following equations describe a deep neural network with l hidden layers.\nu1 = W1 · x + b1, (1) hi = g(ui), ∀1 ≤ i ≤ l, (2)\nui+1 = Wi+1 · hi + bi+1, ∀1 ≤ i ≤ l − 1, (3) y = softmax(Wl+1 · hl + bl+1), (4)\nwhere g, is the activation function, a monotonically increasing non-linear function such as sigmoid, tanh or the rectified linear unit.\nThe loss function used for training is the sum of the classification loss and the regularization term. For a neural network M and a dataset D, the loss function can be written as,\nL(M,D) = Lc(M,D) + λReg(M), (5)\nwhere Lc(M,D) denotes the classification loss between the output of the neural network and the true class labels, Reg(M) denotes the regularization term and λ denotes the regularizer weight. However, dropout cannot be included in the loss function. It is incorporated into the training algorithm."
    }, {
      "heading" : "3 The notion of visualization of a node",
      "text" : "Visualization of a node refers to the visualization of the features learned by the node. Visualization of the nodes of a neural network have been studied by Erhan et. al. [10]. They proposed the activation maximization algorithm for visualizing features learned by a node. Following the notion in [10] we define the visualization of a node as follows.\nVisualization of a node is defined to be the input pattern(s) which activate(s) the node maximally under the restriction of the L2 norm of the input to be equal to unity. The L2 norm of the input is restricted to unity to prevent the input from becoming unbounded. Formally, the visualization vis(n) of a node n is defined as,\nvis(n) = argmax ‖x‖2=1 n(x), (6)\nwhere n(x) denotes the activation of node n for input x. Depending on the non-linearity g used in equation 2, solution to the above equation can be unique or multiple. For example, there exists an unique solution for invertible functions like g = σ (sigmoid) or g = tanh. Whereas there can be multiple solutions for non-invertible functions such as g(·) = max(0, ·) also known as the rectified linear unit (ReLU). The visualization of an internal node of the neural network can be computed using gradient-ascent as described in [10] as activation maximization.\nObserve that the pre-activation of a node n in the first hidden layer for an input vector x is wᵀx, where w denotes the weights of the connections coming into the node n. The pre-activation is maximized when x is aligned in the same direction as w in the appropriate vector space. Since the activation function is an increasing function, maximization of the pre-activation maximizes the\nactivation too. Consequently, we get the following closed form as one of the possible solutions to equation 6 for nodes n in the first hidden layer.\nvis(n) = w\n‖w‖2 (7)\nNote that finding a closed form algebraic expression for the visualization of nodes in the higher hidden layers is difficult due to the non-linearity of the activation function.\nFigures 7a and 7c show example visualizations of internal nodes of neural networks trained on the MNIST dataset, without and with dropout respectively."
    }, {
      "heading" : "4 Proposed visualization based regularizer",
      "text" : "We first define the notion of smoothness of a visualization."
    }, {
      "heading" : "4.1 Smoothness of a visualization",
      "text" : "Intuitively, one can determine whether an image is smooth or noisy by looking at the gradients in the image. An image is smooth if it has small gradients. The gradient of an image can be computed by convoluting it with a high pass filter. Larger the pixel values in the convolution, the larger the gradients in the original image, and noisier the image. We utilize this intuition to give a formal definition of smoothness.\nConsider a convolution I ⊗ K of image I with kernel K, where K is a high pass filter like the laplacian kernel. We define the smoothness of an image to be the negative sum of squares of pixel values of I ⊗K. Equivalently, we can define the visualization loss VL of image I as,\nI ′ = I ⊗K, VL(I) = ∑ i,j (I ′ · I ′)ij , (8)\nwhere “·” denotes the element-wise product, also known as the Schur or Hadamard product. The visualization loss is the negative of the smoothness of an image. Lower the visualization loss, smoother is the image. Table 1 shows the visualization loss for some example visualizations."
    }, {
      "heading" : "4.2 Visualization loss as a regularizer",
      "text" : "Classification tasks using deep neural networks benefit from the high-level of abstractions achieved in the higher layers of the neural network. Deep neural networks are intended to utilize low-level\npixels to learn mid-level features and finally high-level features. We propose the visualization regularizer (VR) to constrain the nodes in the first hidden layer to learn features with qualities similar to mid-level visual features. This constraint is intended to facilitate the discovery of high-level abstractions more effectively.\nInformally, we define the VR regularizer as a regularizer to reduce the visualization loss of the nodes of the neural network. In other words the VR regularizer makes the nodes learn smooth or less noisy features.\nThe following sub-sections give a more detailed description of the VR regularizer:"
    }, {
      "heading" : "4.2.1 Regularizer expression",
      "text" : "Let U(M) denote the set of nodes of the first hidden layer of a neural network M and for a node n ∈ U(M), let wn denote the weights of the connections incoming into the node. From equation 7 we know that visualization of a node in the first hidden layer is proportional to the weights of the connections coming into the node. Hence the visualization loss of the node is proportional to the visualization loss of the weight vector coming into the node. Therefore we can use VL(wn) as a surrogate for the visualization loss of the visualization of the node n. The difficulty of computing an algebraic expression for the visualization of nodes in higher hidden layers, limits the usage of the surrogate to nodes in the first hidden layer only.\nWe define the visualization loss VL of a neural network M as,\nVL(M) = ∑\nn∈U(M)\nVL(wn). (9)\nThe network training loss function can thus be defined as,\nL(M,D) = Lc(M,D) + µVL(M) + λL ′ 2(M), (10)\nwhere L′2(M) denotes the L2 norm regularization term for all weights except the weights coming into the first hidden layer. The remaining notation are as described in section 2.\nNote that instead of defining the visualization of loss as the sum of squares as in equation 8, equivalently one can also define it as the mean of squares. The only change required is to divide the sum by the number of elements (a constant) over which the mean is taken. We denote this by VLm(I). Analogously, we can also define VLm(M) (see eqn. 9) as,\nVLm(M) = mean n∈U(M) VLm(wn). (11)"
    }, {
      "heading" : "4.2.2 Gradient of the visualization regularizer",
      "text" : "For the visualization loss to be used as a regularizer, its gradient must be computed with respect to the model parameters. We derive the expression of the derivative for a general kernel of size (2k + 1)× (2k + 1). For simplicity in computing the expression of the gradient, we index the elements of the kernel relative to the central element as shown in equation 12. The element at the center is indexed (0, 0). All other elements are indexed according to their position relative to the central element.\nK =  a−k,−k . . . a−k,0 . . . a−k,k ... . . . ... . . . ... a0,−k . . . a0,0 . . . a0,k\n... . . .\n... . . . ... ak,−k . . . ak,0 . . . ak,k\n (12)\nLet NK denote the set of indices of the kernel matrix.\nNK = {(i, j) | i, j ∈ {−k, . . . , k}} (13)\nConsider an image I with dimensions n,m. Let S(i, j), corresponding to the (i, j)th pixel of I , be defined as follows.\nS(i, j) = {(r, (p, q)) | (p, q) = (i, j) + r, r ∈ NK and 0 ≤ p < n, 0 ≤ q < m} (14)\nInformally, S(i, j) contains the set of valid indices (p, q), along with their position r relative to (i, j), that need to be considered while computing the convolution for the (i, j)th pixel.\nA full convolution of the image I can be described as\nI ′ = I ⊗K =  ∑ (r,(p,q))∈S(i,j) arIpq  ij . (15)\nIt follows from the definition that a pixel Ipq present at a position r, relative to (i, j), has the coefficient ar in I ′ij .\nThe visualization loss is VL(I) = ∑ i,j I ′ 2 ij . (16)\nThe partial derivative of the visualization loss with respect to a pixel Iij of the image I is\n∂\n∂Iij VL(I) = ∑ p,q ∂ ∂Iij I ′ 2 pq. (17)\nObserve that in the above equation Iij occurs in I ′pq only for (p, q) such that (r, (p, q)) ∈ S(i, j). Moreover, if (p, q) is present at position r relative to (i, j), then (i, j) is present at position −r relative to (p, q). It follows that Iij has the coefficient a−r in I ′pq . Hence the derivative can be computed as,\n∂\n∂Iij VL(I) = ∑ (r,(p,q))∈S(i,j) ∂ ∂Iij I ′ 2 pq\n= ∑\n(r,(p,q))∈S(i,j)\n2I ′pq ∂\n∂Iij I ′pq\n= ∑\n(r,(p,q))∈S(i,j)\n2I ′pqa−r.\nFurther, using equation 15, we can write,\n∂\n∂I VL(I) = 2(I ′ ⊗KI) = 2((I ⊗K)⊗KI), (18)\nwhereKI denotes the kernel matrix formed by flippingK both horizontally and vertically. Formally,\nKIij = K(−i,−j). (19)\nEquation 18 allows us to compute the gradient efficiently and in a scalable manner. All popular GPU programming frameworks provide libraries for scalable convolutions. Figure 1 illustrates the gradient computation for the visualization loss."
    }, {
      "heading" : "4.2.3 Regularized training algorithm",
      "text" : "Training requires computing the gradient for the regularized loss function with respect to the parameters of the network. As evident from equation 10, the gradient of the loss function can be computed by first computing the gradients of Lc(M,D), L′2(M) and VL(M), and then computing their sum. The gradients of Lc(M,D) and L′2(M) can be computed using back-propagation and partial derivatives respectively.\nBy equation 9, the gradient of VL(M) is the sum of gradients of VL(wn) for n ∈ U(M). Note that the gradient ∂∂w VL(wn) is zero if w /∈ wn. In other words, the gradient is zero if w does not belong to the set of weights incoming to node n. Hence we only need to compute the gradients ∂ ∂wn VL(wn) for all n ∈ U(M). These can be computed using equation 18. The full algorithm described in figure 2. The algorithm can be extended to using dropout and momentum.\nComputing each zn in figure 2 takes O(|wn|). Consequently, computing the gradient of the VR regularizer takes O(|w|) time, which is of the same order as computing the gradient of the L2 norm regularizer. Thus, the VR regularizer does not impose additional overhead in the computational complexity per iteration."
    }, {
      "heading" : "5 Relationship with Tikhonov regularization",
      "text" : "Tikhonov regularization was originally developed for solutions to ill-posed problems [7]. For example, L2 regularization, a special case of Tikhonov regularization is used to compute solutions to regression problems for which rank deficient matrices are encountered while computing their solutions. The solution to regularized least squares regression is given by,\nw = argmin w ‖Xw − y‖2 + λ‖Γw‖2, (20)\nwhere Γ is the Tikhonov matrix. The L2 regularizer corresponds to Γ = I .\nLet w be the concatenation of all the weights in wn for n ∈ U(M). From equation 16, we can see that VL(wn) is the sum of squared terms of the form w′ij 2 = ( ∑ t ctwt)\n2, where wt ∈ w, ct ∈ {−1, 0, 1} and w′ = wn ⊗K. For example w′ij = (wi −wj −wk)2 can be represented in the given form where ci = 1, cj = −1, ck = −1, and all other ct are zero. The Tikhonov matrix Γ can be constructed as follows. Consider the expression of VL(M) =∑\nn∈U(M) VL(wn) consisting of the sum of p such squared terms. Let the ith term in this expression be si = ( ∑ t citwt) 2. Then, Γ and consequently VL(M) in terms of Γ are, respectively,\nΓ = [cit]it , and VL(M) = ‖Γw‖ 2. (21)\nThe expression of si consists of only a constant number of non-zero cit. The maximum number of non-zero cit is bounded by the maximum number of neighbors a pixel can have, which is 8. Hence, the number of non-zero entries in Γ is O(p), whereas the total number of entries in Γ is p|w|. This establishes the sparsity of Γ."
    }, {
      "heading" : "6 Experiments and observations",
      "text" : "We experimented on the MNIST [8] and CIFAR-10 [9] datasets and compared the classification accuracy of our algorithm using the VR regularizer with other regularizers.1"
    }, {
      "heading" : "6.1 Experimental setting",
      "text" : "We used a neural network with two fully connected hidden layers for both the datasets. For the CIFAR-10 dataset, we additionally used a convolution and max pooling layer before the fully connected layers. For simplicity in searching for optimal parameters, we fixed the number of fully connected hidden layers to two, with the number of nodes in each hidden layer equal to 1000. We also fixed the number of feature maps of the convolution layer to 10, and the kernel size to 3× 3. We used the laplacian kernel for the VR regularizer defined as follows.\nK = [−1 −1 −1 −1 8 −1 −1 −1 −1 ] (22)\nWe used the mean cross-entropy loss over the mini-batch examples as the classification loss, mean of squares of the weight parameters as the L2 regularizer, and VLm(M) as the VR regularizer. We trained the neural network using stochastic gradient descent as described in figure 2, and also used Nesterov’s accelerated gradient (momentum) [11]. Note that the first fully connected layer is connected to the input for MNIST, whereas it is connected to the max-pooling layer in the case of CIFAR-10. Thus for the CIFAR-10 network, U(M) is defined as the set of nodes in the first fully connected layer. The VR regularizer utilizes this modified definition of U(M) in the case of CIFAR-10.\nWe fixed the number of training epochs to 200 and a mini-batch size of 100. For both MNIST and CIFAR-10, we reduced the learning rate by a factor of 10, respectively after every 100 and 70 epochs. We fixed the momentum parameter to 0.9 and dropout rate to 0.3 for both hidden layers."
    }, {
      "heading" : "6.2 Accuracy",
      "text" : "We compared the different neural networks with combinations of L2, VR, and dropout regularizers. For finding the optimal parameters, we performed a randomized hyper-parameter search with manual fine-tuning. The accuracy and optimal hyper-parameters for various regularizer settings are given in table 2. The parameters µ, λ and α respectively denote the VR regularizer weight, the L2 regularizer weight, and the learning rate.\nFrom the table it is observed that the VR regularizer is an improvement over the standard L2 regularizer. This is observed both for MNIST and CIFAR-10."
    }, {
      "heading" : "6.3 Convergence",
      "text" : "Figures 3 and 4 show the variation of the test set accuracy over the training epochs.\nFigures 5 and 6 show the variation of the regularization terms over the training epochs. Since loss values for different parameter settings can be of different orders of magnitude, for purposes of visualization, the loss values have been linearly transformed, so as to map to 1 after the first epoch and map to 0 after the last epoch."
    }, {
      "heading" : "6.4 Visualizations of hidden nodes",
      "text" : "Figure 7 shows the visualizations of the internal nodes of the MNIST classifier. The first two rows of the sub-figures correspond to nodes in the first hidden layer, and the last two rows correspond to nodes in the second hidden layer.\n1The experiment code is publicly available at https://github.com/biswajitsc/VisRegDL\nThe difference between the visualizations of networks trained using the VR regularizer and the ones not trained using it is apparent. The visualizations of architectures using VR regularizer are smoother and less noisy as was expected."
    }, {
      "heading" : "7 Conclusion and future work",
      "text" : "We observe that the VR regularizer is an improvement over the popular L2 regularizer in terms of classification accuracy. It is also observed that the networks trained using a VR regularizer have smoother visualizations. Though the VR regularizer imposes the smoothness constraint only on the first hidden layer nodes, our observations from figure 7 show that the smoothness constraint is propagated to the next layer.\nThe VR regularizer may be extended to a new class of training algorithms which use domain knowledge in the form of regularizers."
    } ],
    "references" : [ {
      "title" : "Simplifying neural networks by soft weight-sharing,",
      "author" : [ "S.J. Nowlan", "G.E. Hinton" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1992
    }, {
      "title" : "and S",
      "author" : [ "D. Erhan", "Y. Bengio", "A. Courville", "P.-A. Manzagol", "P. Vincent" ],
      "venue" : "Bengio, “Why does unsupervised pre-training help deep learning?,” The Journal of Machine Learning Research, vol. 11, pp. 625–660",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "and R",
      "author" : [ "N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever" ],
      "venue" : "Salakhutdinov, “Dropout: A simple way to prevent neural networks from overfitting,” The Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929–1958",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Adaptive dropout for training deep neural networks,",
      "author" : [ "J. Ba", "B. Frey" ],
      "venue" : "Advances in Neural Information Processing Systems,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2013
    }, {
      "title" : "and G",
      "author" : [ "Y. LeCun", "Y. Bengio" ],
      "venue" : "Hinton, “Deep learning,” Nature, vol. 521, no. 7553, pp. 436–444",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Learning deep architectures for ai,",
      "author" : [ "Y. Bengio" ],
      "venue" : "Foundations and trends R  © in Machine Learning,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2009
    }, {
      "title" : "Solutions of ill-posed problems",
      "author" : [ "A. Tikhonov", "V. Arsenin" ],
      "venue" : "VH Winston and Sons",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 1977
    }, {
      "title" : "The MNIST database of handwritten digits,",
      "author" : [ "Y. LeCun", "C. Cortes" ],
      "venue" : null,
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1998
    }, {
      "title" : "Learning multiple layers of features from tiny images,",
      "author" : [ "A. Krizhevsky" ],
      "venue" : "Technical Report, Univ. Toronto,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2009
    }, {
      "title" : "and P",
      "author" : [ "D. Erhan", "Y. Bengio", "A. Courville" ],
      "venue" : "Vincent, “Visualizing higher-layer features of a deep network,” Technical Report, Univ. Montreal",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "A method of solving a convex programming problem with convergence rate {O}(1/kˆ2),",
      "author" : [ "Y. Nesterov" ],
      "venue" : "Soviet Mathematics Doklady,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1983
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Other regularizers include soft-weight sharing, introduced by Nowlan and Hinton [1].",
      "startOffset" : 80,
      "endOffset" : 83
    }, {
      "referenceID" : 1,
      "context" : "showed that layer-wise unsupervised pre-training acts as a regularizer for deep neural networks [2].",
      "startOffset" : 96,
      "endOffset" : 99
    }, {
      "referenceID" : 2,
      "context" : "Dropout [3] is a more recent regularizer proposed for deep neural networks.",
      "startOffset" : 8,
      "endOffset" : 11
    }, {
      "referenceID" : 3,
      "context" : "There has also been recent works on adaptive dropout [4], which is an improvement over the original dropout.",
      "startOffset" : 53,
      "endOffset" : 56
    }, {
      "referenceID" : 4,
      "context" : "Deep neural networks have been known to learn hierarchical layers of feature representation [5, 6].",
      "startOffset" : 92,
      "endOffset" : 98
    }, {
      "referenceID" : 5,
      "context" : "Deep neural networks have been known to learn hierarchical layers of feature representation [5, 6].",
      "startOffset" : 92,
      "endOffset" : 98
    }, {
      "referenceID" : 6,
      "context" : "We show that the VR regularizer is a special case of Tikhonov regularization [7].",
      "startOffset" : 77,
      "endOffset" : 80
    }, {
      "referenceID" : 7,
      "context" : "Using our regularizer, we achieve better performance in terms of accuracy than the baselines when compared on benchmark datasets such as MNIST [8] and CIFAR-10 [9].",
      "startOffset" : 143,
      "endOffset" : 146
    }, {
      "referenceID" : 8,
      "context" : "Using our regularizer, we achieve better performance in terms of accuracy than the baselines when compared on benchmark datasets such as MNIST [8] and CIFAR-10 [9].",
      "startOffset" : 160,
      "endOffset" : 163
    }, {
      "referenceID" : 9,
      "context" : "[10].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 9,
      "context" : "Following the notion in [10] we define the visualization of a node as follows.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 9,
      "context" : "The visualization of an internal node of the neural network can be computed using gradient-ascent as described in [10] as activation maximization.",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 6,
      "context" : "Tikhonov regularization was originally developed for solutions to ill-posed problems [7].",
      "startOffset" : 85,
      "endOffset" : 88
    }, {
      "referenceID" : 7,
      "context" : "We experimented on the MNIST [8] and CIFAR-10 [9] datasets and compared the classification accuracy of our algorithm using the VR regularizer with other regularizers.",
      "startOffset" : 29,
      "endOffset" : 32
    }, {
      "referenceID" : 8,
      "context" : "We experimented on the MNIST [8] and CIFAR-10 [9] datasets and compared the classification accuracy of our algorithm using the VR regularizer with other regularizers.",
      "startOffset" : 46,
      "endOffset" : 49
    }, {
      "referenceID" : 10,
      "context" : "We trained the neural network using stochastic gradient descent as described in figure 2, and also used Nesterov’s accelerated gradient (momentum) [11].",
      "startOffset" : 147,
      "endOffset" : 151
    } ],
    "year" : 2017,
    "abstractText" : "The success of deep neural networks is mostly due their ability to learn meaningful features from the data. Features learned in the hidden layers of deep neural networks trained in computer vision tasks have been shown to be similar to midlevel vision features. We leverage this fact in this work and propose the visualization regularizer for image tasks. The proposed regularization technique enforces smoothness of the features learned by hidden nodes and turns out to be a special case of Tikhonov regularization. We achieve higher classification accuracy as compared to existing regularizers such as the L2 norm regularizer and dropout, on benchmark datasets with no change in the training computational complexity.",
    "creator" : "LaTeX with hyperref package"
  }
}