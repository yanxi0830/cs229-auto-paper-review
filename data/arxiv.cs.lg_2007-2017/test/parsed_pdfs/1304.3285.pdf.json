{
  "name" : "1304.3285.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Scaling the Indian Buffet Process via Submodular Maximization",
    "authors" : [ "Zoubin Ghahramani" ],
    "emails" : [ "cr478@cam.ac.uk", "zoubin@eng.cam.ac.uk" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Nonparametric latent feature models experienced a surge of interest in the machine learning community following Griffiths & Ghahramani (2006)’s formulation of the Indian Buffet Process (IBP)—a nonparametric prior for equivalence classes of sparse binary matrices. These binary matrices have a finite number of exchangeable rows and an unbounded number of columns, where a 1 in row n and column k indicates that observation n expresses latent feature k. For example, given an image dataset of human faces, each observation is an image, and the latent features might be “is smiling,” “is wearing glasses,” etc. More generally, feature models can be viewed as a generalization of unsupervised clustering, see Broderick et al. (2012).\nProceedings of the 30 th International Conference on Machine Learning, Atlanta, Georgia, USA, 2013. JMLR: W&CP volume 28. Copyright 2013 by the author(s).\nThe IBP prior is often used in sparse matrix factorization models where a data matrix of N D-dimensional observations is expressed as a product of two matrices that factor over K latent factors plus a noise term: X = ZA+E. Formally, this model has a binary feature matrix Z ∈ {0, 1}N×K that linearly combines a latent factor matrix A ∈ RK×D plus a noise matrix E ∈ RN×D to form the observed data X ∈ RN×D. Placing an IBP prior on Z lets K be unbounded and allows the number of active features K+ (those with non-zero Z column sums) to be learned from the data while remaining finite with probability one. The IBP inspired several infinite-limit versions of classic matrix factorization models, e.g. infinite independent component analysis models (Knowles & Ghahramani, 2007).\nInference with IBP models is challenging as its discrete state space has 2NK+ possible assignments. In turn, the IBP has found limited application to large data in comparison to the Chinese Restaurant Process, which assigns one feature to each observation. In this paper, we use Kurihara & Welling (2008)’s MaximizationExpectation (ME) framework to perform approximate MAP inference with IBP matrix factorization models, termed MEIBP inference. For nonnegativeA, we show that we can obtain approximate MAP solutions for Z by maximizingN submodular cost functions. The submodularity property enables the use of a simple greedy algorithm that obtains at least a 13 -approximation to the optimal solution. While the worst-case complexity of MEIBP inference is comparable to sampling and variational approaches, in §5 we show that MEIBP inference often converges to better solutions than variational methods and similar solutions as the best sampling techniques but in a fraction of the time.\nThis paper is structured as follows: in §2 we present background material that sets the foundation for our presentation of MEIBP inference in §3 and the resulting submodular maximization problem that arises, then in §4 we discuss related work, and in §5 we compare the MEIBP with other IBP inference techniques using both synthetic and real-world datasets.\nar X\niv :1\n30 4.\n32 85\nv4 [\nst at\n.M L\n] 2\n4 Ju\nl 2 01\n3"
    }, {
      "heading" : "2. Background",
      "text" : ""
    }, {
      "heading" : "2.1. The Indian Buffet Process",
      "text" : "Griffiths & Ghahramani (2006) derived the IBP prior by placing independent beta priors on Bernoulli generated entries of an N×K binary matrix Z, marginalizing over the beta priors, and letting K go to inifinity. In this infinite limit, however, P (Z) is zero for any particular Z. Griffiths & Ghahramani (2006) therefore take the limit of an equivalence classes of binary matrices, [Z], defined by the “left-order form” (lof ) ordering of the columns and show that P ([Z]lof) has a non-zero probability as K goes to infinity.\nThe lof ordering arranges the columns of Z such that the binary values of the columns are non-increasing, where the first row is the most significant bit. Ding et al. (2010) examine different “shifted” equivalence classes formed by shifting all-zero columns to the right of non-zero columns while maintaining the non-zero column ordering. Given K+ non-zero columns, the IBP prior for the shifted equivalence classes is\nP ([Z]|α) = α K+\nK+! e−αHN K+∏ k=1 (N −mk)!(mk − 1)! N ! (1)\nwhere α is a hyperparameter, HN is the N th harmonic number, and mk = ∑N n=1 znk. The supplementary material has a derivation of P ([Z]|α) as well as a comparison to the lof equivalence classes. The derivations in §3 can be applied using either equivalence class. However, the shifted equivalence classes simplify the mathematics and produce the same results in practice."
    }, {
      "heading" : "2.2. Maximization-Expectation",
      "text" : "Kurihara & Welling (2008) presented the ME algorithm: an inference algorithm that exchanges the expectation and maximization variables in the EM algorithm. Consider a general probabilistic model p(X,Z,A), where X are the observed random variables (RVs), Z are the local latent RVs, and A are the global latent RVs. RVs are qualified as “local” if there is one RV for each observation, and RVs are “global” if their multiplicity is constant or inferred from the data.\nME can be viewed as a special case of a Mean-Field Variational Bayes (MFVB) approximation to a posterior that cannot be computed analytically, p(Z,A|X). MFVB operates by approximating the posterior distribution of a given probabilistic model by assuming independent variational distributions, p(Z,A|X) ≈ q(Z)q(A) (Attias, 2000; Ghahramani & Beal, 2001). The independence constraint lets us compute the variational distribution q that minimizes the KL diver-\ngence between the variational distribution and true posterior. Without this constraint, the distribution that minimizes the KL-divergence is the true posterior, returning us to our original problem. In MFVB, we determine the variational distributions and their parameters using coordinate ascent optimization in which we iteratively update:\nq(Z) ∝ exp E q(A) [ln p(X,Z,A)] (2) q(A) ∝ exp E q(Z) [ln p(X,Z,A)] , (3)\nwhich commonly has closed-form solutions.\nThe EM algorithm can be viewed as a special case of MFVB that obtains MAP values of the global RVs by letting q(A) = δ(A −A∗), where δ(·) is the delta function and A∗ is the MAP assignment. The ME algorithm instead maximizes the local RVs Z and computes the expectation over the global RVs A, which can be viewed as MFVB with q(Z) = δ(Z − Z∗). In the limit of large N , the ME algorithm recovers a Bayesian information criterion regularization term (Kurihara & Welling, 2008). Also, maintaining a variational distribution over the global RVs retains the model selection ability of MFVB, while using point estimates of the local RVs allows the use of efficient data structures and optimization techniques. As we will show, the ME algorithm leads to a scalable submodular optimization problem for latent feature models."
    }, {
      "heading" : "2.3. Submodularity",
      "text" : "Submodularity is a set function property that makes optimization of the function tractable or approximable. Given ground set V and set function f : 2V → R, f is submodular if for all A ⊆ B ⊆ V and e ∈ V \\B:\nf(A ∪ {e})− f(A) ≥ f(B ∪ {e})− f(B), (4) which expresses a “diminishing returns” property, where the incremental benefit of element e diminishes as we include it in larger solution sets. Submodularity is desirable in discrete optimization because submodular functions are discrete analogs of convex functions and can be globally minimized in polynomial time (Lovász, 1983). However, global submodular maximization is NP-hard, but submodularity often enables approximation bounds via greedy algorithms. In the next section, we show that determining a MAP estimate of Z in the ME algorithm is a scalable submodular maximization problem."
    }, {
      "heading" : "3. Maximization-Expectation IBP",
      "text" : "Here we present the ME algorithm for nonnegative linear-Gaussian IBP models and show that approxi-\nmate MAP inference arises as a submodular maximization problem. Boldface variables are matrices with (row, column) subscripts; a dot indicates all elements of the dimension, and lowercase variables are scalars."
    }, {
      "heading" : "3.1. Nonnegative Linear-Gaussian IBP Model",
      "text" : "We consider the following probabilistic model:\np(X,Z,A|θ) = p(X|Z,A, σ2X)p(A|σ2A)p(Z|α) (5)\np(X|Z,A, σ2A) = N∏ n=1 N (Xn·;Zi·A, σ2AI) (6)\np(A|0, σ2A) = K∏ k=1 D∏ d=1 TN (akd; 0, σ2A) (7)\nwith p([Z]|α) specified in Eq. 1. This is a nonnegative linear-Gaussian IBP model, where the prior over the latent factors, p(A|0, σ2A), is a zero-mean i.i.d. truncated Gaussian with nonnegative support, denoted TN . As we show below, this nonnegative prior yields a submodular maximization problem when optimizing Z. We use a truncated Gaussian as it is conjugate to the Gaussian likelihood, but other nonnegative priors (e.g. exponential) can be used. For brevity we assume the hyperparameters, θ = {α, σ2A, σ2X}, are known and discuss θ inference in the supplementary material."
    }, {
      "heading" : "3.2. MEIBP Evidence",
      "text" : "In the ME framework, we approximate the true posterior distribution via a MFVB assumption:\np(Z,A|X,θ) ≈ q(A)δ(Z −Z∗). (8) That is, we maintain a variational distribution over the latent factors A and optimize the latent features Z. Given the MFVB constraint, we determine the variational distributions by minimizing the KL-divergence between the variational distributions and the true posterior, which is equivalent to maximizing a lower bound on the evidence (Attias, 2000):\nln p(X|θ) = E q [ln p(X,A,Z|θ)] +H[q] +D(q‖p)\n≥ E q [ln p(X,A,Z|θ)] +H[q] ≡ F (9)\nwhere H[q] is the entropy of q and D(q‖p) represents the KL-divergence between the variational distribution and the true posterior. The evidence lower bound, F , for the nonnegative linear-Gaussian IBP model is:\n1\nσ2X N∑ n=1 [ −1 2 Zn·ΦΦ TZTn· +Zn·ξ T n· ] − lnK+!\n+ K+∑ k=1 [ ln (N −mk)!(mk − 1)! N ! + ηk ] + const (10)\nwith\nξnk = Φk·X T n· +\n1\n2 D∑ d=1 [ E[akd]2 − E[a2kd] ] (11)\nand\nηk = D∑ d=1 [ − ln πσ2A 2α2/D 2 − E[a 2 kd] 2σ2A +H(q(akd)) ] (12)\nwhere Φk· = (E [ak1] , . . . ,E [akD]), and all expectations are with respect to q(A), which is defined in the next subsection. In §3.5 we show that maximizing this lower bound with respect to Z can be formulated as a submodular maximization problem."
    }, {
      "heading" : "3.3. Variational Factor Updates",
      "text" : "Maximizing Eq. 10 with respect to q(A) yields\nq(A) = K∏ k=1 D∏ d=1 TN (akd; µ̃kd, σ̃2kd), (13)\nwith parameter updates\nµ̃kd = ρk N∑ n=1 znk ( xnd − ∑ k′ 6=k znk′ E [ak′d] )\n(14)\nσ̃2kd = ρkσ 2 X , (15)\nwhere ρk = ( mk +\nσ2X σ2A\n)−1 . These updates take\nO(NK2D), and the relevant moments are:\nE [akd] = µ̃kd + σ̃kd\n√ 2/π\nerfcx (℘kd) (16)\nE [ a2kd ] = µ̃2kd + σ̃ 2 kd + σ̃kdµ̃kd\n√ 2/π\nerfcx (℘kd) (17)\nwith ℘kd = − µ̃kdσ̃kd√2 and erfcx (y) = e y2(1 − erf(y)) representing the scaled complementary error function."
    }, {
      "heading" : "3.4. Evidence Lower Bound as K →∞",
      "text" : "Here we show that the evidence lower bound [Eq. 10] is well-defined in the limit K →∞; in fact, all instances of K are simply replaced by K+. Therefore, similar to variational IBP methods, a user must specify a maximum model complexity K+. A benefit over variational IBP methods, however, is that the q(Z) updates are not affected by inactive features—see §4. We take this limit by breaking the evidence into components 1, . . . ,K+ and K+ + 1, . . . ,K and note that\nwhen mk = 0: µ̃kd = 0, σ̃ 2 kd = σ 2 A, and H(akd) = 1 2 ln πeσ2A 2 . After some algebra, the evidence becomes:\nψK+ + 1\n2 K∑ k=K++1 D∑ d=1 [ − ln πσ 2 A 2 − E[a 2 kd] σ2A + ln πeσ2A 2 ] (18)\nwhere ψK+ is Eq. 10 but with K+ replacing all K. From Eq. 25, we see that E[a2kd] = σ2A when mk = 0, which causes all terms to cancel in Eq. 18 except ψK+ .\nThe evidence lower bound remains well-defined because both the likelihood and IBP prior terms do not depend on inactive features, so for inactive features the KL-divergence between the posterior and variational distributions is simply the KL-divergence between p(A) and q(A). For inactive features, p(A) = q(A), and as a result, the KL-divergence is zero."
    }, {
      "heading" : "3.5. Z Objective Function",
      "text" : "Given q(A), we compute MAP estimates of Z by maximizing the evidence [Eq. 10] for each n ∈ {1, . . . , N} while holding constant all n′ ∈ {1, . . . , N}\\n. Decomposing Eq. 10 into terms that depend on Zn· and those that do not yields (see the supplementary material):\nF(Zn·) =− 1\n2σ2X Zn·ΦΦ\nTZTn· +Zn·ω T n· + const\n− ln ( K+\\n + K+∑ k=1 [ 1{mk\\n=0}znk ]) ! (19)\nΦk· = ( E [ak1] , . . . ,E [akD] ) ωnk = 1\nσ2X\n( Φk·X T n· + 1\n2 D∑ d=1 [ E[akd]2 − E[a2kd] ]) + ν(znk = 1)− ν(znk = 0) + 1{mk\\n=0}ηk,\nwhich is a quadratic pseudo-Boolean function plus a term that penalizes K+, where 1{·} is the indicator function, a “\\n” subscript indicates the given variable is determined after removing the nth row from Z, and\nν(znk) =  0, if mk\\n = 0 and znk = 0\nln (N −mk\\n − znk)!/N ! + ln (mk\\n + znk − 1)!, otherwise\nWe can prove F(Zn·) is submodular given the following two well-known propositions, see Fujishige (2005):\nProposition 1. Nonnegative linear combinations of submodular functions are submodular.\nProposition 2. A quadratic pseudo-Boolean function with quadratic weight matrix W is submodular if and only if Wij ≤ 0 for all i, j.\nVia Proposition 2, we see that − 1 2σ2X Zn·ΦΦTZTn· + Zn·ωTn· is submodular when Φ is nonnegative. From Proposition 1, Eq. 19 is submodular if and only if\nG(Zn·) = − ln ( K+\\n + K+∑ k=1 [ 1{mk\\n=0}znk ]) ! (20)\nis submodular. We prove this property by rephrasing G(Zn·) as a set function and using the definition of submodularity given by Eq. 4. Let An ⊆ Bn ⊆ V where V = {1, . . . ,K+} and An, Bn ∈ 2V with G(An) = − ln (K+\\n +KAn)!. Here we let KAn = ∑K+ k=1 1{mk\\n=0}1{k∈An} where k ∈ An indicates znk = 1. G is submodular if for all e ∈ V \\Bn:\nG(An ∪ {e})− G(An) ≥ G(Bn ∪ {e})− G(Bn)\nln\n( K+\\n +KAn ) !(\nK+\\n +KAn∪{e} ) ! ≥ ln\n( K+\\n +KBn ) !(\nK+\\n +KBn∪{e} ) !\n(21)\nEq. 21 has two cases: (1) me\\n > 0 so KBn∪{e} = KBn and KAn∪{e} = KAn , yielding 0 ≥ 0 for Eq. 21, which is true for all e ∈ V \\ Bn and An ⊆ Bn, (2) me\\n = 0 so KBn∪{e} = KBn + 1 and KAn∪{e} = KAn + 1. After some algebra this yields KBn∪{e} ≥ KAn∪{e} for Eq. 21, which is again true for all e ∈ V \\ Bn and An ⊆ Bn. As a result, both components of Eq. 19 are submodular, and by Proposition 1, adding these terms yields a submodular function."
    }, {
      "heading" : "3.6. Z Optimization",
      "text" : "Eq. 19 is an unconstrained nonmonotone submodular function. Feige et al. (2011) prove that an approximibility guarantee is NP-hard for this class of functions. However, Feige et al. (2011) also show that a local-search (ls) algorithm obtains a constant-factor approximation to the optimal solution, provided the submodular objective function is nonnegative. For a submodular function F : 2V → R with ground set V = {1, . . . ,K+} and solution set A ⊆ V , the lsalgorithm operates as follows:\n1. initialize: let A = {arg maxw∈V F({w})} 2. grow : while there is an element w ∈ V \\ A s.t. F(A ∪ {w}) > (1 + |V |2 )F(A): let A := A ∪ {w}\n3. prune: if there is an element w ∈ A s.t. F(A \\ {w}) > (1 + |V |2 )F(A): let A := A \\ {w}, goto 2.\n4. return: maximum of F(A) and F(V \\A).\nThe ls-algorithm obtains a solution that is greater than 1 3 (1 − |V | )OPT—where is a parameter and OPT is\nthe maximum value of F . The ls-algorithm performs O( 1 |V |3 log |V |) function calls in the grow/prune steps.\nSince Eq. 19 is not strictly nonnegative, we use its normalized cost function to interpret the lsapproximability guarantee: F(Zn·)− Fn0, where Fn0 is the minimum value of F(Zn·). Using the normalized cost function, we obtain the following optimality guarantee:\nF(Z lsn·) ≥ Fn0 + 1\n3 ( 1− |V | ) (F(Z∗n·)−Fn0) (22)\nwhere the superscript “ls” denotes the solution from the greedy ls-algorithm and an asterisk denotes the set that obtains the true maximum. This inequality states that the ls-algorithm solution is guaranteed to perform better than the minimum by an amount proportional to the difference between the optimum and the minimum. However, we emphasize that this inequality does not provide an optimality guarantee for the global MAP solution.\nWe studied the empirical performance of the lsalgorithm by generating high noise data (σX = 1) from the nonnegative linear-Gaussian model with N = 500, D = 50 and compared the ls-algorithm with the brute-force optimal solution as K varied from 2 to 12, performing 1000K total optimizations for each of ten randomly generated datasets. Furthermore, we compared the ls-algorithm with randomly sampled Zn· solutions to demonstrate that the optimization space was not skewed to favor solutions near the optimal value.\nFigure 1 shows the fraction of solutions that obtain the true optimum as well as the fraction of solutions that were greater than 95% of F(Z∗n·) − Fn0, where the error bars indicate the combined standard deviation over the 10 × 1000K optimizations. The ls-algorithm found the optimal solution roughly 70% of the time\nfor K = 12 and obtained within 95% of the optimal solution over 99.9% of the time for all K—meaning we could empirically replace the 13 in Eq. 22 with 19 20 . The random sampling comparison indicated that the optimization space did not favor nearly-optimal solutions: its convergence to 5% for within-95% optimal solutions was characteristic of a uniform solution space.\nBy precomputing ΦΦT and maintaining an auxiliary vector of K+ weights, we can evaluate Eq. 19 in constant time when adding/removing elements to the solution set. In turn, the ls-algorithm optimizes F(Zn·) in K2+D + O( 1 K3+ logK+) operations. The O( 1 K\n3 + logK+) component arises from the\nadd/removal operations, but as we show in Figure 2, it is a loose upper bound that scales sub-quadratically in practice."
    }, {
      "heading" : "4. Related Work",
      "text" : "Several proposals have been made for efficient inference with latent feature models. Table 1 summarizes the per-iteration complexity of the methods discussed below. In the next section we compare these methods on two synthetic and three real-world datasets.\nDoshi-Velez et al. (2009) formulated a coordinate ascent variational inference technique for IBP models (VIBP). This method used the “stick breaking” formulation of the IBP, which maintained coupled betadistributed priors on the entries of Z—marginalizing these priors does not allow closed-form MFVB updates. Unlike MEIBP inference, maintaining the beta priors has the undesirable consequence that inactive features contribute to the evidence lower bound and must be ignored when updating the variational distributions. This was not a problem for Doshi-Velez et al. (2009)’s finite variational IBP, which computes variational distributions for a linear-Gaussian likelihood with a parametric beta-Bernoulli prior on the latent features. The inference complexity for both methods\nis O(NK2+D), which is dominated by updating q(Z).\nDing et al. (2010) used mixed expectation-propagation style updates with MFVB inference in order to perform variational inference for a nonnegative linearGaussian IBP model (INMF). The expectationpropagation style updates are more complicated than MFVB updates and have per-iteration complexity O(N(K3D+KD2)). Ding et al. (2010) motivated this framework by stating that the evidence lower bound of a linear-Gaussian likelihood with a truncated Gaussian prior on the latent factors is negative infinity. This is only true if the variational distribution is a Gaussian, however the free-form variational distribution for their model is a truncated Gaussian, which has a welldefined evidence lower bound.\nDoshi-Velez & Ghahramani (2009) presented a lineartime “accelerated” Gibbs sampler for conjugate IBP models that effectively marginalized over the latent factors (AIBP). The per-iteration complexity was O(N(K2 + KD)). This is comparable to the uncollapsed IBP sampler (UGibbs) that has per-iteration complexity O(NDK2) but does not marginalize over the latent factors, and as a result, takes longer to mix. In terms of both complexity and empirical performance, the accelerated Gibbs sampler is the most scalable sampling-based IBP inference technique currently available. One constraint of the accelerated IBP is that the latent factor distribution must be conjugate to the likelihood, which for instance, does not allow nonnegative priors on the latent factors.\nRai & Daume III (2011) introduced a beam-search heuristic for locating approximate MAP solutions to linear-Gaussian IBP models (BS-IBP). This heuristic sequentially adds a single data point to the model and determines the latent feature assignments by scoring all 2K+ latent feature combinations. The scoring heuristic uses an estimate of the joint probability, P (X,Z) to score assignments, which evaluates the collapsed likelihood P (X|Z) for all 2K+ possible assignments: an expensive N3(K+ +D) operation, yielding a per-iteration complexity of O(N3(K+ +D)2 K+)."
    }, {
      "heading" : "5. Experiments",
      "text" : "We evaluated the inference quality and efficiency of MEIBP inference on two synthetic and three realworld datasets. We used the runtime and predictive likelihood of held-out observations as our performance criteria and compared MEIBP inference with the methods listed in Table 1 (the finite and infinite VIBP are differentiated with an “f-” and “i-” prefix). We used a truncated Gaussian prior on the latent fac-\ntors for UGibbs and INMF, and Gaussian priors for the AIBP and variational methods. In our evaluations, we also included Schmidt et al. (2009)’s iterated conditional modes algorithm, which computes a MAP estimate of a parametric nonnegative matrix factorization model: X = BA+E, where B and A have exponential priors and E is zero-mean Gaussian noise. We abbreviate this model “BNMF”; it has a per-iteration complexity of O(N(K2+ +K+D)).\nThe VIBP and MEIBP inference methods specify a maximum K value, while the sampling methods are unbounded. Therefore, we also included truncated versions of the sampling methods (indicated by a “t-” prefix) for a fairer comparison. We centered all input data to have a 0-mean for the models with 0- mean Gaussian priors and a 0-minimum for nonnegative models, and all inferred matrices were initialized randomly from their respective priors. Following Doshi-Velez & Ghahramani (2009), we fixed the hyperparameters σX and σA to 3 4σ, where σ was the standard deviation across all dimensions of the data, and set α = 3. We ran each algorithm until the multiplicative difference of the average training log-likelihood differed by less than 10−4 between blocks of five iterations with a maximum runtime of 36 hours. Our experiments used MATLAB implementations of the algorithms, as provided by the respective authors, on 3.20 GHz processors.\nSynthetic Data We created high-noise synthetic datasets in the following way: (1) sample zn,k ∼ Bernoulli(p = 0.4), (2) generate A with K random, potentially overlapping binary factors, (3) let X = ZA + E, where E ∼ N (0, 1). We evaluated the predictive likelihood on 20% of the dimensions from the last half of the data (see supplementary information).\nFigure 3 shows the evolution of the test log-likelihood over time for a small dataset with N = 500, D =\n500,K = 20 and a large dataset with N = 105, D = 103,K = 50. All models were initialized randomly with the true number of latent features, and the error regions display the standard deviation over five random restarts. The BS-IBP and INMF methods were removed from our experiments following the synthetic dataset tests as both methods took at least an order of magnitude longer than the other methods: in 36 hours, the BS-IBP did not complete a single iteration on the small dataset, and the INMF did not complete a single iteration on the large dataset.\nMEIBP converged quickest among the IBP models for both the small and large dataset, while the parametric BNMF model converged much faster than all IBP models. However, the IBP models captured the sparsity of the latent features and the MEIBP and UGibbs eventually outperformed the BNMF on the small dataset, while only the MEIBP outperformed the BNMF on the large dataset. The VIBP methods converged quicker than the sampling counterparts but had trouble escaping local optima. The uncollapsed samplers eventually performed as well as the MEIBP on the small dataset but did not mix to a well-performing distribution for the large dataset.\nReal Data Table 2 summarizes the real-world datasets used in our experiments. Piano and YaleBC are dense real-valued datasets, whereas the Flickr dataset is a sparse binary dataset (0.81% filled). For the Piano and Flickr datasets, we evaluated the predictive likelihood on a held-out portion of 20% of the dimensions from the last half of the datasets. The Yale-BC dataset had roughly sixty-four facial images of thirty-eight subjects, and we removed the bottom half of five images from each subject for testing.\nFigure 4 shows the test log-likelihood and convergence time for all inference methods applied to the real-world datasets, averaged over five random restarts. All inference methods were initialized with K = {10, 25, 50}\nas indicated by the size of the marker (the smallest marker shows the K = 10 results). The sampling methods (AIBP, UGibbs) also include a large faded marker that shows the results for unbounded K.\nThe Piano results were similar to the small synthetic dataset. The BNMF converged much faster than the IBP models, and the MEIBP performed best among the IBP models in terms of runtime and test loglikelihood—it converged to a similar solution as the AIBP in one-third the time. Though UGibbs has the best per-iteration complexity, it got stuck in poor local optima when randomly initialized. The VIBP methods and MEIBP expressed large uncertainty about the latent factors early on and overcame these poor local optima. By using hard latent feature assignments, the MEIBP took larger steps in the inference space than the VIBP methods, which was beneficial for this dataset, and achieved similar results to the AIBP.\nMEIBP inference performed comparable to the best IBP sampling technique for the sparse binary Flickr dataset and converged over an order of magnitude faster. Surprisingly, the dense BNMF inference performed very well on this dataset even though the dataset was sparse and binary. The BNMF converged slower than the MEIBP because it inferred a sparse matrix from a dense prior, which took over four times as many iterations to converge compared to the dense datasets. While the t-AIBP converged to a better solution than the MEIBP, it took over an order-\nof-magnitude longer to surpass the MEIBP’s performance. As we demonstrate with the Flickr results, initializing the AIBP with the MEIBP outcome obtained a similar solution in a fraction of the time (indicated as “meibp+aibp” on the figure).\nThe MEIBP converged faster than the other IBP methods for the Yale-BC dataset but to a lower test likelihood. The UGibbs and BNMF also experienced difficulty for this dataset, where BNMF converged to a test log-likelihood around −3.6 × 106 (not visible in the figure). These linear-Gaussian models with nonnegative priors performed worse than the models with Gaussian priors because the dataset contained many images with dark shadows covering part of the face. The nonnegative priors appeared to struggle with reconstructing these shadows, because unlike the Gaussian priors, they could not infer negative-valued “shadow” factors that obscured part of the image.\nIn the above experiments, the MEIBP consistently exhibited a sudden convergence whereby it obtained a local optima and the ls-algorithm did not change any Z assignments. This is a characteristic of using hard assignments with a greedy algorithm: at a certain point, changing any latent feature assignments decreased the objective function. This abrupt convergence, in combination with the speed of the ls-algorithm, helped the MEIBP consistently converge faster than other IBP methods. Furthermore, the submodular maximization algorithm converged to local optima that were comparable or better than the sampling or variational results, though at the cost of only obtaining a MAP solution. Like the variational methods, it maintained a distribution over A that prevented it from getting stuck in local optima early on, and like the sampling methods,\nthe MEIBP used hard Z assignments to take larger steps in the inference space and obtain better optima."
    }, {
      "heading" : "6. Summary and Future Work",
      "text" : "We presented a new inference technique for IBP models that used Kurihara & Welling (2008)’s ME framework to perform approximate MAP inference via submodular maximization. Our key insight was to exploit the submodularity inherent in the evidence lower bound formulated in §3, which arose from the quadratic pseudo-Boolean component of the linearGaussian model. MEIBP inference converged faster than competing IBP methods and obtained comparable solutions on various datasets.\nThere are many discrete Bayesian nonparametric priors, such as the Dirichlet process, and an interesting area for future research will be to generalize our results in order to phrase inference with these priors as submodular optimization problems. Furthermore, we used a simple local-search algorithm to obtain a 13 -approximation bound, but concurrently with this work, Buchbinder et al. (2012) proposed a simpler stochastic algorithm for unconstrained submodular maximization that obtains an expected 12 - approximation bound. Using this algorithm, MEIBP inference has an improved worst case complexity of O(NK2+D). We will investigate this algorithm in an extended technical version of this paper.\nCode: A MATLAB implementation of MEIBP is available at https://github.com/cjrd/MEIBP.\nAcknowledgements: CR was supported by the Winston Churchill Foundation of the United States, and\nZG was supported by EPSRC grant EP/I036575/1 and grants from Google and Microsoft. We thank the anonymous reviewers for their helpful comments."
    }, {
      "heading" : "Supplementary Material",
      "text" : ""
    }, {
      "heading" : "S.1. Truncated Gaussian Properties",
      "text" : "In the main text we examined a truncated Gaussian of the form:\nTN (µ̃kd, σ̃2kd) = 2 erfc ( − µ̃kd σ̃kd √ 2 )N (µ̃kd, σ̃2kd) (23) with N representing a Gaussian distribution. The first two moments of TN (µ̃kd, σ̃2kd) are:\nE [akd] = µ̃kd + σ̃kd\n√ 2/π\nerfcx (℘kd) (24)\nE [ a2kd ] = µ̃2kd + σ̃ 2 kd + σ̃kdµ̃kd\n√ 2/π\nerfcx (℘kd) (25)\nwith ℘kd = − µ̃kdσ̃kd√2 and erfcx (y) = e y2(1 − erf(y)) representing the scaled complementary error function. The entropy is\nH(q(akd)) = 1 2 ln πeσ̃2kd 2 + ln erfc ( − µ̃kd σ̃kd √ 2 ) (26)\n+ µ̃kd σ̃kd\n√ 1\n2π\n( erfcx ( − µ̃kd σ̃kd √ 2 ))−1 .\n(27)"
    }, {
      "heading" : "S.2. Shifted Equivalence Classes",
      "text" : "Here we discuss the “shifted” equivalence class of binary matrices first proposed by Ding et al. (2010). For a given N ×K binary matrix Z, the equivalence class for this binary matrix [Z] is obtained by shifting allzero columns to the right of the non-zero columns while maintaining the non-zero column orderings, see Figure 5. Placing independent Beta( αK , 1) priors on the Bernoulli entries of Z and integrating over these priors yields the following probability for Z, see Eq. 27 in Griffiths & Ghahramani (2005):\nP (Z) = K∏ k=1 α KΓ(mk + α K )Γ(N −mk + 1) Γ(N + 1 + αK ) (28) where mk = ∑N n=1 znk. Letting K → ∞ yields P (Z) = 0 for all Z. However, the probability of certain equivalence classes of binary matrices, P ([Z]), can remain non-zero as K → ∞. Specifically, Griffiths & Ghahramani (2005) show P ([Z]) remains non-zero for the “left-ordered form” equivalence class of binary matrices, whereby the columns of Z are ordered such that the binary values of the columns are non-increasing, where the first row is the most significant bit. Here\nwe outline a similar result for the shifted equivalence class.1\nWe obtain the probability of the shifted equivalence class by multiplying the multiplicity of the equivalence class by the probability of a matrix within the class. For a given matrix with K columns and K+ non-zero columns, each shifted equivalence class has ( K K+ ) matrices that map to it, yielding:\nP ([Z]) =\n( K\nK+ ) K∏ k=1 α KΓ(mk + α K )Γ(N −mk + 1) Γ(N + 1 + αK ) .\n(29)\nFollowing a similar algebraic rearrangement as Griffiths & Ghahramani (2005) Eqs. 30-33, except replacing the K!∏2N−1\nh=0 Kh! term with\n( K K+ ) —which occurs be-\ncause of the different equivalence class multiplicities— results in:\nP ([Z]) = αK+ K+! · K! (K −K+)!KK+ · ( N !∏2N−1 j=1 (j + α K ) )K\n· K+∏ k=1\n(N −mk)! ∏mk−1 j=1 (j + α K )\nN ! . (30)\nWe then take the limit K → ∞ for each of the four terms. The first term has no K dependence and does not change in the infinite limit. For the second term we let K0 = K −K+ and have K!K0!KK+ . Equations 60-62 in Griffiths & Ghahramani (2005) show that this term becomes 1 as K → ∞. The infinite limit of the third and fourth terms are determined in the Appendix of Griffiths & Ghahramani (2005). Combining all four terms together yields:\nP ([Z]) = αK+\nK+! e−αHN K+∏ k=1 (N −mk)!(mk − 1)! N ! (31)\nwhere HN is the N th harmonic number.\nThe probability of the shifted equivalence class is nearly identical to the probability of the left-orderedform equivalence class:\nP ([Z]lof) = αK+∏2N−1 h=1 Kh e−αHN K+∏ k=1 (N −mk)!(mk − 1)! N ! ,\n(32)\nwhere Kh is the number of columns of Z with binary value h ∈ {1, . . . , 2N−1} when the first row is taken\n1Ding et al. (2010) proposed this equivalence class but did not explicitly show that it remains well defined as K → ∞. Furthermore, they did not discuss the collapsed case where we first marginalize over the beta priors on Z.\nto be the most significant bit. The only difference between Eq. 31 and Eq. 32 is the denominator of the first fraction. For the left-ordered-form, this term penalizes Z matrices with identical columns. In the feature assignment view, this term penalizes features that are assigned to the exact same set of observations. The K+! term in the shifted equivalence class prior does not distinguish between identical and distinct columns of Z, and in turn, does not penalize repeated feature assignments. These two equivalence class probabilities are proportional in the limit of large N as the probability of two columns being identical approaches 0."
    }, {
      "heading" : "S.3. Hyperparameter Inference",
      "text" : "In the main text we assumed the hyperparameters θ = {σX , σA, α} were known (i.e. estimated from the data). Placing conjugate gamma hyperpriors on these parameters allows for a straightforward extension in which we infer their values. Formally, let\np(τX) = Gamma(τX ; aX , bX) (33)\np(τA) = Gamma(τA; aA, bA) (34)\np(α) = Gamma(α; aα, bα) (35)\nwhere τ represents the precision, equivalent to the inverse variance 1σ2 , for the variance parameter indicated in the subscript. Update equations for the variational distributions follow from standard update equations for variational inference in exponential families, cf. Attias (2000), and yield:\nq(τX) = Gamma(ãX , b̃X) (36)\nq(τA) = Gamma(ãA, b̃A) (37)\nq(α) = Gamma(ãα, b̃α) (38)\nwith variance updates\nãA = aA + KD\n2 (39)\nb̃A = bA + 1\n2 K+∑ k=1 D∑ d=1 E [ a2kd ]\n(40)\nand\nãX = aX + ND\n2 (41)\nb̃X = bX + 1\n2 N∑ n=1 D∑ d=1 [ x2nd + K+∑ k=1 [ E [ a2kd ] znk (42)\n− 2E[akd]znkxnd + 2 K+∑\nk′=k+1\nznkznk′akdak′d ]] (43)\nand q(α) updates\nãα = aα +K+ (44)\nb̃α = bα +HN . (45)\nMEIBP inference is carried out exactly as discussed in the main text except all instances of σX , σA, and α are replaced with the expectation from their respective variational distribution. Furthermore the variational lower bound also has three additional entropy terms for gamma distributions, one for each hyperparameter."
    }, {
      "heading" : "S.4. Evidence as a function of Zn·",
      "text" : "As shown in the main text, we obtain a submodular objective function for each Zn·, n ∈ {1, . . . , N} by examining the evidence as a function ofZn· while holding constant all n′ ∈ {1, . . . , N} \\ n. The evidence is\n1\nσ2X N∑ n=1 [ −1 2 Zn·ΦΦ TZTn· +Zn·ξ T n· ] − lnK+!\n+ K+∑ k=1 [ ln (N −mk)!(mk − 1)! N ! + ηk ] + const\n(46)\nξnk = Φk·X T n· +\n1\n2 D∑ d=1 [ E[akd]2 − E[a2kd] ] (47)\nηk = D∑ d=1\n[ − ln πσ2A 2α2/D\n2 − E[a 2 kd] 2σ2A +H(q(akd))\n] , (48)\nwhich nearly factorizes over the Zn· because the likelihood component and parts of the prior components naturally fit into a quadratic function of Zn·. The lnK+! and ηk only couple the rows of Z when K+ changes, while the log-factorial term couples the rows of Z through the sums of the columns. Both of these terms only depend on statistics of Z (the mk values and K+), not the Z matrix itself, e.g. permuting the rows of Z would not affect these terms. Furthermore, lnK+ and ηk have no N dependence and become insignificant as N increases. These observations, in conjunction with the MEIBP performance in the experimental section of the main text, indicate that optimizing Eq. 46 for Zn· is a reasonable surrogate for optimizing Z.\nHere we explicitly decompose Eq. 46 to show its Zn· dependency. Decomposing ln (N−mk)!(mk−1)!\nN ! is straightforward if we first define the function:\nν(znk) = { ln (N −mk\\n − znk)!(mk\\n + znk − 1)!/N ! 0, if mk\\n = 0 and znk = 0. (49) where the “\\n” subscript indicates the variable with the nth row removed from Z. For a given n we have:\nK+∑ k=1 ν(znk) = K+∑ k=1 ln (N −mk)!(mk − 1)!/N !\n= K+∑ k=1 znk (ν(znk = 1)− ν(znk = 0))\n+ ν(znk = 0), (50)\nwhich makes the Zn· dependency explicit and lets us add ν(znk = 1) − ν(znk = 0) into the inner-product term, ξn·, and place ν(znk = 0) into a constant term. We can incorporate ηk into the inner-product term in a similar manner for a given n ∈ {1, . . . , N} :\nK+∑ k=1 ηk = ∑\nk:mk\\n>0\nηk + K+∑ k=1 1{mk\\n=0}znkηk, (51)\nwhere the first term does not depend on Zn· and is added to the constant term, while the second term is added to the inner-product term. Finally, for a given n ∈ {1, . . . , N} the lnK! term becomes\nlnK+! = ln K+\\n + K+∑ k=1 [ 1{mk\\n=0}znk ]!, (52) where 1{·} is the indicator function. As stated in the main text, combining the above terms yields\nthe following submodular objective function for n = 1, . . . , N :\nF(Zn·) =− 1\n2σ2X Zn·ΦΦ\nTZTn· +Zn·ω T n· + const\n− ln K+\\n + K+∑ k=1 [ 1{mk\\n=0}znk ]! (53) Φk· = (E [ak1] , . . . ,E [akD]) (54)\nωnk = 1\nσ2X\n( Φk·X T n· + 1\n2 D∑ d=1 [ E[akd]2 − E[a2kd] ]) + ν(znk = 1)− ν(znk = 0) + 1{mk\\n=0}ηk,\n(55)\n1{·} is the indicator function, and the subscript “ \\n” is the value of the given variable after removing the nth row from Z."
    }, {
      "heading" : "S.5. Additional MEIBP Characterization",
      "text" : "In this section, we will maintain a growing list of additional MEIBP characterization experiments. See http://arxiv.org/abs/1304.3285 for the current version."
    }, {
      "heading" : "S.5.1. Learning K+",
      "text" : "An ostensible advantage of using Bayesian nonparametric priors is that a user does not need to specify the multiplicity of the prior parameters. Clever sampling techniques such as slice sampling and retrospective sampling allow samples to be drawn from these nonparametric priors, c.f. Teh et al. (2007) and Papaspiliopoulos & Roberts (2008). However variational methods are not directly amenable to Bayesian nonparametric priors as the variational optimization cannot be performed over an unbounded prior space. Instead, variational methods must specify a maximum model complexity (parameter multiplicity). Several heuristics have been proposed to address this limitation: Wang & Blei (2012) sampled from the variational distribution for the local parameters—which included sampling from the unbounded prior— and used the empirical distributions of the local samples to update the global parameters, while Ding et al. (2010) simply started with K+ = 1 and greedily added features. We did not address these techniques in this work as the MEIBP performed competitively with the unbounded sampling techniques without employing these types of heuristics. Furthermore, here we demonstrate that the MEIBP can robustly infer the true number of latent features when the K+ bound is greater than the true number of latent features.\nFor this experiment we generated the binary images dataset used in Griffiths & Ghahramani (2005), where the dataset, X, consisted of 2000 6 × 6 images. Each row of X was a 36 dimensional vector of pixel intensity values that was generated by using Z to linearly combine a subset of the four binary factors shown in Figure 6. Gaussian white noise, N (0, σX), was then added to each image, yielding X = ZA + E. The feature vectors, Zn· were sampled from a distribution in which each factor was present with probability 0.5. Figure 7 shows four of these images with different σX values.\nWe initialized the MEIBP with K = 20, σX=1.0, σA = 1.0, α = 2, µ̃kd ∼ |N (0, 0.05)| (variational factor means), σ̃kd ∼ |N (0, 0.1)| (variational factor standard deviations), znk ∼ Bernoulli( 13 ). With this initialization, we tested the MEIBP robustness by performing MEIBP inference on X for σX = 0.1, . . . , 1.0 in 100 evenly spaced increments with all hyperparameters and algorithm options unchanged during the experiment. MEIBP convergence was determined in the same way as the main experimental section. Figure 8 (top) shows a histogram of the final number of MEIBP features (Ktrue = 4) and Figure 8 (bottom) shows the final number of MEIBP features as a function of σX .\nThese results indicate that the regularizing nature of the IBP prior tends to lead to the correct number of latent features even when the K+ bound is much larger than the true K+. Furthermore this experiment indicates that MEIBP inference is robust to model noise, at least, for the simple data used in this experiment. At a medium level of data noise, the inference occasionally finished with K+ = 3, which resulted from two true latent factors collapsing to the same inferred latent feature. Once this occurred, MEIBP did not have a mechanism for splitting the features. For σX comparable to the latent factors, σX ≥ 0.9, MEIBP often inferred “noise features,” which were essentially whitenoise and were typically active for less than 4% of the data instances. In future experiments we will attempt to flesh out the practical differences between unbounded priors and priors that operate in a large bounded latent space."
    } ],
    "references" : [ {
      "title" : "A variational bayesian framework for graphical models",
      "author" : [ "H. Attias" ],
      "venue" : "Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Attias,? \\Q2000\\E",
      "shortCiteRegEx" : "Attias",
      "year" : 2000
    }, {
      "title" : "Clusters and features from combinatorial stochastic processes",
      "author" : [ "T. Broderick", "M.I. Jordan", "J. Pitman" ],
      "venue" : null,
      "citeRegEx" : "Broderick et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Broderick et al\\.",
      "year" : 2012
    }, {
      "title" : "A tight linear time (1/2)-approximation for unconstrained submodular maximization",
      "author" : [ "N. Buchbinder", "M. Feldman", "J. Naor", "R. Schwartz" ],
      "venue" : "In 53rd Annual Symposium on Foundations of Computer Science,",
      "citeRegEx" : "Buchbinder et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Buchbinder et al\\.",
      "year" : 2012
    }, {
      "title" : "Nonparametric Bayesian matrix factorization by Power-EP",
      "author" : [ "N. Ding", "Y.A. Qi", "R. Xiang", "I. Molloy", "N. Li" ],
      "venue" : "In 14th Int’l Conf. on AISTATS,",
      "citeRegEx" : "Ding et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Ding et al\\.",
      "year" : 2010
    }, {
      "title" : "Accelerated sampling for the Indian buffet process",
      "author" : [ "F. Doshi-Velez", "Z. Ghahramani" ],
      "venue" : "In Proceedings of the 26th Annual Int’l Conference on Machine Learning,",
      "citeRegEx" : "Doshi.Velez and Ghahramani,? \\Q2009\\E",
      "shortCiteRegEx" : "Doshi.Velez and Ghahramani",
      "year" : 2009
    }, {
      "title" : "Variational inference for the Indian buffet process",
      "author" : [ "F. Doshi-Velez", "K.T. Miller", "J. Van Gael", "Y.W. Teh" ],
      "venue" : "In 13th Int’l Conf. on AISTATS,",
      "citeRegEx" : "Doshi.Velez et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Doshi.Velez et al\\.",
      "year" : 2009
    }, {
      "title" : "Maximizing non-monotone submodular functions",
      "author" : [ "U. Feige", "V.S. Mirrokni", "J. Vondrak" ],
      "venue" : "SIAM Journal on Computing,",
      "citeRegEx" : "Feige et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Feige et al\\.",
      "year" : 2011
    }, {
      "title" : "Submodular functions and optimization, volume 58",
      "author" : [ "S. Fujishige" ],
      "venue" : "Elsevier Science Limited,",
      "citeRegEx" : "Fujishige,? \\Q2005\\E",
      "shortCiteRegEx" : "Fujishige",
      "year" : 2005
    }, {
      "title" : "Propagation algorithms for variational bayesian learning",
      "author" : [ "Z. Ghahramani", "M.J. Beal" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Ghahramani and Beal,? \\Q2001\\E",
      "shortCiteRegEx" : "Ghahramani and Beal",
      "year" : 2001
    }, {
      "title" : "Infinite latent feature models and the Indian buffet process",
      "author" : [ "T. Griffiths", "Z. Ghahramani" ],
      "venue" : "Technical report, Gatsby Unit,",
      "citeRegEx" : "Griffiths and Ghahramani,? \\Q2005\\E",
      "shortCiteRegEx" : "Griffiths and Ghahramani",
      "year" : 2005
    }, {
      "title" : "Infinite latent feature models and the Indian buffet process",
      "author" : [ "T.L. Griffiths", "Z. Ghahramani" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Griffiths and Ghahramani,? \\Q2006\\E",
      "shortCiteRegEx" : "Griffiths and Ghahramani",
      "year" : 2006
    }, {
      "title" : "Infinite sparse factor analysis and infinite independent components analysis",
      "author" : [ "D. Knowles", "Z. Ghahramani" ],
      "venue" : "Independent Component Analysis and Signal Separation,",
      "citeRegEx" : "Knowles and Ghahramani,? \\Q2007\\E",
      "shortCiteRegEx" : "Knowles and Ghahramani",
      "year" : 2007
    }, {
      "title" : "Utilizing object-object and object-scene context when planning to find things",
      "author" : [ "T. Kollar", "N. Roy" ],
      "venue" : "In IEEE International Conference on Robotics and Automation,",
      "citeRegEx" : "Kollar and Roy,? \\Q2009\\E",
      "shortCiteRegEx" : "Kollar and Roy",
      "year" : 2009
    }, {
      "title" : "Bayesian k-means as a maximization-expectation algorithm",
      "author" : [ "K. Kurihara", "M. Welling" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Kurihara and Welling,? \\Q2008\\E",
      "shortCiteRegEx" : "Kurihara and Welling",
      "year" : 2008
    }, {
      "title" : "Acquiring linear subspaces for face recognition under variable lighting",
      "author" : [ "K.C. Lee", "J. Ho", "D. Kriegman" ],
      "venue" : "IEEE Trans. Pattern Anal. Mach. Intelligence,",
      "citeRegEx" : "Lee et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2005
    }, {
      "title" : "Submodular functions and convexity",
      "author" : [ "L. Lovász" ],
      "venue" : "Mathematical programming: the state of the art,",
      "citeRegEx" : "Lovász,? \\Q1983\\E",
      "shortCiteRegEx" : "Lovász",
      "year" : 1983
    }, {
      "title" : "Retrospective Markov chain Monte Carlo methods for Dirichlet process hierarchical models",
      "author" : [ "O. Papaspiliopoulos", "G.O. Roberts" ],
      "venue" : null,
      "citeRegEx" : "Papaspiliopoulos and Roberts,? \\Q2008\\E",
      "shortCiteRegEx" : "Papaspiliopoulos and Roberts",
      "year" : 2008
    }, {
      "title" : "A discriminative model for polyphonic piano transcription",
      "author" : [ "G.E. Poliner", "D.P.W. Ellis" ],
      "venue" : "EURASIP Journal on Advances in Signal Processing,",
      "citeRegEx" : "Poliner and Ellis,? \\Q2006\\E",
      "shortCiteRegEx" : "Poliner and Ellis",
      "year" : 2006
    }, {
      "title" : "Beam search based map estimates for the Indian buffet process",
      "author" : [ "P. Rai", "H. Daume III" ],
      "venue" : "In Proceedings of the 28th Annual Int’l Conference on Machine Learning,",
      "citeRegEx" : "Rai and III,? \\Q2011\\E",
      "shortCiteRegEx" : "Rai and III",
      "year" : 2011
    }, {
      "title" : "Stickbreaking construction for the indian buffet process",
      "author" : [ "Y.W. Teh", "D. Gorur", "Z. Ghahramani" ],
      "venue" : "In Int’l Conference on AISTATS,",
      "citeRegEx" : "Teh et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Teh et al\\.",
      "year" : 2007
    }, {
      "title" : "Truncation-free online variational inference for bayesian nonparametric models",
      "author" : [ "C. Wang", "D. Blei" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Wang and Blei,? \\Q2012\\E",
      "shortCiteRegEx" : "Wang and Blei",
      "year" : 2012
    }, {
      "title" : "Kh is the number of columns of Z with binary value h ∈",
      "author" : [ "Ding" ],
      "venue" : null,
      "citeRegEx" : "Ding,? \\Q2010\\E",
      "shortCiteRegEx" : "Ding",
      "year" : 2010
    }, {
      "title" : "However variational methods are not directly amenable to Bayesian nonparametric priors as the variational optimization cannot be performed over an unbounded prior space",
      "author" : [ "nonparametric priors", "c.f. Teh" ],
      "venue" : null,
      "citeRegEx" : "priors and Teh,? \\Q2008\\E",
      "shortCiteRegEx" : "priors and Teh",
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "More generally, feature models can be viewed as a generalization of unsupervised clustering, see Broderick et al. (2012).",
      "startOffset" : 97,
      "endOffset" : 121
    }, {
      "referenceID" : 3,
      "context" : "Ding et al. (2010) examine different “shifted” equivalence classes formed by shifting all-zero columns to the right of non-zero columns while maintaining the non-zero column ordering.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 0,
      "context" : "MFVB operates by approximating the posterior distribution of a given probabilistic model by assuming independent variational distributions, p(Z,A|X) ≈ q(Z)q(A) (Attias, 2000; Ghahramani & Beal, 2001).",
      "startOffset" : 160,
      "endOffset" : 199
    }, {
      "referenceID" : 15,
      "context" : "Submodularity is desirable in discrete optimization because submodular functions are discrete analogs of convex functions and can be globally minimized in polynomial time (Lovász, 1983).",
      "startOffset" : 171,
      "endOffset" : 185
    }, {
      "referenceID" : 0,
      "context" : "Given the MFVB constraint, we determine the variational distributions by minimizing the KL-divergence between the variational distributions and the true posterior, which is equivalent to maximizing a lower bound on the evidence (Attias, 2000): ln p(X|θ) = E q [ln p(X,A,Z|θ)] +H[q] +D(q‖p) ≥ E q [ln p(X,A,Z|θ)] +H[q] ≡ F (9) where H[q] is the entropy of q and D(q‖p) represents the KL-divergence between the variational distribution and the true posterior.",
      "startOffset" : 228,
      "endOffset" : 242
    }, {
      "referenceID" : 7,
      "context" : "We can prove F(Zn·) is submodular given the following two well-known propositions, see Fujishige (2005): Proposition 1.",
      "startOffset" : 87,
      "endOffset" : 104
    }, {
      "referenceID" : 6,
      "context" : "Feige et al. (2011) prove that an approximibility guarantee is NP-hard for this class of functions.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 6,
      "context" : "Feige et al. (2011) prove that an approximibility guarantee is NP-hard for this class of functions. However, Feige et al. (2011) also show that a local-search (ls) algorithm obtains a constant-factor approximation to the optimal solution, provided the submodular objective function is nonnegative.",
      "startOffset" : 0,
      "endOffset" : 129
    }, {
      "referenceID" : 3,
      "context" : "Ding et al. (2010) used mixed expectation-propagation style updates with MFVB inference in order to perform variational inference for a nonnegative linearGaussian IBP model (INMF).",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 3,
      "context" : "Ding et al. (2010) used mixed expectation-propagation style updates with MFVB inference in order to perform variational inference for a nonnegative linearGaussian IBP model (INMF). The expectationpropagation style updates are more complicated than MFVB updates and have per-iteration complexity O(N(KD+KD)). Ding et al. (2010) motivated this framework by stating that the evidence lower bound of a linear-Gaussian likelihood with a truncated Gaussian prior on the latent factors is negative infinity.",
      "startOffset" : 0,
      "endOffset" : 327
    }, {
      "referenceID" : 5,
      "context" : "Algorithm Iteration Complexity MEIBP O(N(K +D+K 3 + lnK+)) VIBP (Doshi-Velez et al., 2009) O(NK +D) AIBP (Doshi-Velez & Ghahramani, 2009) O(N(K + +K+D)) UGibbs (Doshi-Velez & Ghahramani, 2009) O(NK +D) BS-IBP (Rai & Daume III, 2011) O(N(K+ +D)2 +) INMF (Ding et al.",
      "startOffset" : 64,
      "endOffset" : 90
    }, {
      "referenceID" : 3,
      "context" : ", 2009) O(NK +D) AIBP (Doshi-Velez & Ghahramani, 2009) O(N(K + +K+D)) UGibbs (Doshi-Velez & Ghahramani, 2009) O(NK +D) BS-IBP (Rai & Daume III, 2011) O(N(K+ +D)2 +) INMF (Ding et al., 2010) O(N(K +D +K+D ))",
      "startOffset" : 170,
      "endOffset" : 189
    }, {
      "referenceID" : 14,
      "context" : "Dataset Size (N ×D) Details Piano (Poliner & Ellis, 2006) 16000× 161 DFT of piano recordings Flickr (Kollar & Roy, 2009) 25000× 1500 binary image-tag indicators Yale-BC (Lee et al., 2005) 2414× 32256 face images with various lightings",
      "startOffset" : 169,
      "endOffset" : 187
    }, {
      "referenceID" : 2,
      "context" : "Furthermore, we used a simple local-search algorithm to obtain a 13 -approximation bound, but concurrently with this work, Buchbinder et al. (2012) proposed a simpler stochastic algorithm for unconstrained submodular maximization that obtains an expected 12 approximation bound.",
      "startOffset" : 123,
      "endOffset" : 148
    } ],
    "year" : 2013,
    "abstractText" : "Inference for latent feature models is inherently difficult as the inference space grows exponentially with the size of the input data and number of latent features. In this work, we use Kurihara & Welling (2008)’s maximization-expectation framework to perform approximate MAP inference for linearGaussian latent feature models with an Indian Buffet Process (IBP) prior. This formulation yields a submodular function of the features that corresponds to a lower bound on the model evidence. By adding a constant to this function, we obtain a nonnegative submodular function that can be maximized via a greedy algorithm that obtains at least a 13 approximation to the optimal solution. Our inference method scales linearly with the size of the input data, and we show the efficacy of our method on the largest datasets currently analyzed using an IBP model.",
    "creator" : "LaTeX with hyperref package"
  }
}