{
  "name" : "1502.06362.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Contextual Dueling Bandits",
    "authors" : [ "Miroslav Dudı́k", "Katja Hofmann", "Robert E. Schapire", "Aleksandrs Slivkins", "Masrour Zoghi", "SLIVKINS ZOGHI" ],
    "emails" : [ "MDUDIK@MICROSOFT.COM", "KATJA.HOFMANN@MICROSOFT.COM", "SCHAPIRE@MICROSOFT.COM", "SLIVKINS@MICROSOFT.COM", "M.ZOGHI@UVA.NL" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "We study how to learn to act based on contextual information when provided only with partial, relative feedback. This problem naturally arises in information retrieval (IR) and recommender systems, where the user feedback is considerably more reliable when interpreted as relative comparisons rather than absolute labels (Radlinski et al., 2008). For instance, in web search, for a particular query, the IR system may have several candidate rankings of documents that could be presented, with the best option being dependent upon the specific user. By presenting a mix or interleaving of two of the candidate rankings and observing the user’s response (Chapelle et al., 2012; Hofmann et al., 2013), it is possible for such a system to get feedback about user preferences. ∗ On leave from Princeton University. † Part of this research was conducted during an internship with Microsoft Research.\nc© 2015 M. Dudı́k, K. Hofmann, R.E. Schapire, A. Slivkins & M. Zoghi.\nar X\niv :1\n50 2.\n06 36\n2v 2\n[ cs\n.L G\nHowever, this feedback is partial since it is only with respect to the two rankings that were chosen, and it is relative since it only tells which of the two rankings is preferred to the other.\nThe dueling-bandits problem of Yue et al. (2009) formalizes this setting. Abstractly, the learner is repeatedly faced with a set of possible actions, and may select two of these actions to face off in a duel whose stochastically determined winner is then revealed. Through such experimentation, the learner attempts to find the “best” of the actions.\nIn this paper, we focus on the contextual dueling bandit setting, where context can provide information that helps identify the best action. For instance, in the example above, the actions may be the candidate rankings to choose among, and the context may be additional information about the user or query that might help in choosing the best ranking. The learner’s goal now is to find a good policy, a rule for choosing actions based on context.\nSimilar to prior work on contextual (non-dueling) bandits (Auer et al., 2002; Langford and Zhang, 2007; Dudı́k et al., 2011; Agarwal et al., 2014), we propose a setting in which the learner has access to a space of policies Π, with the goal of performing as well as the “best” in the space. This space plays a role analogous to the hypothesis space in supervised learning. It will typically be extremely large or even infinite. We therefore explicitly aim for methods that will be applicable when this is the case.\nMerely defining the precise goal of learning can be problematic in such a relative-feedback setting. When rewards are absolute, the best policy in Π is clearly and easily defined as the one that achieves the highest expected reward, because, by such an absolute measure, this policy beats every other policy. In a relative-feedback setting, since we have a means of obtaining pairwise comparisons between actions or policies, we might aim to choose the policy in Π that (on average) beats every other policy in the class in such head-to-head competitions. Most previous work on dueling bandits (Yue et al., 2009; Yue and Joachims, 2011; Urvoy et al., 2013; Zoghi et al., 2014b) has in fact explicitly or implicitly assumed that such a Condorcet winner exists. But there are good reasons to doubt such a strong assumption, particularly when working with large and rich policy spaces. There are numerous examples, even in natural situations, where this assumption (and more generally, transitivity among policies) is known to fail (see, for instance, Gardner, 1970; Zoghi et al., 2014a). Indeed, the preferences of a population of users do not need to be transitive, even if each individual user has transitive preferences.\nIn this paper, we seek to improve the dueling bandits techniques in two respects. First, we seek to relax the modeling restrictions on which previous methods have depended so as to develop methods that are more generally applicable. Second, we seek to achieve a similar level of flexibility in the design of policies as for supervised learning algorithms. Contributions. Our first contribution (in Section 2) is the introduction of a new solution concept, called the von Neumann winner, which is based on a game-theoretic interpretation. Like a Condorcet winner, when facing any other policy in a duel, a von Neumann winner has at least a 50% chance of winning; in this sense, a von Neumann winner is at least as good as every policy in the space. On the other hand, a von Neumann winner is always guaranteed to exist, without any extraneous assumptions. This guarantee is made possible by allowing policies to be selected in a randomized fashion, as is quite natural in such a learning setting.\nWith the goal of learning clarified, we turn to algorithms. As a warm-up, in Section 5, we give a fully online algorithm in which two copies of the Exp4.P multi-armed bandit algorithm (Beygelzimer et al., 2011) are run against one another (using a “sparring” approach previously suggested\nby Ailon et al., 2014). Although yielding good regret, this algorithm requires time and space linear in |Π|, which is impractical in most realistic settings where we would expect Π to be enormous.\nTo address this difficulty, we propose an approach used previously in other works on contextual bandits (Langford and Zhang, 2007; Dudı́k et al., 2011; Agarwal et al., 2014). Specifically, we assume that we have access to a classification oracle for our policy class that can find the minimumcost policy in Π when given the cost of each action on each of a sequence of contexts. In fact, an ordinary cost-sensitive, multiclass classification learning algorithm can be used for this purpose, which suggests that, practically, this may be a reasonable and natural assumption.\nWe then consider techniques for constructing a von Neumann winner from empirical exploration data. (Although we focus on a batch-like setting, the resulting algorithms can be used online as well.) We analyze the statistical efficiency of this approach in Section 6. In Sections 7 and 8, we give two polynomial-time algorithms for computing an approximate von Neumann winner from data: one based on Kalai and Vempala’s Follow-the-Perturbed-Leader algorithm (2003), and the other based on projected gradient ascent as studied by Zinkevich (2003). These techniques yield learning algorithms that approximate or perform as well as the von Neumann winner, using data, time, and space that only depend logarithmically on the cardinality of the space Π, and therefore, are applicable even with huge policy spaces. Other related work. Numerous algorithms have been proposed for the (non-contextual) dueling bandits problem: Interleaved Filter (Yue et al., 2009); Beat the Mean (BTM) (Yue and Joachims, 2011); Sensitivity Analysis of VAriables for Generic Exploration (SAVAGE) (Urvoy et al., 2013); Relative Confidence Sampling (Zoghi et al., 2014a); Relative Upper Confidence Bound (RUCB) (Zoghi et al., 2014b); Doubler, MultiSBM and Sparring (Ailon et al., 2014) and mergeRUCB (Zoghi et al., 2015b). These methods impose various constraints on the problem at hand, ranging from the requirement that it arise from an underlying utility function (e.g., MultiSBM) to no constraint at all (e.g., SAVAGE); they mainly provide regret bounds that are logarithmic in the number of rounds, and at least linear in the number of actions. In principle, these methods could be applied to contextual dueling bandits by treating policies as actions. But this would lead to regret at least linear in the number of policies which is far worse than the logarithmic bounds obtained in this paper.\nThe method that is the most closely related to our work is Dueling Bandit Gradient Descent (DBGD) (Yue and Joachims, 2009), a policy gradient method that iteratively improves upon the current policy by conducting comparisons with nearby policies, assuming that the policy space comes equipped with a distance metric, and incrementally adapting the policy if a better alternative is encountered. As with all local optimization methods, DBGD imposes a convexity assumption on the dueling bandit problem for its performance guarantee: the dueling bandit problem is assumed to arise from the noisy observations of an underlying convex objective function. In this paper, we both relax the assumptions imposed by DBGD and improve upon the regret bound."
    }, {
      "heading" : "2. Dueling bandits and the von Neumann winner",
      "text" : "In the dueling bandits problem (Yue et al., 2009), the learner has access to K possible actions, 1, . . . ,K, and attempts to determine the “best” action through repeated stochastic pairwise comparisons of actions, called duels. Thus, at each time step, the learner chooses a pair of actions (a, b) for a duel; the outcome of the duel is +1 if a wins, and −1 if b wins. The (unknown) expected value of this outcome is denoted P (a, b), and is assumed to depend only on the selected pair (a, b). In other words, the probability that a beats b in a duel is (P (a, b) + 1)/2, and the two actions are exactly\nevenly matched if P (a, b) = 0. We say that a beats b to mean that the chance of a winning a duel with b is strictly greater than 1/2; similarly, a ties b if this probability is exactly 1/2.\nThe K × K matrix P of all such expectations P (a, b) is called the preference matrix.1 This matrix is initially unknown to the learner, but can be discovered bit-by-bit through experimentation. We assume, of course, that all of the entries of P are in [−1,+1], and furthermore, that P is skew-symmetric, meaning that P> = −P so that a duel (b, a) is equivalent to (the negation of) a duel (a, b). (This also implies P (a, a) = 0 for every action a, as is natural.) Other than this, we strenuously avoid making any assumptions in the current work about the matrix P. For instance, we do not make any assumptions regarding transitivity among the various actions.\nIn such a relative-feedback setting, the “best” action is not always well-defined because there is no measure of the absolute quality of actions. Existing work typically assumes the existence of a Condorcet winner (Urvoy et al., 2013; Zoghi et al., 2014b), that is, an action a∗ that beats every other action a 6= a∗. This is a very natural definition from a preference learning perspective, since a∗ is indeed preferred to every other action. However, it has been shown that dueling bandit problems without Condorcet winners arise regularly in practice (Zoghi et al., 2014a).2\nAlthough there is no guarantee of a single action beating all others, the situation changes considerably if we simply allow actions to be selected in a randomized fashion. With this natural relaxation, the problem of non-existence entirely vanishes. Thus, the idea is to find a probability vector w in ∆K (where ∆K is the simplex of vectors in [0, 1]K whose entries sum to 1) such that∑K\na=1w(a)P (a, b) ≥ 0 for all actions b. (1)\nIn words, for every action b, if a is selected randomly according to distribution w, then the chance of beating b in a duel is at least 1/2. (Note that this property implies that the same will be true if b is itself selected in a randomized way.) A distribution w with this property is said to be a von Neumann winner for the preference matrix P.\nAs the name reflects, this notion is intimately connected to a game-theoretic interpretation. Indeed, we can view preference matrix P as describing a zero-sum matrix game. In such a game, the two players simultaneously choose distributions (or mixed strategies) w and u over rows and columns, respectively, yielding a gain to the row player of w>Pu. According to von Neumann’s celebrated minmax theorem, for any matrix P,\nmax w∈∆K min u∈∆K w>Pu = min u∈∆K max w∈∆K w>Pu,\nthe common value being the value of the game P. A maxmin strategy w or a minmax strategy u is one realizing the max or min on the left- or right-hand side of this equality, respectively. Finding these strategies is called solving the game.\nWe have assumed that the matrix P is itself skew-symmetric, so the game it describes is a symmetric game. Such games are known to have value exactly equal to zero (see, for instance, Owen, 1995, Theorem II.6.2). Working through definitions, this means that w is a maxmin strategy if and only if minu∈∆K w >Pu ≥ 0. But this is exactly equivalent to Eq. (1). Therefore:\nProposition 1 A probability vector w ∈ ∆K is a von Neumann winner for preference matrix P if and only if it is a maxmin strategy for the game P. Consequently, every preference matrix P has a von Neumann winner.\n1. In the literature, the preference matrix P often refers to the matrix of probabilities (P (a, b) + 1)/2. With our modification, P (a, b) becomes anti-symmetric around 0, which simplifies the arguments considerably. 2. See also Appendix A for more compelling evidence that this is indeed the case.\nBefore continuing, we briefly mention some of the other solution concepts that have been proposed to remedy the potential non-existence of a Condorcet winner (Schulze, 2011). Two of these are the Borda winner, the action that has the highest probability of winning a duel against a uniformly random action; and the Copeland winner, the action that wins the most pairwise comparisons (Urvoy et al., 2013). Both of these fail the independence of clones criterion (Schulze, 2011), meaning that adding multiple identical copies of an action can change the Borda or Copeland winner. This criterion is particularly crucial in a dueling bandit setting because a given policy class may contain many identical policies. In contrast, the von Neumann winner performs at least as well as any individual policy, and is thus unaffected by the presence or absence of clones. See Appendix B for a more detailed discussion. Note that if there does happen to exist a Condorcet winner, then it will also be the unique von Neumann winner."
    }, {
      "heading" : "3. Incorporating context",
      "text" : "Next, we consider how the preceding development can be extended to a much more realistic setting in which the best way of acting may depend on additional, observable information, called context. Thus, prior to choosing actions, the learner is allowed to observe some value x, the context, selected by Nature from some unspecified space X . For instance, x might be a feature-vector description of a web user. In this setting, the preference matrix is no longer static; rather, which actions are better than which others now varies and depends on the context, which therefore must be taken into account to fully optimize the choice of actions.\nFormally, we assume that on every round t of the learning process, a context xt and preference matrix Pt are chosen by Nature. The context xt is revealed to the learner, but the preference matrix Pt remains hidden. Based on xt, the learner selects two actions (at, bt) for a duel, whose outcome has expectation determined by the current (hidden) preference matrix Pt in the usual way. Except where noted otherwise, in this paper, we always assume that each pair (xt,Pt) is chosen at random according to independent draws from some unknown joint distribution D.\nThe goal is to determine which action to select as a function of the context. Such a mapping π from contexts x to actions a is called a policy. Typically, we are working with policies of a particular form, that is, from some policy space Π. For instance, this space might represent the set of all decision trees. For simplicity, we assume that Π has finite cardinality. However, we generally think of Π as an extremely large space, exponential in any reasonable measure of complexity.\nThe notion of von Neumann winner (as well as other concepts, like Condorcet winner) can be extended to incorporate context essentially by reducing to the non-contextual setting. We can regard each policy π as a “meta-action,” and define a |Π| × |Π| preference matrix M over these meta-actions. Thus, the rows and columns of M are each indexed by policies in Π, and\nM(π, ρ) = E(x,P)∼D [P (π(x), ρ(x))]. (2)\nThis quantity is the expected outcome when a “meta-duel” is held between the two policies π and ρ, whose stochastic outcome is determined by randomly selecting (x,P) ∼ D, and then holding an ordinary duel on P between the actions π(x) and ρ(x). This huge matrix M thus encodes the probability of any policy beating any other policy in a duel.\nWe can now define von Neumann winner in the contextual setting to be an (ordinary) von Neumann winner for the matrix M (regarded here as a kind of “meta-preference-matrix”). Thus, unraveling definitions, a (contextual) von Neumann winner is a probability distribution W over policies\nsuch that for every opposing policy ρ, if π is chosen at random from W, then the probability that π beats ρ in a duel is at least 1/2. That is, the randomized policy defined by W beats or ties every policy in the space Π. By Proposition 1 (applied to M), such a von Neumann winner must exist.\nFor the rest of the paper, we study how to compute (or approximate) contextual von Neumann winners. Of course, because the space Π and corresponding matrix M are both gigantic, this will present significant computational challenges."
    }, {
      "heading" : "4. Learning scenarios",
      "text" : "We consider two possible learning scenarios. In the simpler of these, called explore-then-exploit, we suppose that the learner is allowed to explore for some number of rounds m (where, as described above, on each round, the learner is presented with a random context and permitted to run and observe the outcome of a duel between a pair of actions of its choosing). At the end of these m rounds, the learner outputs a distribution Ŵ over policies in Π. The learner’s goal is to produce Ŵ which is an ε-approximate von Neumann winner, that is, for which\nmin U∈∆|Π|\nŴ>MU ≥ −ε\nfor some small ε > 0. In other words, for all π ∈ Π, Ŵ should beat π with probability at least 1/2− ε/2. Naturally, m should be “reasonable” as a function ε. This setting is almost like learning from a passively selected batch of training examples, except that the learner has an active role in selecting which actions to play in each duel.\nIn the alternative full-explore-exploit setting, learning occurs in a fully online manner across T rounds (in the manner described earlier), with performance measured using some notion of regret. In this paper, where we are working with policies and changing preference matrices, we propose to define regret to be\nmax π∈Π\n1\n2 T∑ t=1 [ Pt(π(xt), at) + Pt(π(xt), bt) ] . (3)\nIf we can find an algorithm for which this regret is o(T ), then eventually the algorithm selects actions (at, bt) which cannot be beaten by any other policy π ∈ Π.\nIn the standard dueling-bandits setting with a static preference matrix, a seemingly different definition of regret was used by Yue et al. (2009) in terms of an assumed Condorcet winner. However, when specialized to their setting, and when provided with their same assumptions, their definition can be shown to be equivalent (up to constant factors) to Eq. (3).\n5. Sparring Exp4.P\nOur goal then is to find, approximate or perform as well as a von Neumann winner, which, as we have seen, is a maxmin strategy for a particular game. Under this interpretation, it becomes especially natural to use ordinary no-regret learning algorithms as players of this game since it is known that such algorithms, when properly configured for this purpose, will converge to maxmin or minmax strategies (Freund and Schapire, 1999). The idea is simply to run two independent copies of such an algorithm against one another. Such a “sparring” approach was previously proposed for dueling bandits by Ailon et al. (2014), though without details, and not in the contextual setting.\nWe consider using the multi-armed bandit algorithm Exp4.P (Beygelzimer et al., 2011) for this purpose in the full-explore-exploit setting. Exp4.P is well-suited since it is designed to work with partial information as in our bandit setting, and since it can handle the kind of adversarially generated data that arises unavoidably when playing a game. It also is designed to work with policies in a contextual setting like ours (or, more generally, to accept the advice of “experts”).\nThe learning setting for Exp4.P is as follows (somewhat, but straightforwardly, modified for our present purposes). There are K possible actions, 1, . . . ,K, and a finite space Π of policies. On each round t = 1, . . . , T , an adversary chooses and reveals a context xt, and also chooses, but does not reveal rewards rt(1), . . . , rt(K) ∈ [−1,+1] for each of the K actions. The learner then selects an action at, and receives the revealed reward rt(at). The learner’s total reward is thus GA = ∑T t=1 rt(at), while the reward of each policy π is Gπ = ∑T t=1 rt(π(xt)). The learner’s goal is to receive reward close to that of the best policy. Beygelzimer et al. (2011) prove that (subject to very benign conditions) with probability at least 1− δ, Exp4.P achieves reward\nGA ≥ max π∈Π\nGπ − 12 √ KT ln(|Π|/δ). (4)\n(This holds for any δ > 0; the δ is passed as a parameter to the algorithm.) For contextual dueling bandits, we run two separate copies of Exp4.P which are played against one another; let us call them row-Exp and column-Exp. We use the same actions, contexts, and policies for the two copies as for the original problem. On each round t, Nature chooses a context xt and a preference matrix Pt. The context (but not the preference matrix) is revealed to row-Exp and column-Exp, which select actions at and bt, respectively. A duel is then held between these two actions; the outcome r is passed as feedback to row-Exp (for its chosen action at), and its negation −r is similarly passed to column-Exp. We call this algorithm SparringEXP4.P.\nTheorem 2 Consider K actions, policy space Π, and time horizon T . Fix parameter δ > 0. Then with probability at least 1− δ, SparringEXP4.P achieves regret at most O( √ KT ln(|Π|/δ)).\nThe proof is in Appendix C. This result holds also for an adversarial environment in which the pairs (xt,Pt) are selected by an adversary rather than at random. Also, we can adapt this algorithm for explore-then-exploit learning using the following standard technique for online-tobatch conversion. Run SparringEXP4.P for m exploration rounds. In each round i, row-Exp internally computes a distribution wi over policies. Then w = (1/m) ∑m i=1 wi is an ε-approximate\nvon Neumann winner where ε = O( √ K ln(|Π|/δ)/m).\nAlthough yielding very good regret bounds and handling adversaries, this approach requires time and space proportional to |Π|, and is therefore not practical for extremely large policy spaces."
    }, {
      "heading" : "6. Explore-then-exploit algorithms with a classification oracle",
      "text" : "We next begin a development that will lead to efficient methods (in terms of time, space and data) for handling even extremely large policy spaces, under a particular assumption discussed below. We describe a general approach for exploration, for using the collected data to find a statistically sound solution, and for reducing the problem that must be solved to a more tractable form.\nWe focus mainly on the explore-then-exploit problem. Thus, we have m exploration rounds, and on each round i, a pair (xi,Pi) is selected at random, and the learner is permitted to choose and observe the outcome of a single duel (ai, bi). Although xi is observed, Pi is not. Here, we propose\na simple exploration strategy, called uniform exploration, in which each dueling pair (ai, bi) is selected uniformly at random. Let ri be the resulting observed outcome. Based on these, the learner can obtain a noisy but unbiased version of the hidden preference matrix Pi. Specifically, let us define a matrix P̂i where P̂i(ai, bi) = K2ri, and all other entries P̂i(a, b) are set to zero. It can be verified that the expected value of each entry P̂i(a, b) is exactly Pi(a, b); that is, E[P̂i |Pi] = Pi.\nIn Appendix D, we extend our setting to an arbitrary unbiased estimator P̂i of Pi, and in particular to an arbitrary exploration strategy that does not change adaptively over time. This extension is parameterized by upper bounds on the absolute value and the variance of P̂i(a, b) for all rounds i and all actions (a, b). For uniform exploration, both upper bounds are K2.\nWhile non-adaptive exploration strategies usually lead to suboptimal statistical performance, they are often preferable in practice. This is because in large-scale industrial applications the existing infrastructure is often insufficient to support a feedback loop that would update the exploration strategy adaptively over time, and upgrading the infrastructure may be infeasible in the near term. Statistical guarantees. With these noisy versions of the empirical preference matrices, we can estimate the expected outcome in a “meta-duel” between two policies π and ρ, that is, an entry of the matrix M defined in Eq. (2). In particular, let\nM̂(π, ρ) = 1\nm m∑ i=1 P̂i(π(xi), ρ(xi)). (5)\nThen the expected value of this quantity isM(π, ρ), the corresponding entry of M. Moreover, using Bernstein’s inequality and the union bound, we can show that, with probability at least 1− δ,∣∣∣M̂(π, ρ)−M(π, ρ)∣∣∣ ≤ ε′ for all (π, ρ) ∈ Π×Π, (6) where ε′ = O ( K √ ln(|Π|/δ)/m ) . 3 Thus, although huge, the matrix M is well-approximated\nby the matrix M̂ using only a moderately sized sample. In fact, to find an approximate maxmin strategy for M it suffices to find one for M̂, which will be the approach taken by our algorithms.\nLemma 3 Given the set-up above, suppose that Eq. (6) holds (as will be the case with probability at least 1− δ), and suppose further that Ŵ ∈ ∆|Π| is a probability vector for which\nmin U∈∆|Π| Ŵ>M̂U ≥ max W∈∆|Π| min U∈∆|Π| W>M̂U− ε.\nThen Ŵ is a (2ε′ + ε)-approximate von Neumann winner for M.\nA more compact version of the problem. Our aim now is to find an approximate maxmin strategy for the matrix M̂. Although this matrix is gigantic in both dimensions, by leveraging how it was constructed from only a small number of empirical observations, we can re-express the problem in a far more compact form. To this end, let us define, for each policy π ∈ Π, a policy vector vπ ∈ RmK that encodes the behavior of π on the exploration data. For readability, although a vector, we index entries of vπ by pairs (i, a), where i is a round and a is an action, and we define\nvπ(i, a) = 1{π(xi) = a}/ √ m.\n3. The proof for Eq. (6) and the subsequent Lemma 3 are in Appendix D.1.\nThus, vπ is broken into m length-K blocks, with block i encoding in a natural way the action selected by π on xi. (The constant 1/ √ m is for normalization.)\nWe also define an mK ×mK block-diagonal matrix B, where the m blocks along the diagonal are exactly the K ×K matrices P̂i described above. Formally, using the earlier indexing,\nB((i, a), (j, b)) = 1{i = j}P̂i(a, b).\nWorking through these definitions, it can be verified that for any two policies π and ρ, the quantity v>πBvρ is exactly equal to M̂(π, ρ) as defined in Eq. (5). This means that if W and U are probability vectors over Π, then\nW>M̂U = (∑ π∈ΠW (π)vπ )> B (∑ ρ∈Π U(ρ)vρ ) .\nTherefore, the problem of finding a maxmin strategy for M̂ is equivalent to solving max w∈C min u∈C w>Bu (7)\nwhere C is the convex hull of the set of all policy vectors {vπ : π ∈ Π} (henceforth, the policy hull). Furthermore, a solution w ∈ C is necessarily a convex combination of vectors vπ, and therefore corresponds to a probability vector over policies.\nThe formulation given in Eq. (7) shows that B should itself be viewed as a game matrix, and that our remaining goal is to approximately solve this game. This matrix has the advantage of being far smaller than M̂. However, unlike a conventional matrix game, the space from which the players’ vectors w and u are chosen is not the standard space of probability vectors over actions, but rather the convex hull of an exponentially large set of vectors. Classification oracle. Our algorithms assume that the policy space Π is structured in a way that admits a certain computational operation that is quite natural in the realm of learning. Specifically, we assume the existence of a classification oracle. The input to this oracle is a sequence of cost vectors c1, . . . , cm, each in RK , with the interpretation that ci(a) is the cost of choosing action a on context xi. The output of the oracle is the policy in Π with minimum cost, that is,\nargmin π∈Π m∑ i=1 ci(π(xi)). (8)\nIndeed, regarding the xi’s as examples, the actions a as labels or classes, and the policies π as classifiers, we see that this oracle is in fact solving an empirical, cost-sensitive, multi-class classification problem. Thus, the assumption of such an oracle is an idealization based on the numerous cases in which effective classification algorithms already exist. In practice, the policy space Π is usually defined as the space of all possible policies returned by a given classification algorithm, and we hope that our methods will be effective when using ordinary off-the-shelf classification algorithms as oracle.\nEquivalently, the classification oracle can be described in terms of policy vectors. Specifically, the cost vectors above can be identified with their concatenation, a single vector c ∈ RmK , divided naturally into m blocks. Then the problem given in Eq. (8) is the same as\nargmin w∈C\nc ·w,\nwhere the argmin is over the policy hull C defined above. This is because the minimum, without loss of generality, will be a policy vector vπ, where π minimizes Eq. (8). Therefore, in what follows, we use expressions of this latter form to indicate an invocation of our assumed classification oracle. Algorithms and end-to-end guarantees. We design algorithms that compute an approximate von Neumann winner Ŵ by solving the optimization problem in Eq. (7). Although there exist many methods for solving such a game, the challenge here is the requirement that the solution be in the policy hull C. As already seen in Section 5, regret minimization algorithms are a natural choice. However, most standard algorithms will not conform to this constraint. In the sections that follow, we provide two algorithms: Algorithm SparringFPL that builds on the Follow-the-PerturbedLeader algorithm of Kalai and Vempala (2003), and Algorithm ProjectedGD that builds on online projected gradient descent methods of Zinkevich (2003).\nFor a given approximation quality, the performance of either algorithm is characterized by several quantities: the sufficient number of exploration rounds, the running time, the storage requirement, and the number of policies in the support of Ŵ. As it turns out, the key quantities are the number of exploration rounds and the number of oracle calls. We assume each oracle call returns both a policy vector and a corresponding policy, each representable using b bits.4 The solution Ŵ is specified by explicitly listing the probabilities and policies in its support.\nTheorem 4 Consider K actions and a policy class Π with b-bit representation. Fix parameters ε, δ > 0. Both SparringFPL and ProjectedGD compute an ε-approximate von Neumann winner Ŵ with probability 1 − δ using m = O((K2/ε2) ln(|Π|/δ)) exploration rounds with uniform exploration strategy. The number of oracle calls is N = O((K6/ε4) ln(|Π|/δ)) for SparringFPL andN = O(K8/ε4) for ProjectedGD. For both algorithms, disregarding oracle calls, the running time is O(mKN), the storage requirement is O(bN), and Ŵ is a distribution over at most N policies.\nWhile SparringFPL is very simple and intuitive, ProjectedGD achieves a better number of oracle calls whenever K √ ln(|Π|/δ).\nOur algorithms can be used in the full-explore-exploit setting as well: afterm exploration rounds with uniform exploration strategy, Ŵ is computed and used in the remaining rounds for both actions. The parameter m is chosen in advance as a function of the time horizon T . The statistical performance is expressed via regret, as defined in Eq. (3). The total running time is dominated by the time to compute Ŵ.5\nTheorem 5 (regret) Consider K actions, time horizon T ≥ K, and a policy class Π with b-bit representation. Fix a parameter δ > 0. Both SparringFPL and ProjectedGD achieve regret O(K2/3 T 2/3 Ψ1/3) with probability 1 − δ, where Ψ = ln(|Π|/δ). The number of oracle calls is N = O(K10/3 T 4/3 Ψ−1/3) for SparringFPL and N = O(K16/3 T 4/3 Ψ−4/3) for ProjectedGD. For both algorithms, disregarding oracle calls, the total running time is O(mKN), the storage requirement is O(bN), and the number of exploration rounds is m = O(K2/3 T 2/3 Ψ1/3).\n4. Often, policies are specified as a parameter vector to some algorithm that implements them. For finite classes, it is usually the case that b is roughly O(ln |Π|). 5. The running time to execute Ŵ in each of the exploitation rounds (i.e., to compute the random action for a given context) is a low-order term; we omit further details from this version.\n7. Solving the compact game with SparringFPL\nOur first algorithm to solve Eq. (7), SparringFPL, is based on the Follow-the-Perturbed-Leader (FPL) algorithm of Kalai and Vempala (2003). FPL is designed for a standard online learning problem: Let D and L be subsets of RmK . On each round t = 1, . . . , N , the learner chooses a decision vector dt ∈ D, and then receives a loss vector `t ∈ L. The learner’s goal is to minimize its cumulative loss ∑N t=1 dt · `t relative to the best possible loss using a fixed decision, that is,\nmind∈D ∑N\nt=1 d ·`t. FPL chooses dt as the best such vector based on a slightly perturbed version of the preceding losses. Namely, letting pt ∈ RmK be chosen uniformly at random from [0, 1/α]mK ,\ndt = argmin d∈D\nd · (∑t−1\nτ=1 `τ + pt\n) .\nWe solve Eq. (7) by sparring two copies of FPL, called row-FPL and column-FPL, in the fashion of a repeated game. On every round t, row-FPL uses FPL to select a vector wt, while column-FPL uses a different copy of FPL to select a vector ut. We then define the resulting loss vectors to be −But for row-FPL, and B>wt for column-FPL. Here is the complete algorithm:\n• For t = 1, . . . , N : – Choose uniform random perturbations pt, qt from [0, 1/α]mK . – Let wt = argminw∈C w · [−B(u1 + · · ·+ ut−1) + pt]. – Let ut = argminu∈C u · [ B>(w1 + · · ·+ wt−1) + qt ] .\n• Output w = 1N ∑N t=1 wt\nThe argmin expressions in the algorithm are implemented using the classification oracle. The returned vector w is in C, and in fact corresponds to a uniform mixture of N policies.\nIn Appendix D.2, we show that to find an ε-approximate solution to Eq. (7) with probability 1−δ, it suffices to useN = O(K4/ε2)(m+ln(1/δ)) steps of the algorithm with α = √ 2/(K4N), which in turn implies Theorems 4 and 5 for SparringFPL.\n8. Solving the compact game with ProjectedGD\nOur second algorithm, called ProjectedGD, solves Eq. (7) using online projected gradient descent methods as studied by Zinkevich (2003). The algorithm maintains a vector wt ∈ C corresponding to a strategy for the row player. On every round, a column strategy ut ∈ C is chosen that is a “best response” to wt. The strategy wt is updated by taking a small gradient step. The resulting vector zt+1 is likely to be outside the set C, and therefore is (approximately) projected back to C, yielding wt+1. The algorithm is as follows:\n• Choose any w1 ∈ C • For t = 1, . . . , Nout:\n– ut = arg minu∈C w>t Bu – zt+1 = wt + ηBut – wt+1 = ApproxProject(zt+1,wt)\n• Output w = 1Nout ∑Nout t=1 wt\nIdeally, we would like for wt+1 to be the exact Euclidean projection of zt+1 onto C, but instead need to settle for an approximation. For this purpose, the procedure ApproxProject(z,v1), described below, computes an approximate projection of an arbitrary vector z onto C. It takes as an input a second vector v1 that is already in C, and which we can think of as an initial guess at the actual projection. The quality (as an approximation) of the returned vector v is allowed to depend on how close v1 is to z. Specifically, we require that, for all s ∈ C, and a constant α specified later,\n‖s− v‖2 ≤ ‖s− z‖2 + α · ‖v1 − z‖. (9)\nIn Appendix D.3, we show that with the parameter η = 2/(L √ Nout) our algorithm finds an\nε-approximate solution to Eq. (7), where ε = 2L/ √ Nout + Lα/2.\nComputing approximate projections. It remains to describe the approximate-projection procedure ApproxProject(z,v1). Given an arbitrary vector z and another vector v1 ∈ C, the goal of the algorithm, as in Eq. (9), can be restated as that of finding a vector v ∈ C for which\nmin s∈C\nF (s,v) ≥ −α · ‖v1 − z‖ (10)\nwhere we define F (s,v) = ‖s− z‖2−‖s− v‖2 = 2s ·(v−z)+‖z‖2−‖v‖2. Note that F is linear in s (for each v), and concave in v (for each s). To ensure that Eq. (10) holds, we give an algorithm that aims to maximize the left-hand side of this inequality. (As a side note, the maximizing vector turns out to be exactly the projection of z onto C, although we do not require that fact for our algorithm and analysis.)\nTo this end, we use an algorithm that resembles repeated play of a game in which the payoff is defined by F . The s player uses best response on each round, while the v player again uses a variant of online gradient ascent applied to the function F (st, ·). The algorithm takes a parameter ν ∈ (0, 1], and uses v1 ∈ C, which was provided as an argument to ApproxProject(z,v1), as the initial vector. Here is the algorithm:\n• For t = 1, . . . , Nin:\n– st = arg mins∈C s · (vt − z) – vt+1 = (1− ν)vt + νst\n• Output v = 1Nin ∑ t vt\nNote that vt is in C for every t (by convexity of C), and therefore v is as well. In Appendix D.4, we show that ApproxProject(z,v1) with parameter ν = ‖z− v1‖/ √ Nin computes v that satisfies Eq. (10) with α = 8/ √ Nin. We optimize the choice of Nin and Nout to show that one can obtain an ε-approximate solution to Eq. (7) using only O(K8/ε4) oracle calls. This in turn implies Theorems 4 and 5 for ProjectedGD."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We would like to thank Alekh Agarwal for insightful comments and discussions."
    }, {
      "heading" : "Appendix A. Failure of the Condorcet winner to exist",
      "text" : "Here, we investigate the reliability of the Condorcet assumption by replicating the experiment of (Zoghi et al., 2014a, Section 6.1) with a small modification. As in their setting, we consider a family of K-armed dueling bandit problems arising from the ranker evaluation problem in IR, where the comparisons are carried out using Probabilistic Interleave (Hofmann et al., 2011) and the preferences are generated using click models simulating user behavior (Guo et al., 2009a,b). The rankers are sampled randomly from the set of 136 rankers provided with the MSLR dataset.6 However, unlike the experiments of Zoghi et al. (2014a), we use an informational click model, rather than a perfect one (Hofmann et al., 2013). The former simulates the behavior of a user who is seeking general information about a broad topic, while the latter represents an idealized user, who meticulously examines every document in the retrieved list. We believe that the informational click model is more realistic and therefore use it here.\nThe plot in Figure 1 shows the probability with which the encountered dueling bandit problems contain Condorcet winners. As this figure demonstrates, in this setting, the occurrence of the Condorcet winner drops rapidly as the number of rankers grows.\nThis shows that even in this simple non-contextual example the assumption that there exists a Condorcet winner is too unreliable to be practical. Needless to say that in the contextual dueling bandit problem, where one is dealing with a potentially very large and diverse set of policies, the likelihood of one policy dominating every single other policy is even more unrealistic."
    }, {
      "heading" : "Appendix B. Comparison between the Copeland and von Neumann winners",
      "text" : "A Copeland winner is defined to be any arm that beats the largest number of other arms. It is a generalization of the Condorcet winner in the sense that if the Condorcet winner exists, it will be a Copeland winner. However, we claim that the von Neumann winner is a more natural generalization than the Copeland winner for the following two reasons: first, in the absence of a Condorcet winner, Copeland winners, both individually and as a collective, can lose to an arm that is not a Copeland winner, whereas the von Neumann winner beats or ties with every single arm; second, the set of Copeland winners can be altered by the introduction of “clones,” i.e., arms whose corresponding rows of the preference matrix are identical to each other.\nTo demonstrate this lack of stability of Copeland winners, consider any K + 3-armed example with K > 4, where arms a1, a2 and a3 beat all other arms and the three of them are in a cycle, with a1 beating a2, a2 beating a3 and a3 beating a1 all with probability 1. It is easy to see that these three arms are the only Copeland winners with Copeland score equal to K + 1 and also form the support of the von Neumann distribution: indeed, the von Neumann distribution is simply the uniform distribution on these three arms. Now, let us consider a slight modification of this problem, where we add one more arm, called a0, which is a duplicate of arm a1; hence P (0, 1) = 0 and P (0, j) = P (1, j) for all j > 1. In the following we explain what happens to the set of Copeland winners after this modification. In the presence of ties, there are three sensible definitions that one could use for the Copeland score; these definitions and the corresponding scores for the top four arms in our modified example can be found in Table 1. As the quantities in this table show, regardless of the definition of the Copeland score used, the set of Copeland winners for our new\n6. http://research.microsoft.com/en-us/projects/mslr/default.aspx\nK+ 4-armed dueling bandit problem does not contain all of a0, . . . , a3. Indeed, under no definition can arm a2 be considered a Copeland winner.\nOn the other hand, arms a0, a1, a2, a3 still form the support of the von Neumann distribution of this modified dueling bandit problem: if we assign weights w(0), w(1), w(2), w(3) to these four arms such that\nw(0) + w(1) = w(2) = w(3) = 1\n3\nand sample an arm ai according to these weights, we will (on average) beat any aj with j > 3 and tie with all aj with j ≤ 3.\nWe consider the lack of stability under cloning illustrated by this example to be a major drawback of the Copeland score as a measure of quality.\nFurthermore, as the following 5-armed preference matrix illustrates, the von Neumann winner does not necessarily contain the Copeland winner in its support:\nP =  0 0.5 −0.5 0.5 −0.95 −0.5 0 0.5 −0.2 0.5 0.5 −0.5 0 −0.2 0.5 −0.5 0.2 0.2 0 0.5\n0.95 −0.5 −0.5 −0.5 0  Indeed, the von Neumann winner of this matrix is the uniform distribution on the first three arms, as can be easily checked by multiplying the row vector [1/3 1/3 1/3 0 0] with P, while the Copeland winner is the fourth arm, since the Copeland scores of the 5 arms are [2 2 2 3 1]. Moreover, the fourth arm also happens to be both the Borda winner (Urvoy et al., 2013) and the Random Walk winner (Negahban et al., 2012; Busa-Fekete and Hüllermeier, 2014). The Borda winner is the arm with the highest chance of winning a comparison against a uniformly randomly chosen opponent, i.e., the arm corresponding to the row in the preference matrix whose entries have the highest sum: in this case the Borda scores are [0.455 0.53 0.53 0.54 0.445]. The Random Walk winner is obtained as follows: first, we convert the “probabilistic” preference matrix (i.e., P/2 + 0.5) into a columnwise stochastic matrix (by dividing each column by its sum), then find the stationary distribution of the Markov chain defined by this matrix (by finding the right eigenvector of the stochastic matrix corresponding to eigenvalue 1), and, finally, declare the arm with the highest probability under this\nstationary distribution to be the Random Walk winner. In this particular example, the stationary distribution is [0.198 0.212 0.204 0.217 0.169] and so the Random Walk winner is the fourth arm, as mentioned before.\nDespite the above observations, in practice, the Copeland winner and the von Neumann winner tend to agree to a large extent. For instance, in preference matrices sampled from the MSLR dataset, as described in Appendix A, in over 99.9% of the examples, the von Neumann winner contained at least one Copeland winner. Moreover, in the overwhelming majority of the cases, all Copeland winners were assigned non-zero probability by the von Neumann winner, although the percentage of cases where this phenomenon occurs is slightly lower than the above figure and dependent on the number of arms (see Figure 2). Based on these observations, the Copeland winner can roughly be thought of as a more restrictive notion than the von Neumann winner.\nFurthermore, as Figure 3 demonstrates, the arms in the support of the von Neumann winner tend to have high Copeland scores (or equivalently, low Copeland losses) in practice. Given this close relation between these two notions of winners, a natural question becomes whether the recent improvements made in solving the Copeland dueling bandit problem (Zoghi et al., 2015a) can be used to speed up the task of finding the von Neumann winner.\nAnother aspect of the von Neumann winner that might be disconcerting when first encountered is the fact that it is a distribution, which in theory can put non-zero probability on all arms; however, in practice, this is very far from being the case. Indeed, among the over a million preference matrices sampled from the MSLR dataset, not a single one had a von Neumann winner that assigned non-zero probability to more than 5 arms. In fact, in the vast majority of the cases, the von Neumann winners had supports of size 1 or 3 (see Figure 4). Note that fewer than 0.03% of preference matrices had von Neumann winners whose support contains 5 arms.\nAppendix C. Analysis of SparringEXP4.P (proof of Theorem 2)\nTo fulfill the requirements of the learning model for Exp4.P, we also need to define rewards rt(a) for all of the actions that were not chosen. Furthermore, these rewards need to be defined before each copy of the algorithm chooses its action (or, more technically, in a manner that is conditionally independent of each copy’s choice). To this end, for every pair of actions (a, b), we define a {−1,+1}-valued random variable Rt(a, b) with the expected value Pt(a, b). Thus, Rt(a, b) can be viewed as the outcome of a hypothetical duel between actions a and b. These values are only used\nfor the mathematical argument, and do not literally have to be computed. Only the pair (at, bt) is actually used in a duel.\nFor row-Exp, based on column-Exp’s chosen action bt, we then define rewards r (row) t (a) = Rt(a, bt) for all a. And similarly, for column-Exp, based on row-Exp’s chosen action at, we define rewards r(col)t (b) = −Rt(at, b) for all b. In particular, this means that row-Exp receives, for its chosen action at, the rewardRt(at, bt) (that is, the result of a duel between at and bt), while columnExp receives the reward −Rt(at, bt) for its chosen action bt.\nLet us first take the point of view of row-Exp. Plugging in to Eq. (4), we have that, with probability at least 1− δ/4, for all π ∈ Π,\nT∑ t=1 Rt(at, bt) ≥ T∑ t=1 Rt(π(xt), bt)−O( √ KT ln(|Π|/δ)).\nFurther, using Azuma’s lemma and union bound, we can show that, with probability at least 1−δ/4, for every π ∈ Π\nT∑ t=1 Rt(π(xt), bt) ≥ T∑ t=1 Pt(π(xt), bt)−O( √ KT ln(|Π|/δ)).\nSimilarly, from column-Exp’s perspective, with probability at least 1− δ/4, for all π ∈ Π,\n− T∑ t=1 Rt(at, bt) ≥ − T∑ t=1 Rt(at, π(xt))−O( √ KT ln(|Π|/δ))\nand, by Azuma’s lemma and the skew-symmetry of Pt, with probability at least 1− δ/4, for every π ∈ Π,\n− T∑ t=1 Rt(at, π(xt)) ≥ T∑ t=1 Pt(π(xt), at)−O( √ KT ln(|Π|/δ)).\nCombining and rearranging now yields the theorem.\nAppendix D. Analysis of SparringFPL and ProjectedGD\nCompared to the presentation in the body of the paper, we extend our setting from uniform exploration strategy to an arbitrary unbiased estimator P̂i of Pi, i.e., to any matrix P̂i which satisfies E[P̂i |Pi] = Pi. Our results are parameterized by two numbers, L, V , such that |P̂i(a, b)| ≤ L and Var(P̂i(a, b)) ≤ V for all exploration rounds i and all action pairs (a, b). For uniform exploration, both upper bounds are K2.\nD.1. Reduction from M to M̂ (proof of Lemma 3)\nIn this subsection, we prove Lemma 3 which reduces the optimization problem to that on the approximate matrix M̂ computed from the data. As a first step, we prove Eq. (6) which relates M̂ to the true preference matrix for M.\nProof [Eq. (6)] Let Z1, . . . , Zm be independent, identically distributed random variables, each taking values in [−R,R], and each having mean zero and variance V . Then according to Bernstein’s inequality, the probability that the average A = (1/m) ∑m i=1 Zi exceeds some value s is at most\nexp ( − s 2/2\nmV +Rs/3\n) .\nFor an appropriate choice of s, this implies that, with probability at least 1− δ,\nA ≤ 2R ln(1/δ) 3m +\n√ 2V ln(1/δ)\nm . (11)\nTo derive Eq. (6), for a fixed pair of policies π and ρ, we can letZi = P̂i(π(xi), ρ(xi))−M(π, ρ) whose mean is zero and variance is at most V ; further, |Zi| ≤ 1 +L. Plugging into Eq. (11) implies that Eq. (6) holds with\nε′ = 2(1 + L) ln(|Π|2/δ)\n3m +\n√ 2V ln(|Π|2/δ)\nm\nwith probability at least 1 − δ/|Π|2. By the union bound, with probability at least 1 − δ, this will hold simultaneously for all policies π and ρ.\nProof [Lemma 3] Eq. (6) implies that W>M̂U is within ε′ of W>MU, for all probability vectors W and U. Therefore,\nmin U∈∆|Π| Ŵ>MU ≥ min U∈∆|Π| Ŵ>M̂U− ε′\n≥ max W∈∆|Π| min U∈∆|Π|\nW>M̂U− ε′ − ε\n≥ max W∈∆|Π| min U∈∆|Π|\nW>MU− 2ε′ − ε\n= −(2ε′ + ε).\nD.2. Analysis of SparringFPL\nTo analyze SparringFPL, we build on the provable guarantees for FPL. For convenience, let us recap the learning setting for FPL. Let D and L be subsets of RmK . On each round t = 1, . . . , N , the learner chooses a decision vector dt ∈ D, and then receives a loss vector `t ∈ L. The learner’s goal is to minimize its cumulative loss ∑N t=1 dt · `t relative to the best\npossible loss using a fixed decision, that is, mind∈D ∑N\nt=1 d · `t. Kalai and Vempala (2003, Theorem 1.1) prove the following (slightly simplified) result: assume that D, R and A are such that for all d ∈ D and ` ∈ L we have that ‖d‖1 ≤ D, |d · `| ≤ R and ‖`‖1 ≤ A. Also, let α = √ 2D/(RAN). Then, for any sequence `1, . . . , `N ∈ L,\nE [ N∑ t=1 dt · `t ] ≤ min d∈D N∑ t=1 d · `t + 2 √ 2DRAN. (12)\nwhere the expectation is over the random choice of perturbations. Kalai and Vempala prove this in the oblivious case when the adversary has fixed the `t’s ahead of time. However, this restriction can be relaxed to allow each `t to be selected adaptively in a possibly stochastic fashion that may depend on the entire preceding history through round t − 1, but not on the perturbation pt for the current round. Using a martingale argument and Azuma’s lemma (see also Cesa-Bianchi and Lugosi, 2006, Lemma 4.1), it can then be shown that, with probability at least 1− δ′,\n1\nN N∑ t=1 dt · `t ≤ min d∈D 1 N N∑ t=1 d · `t + 2 √ 2DRA/N + 2R √ 2 ln(1/δ′)/N. (13)\nTheorem 6 In SparringFPL, set parameter α = √\n2/(L2N). Then with probability at least 1−δ, the vector w returned by the algorithm satisfies\nmin u∈C w>Bu ≥ max w∈C min u∈C w>Bu− 2ε\nwhere ε = 2L √ 2m/N + 2L √ 2 ln(2/δ)/N .\nThus, to find an ε-approximate solution, we can choose N to be O(L2/ε2)(m+ ln(1/δ)). This also gives a bound on the number of oracle calls (it is called twice per round). Proof Note that in our case, we can choose D = √ m, A = L √ m and R = L.\nLet u = 1N ∑N\nt=1 ut. Then we have the following chain of inequalities holding with probability at least 1− δ:\nmin u∈C max w∈C w>Bu− ε ≤ max w∈C w>Bu− ε\n≤ 1 N N∑ t=1 w>t But (14)\n≤ min u∈C\nw>Bu + ε (15)\n≤ max w∈C min u∈C\nw>Bu + ε.\nHere, Eqs. (14) and (15) follow directly from Eq. (13) applied, respectively, to row-FPL and columnFPL with δ′ = δ/2. Noting that\nmax w∈C min u∈C w>Bu = min u∈C max w∈C w>Bu,\nthe theorem now follows.\nD.3. Analysis of ProjectedGD: outer loop\nUsing an analysis similar to Zinkevich (2003), but for a fixed learning rate, and taking into account the errors introduced by imperfect projections, we can show the following:\nLemma 7 For the algorithm ProjectedGD with η = 2/(L √ Nout), we have\n1\nNout Nout∑ t=1 w>t But ≥ max w∈C 1 Nout Nout∑ t=1 w>But − ε\nwhere ε = 2L/ √ Nout + Lα/2.\nProof For all w ∈ C, we have\n‖w −wt+1‖2 − ‖w −wt‖2 ≤ ‖w − zt+1‖2 − ‖w −wt‖2 + αηL (16) = −2η(w −wt)>But + η2‖But‖2 + αηL (17) ≤ −2η(w −wt)>But + η2L2 + αηL.\nHere, Eq. (16) uses Eq. (9), applied to our case where we have zt+1−wt = ηBut. Eq. (17) follows from straightforward algebra. Since ‖w −w1‖ ≤ 2, summing over t = 1, . . . , Nout yields, for all w ∈ C,\n−4 ≤ ‖w −wNout+1‖2 − ‖w −w1‖2\n≤ −2η Nout∑ t=1 w>But + 2η Nout∑ t=1 w>t But + η 2L2Nout + αηLNout.\nRe-arranging completes the lemma.\nWe can prove that the returned vector w is an ε-approximate maxmin solution using a technique similar to Freund and Schapire (1999). (Alternatively, we could use the average of the ut’s which is an ε-approximate minmax solution by the same proof.)\nTheorem 8 The vector w satisfies min u∈C w>Bu ≥ max w∈C min u∈C w>Bu− ε where ε is as Lemma 7.\nProof Let u = 1Nout ∑Nout t=1 ut. Then\nmax w∈C min u∈C w>Bu ≥ min u∈C w>Bu\n= min u∈C\n1\nNout Nout∑ t=1 w>t Bu\n≥ 1 Nout Nout∑ t=1 min u∈C w>t Bu\n= 1\nNout Nout∑ t=1 w>t But\n≥ max w∈C\n1\nNout Nout∑ t=1 w>But − ε\n= max w∈C\nw>Bu− ε\n≥ min u∈C max w∈C\nw>Bu− ε.\nD.4. Analysis of ProjectedGD: inner loop\nIt remains to analyze the inner loop of ProjectedGD, i.e., the approximate-projection procedure ApproxProject(z,v1).\nLet v∗ be the projection of z onto the policy hull C. We can prove the following for this algorithm using ‖v∗ − vt‖2 as a potential function. Lemma 9 For the algorithm ApproxProjectwith ν = ‖z− v1‖/ √ Nin and δ = 8‖z− v1‖/ √ Nin, we have 1\nNin Nin∑ t=1 F (st,vt) ≥ 1 Nin Nin∑ t=1 F (st,v ∗)− δ.\nProof We have\n‖v∗ − vt+1‖2 − ‖v∗ − vt‖2 = ‖v∗ − vt + ν(vt − st)‖2 − ‖v∗ − vt‖2\n= 2ν(v∗ − vt) · (vt − st) + ν2‖vt − st‖2 ≤ 2ν(v∗ − vt) · (vt − st) + 4ν2 (18) ≤ ν(F (st,vt)− F (st,v∗)) + 4ν2. (19)\nEq. (18) uses ‖vt − st‖ ≤ ‖vt‖+ ‖st‖ ≤ 2. To see Eq. (19), note that\n2(v∗ − vt) · (vt − st) = 2v∗ · vt − 2‖vt‖2 − 2st · (v∗ − vt) ≤ ‖v∗‖2 + ‖vt‖2 − 2‖vt‖2 − 2st · (v∗ − vt) = [ 2st · (vt − z) + ‖z‖2 − ‖vt‖2\n] − [ 2st · (v∗ − z) + ‖z‖2 − ‖v∗‖2\n] = F (st,vt)− F (st,v∗).\nThe inequality here uses the fact that, for any two vectors u and w, we have 2u ·w ≤ ‖u‖2 +‖w‖2. Also,\n‖v∗ − v1‖ ≤ ‖v∗ − z‖+ ‖z− v1‖ = min\nv∈C ‖v − z‖+ ‖z− v1‖\n≤ ‖v1 − z‖+ ‖z− v1‖. (20)\nSumming Eq. (19) for t = 1, . . . , Nin and combining with Eq. (20) gives\n−4‖z− v1‖2 ≤ ‖v∗ − vNin+1‖2 − ‖v∗ − v1‖2\n≤ ν Nin∑ t=1 F (st,vt)− ν Nin∑ t=1 F (st,v ∗) + 4ν2Nin.\nRe-arranging and applying our choice of ν completes the lemma.\nNext, we show that v satisfies the specification given in Eq. (10).\nTheorem 10 For the algorithm ApproxProject, mins∈C F (s,v) ≥ −δ with δ set as in Lemma 9. Thus, Eq. (10) holds for v if we set α = 8/ √ Nin.\nProof Let\ns = 1\nNin Nin∑ t=1 st.\nThen\nmin s∈C F (s,v) ≥ min s∈C\n1\nNin Nin∑ t=1 F (s,vt) (21)\n≥ 1 Nin Nin∑ t=1 min s∈C F (s,vt)\n= 1\nNin Nin∑ t=1 F (st,vt) (22)\n≥ 1 Nin Nin∑ t=1 F (st,v ∗)− δ = F (s,v∗)− δ (23) ≥ min\ns∈C F (s,v∗)− δ\n≥ ‖z− v∗‖2 − δ ≥ −δ. (24)\nEq. (21) uses Jensen’s inequality and the fact that F (s, ·) is concave for each s. Eq. (22) follows from our choice of st (which minimizes F (·,vt)). Eq. (23) uses linearity of F (·,v) for each v. And Eq. (24) uses F (s,v∗) ≥ ‖z− v∗‖2 for all s ∈ C, which follows from simple Euclidean geometry and the Pythagorean theorem.\nFinally, combining with Lemma 7 and Theorem 8, this shows that the overall solution w will be an ε-approximate maxmin solution where ε = 2L/ √ Nout + 4L/ √ Nin. Thus, we can obtain\nany desired value of ε by setting Nin = Nout = ⌈ 36L2/ε2 ⌉ . The resulting number of calls to the classification oracle will be Nout + NinNout = O(L4/ε4). As earlier noted, compared to SparringFPL, this bound gives a different trade-off between m and ε. For the case that ε = O(ε′), and with ε′ as in Section 6, this algorithm gives a better bound by a factor of O((ln |Π|)/K2)."
    } ],
    "references" : [ {
      "title" : "Taming the monster: A fast and simple algorithm for contextual bandits",
      "author" : [ "Alekh Agarwal", "Daniel Hsu", "Satyen Kale", "John Langford", "Lihong Li", "Robert E. Schapire" ],
      "venue" : "In Proceedings of the International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Agarwal et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Agarwal et al\\.",
      "year" : 2014
    }, {
      "title" : "Reducing dueling bandits to cardinal bandits",
      "author" : [ "Nir Ailon", "Zohar Karnin", "Thorsten Joachims" ],
      "venue" : "In Proceedings of the International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Ailon et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Ailon et al\\.",
      "year" : 2014
    }, {
      "title" : "The nonstochastic multiarmed bandit problem",
      "author" : [ "Peter Auer", "Nicolò Cesa-Bianchi", "Yoav Freund", "Robert E. Schapire" ],
      "venue" : "SIAM J. Computing,",
      "citeRegEx" : "Auer et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Auer et al\\.",
      "year" : 2002
    }, {
      "title" : "Contextual bandit algorithms with supervised learning guarantees",
      "author" : [ "Alina Beygelzimer", "John Langford", "Lihong Li", "Lev Reyzin", "Robert Schapire" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "Beygelzimer et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Beygelzimer et al\\.",
      "year" : 2011
    }, {
      "title" : "A survey of preference-based online learning with bandit algorithms",
      "author" : [ "Róbert Busa-Fekete", "Eyke Hüllermeier" ],
      "venue" : "In Algorithmic Learning Theory (ALT),",
      "citeRegEx" : "Busa.Fekete and Hüllermeier.,? \\Q2014\\E",
      "shortCiteRegEx" : "Busa.Fekete and Hüllermeier.",
      "year" : 2014
    }, {
      "title" : "Prediction, Learning, and Games",
      "author" : [ "Nicolò Cesa-Bianchi", "Gábor Lugosi" ],
      "venue" : null,
      "citeRegEx" : "Cesa.Bianchi and Lugosi.,? \\Q2006\\E",
      "shortCiteRegEx" : "Cesa.Bianchi and Lugosi.",
      "year" : 2006
    }, {
      "title" : "Large-scale validation and analysis of interleaved search evaluation",
      "author" : [ "Olivier Chapelle", "Thorsten Joachims", "Filip Radlinski", "Yisong Yue" ],
      "venue" : "ACM Transactions on Information Systems (TOIS),",
      "citeRegEx" : "Chapelle et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Chapelle et al\\.",
      "year" : 2012
    }, {
      "title" : "Efficient optimal learning for contextual bandits",
      "author" : [ "Miroslav Dudı́k", "Daniel Hsu", "Satyen Kale", "Nikos Karampatziakis", "John Langford", "Lev Reyzin", "Tong Zhang" ],
      "venue" : "In Uncertainty in Artificial Intelligence (UAI),",
      "citeRegEx" : "Dudı́k et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Dudı́k et al\\.",
      "year" : 2011
    }, {
      "title" : "Adaptive game playing using multiplicative weights",
      "author" : [ "Yoav Freund", "Robert E. Schapire" ],
      "venue" : "Games and Economic Behavior,",
      "citeRegEx" : "Freund and Schapire.,? \\Q1999\\E",
      "shortCiteRegEx" : "Freund and Schapire.",
      "year" : 1999
    }, {
      "title" : "Mathematical games: The paradox of the nontransitive dice and the elusive principle of indifference",
      "author" : [ "Martin Gardner" ],
      "venue" : "Scientific American,",
      "citeRegEx" : "Gardner.,? \\Q1970\\E",
      "shortCiteRegEx" : "Gardner.",
      "year" : 1970
    }, {
      "title" : "Tailoring click models to user goals",
      "author" : [ "Fan Guo", "Lei Li", "Christos Faloutsos" ],
      "venue" : "In Workshop on Web Search Click Data (WSCD),",
      "citeRegEx" : "Guo et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2009
    }, {
      "title" : "Efficient multiple-click models in web search",
      "author" : [ "Fan Guo", "Chao Liu", "Yi-Min Wang" ],
      "venue" : "In Proceedings of the International Conference on Web Search and Data Mining (WSDM),",
      "citeRegEx" : "Guo et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2009
    }, {
      "title" : "A probabilistic method for inferring preferences from clicks",
      "author" : [ "Katja Hofmann", "Shimon Whiteson", "Maarten de Rijke" ],
      "venue" : "In Proceedings of the International Conference on Information and Knowledge Management (CIKM),",
      "citeRegEx" : "Hofmann et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Hofmann et al\\.",
      "year" : 2011
    }, {
      "title" : "Fidelity, soundness, and efficiency of interleaved comparison methods",
      "author" : [ "Katja Hofmann", "Shimon Whiteson", "Maarten de Rijke" ],
      "venue" : "ACM Transactions on Information Systems (TOIS),",
      "citeRegEx" : "Hofmann et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Hofmann et al\\.",
      "year" : 2013
    }, {
      "title" : "Efficient algorithms for the online decision problem",
      "author" : [ "Adam Kalai", "Santosh Vempala" ],
      "venue" : "In Conference on Learning Theory (COLT),",
      "citeRegEx" : "Kalai and Vempala.,? \\Q2003\\E",
      "shortCiteRegEx" : "Kalai and Vempala.",
      "year" : 2003
    }, {
      "title" : "The epoch-greedy algorithm for contextual multi-armed bandits",
      "author" : [ "John Langford", "Tong Zhang" ],
      "venue" : "In Annual Conference on Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Langford and Zhang.,? \\Q2007\\E",
      "shortCiteRegEx" : "Langford and Zhang.",
      "year" : 2007
    }, {
      "title" : "Iterative ranking from pair-wise comparisons",
      "author" : [ "Sahand Negahban", "Sewoong Oh", "Devavrat Shah" ],
      "venue" : "In Annual Conference on Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Negahban et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Negahban et al\\.",
      "year" : 2012
    }, {
      "title" : "How does clickthrough data reflect retrieval quality",
      "author" : [ "Filip Radlinski", "Madhu Kurup", "Thorsten Joachims" ],
      "venue" : "In Proceedings of the International Conference on Information and Knowledge Management (CIKM),",
      "citeRegEx" : "Radlinski et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Radlinski et al\\.",
      "year" : 2008
    }, {
      "title" : "A new monotonic, clone-independent, reversal symmetric, and Condorcetconsistent single-winner election method",
      "author" : [ "Markus Schulze" ],
      "venue" : "Social Choice and Welfare,",
      "citeRegEx" : "Schulze.,? \\Q2011\\E",
      "shortCiteRegEx" : "Schulze.",
      "year" : 2011
    }, {
      "title" : "Generic exploration and karmed voting bandits",
      "author" : [ "Tanguy Urvoy", "Fabrice Clerot", "Raphael Féraud", "Sami Naamane" ],
      "venue" : "In Proceedings of the International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Urvoy et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Urvoy et al\\.",
      "year" : 2013
    }, {
      "title" : "Interactively optimizing information retrieval systems as a dueling bandits problem",
      "author" : [ "Yisong Yue", "Thorsten Joachims" ],
      "venue" : "In Proceedings of the International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Yue and Joachims.,? \\Q2009\\E",
      "shortCiteRegEx" : "Yue and Joachims.",
      "year" : 2009
    }, {
      "title" : "Beat the mean bandit",
      "author" : [ "Yisong Yue", "Thorsten Joachims" ],
      "venue" : "In Proceedings of the International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Yue and Joachims.,? \\Q2011\\E",
      "shortCiteRegEx" : "Yue and Joachims.",
      "year" : 2011
    }, {
      "title" : "The k-armed dueling bandits problem",
      "author" : [ "Yisong Yue", "J. Broder", "R. Kleinberg", "T. Joachims" ],
      "venue" : "In Conference on Learning Theory (COLT),",
      "citeRegEx" : "Yue et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Yue et al\\.",
      "year" : 2009
    }, {
      "title" : "Online convex programming and generalized infinitesimal gradient ascent",
      "author" : [ "Martin Zinkevich" ],
      "venue" : "In Proceedings of the International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Zinkevich.,? \\Q2003\\E",
      "shortCiteRegEx" : "Zinkevich.",
      "year" : 2003
    }, {
      "title" : "Relative confidence sampling for efficient on-line ranker evaluation",
      "author" : [ "Masrour Zoghi", "Shimon Whiteson", "Maarten de Rijke", "Rémi Munos" ],
      "venue" : "In Proceedings of the International Conference on Web Search and Data Mining (WSDM),",
      "citeRegEx" : "Zoghi et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Zoghi et al\\.",
      "year" : 2014
    }, {
      "title" : "Relative upper confidence bound for the k-armed dueling bandits problem",
      "author" : [ "Masrour Zoghi", "Shimon Whiteson", "Rémi Munos", "Maarten de Rijke" ],
      "venue" : "In Proceedings of the International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Zoghi et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Zoghi et al\\.",
      "year" : 2014
    }, {
      "title" : "MergeRUCB: A method for large-scale online ranker evaluation",
      "author" : [ "Masrour Zoghi", "Shimon Whiteson", "Maarten de Rijke" ],
      "venue" : "In Proceedings of the International Conference on Web Search and Data Mining (WSDM),",
      "citeRegEx" : "Zoghi et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Zoghi et al\\.",
      "year" : 2015
    }, {
      "title" : "Failure of the Condorcet winner to exist Here, we investigate the reliability of the Condorcet assumption by replicating the experiment of (Zoghi et al., 2014a, Section 6.1) with a small modification. As in their setting, we consider a family of K-armed dueling bandit problems arising from the ranker evaluation problem in IR",
      "author" : [ ],
      "venue" : null,
      "citeRegEx" : "Appendix,? \\Q2014\\E",
      "shortCiteRegEx" : "Appendix",
      "year" : 2014
    }, {
      "title" : "Theorem 1.1) prove the following (slightly simplified) result: assume that D, R and A are such that for all d ∈ D",
      "author" : [ "t. Kalai", "Vempala" ],
      "venue" : null,
      "citeRegEx" : "Kalai and Vempala,? \\Q2003\\E",
      "shortCiteRegEx" : "Kalai and Vempala",
      "year" : 2003
    } ],
    "referenceMentions" : [ {
      "referenceID" : 22,
      "context" : "We study this problem in the dueling-bandits framework of Yue et al. (2009), which we extend to incorporate context.",
      "startOffset" : 58,
      "endOffset" : 76
    }, {
      "referenceID" : 17,
      "context" : "This problem naturally arises in information retrieval (IR) and recommender systems, where the user feedback is considerably more reliable when interpreted as relative comparisons rather than absolute labels (Radlinski et al., 2008).",
      "startOffset" : 208,
      "endOffset" : 232
    }, {
      "referenceID" : 6,
      "context" : "By presenting a mix or interleaving of two of the candidate rankings and observing the user’s response (Chapelle et al., 2012; Hofmann et al., 2013), it is possible for such a system to get feedback about user preferences.",
      "startOffset" : 103,
      "endOffset" : 148
    }, {
      "referenceID" : 13,
      "context" : "By presenting a mix or interleaving of two of the candidate rankings and observing the user’s response (Chapelle et al., 2012; Hofmann et al., 2013), it is possible for such a system to get feedback about user preferences.",
      "startOffset" : 103,
      "endOffset" : 148
    }, {
      "referenceID" : 2,
      "context" : "Similar to prior work on contextual (non-dueling) bandits (Auer et al., 2002; Langford and Zhang, 2007; Dudı́k et al., 2011; Agarwal et al., 2014), we propose a setting in which the learner has access to a space of policies Π, with the goal of performing as well as the “best” in the space.",
      "startOffset" : 58,
      "endOffset" : 146
    }, {
      "referenceID" : 15,
      "context" : "Similar to prior work on contextual (non-dueling) bandits (Auer et al., 2002; Langford and Zhang, 2007; Dudı́k et al., 2011; Agarwal et al., 2014), we propose a setting in which the learner has access to a space of policies Π, with the goal of performing as well as the “best” in the space.",
      "startOffset" : 58,
      "endOffset" : 146
    }, {
      "referenceID" : 7,
      "context" : "Similar to prior work on contextual (non-dueling) bandits (Auer et al., 2002; Langford and Zhang, 2007; Dudı́k et al., 2011; Agarwal et al., 2014), we propose a setting in which the learner has access to a space of policies Π, with the goal of performing as well as the “best” in the space.",
      "startOffset" : 58,
      "endOffset" : 146
    }, {
      "referenceID" : 0,
      "context" : "Similar to prior work on contextual (non-dueling) bandits (Auer et al., 2002; Langford and Zhang, 2007; Dudı́k et al., 2011; Agarwal et al., 2014), we propose a setting in which the learner has access to a space of policies Π, with the goal of performing as well as the “best” in the space.",
      "startOffset" : 58,
      "endOffset" : 146
    }, {
      "referenceID" : 22,
      "context" : "Most previous work on dueling bandits (Yue et al., 2009; Yue and Joachims, 2011; Urvoy et al., 2013; Zoghi et al., 2014b) has in fact explicitly or implicitly assumed that such a Condorcet winner exists.",
      "startOffset" : 38,
      "endOffset" : 121
    }, {
      "referenceID" : 21,
      "context" : "Most previous work on dueling bandits (Yue et al., 2009; Yue and Joachims, 2011; Urvoy et al., 2013; Zoghi et al., 2014b) has in fact explicitly or implicitly assumed that such a Condorcet winner exists.",
      "startOffset" : 38,
      "endOffset" : 121
    }, {
      "referenceID" : 19,
      "context" : "Most previous work on dueling bandits (Yue et al., 2009; Yue and Joachims, 2011; Urvoy et al., 2013; Zoghi et al., 2014b) has in fact explicitly or implicitly assumed that such a Condorcet winner exists.",
      "startOffset" : 38,
      "endOffset" : 121
    }, {
      "referenceID" : 3,
      "context" : "P multi-armed bandit algorithm (Beygelzimer et al., 2011) are run against one another (using a “sparring” approach previously suggested",
      "startOffset" : 31,
      "endOffset" : 57
    }, {
      "referenceID" : 13,
      "context" : "The dueling-bandits problem of Yue et al. (2009) formalizes this setting.",
      "startOffset" : 31,
      "endOffset" : 49
    }, {
      "referenceID" : 15,
      "context" : "To address this difficulty, we propose an approach used previously in other works on contextual bandits (Langford and Zhang, 2007; Dudı́k et al., 2011; Agarwal et al., 2014).",
      "startOffset" : 104,
      "endOffset" : 173
    }, {
      "referenceID" : 7,
      "context" : "To address this difficulty, we propose an approach used previously in other works on contextual bandits (Langford and Zhang, 2007; Dudı́k et al., 2011; Agarwal et al., 2014).",
      "startOffset" : 104,
      "endOffset" : 173
    }, {
      "referenceID" : 0,
      "context" : "To address this difficulty, we propose an approach used previously in other works on contextual bandits (Langford and Zhang, 2007; Dudı́k et al., 2011; Agarwal et al., 2014).",
      "startOffset" : 104,
      "endOffset" : 173
    }, {
      "referenceID" : 22,
      "context" : "Numerous algorithms have been proposed for the (non-contextual) dueling bandits problem: Interleaved Filter (Yue et al., 2009); Beat the Mean (BTM) (Yue and Joachims, 2011); Sensitivity Analysis of VAriables for Generic Exploration (SAVAGE) (Urvoy et al.",
      "startOffset" : 108,
      "endOffset" : 126
    }, {
      "referenceID" : 21,
      "context" : ", 2009); Beat the Mean (BTM) (Yue and Joachims, 2011); Sensitivity Analysis of VAriables for Generic Exploration (SAVAGE) (Urvoy et al.",
      "startOffset" : 29,
      "endOffset" : 53
    }, {
      "referenceID" : 19,
      "context" : ", 2009); Beat the Mean (BTM) (Yue and Joachims, 2011); Sensitivity Analysis of VAriables for Generic Exploration (SAVAGE) (Urvoy et al., 2013); Relative Confidence Sampling (Zoghi et al.",
      "startOffset" : 122,
      "endOffset" : 142
    }, {
      "referenceID" : 1,
      "context" : ", 2014b); Doubler, MultiSBM and Sparring (Ailon et al., 2014) and mergeRUCB (Zoghi et al.",
      "startOffset" : 41,
      "endOffset" : 61
    }, {
      "referenceID" : 20,
      "context" : "The method that is the most closely related to our work is Dueling Bandit Gradient Descent (DBGD) (Yue and Joachims, 2009), a policy gradient method that iteratively improves upon the current policy by conducting comparisons with nearby policies, assuming that the policy space comes equipped with a distance metric, and incrementally adapting the policy if a better alternative is encountered.",
      "startOffset" : 98,
      "endOffset" : 122
    }, {
      "referenceID" : 0,
      "context" : ", 2011; Agarwal et al., 2014). Specifically, we assume that we have access to a classification oracle for our policy class that can find the minimumcost policy in Π when given the cost of each action on each of a sequence of contexts. In fact, an ordinary cost-sensitive, multiclass classification learning algorithm can be used for this purpose, which suggests that, practically, this may be a reasonable and natural assumption. We then consider techniques for constructing a von Neumann winner from empirical exploration data. (Although we focus on a batch-like setting, the resulting algorithms can be used online as well.) We analyze the statistical efficiency of this approach in Section 6. In Sections 7 and 8, we give two polynomial-time algorithms for computing an approximate von Neumann winner from data: one based on Kalai and Vempala’s Follow-the-Perturbed-Leader algorithm (2003), and the other based on projected gradient ascent as studied by Zinkevich (2003).",
      "startOffset" : 8,
      "endOffset" : 893
    }, {
      "referenceID" : 0,
      "context" : ", 2011; Agarwal et al., 2014). Specifically, we assume that we have access to a classification oracle for our policy class that can find the minimumcost policy in Π when given the cost of each action on each of a sequence of contexts. In fact, an ordinary cost-sensitive, multiclass classification learning algorithm can be used for this purpose, which suggests that, practically, this may be a reasonable and natural assumption. We then consider techniques for constructing a von Neumann winner from empirical exploration data. (Although we focus on a batch-like setting, the resulting algorithms can be used online as well.) We analyze the statistical efficiency of this approach in Section 6. In Sections 7 and 8, we give two polynomial-time algorithms for computing an approximate von Neumann winner from data: one based on Kalai and Vempala’s Follow-the-Perturbed-Leader algorithm (2003), and the other based on projected gradient ascent as studied by Zinkevich (2003). These techniques yield learning algorithms that approximate or perform as well as the von Neumann winner, using data, time, and space that only depend logarithmically on the cardinality of the space Π, and therefore, are applicable even with huge policy spaces.",
      "startOffset" : 8,
      "endOffset" : 974
    }, {
      "referenceID" : 22,
      "context" : "Dueling bandits and the von Neumann winner In the dueling bandits problem (Yue et al., 2009), the learner has access to K possible actions, 1, .",
      "startOffset" : 74,
      "endOffset" : 92
    }, {
      "referenceID" : 19,
      "context" : "Existing work typically assumes the existence of a Condorcet winner (Urvoy et al., 2013; Zoghi et al., 2014b), that is, an action a∗ that beats every other action a 6= a∗.",
      "startOffset" : 68,
      "endOffset" : 109
    }, {
      "referenceID" : 18,
      "context" : "Before continuing, we briefly mention some of the other solution concepts that have been proposed to remedy the potential non-existence of a Condorcet winner (Schulze, 2011).",
      "startOffset" : 158,
      "endOffset" : 173
    }, {
      "referenceID" : 19,
      "context" : "Two of these are the Borda winner, the action that has the highest probability of winning a duel against a uniformly random action; and the Copeland winner, the action that wins the most pairwise comparisons (Urvoy et al., 2013).",
      "startOffset" : 208,
      "endOffset" : 228
    }, {
      "referenceID" : 18,
      "context" : "Both of these fail the independence of clones criterion (Schulze, 2011), meaning that adding multiple identical copies of an action can change the Borda or Copeland winner.",
      "startOffset" : 56,
      "endOffset" : 71
    }, {
      "referenceID" : 22,
      "context" : "In the standard dueling-bandits setting with a static preference matrix, a seemingly different definition of regret was used by Yue et al. (2009) in terms of an assumed Condorcet winner.",
      "startOffset" : 128,
      "endOffset" : 146
    }, {
      "referenceID" : 8,
      "context" : "Under this interpretation, it becomes especially natural to use ordinary no-regret learning algorithms as players of this game since it is known that such algorithms, when properly configured for this purpose, will converge to maxmin or minmax strategies (Freund and Schapire, 1999).",
      "startOffset" : 255,
      "endOffset" : 282
    }, {
      "referenceID" : 1,
      "context" : "Such a “sparring” approach was previously proposed for dueling bandits by Ailon et al. (2014), though without details, and not in the contextual setting.",
      "startOffset" : 74,
      "endOffset" : 94
    }, {
      "referenceID" : 3,
      "context" : "P (Beygelzimer et al., 2011) for this purpose in the full-explore-exploit setting.",
      "startOffset" : 2,
      "endOffset" : 28
    }, {
      "referenceID" : 3,
      "context" : "P (Beygelzimer et al., 2011) for this purpose in the full-explore-exploit setting. Exp4.P is well-suited since it is designed to work with partial information as in our bandit setting, and since it can handle the kind of adversarially generated data that arises unavoidably when playing a game. It also is designed to work with policies in a contextual setting like ours (or, more generally, to accept the advice of “experts”). The learning setting for Exp4.P is as follows (somewhat, but straightforwardly, modified for our present purposes). There are K possible actions, 1, . . . ,K, and a finite space Π of policies. On each round t = 1, . . . , T , an adversary chooses and reveals a context xt, and also chooses, but does not reveal rewards rt(1), . . . , rt(K) ∈ [−1,+1] for each of the K actions. The learner then selects an action at, and receives the revealed reward rt(at). The learner’s total reward is thus GA = ∑T t=1 rt(at), while the reward of each policy π is Gπ = ∑T t=1 rt(π(xt)). The learner’s goal is to receive reward close to that of the best policy. Beygelzimer et al. (2011) prove that (subject to very benign conditions) with probability at least 1− δ, Exp4.",
      "startOffset" : 3,
      "endOffset" : 1100
    }, {
      "referenceID" : 14,
      "context" : "In the sections that follow, we provide two algorithms: Algorithm SparringFPL that builds on the Follow-the-PerturbedLeader algorithm of Kalai and Vempala (2003), and Algorithm ProjectedGD that builds on online projected gradient descent methods of Zinkevich (2003).",
      "startOffset" : 137,
      "endOffset" : 162
    }, {
      "referenceID" : 14,
      "context" : "In the sections that follow, we provide two algorithms: Algorithm SparringFPL that builds on the Follow-the-PerturbedLeader algorithm of Kalai and Vempala (2003), and Algorithm ProjectedGD that builds on online projected gradient descent methods of Zinkevich (2003). For a given approximation quality, the performance of either algorithm is characterized by several quantities: the sufficient number of exploration rounds, the running time, the storage requirement, and the number of policies in the support of Ŵ.",
      "startOffset" : 137,
      "endOffset" : 266
    }, {
      "referenceID" : 14,
      "context" : "(7), SparringFPL, is based on the Follow-the-Perturbed-Leader (FPL) algorithm of Kalai and Vempala (2003). FPL is designed for a standard online learning problem: Let D and L be subsets of RmK .",
      "startOffset" : 81,
      "endOffset" : 106
    }, {
      "referenceID" : 23,
      "context" : "(7) using online projected gradient descent methods as studied by Zinkevich (2003). The algorithm maintains a vector wt ∈ C corresponding to a strategy for the row player.",
      "startOffset" : 66,
      "endOffset" : 83
    } ],
    "year" : 2015,
    "abstractText" : "We consider the problem of learning to choose actions using contextual information when provided with limited feedback in the form of relative pairwise comparisons. We study this problem in the dueling-bandits framework of Yue et al. (2009), which we extend to incorporate context. Roughly, the learner’s goal is to find the best policy, or way of behaving, in some space of policies, although “best” is not always so clearly defined. Here, we propose a new and natural solution concept, rooted in game theory, called a von Neumann winner, a randomized policy that beats or ties every other policy. We show that this notion overcomes important limitations of existing solutions, particularly the Condorcet winner which has typically been used in the past, but which requires strong and often unrealistic assumptions. We then present three efficient algorithms for online learning in our setting, and for approximating a von Neumann winner from batch-like data. The first of these algorithms achieves particularly low regret, even when data is adversarial, although its time and space requirements are linear in the size of the policy space. The other two algorithms require time and space only logarithmic in the size of the policy space when provided access to an oracle for solving classification problems on the space.",
    "creator" : "LaTeX with hyperref package"
  }
}