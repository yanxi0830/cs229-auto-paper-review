{
  "name" : "1607.07195.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Higher-Order Factorization Machines",
    "authors" : [ "Mathieu Blondel", "Akinori Fujino", "Naonori Ueda", "Masakazu Ishihata" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Factorization machines (FMs) [12, 13] are a supervised learning approach that can use second-order feature combinations efficiently even when the data is very high-dimensional. The key idea of FMs is to model the weights of feature combinations using a low-rank matrix. This has two main benefits. First, FMs can achieve empirical accuracy on a par with polynomial regression or kernel methods but with smaller and faster to evaluate models [4]. Second, FMs can infer the weights of feature combinations that were not observed in the training set. This second property is crucial for instance in recommender systems, a domain where FMs have become increasingly popular [13, 15]. Without the low-rank property, FMs would fail to generalize to unseen user-item interactions.\nUnfortunately, although higher-order FMs (HOFMs) were briefly mentioned in the original work of [12, 13], there exists to date no efficient algorithm for training arbitrary-order HOFMs. In fact, even just computing predictions given the model parameters naively takes polynomial time in the number of features. For this reason, HOFMs have, to our knowledge, never been applied to any problem. In addition, HOFMs, as originally defined in [12, 13], model each degree in the polynomial expansion with a different matrix and therefore require the estimation of a large number of parameters.\nIn this paper, we propose the first efficient algorithms for training arbitrary-order HOFMs. To do so, we rely on a link between FMs and the so-called ANOVA kernel [4]. We propose linear-time dynamic programming algorithms for evaluating the ANOVA kernel and computing its gradient. Based on these, we propose stochastic gradient and coordinate descent algorithms for arbitrary-order HOFMs. To reduce the number of parameters, as well as prediction times, we also introduce two new kernels derived from the ANOVA kernel, allowing us to define new variants of HOFMs with shared parameters. We demonstrate the proposed approaches on four different link prediction tasks."
    }, {
      "heading" : "2 Factorization machines (FMs)",
      "text" : "Second-order FMs. Factorization machines (FMs) [12, 13] are an increasingly popular method for efficiently using second-order feature combinations in classification or regression tasks even when the data is very high-dimensional. Let w ∈ Rd and P ∈ Rd×k, where k ∈ N is a rank hyper-parameter. We denote the rows of P by p̄j and its columns by ps, for j ∈ [d] and s ∈ [k],\nar X\niv :1\n60 7.\n07 19\n5v 1\n[ st\nat .M\nL ]\n2 5\nJu l 2\n01 6\nwhere [d] := {1, . . . , d}. Then, FMs predict an output y ∈ R from a vector x = [x1, . . . , xd]T by ŷFM(x) := 〈w,x〉+ ∑ j′>j 〈p̄j , p̄j′〉xjxj′ . (1)\nAn important characteristic of (1) is that it considers only combinations of distinct features (i.e., the squared features x21, . . . , x 2 d are ignored). The main advantage of FMs compared to naive polynomial regression is that the number of parameters to estimate is O(dk) instead of O(d2). In addition, we can compute predictions in O(2dk) time1 using\nŷFM(x) = w Tx+\n1\n2\n( ‖PTx‖2 − k∑ s=1 ‖ps ◦ x‖2 ) ,\nwhere ◦ indicates element-wise product [3]. Given a training set X = [x1, . . . ,xn] ∈ Rd×n and y = [y1, . . . , yn] T ∈ Rn,w and P can be learned by minimizing the following non-convex objective\n1\nn n∑ i=1 ` (yi, ŷFM(xi)) + β1 2 ‖w‖2 + β2 2 ‖P ‖2, (2)\nwhere ` is a convex loss function and β1 > 0, β2 > 0 are hyper-parameters. The popular libfm library [13] implements efficient stochastic gradient and coordinate descent algorithms for obtaining a stationary point of (2). Both algorithms have a runtime complexity of O(2dkn) per epoch.\nHigher-order FMs (HOFMs). Although no training algorithm was provided, FMs were extended to higher-order feature combinations in the original work of [12, 13]. Let P (t) ∈ Rd×kt , where t ∈ {2, . . . ,m} is the order or degree of feature combinations considered, and kt ∈ N is a rank hyper-parameter. Let p̄(t)j be the j th row of P (t). Then m-order HOFMs can be defined as\nŷHOFM(x) := 〈w,x〉+ ∑ j′>j 〈p̄(2)j , p̄ (2) j′ 〉xjxj′ + · · ·+ ∑ jm>···>j1 〈p̄(m)j1 , . . . , p̄ (m) jm 〉xj1xj2 . . . xjm (3)\nwhere we defined 〈p̄(t)j1 , . . . , p̄ (t) jt 〉 := ∑kt s=1 ∏t r=1 p (t) jr,s\n(sum of element-wise products). The objective function of HOFMs can be expressed in a similar way as for (2):\n1\nn n∑ i=1 ` (yi, ŷHOFM(xi)) + β1 2 ‖w‖2 + m∑ t=2 βt 2 ‖P (t)‖2, (4)\nwhere β1, . . . , βm > 0 are hyper-parameters. To avoid the combinatorial explosion of hyperparameter combinations to search, in our experiments we will simply set β1 = · · · = βm and k2 = · · · = km. While (3) looks quite daunting, [4] recently showed that FMs can be expressed from a simpler kernel perspective. Let us define the ANOVA2 kernel [18] of degree 2 ≤ m ≤ d by\nAm(p,x) := ∑\njm>···>j1 m∏ t=1 pjtxjt . (5)\nFor later convenience, we also define A0(p,x) := 1 and A1(p,x) := 〈p,x〉. Then it is shown that\nŷHOFM(x) = 〈w,x〉+ k2∑ s=1 A2 ( p(2)s ,x ) + · · ·+ km∑ s=1 Am ( p(m)s ,x ) , (6)\nwhere p(t)s is the sth column of P (t). This perspective shows that we can view FMs and HOFMs as a type of kernel machine whose “support vectors” are learned directly from data. Intuitively, the ANOVA kernel can be thought as a kind of polynomial kernel that uses only combinations of distinct features. A key property of the ANOVA kernel is multi-linearity [4]:\nAm(p,x) = Am(p¬j ,x¬j) + pjxj Am−1(p¬j ,x¬j), (7)\nwhere p¬j denotes the (d − 1)-dimensional vector with pj removed and similarly for x¬j . That is, everything else kept fixed, Am(p,x) is an affine function of pj ∀j ∈ [d]. Although no training\n1We include the constant factor for fair later comparison with arbitrary-order HOFMs. 2The name comes from the ANOVA decomposition of functions. [19, 18]\nalgorithm was provided, [4] showed based on (7) that, although non-convex, the objective function of arbitrary-order HOFMs is convex in w and in each row of P (2), . . . ,P (m), separately.\nInterpretability of HOFMs. An advantage of FMs and HOFMs is their interpretability. To see why this is the case, notice that we can rewrite (3) as\nŷHOFM(x) = 〈w,x〉+ ∑ j′>j W(2)j,j′xjxj′ + · · ·+ ∑ jm>···>j1 W(m)j1,...,jmxj1xj2 . . . xjm ,\nwhere we defined W(t) := ∑k s=1 p (t) s ⊗ · · · ⊗ p(t)s︸ ︷︷ ︸\nt times\n. Intuitively, W(t) ∈ Rdt is a low-rank t-way\ntensor which contains the weights of feature combinations of degree t. For instance, when t = 3, W(3)i,j,k is the weight of xixjxk. Similarly to the ANOVA decomposition of functions, HOFMs consider only combinations of distinct features.\nThis paper. Unfortunately, there exists to date no efficient algorithm for training arbitrary-order HOFMs. Indeed, computing (5) naively takes O(dm), i.e., polynomial time. In the following, we present linear-time algorithms. Moreover, HOFMs, as originally defined in [12, 13] require the estimation of m− 1 matrices P (2), . . . ,P (m). Thus, HOFMs can produce large models when m is large. To address this issue, we propose new variants of HOFMs with shared parameters."
    }, {
      "heading" : "3 Linear-time stochastic gradient algorithms for HOFMs",
      "text" : "The kernel view presented in Section 2 allows us to focus on the ANOVA kernel as the main “computational unit” for training HOFMs. In this section, we develop dynamic programming (DP) algorithms for evaluating it and computing its gradient in only O(dm) time, i.e., linear time.\nEvaluation. The main observation is that we can use (7) to recursively remove features until computing the kernel becomes trivial. Let us denote a subvector of p by p1:j ∈ Rj and similarly for x. Furthermore, let us introduce the shorthand aj,t := At(p1:j ,x1:j). Then we have that\naj,0 = 1 if j ≥ 0 aj,t = 0 if j < t aj,t = aj−1,t + pjxj aj−1,t−1 otherwise.\n(8)\nThe quantity we want to compute is Am(p,x) = ad,m. Instead of naively using the above recursion, which would lead to many redundant computations, we use a bottom-up approach and organize computations in a DP table. We start from the top-left corner to initialize the recursion and go through the table to arrive at the solution in the bottom-right corner. The procedure, summarized in Algorithm 1, takes O(dm) time and memory.\nGradients. For computing the gradient of Am(p,x) w.r.t. p, we use reverse-mode differentiation [2] (a.k.a. backpropagation in a neural network context), since it allows us to compute the entire gradient in a single pass. We supplement each variable aj,t in the DP table by a so-called adjoint ãj,t :=\n∂ad,m ∂aj,t , which represents the sensitivity of ad,m = Am(p,x) w.r.t. aj,t. From recursion (8), except for edge cases, aj,t influences aj+1,t+1 and aj+1,t. Using the chain rule, we then obtain\nãj,t = ∂ad,m ∂aj+1,t ∂aj+1,t ∂aj,t + ∂ad,m ∂aj+1,t+1 ∂aj+1,t+1 ∂aj,t = ãj+1,t + pj+1xj+1 ãj+1,t+1. (9)\nSimilarly, we introduce the adjoint p̃j := ∂ad,m ∂pj ∀j ∈ [d]. Since pj influences aj,t ∀t ∈ [m], we have\np̃j = m∑ t=1 ∂ad,m ∂aj,t ∂aj,t ∂pj = m∑ t=1 ãj,t aj−1,t−1 xj .\nWe can run recursion (9) in reverse order of the DP table (i.e., top-down) starting from ãd,m = 1. Using this approach, we can compute the entire gradient ∇Am(p,x) = [p̃1, . . . , p̃d]T w.r.t. p in O(dm) time and memory. The procedure is summarized in Algorithm 2.\nAlgorithm 1 Evaluating Am(p,x) in O(dm) Input: p ∈ Rd, x ∈ Rd aj,t ← 0 ∀t ∈ {1, . . . ,m}, j ∈ {0, 1, . . . , d} aj,0 ← 1 ∀j ∈ {0, 1, . . . , d}\nfor t := 1, . . . ,m do for j := t, . . . , d do\naj,t ← aj−1,t + pjxjaj−1,t−1 end for\nend for\nOutput: Am(p,x) = ad,m\nAlgorithm 2 Computing∇Am(p,x) in O(dm) Input: p ∈ Rd, x ∈ Rd, {aj,t}d,mj,t=0 ãj,t ← 0 ∀t ∈ {0, 1, . . . ,m}, j ∈ {0, 1, . . . , d} ãd,m ← 1 for t := m, . . . , 1 do\nfor j := d, . . . , t do ãj,t ← ãj+1,t+1pj+1xj+1 if j < d, t < m ãj,t ← ãj,t + ãj+1,t if j < d\nend for end for p̃j := ∑m t=1 ãj,taj−1,t−1xj ∀j ∈ [d] Output: ∇Am(p,x) = [p̃1, . . . , p̃d]T\nStochastic gradient (SG) algorithms. Based on Algorithm 1 and 2, we can easily learn arbitraryorder HOFMs using any gradient-based optimization algorithm. Here we focus our discussion on SG algorithms. If we alternatingly minimize (4) w.r.t P (2), . . . ,P (m), then the sub-problem associated with degree m is of the form\nF (P ) := 1\nn n∑ i=1 `\n( yi,\nk∑ s=1 Am(ps,xi) + oi\n) + β\n2 ‖P ‖2, (10)\nwhere o1, . . . , on ∈ R are fixed offsets which account for the contribution of degrees other than m to the predictions. The sub-problem is convex in each row of P . A SG update for (10) w.r.t. ps for some instance xi can be computed by ps ← ps − η`′(yi, ŷi)∇Am(ps,xi)− ηβps, where η is a learning rate and where we defined ŷi := ∑k s=1Am(ps,xi) + oi. Because evaluating Am(p,x) and computing its gradient both take O(dm), the cost per epoch, i.e., of visiting all instances, is O(mdkn). When m = 2, this is the same cost as the SG algorithm implemented in libfm.\nSparse data. We conclude this section with a few useful remarks on sparse data. Let us denote the support of a vector x = [x1, . . . , xd]T by supp(x) := {j ∈ [d] : xj 6= 0} and let us define xS := [xj : j ∈ S]T. It is easy to see from (7) that the gradient and x have the same support, i.e., supp(∇Am(p,x)) = supp(x). Another useful remark is thatAm(p,x) = Am(psupp(x),xsupp(x)), provided that m ≤ nz(x), where nz(x) is the number of non-zero elements in x. Hence, when the data is sparse, we only need to iterate over non-zero features in Algorithm 1 and 2. Consequently, their time and memory cost is only O(nz(x)m) and thus the cost per epoch of SG algorithms is O(mknz(X))."
    }, {
      "heading" : "4 Coordinate descent algorithm for arbitrary-order HOFMs",
      "text" : "We now describe a coordinate descent (CD) solver for arbitrary-order HOFMs. CD is a good choice for learning HOFMs because their objective function is coordinate-wise convex, thanks to the multilinearity of the ANOVA kernel. Our algorithm can be seen as a generalization to higher orders of the CD algorithms proposed in [13, 4].\nAn alternative recursion. Efficient CD implementations typically require maintaining statistics for each training instance, such as the predictions at the current iteration. When a coordinate is updated, the statistics then need to be synchronized. Unfortunately, the recursion we used in the previous section is not suitable for a CD algorithm because it would require to store and synchronize the DP table for each training instance upon coordinate-wise updates. We therefore turn to an alternative recursion:\nAm(p,x) = 1 m m∑ t=1 (−1)t+1Am−t(p,x)Dt(p,x), (11)\nwhere we defined Dt(p,x) := ∑d j=1(pjxj)\nt. Note that the recursion was already known in the context of traditional kernel methods (c.f., [18, Section 11.8]) but its application to HOFMs is novel. Since we know that A0(p,x) = 1 and A1(p,x) = 〈p,x〉, we can use (11) to compute A2(p,x), then A3(p,x), and so on. The overall evaluation cost for arbitrary m ∈ N is O(md+m2).\nCoordinate-wise derivatives. We can apply reverse-mode differentiation to recursion (11) in order to compute the entire gradient (c.f., Appendix C). However, in CD, since we only need the derivative of one variable at a time, we can simply use forward-mode differentiation:\n∂Am(p,x) ∂pj = 1 m m∑ t=1 (−1)t+1 [ ∂Am−t(p,x) ∂pj Dt(p,x) +Am−t(p,x)∂D t(p,x) ∂pj ] , (12)\nwhere ∂D t(p,x) ∂pj = tpt−1j x t j . The advantage of (12) is that we only need to cacheDt(p,x) for t ∈ [m]. Hence the memory complexity per sample is only O(m) instead of O(dm) for (8).\nUse in a CD algorithm. Similarly to [4], we assume that the loss function ` is µ-smooth and update the elements pj,s of P in cyclic order by pj,s ← pj,s − η−1j,s ∂F (P ) ∂pj,s , where we defined\nηj,s := µ\nn n∑ i=1 ( ∂Am(ps,xi) ∂pj,s )2 + β and ∂F (P ) ∂pj,s = 1 n n∑ i=1 `′(yi, ŷi) ∂Am(ps,xi) ∂pj,s + βpj,s.\nThe update guarantees monotonic decrease of the objective value and is the exact coordinate-wise minimizer when ` is the squared loss. Overall, the total cost per epoch, i.e., updating all coordinates once, is O(τ(m)knz(X)), where τ(m) is the time it takes to compute (12). Assuming Dt(ps,xi) have been previously cached, for t ∈ [m], computing (12) takes τ(m) = m(m+ 1)/2− 1 operations. For fixed m, if we unroll the two loops needed to compute (12), modern compilers can often further reduce the number of operations needed. Nevertheless, this quadratic dependency on m means that our CD algorithm is best for small m, typically m ≤ 4."
    }, {
      "heading" : "5 HOFMs with shared parameters",
      "text" : "HOFMs, as originally defined in [12, 13], model each degree with separate matrices P (2), . . . ,P (m). Assuming that we use the same rank k for all matrices, the total model size of m-order HOFMs is therefore O(kdm). Moreover, even when using our O(dm) DP algorithm, the cost of computing predictions is O(k(2d+ · · ·+md)) = O(kdm2). Hence, HOFMs tend to produce large, expensiveto-evaluate models. To reduce model size and prediction times, we introduce two new kernels which allow us to share parameters between each degree: the inhomogeneous ANOVA kernel and the all-subsets kernel. Because both kernels are derived from the ANOVA kernel, they share the same appealing properties: multi-linearity, sparse gradients and sparse-data friendliness."
    }, {
      "heading" : "5.1 Inhomogeneous ANOVA kernel",
      "text" : "It is well-known that a sum of kernels is equivalent to concatenating their associated feature maps [17, Section 3.4]. Let θ = [θ1, . . . , θm]T. To combine different degrees, a natural kernel is therefore\nA1→m(p,x;θ) := m∑ t=1 θtAt(p,x). (13)\nThe kernel uses all feature combinations of degrees 1 up to m. We call it inhomogeneous ANOVA kernel, since it is an inhomogeneous polynomial of x. In contrast, Am(p,x) is homogeneous. The main difference between (13) and (6) is that all ANOVA kernels in the sum share the same parameters. However, to increase modeling power, we allow each kernel to have different weights θ1, . . . , θm.\nEvaluation. Due to the recursive nature of Algorithm 1, when computing Am(p,x), we also get A1(p,x), . . . ,Am−1(p,x) for free. Indeed, lower-degree kernels are available in the last column of the DP table, i.e., At(p,x) = ad,t ∀t ∈ [m]. Hence, the cost of evaluating (13) is O(dm) time. The total cost for computing ŷ = ∑k s=1A1→m(ps,x;θ) is O(kdm) instead of O(kdm2) for ŷHOFM(x).\nLearning. While it is certainly possible to learn P and θ by directly minimizing some objective function, here we propose an easier solution, which works well in practice. Our key observation is that we can easily turn Am into A1→m by adding dummy values to feature vectors. Let us denote the concatenation of p with a scalar γ by [γ,p] and similarly for x. From (7), we easily obtain\nAm([γ1,p], [1,x]) = Am(p,x) + γ1Am−1(p,x).\nSimilarly, if we apply (7) twice, we obtain:\nAm([γ1, γ2,p], [1, 1,x]) = Am(p,x) + (γ1 + γ2)Am−1(p,x) + γ1γ2Am−2(p,x). Applying the above to m = 2 and m = 3, we obtain\nA2([γ1,p], [1,x]) = A1→2(p,x; [γ1, 1]) and A3([γ1, γ2,p], [1, 1,x]) = A1→3(p,x; [γ1γ2, γ1+γ2, 1]).\nMore generally, by adding m−1 dummy features to p and x, we can convertAm toA1→m. Because p is learned, this means that we can automatically learn γ1, . . . , γm−1. These weights can then be converted to θ1, . . . , θm by “unrolling” recursion (7). Although simple, we show in our experiments that this approach works favorably compared to directly learning P and θ. The main advantage of this approach is that we can use the same software unmodified (we simply need to minimize (10) with the augmented data). Moreover, the cost of computing the entire gradient by Algorithm 2 using the augmented data is just O(dm+m2) compared to O(dm2) for HOFMs with separate parameters."
    }, {
      "heading" : "5.2 All-subsets kernel",
      "text" : "We now consider a closely related kernel called all-subsets kernel [17, Definition 9.5]:\nS(p,x) := d∏ j=1 (1 + pjxj).\nThe main difference with the traditional use of this kernel is that we learn p. Interestingly, it can be shown that S(p,x) = 1 +A1→d(p,x;1) = 1 +A1→nz(x)(p,x;1), where nz(x) is the number of non-zero features in x. Hence, the kernel uses all combinations of distinct features up to order nz(x) with uniform weights. Even if d is very large, the kernel can be a good choice if each training instance contains only a few non-zero elements. To learn the parameters, we simply substitute Am with S in (10). In SG or CD algorithms, all it entails is to substitute ∇Am(p,x) with ∇S(p,x). For computing ∇S(p,x), it is easy to verify that S(p,x) = S(p¬j ,x¬j)(1 + pjxj) ∀j ∈ [d] and therefore we have\n∇S(p,x) = [ x1 S(p¬1,x¬1), . . . , xd S(p¬d,x¬d) ]T = [ x1 S(p,x) 1 + p1x1 , . . . , xd S(p,x) 1 + pdxd ]T .\nTherefore, the main advantage of the all-subsets kernel is that we can evaluate it and compute its gradient in just O(d) time. The total cost for computing ŷ = ∑k s=1 S(ps,x) is only O(kd)."
    }, {
      "heading" : "6 Experimental results",
      "text" : ""
    }, {
      "heading" : "6.1 Application to link prediction",
      "text" : "Problem setting. We now demonstrate a novel application of HOFMs to predict the presence or absence of links between nodes in a graph. Formally, we assume two sets of possibly disjoint nodes of size nA and nB , respectively. We assume features for the two sets of nodes, represented by matricesA ∈ RdA×nA andB ∈ RdB×nB . For instance,A can represent user features andB movie features. We denote the columns of A and B by ai and bj , respectively. We are given a matrix Y ∈ {0, 1}nA×nB , whose elements indicate presence (positive sample) or absence (negative sample) of link between two nodes ai and bj . We denote the number of positive samples by n+. Using this data, our goal is to predict new associations. Datasets used in our experiments are summarized in Table 2. Note that for the NIPS and Enzyme datasets,A = B.\nConversion to a supervised problem. We need to convert the above information to a format FMs and HOFMs can handle. To predict an element yi,j of Y , we simply form xi,j to be the concatenation\nof ai and bj and feed this to a HOFM in order to compute a prediction ŷi,j . Because HOFMs use feature combinations in xi,j , they can learn the weights of feature combinations between ai and bj . At training time, we need both positive and negative samples. Let us denote the set of positive and negative samples by Ω. Then our training set is composed of (xi,j , yi,j) pairs, where (i, j) ∈ Ω. Models compared.\n• HOFM: ŷi,j = ŷHOFM(xi,j) as defined in (3) and as originally proposed in [12, 13]. We minimize (4) by alternating minimization of (10) for each degree.\n• HOFM-shared: ŷi,j = ∑k s=1A1→m(ps,xi,j ;θ). We learn P and θ using the simple augmented\ndata approach described in Section 5.1 (HOFM-shared-augmented). Inspired by SimpleMKL [11], we also report results when learning P and θ directly by minimizing 1|Ω| ∑ (i,j)∈Ω `(yi,j , ŷi,j) + β 2 ‖P ‖ 2 subject to θ ≥ 0 and 〈θ,1〉 = 1 (HOFM-shared-simplex).\n• All-subsets: ŷi,j = ∑k s=1 S(ps,xi,j). As explained in Section 5.2, this model is equivalent to the\nHOFM-shared model with m = nz(xi,j) and θ = 1. • Polynomial network: ŷi,j = ∑k s=1(γs + 〈ps,xi,j〉)m. This model can be thought as factorization\nmachine variant that uses a polynomial kernel instead of the ANOVA kernel.\n• Low-rank bilinear regression: ŷi,j = aiUV Tbj , where U ∈ RdA×k and V ∈ RdB×k. Such model was shown to be state-of-the-art for link prediction in [8] and [9]. We learn U and V by minimizing 1|Ω| ∑ (i,j)∈Ω `(yi,j , ŷi,j) + β 2 (‖U‖ 2 + ‖V ‖2).\nExperimental setup and evaluation. In this experiment, for all models above, we use CD rather than SG to avoid the tuning of a learning rate hyper-parameter. We set ` to be the squared loss. Although we omitted it from our notation for clarity, we also fit a bias term for all models. We evaluated the compared models using the area under the ROC curve (AUC), which is the probability that the model correctly ranks a positive sample higher than a negative sample. We split the n+ positive samples into 50% for training and 50% for testing. We sample the same number of negative samples as positive samples for training and use the rest for testing. We chose β from 10−6, 10−5, . . . , 106 by cross-validation and following [8] we empirically set k = 30. Throughout our experiments, we initialized the elements of P randomly by N (0, 0.01). Results are indicated in Table 3. Overall the two best models were HOFM and HOFM-sharedaugmented, which achieved the best scores on 3 out of 4 datasets. The two models outperformed low-rank bilinear regression on 3 out 4 datasets, showing the benefit of using higher-order feature combinations. HOFM-shared-augmented achieved similar accuracy to HOFM, despite using a smaller model. Surprisingly, HOFM-shared-simplex did not improve over HOFM-shared-augmented except\non the GD dataset. We conclude that our augmented data approach is convenient yet works well in practice. All-subsets and polynomial networks performed worse than HOFM and HOFM-sharedaugmented, except on the GD dataset where they were the best. Finally, we observe that HOFM were quite robust to increasing m, which is likely a benefit of modeling each degree with a separate matrix."
    }, {
      "heading" : "6.2 Solver comparison",
      "text" : "We compared AdaGrad [5], L-BFGS and coordinate descent (CD) for minimizing (10) when varying the degree m on the NIPS dataset with β = 0.1 and k = 30. We constructed the data in the same way as explained in the previous section and added m− 1 dummy features, resulting in n = 8, 280 sparse samples of dimension d = 27, 298 + m− 1. For AdaGrad and L-BFGS, we computed the (stochastic) gradients using Algorithm 2. All solvers used the same initialization.\nResults are indicated in Figure 1. We see that our CD algorithm performs very well when m ≤ 3 but starts to deteriorate when m ≥ 4, in which case L-BFGS becomes advantageous. As shown in Figure 1 d), the cost per epoch of AdaGrad and L-BFGS scales linearly with m, a benefit of our DP algorithm for computing the gradient. However, to our surprise, we found that AdaGrad is quite sensitive to the learning rate η. AdaGrad diverged for η ∈ {1, 0.1, 0.01} and the largest value to work well was η = 0.001. This explains why AdaGrad did not outperform CD despite the lower cost per epoch. In the future, it would be useful to create a CD algorithm with a better dependency on m."
    }, {
      "heading" : "7 Conclusion and future directions",
      "text" : "In this paper, we presented the first training algorithms for HOFMs and introduced new HOFM variants with shared parameters. A popular way to deal with a large number of negative samples is to use an objective function that directly maximize AUC [8, 14]. This is especially easy to do with SG algorithms because we can sample pairs of positive and negative samples from the dataset upon each SG update. We therefore expect the algorithms developed in Section 3 to be especially useful in this setting. Recently, [7] proposed a distributed SG algorithm for training second-order FMs. It should be straightforward to extend this algorithm to HOFMs based on our contributions in Section 3. Finally, it should be possible to integrate Algorithm 1 and 2 into a deep learning framework such as TensorFlow [1], in order to easily compose ANOVA kernels with other layers (e.g., convolutional)."
    }, {
      "heading" : "A Dataset descriptions",
      "text" : "• NIPS: co-author graph of authors at the first twelve editions of NIPS, obtained from [16]. For this dataset, as well as the Enzyme dataset below, we have A = B. The co-author graph comprises nA = nB = 2, 037 authors represented by bag-of-words vectors of dimension dA = dB = 13, 649 (words used by authors in their publications). The number of positive samples is n+ = 4, 140.\n• Enzyme: metabolic network obtained from [20]. The network comprises nA = nB = 668 enzymes represented by three sets of features: a 157-dimensional vector of phylogenetic information, a 145-dimensional vector of gene expression information and a 23-dimensional vector of gene location information. We concatenate the three sets of information to form feature vectors of dimension dA = dB = 325. Original enzyme similarity scores are between 0 and 1. We binarize the scores using 0.95 as threshold. The resulting number of positive samples is n+ = 2, 994.\n• GD: human gene-disease association data obtained from [9]. The bipartite graph is comprised of nA = 3, 209 diseases and nB = 12, 331 genes. We represent each disease using a vector of dA = 3, 209 dimensions, whose elements are similarity scores obtained from MimMiner. The study [9] also used bag-of-words vectors describing each disease but we found these to not help improve performance both for baselines and proposed methods. We represent each gene using a vector of dB = 25, 275 features, which are the concatenation of 12, 331 similarity scores obtained from HumanNet and 12, 944 gene-phenotype associations from 8 other species. See [9] for a detailed description of the features. The number of positive samples is n+ = 3, 954.\n• Movielens 100K: recommender system data obtained from [6]. The bipartite graph is comprised of nA = 943 users and nB = 1, 682 movies. For users, we convert age, gender, occupation and living area (first digit of zipcode) to a binary vector using a one-hot encoding. For movies, we use the release year and genres. The resulting vectors are of dimension dA = 49 and dB = 29, respectively. Original ratings are between 1 and 5. We binarize the ratings using 5 as threshold, resulting in n+ = 21, 201 positive samples."
    }, {
      "heading" : "B Additional experiments",
      "text" : "B.1 Solver comparison\nWe also compared AdaGrad, L-BFGS and coordinate descent (CD) on the Enzyme, Gene-Disease (GD) and Movielens 100K datasets. Results are indicated in Figure 2, 3 and 4, respectively.\nB.2 Recommender system experiments\nWe compared HOFMs, HOFMs (with shared parameters), All-subsets and Polynomial Networks on the following two recommender system datasets. The design matrixX was constructed following [12, 13]. Namely, for each rating yi, the corresponding xi is set to the concatenation of the one-hot encodings of the user and item indices. We split samples uniformly at random between 75% for training and 25% for testing.\nDataset n d Movielens 1M 1,000,209 (ratings) 9,940 = 6,040 (users) + 3,900 (movies)\nLast.fm 108,437 (tag counts) 24,078 = 12,133 (artists) + 11,945 (tags)\nResults are indicated in Figure 5. We see that HOFM-shared outperforms HOFM and that All-subsets performs well too on these tasks."
    }, {
      "heading" : "C Reverse-mode differentiation on the alternative recursion",
      "text" : "We now describe how to apply reverse-mode differentiation to the alternative recursion (11) in order to compute the entire gradient efficiently. Let us introduce the shorthands at := At(p,x) and dt := Dt(p,x). We can then write the recursion as\nam = 1\nm m∑ t=1 (−1)t+1am−tdt.\nFor concreteness, let us illustrate the recursion for m = 3. We have\na1 = a0d1, a2 = 1\n2 (a1d1 − a0d2) and a3 =\n1 3 (a2d1 − a1d2 + a0d3).\nWe see that a2 influences a3, and a1 influences a2 and a3. Likewise, d3 influences a3, d2 influences a2 and a3, and d1 influences a1, a2 and a3. Let us denote the adjoints ãt := ∂am∂at and d̃t := ∂am ∂dt\n. For general m, summing over quantities that influences at and dt, we obtain\nãt = m∑ s=t+1 (−1)s−t+1 s ãsds−t and d̃t = (−1)t+1 m∑ s=t 1 s ãsas−t.\nLet us denote the adjoint of pj by p̃j := ∂am∂pj . We know that pj directly influences only d1, . . . , dm and therefore\np̃j = m∑ t=1 ∂am ∂dm ∂dm ∂pj = m∑ t=1 d̃ttp t−1 j x t j .\nAssuming that d1, . . . , dm and a1, . . . , am have been previously computed, which takesO(dm+m2), the procedure for computing the gradient can be summarized as follows:\n1. Initialize ãm = 1, 2. Compute ãm−1, . . . , ã1 (in that order),\n3. Compute d̃m, . . . , d̃1, 4. Compute ∇Am(p,x) = [p̃1, . . . , p̃d]T.\nSteps 2 and 4 both take O(m2) and step 4 takes O(dm) so the total cost is O(dm + m2). We can improve the complexity of step 4 as follows. We can rewrite∇Am(p,x) in matrix notation:\n∇Am(p,x) =   1 p1x1 (p1x1) 2 . . . (p1x1) m−1 1 p2x2 (p2x2) 2 . . . (p2x2) m−1 ... ... ... . . .\n... 1 pdxd (pdxd) 2 . . . (pdxd) m−1\n  d̃1 2d̃2\n... md̃m\n  ◦ x.\nThe left matrix is called a Vandermonde matrix. The product between a d×m Vandermonde matrix and a m-dimensional vector can be computed using the Moenck-Borodin algorithm (an algorithm similar to the FFT), in O(r log2 l), where r = max(d,m) and l = min(d,m) [10]. Since m ≤ d, the cost of step 4 can therefore be reduced to O(d log2m)."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "Factorization machines (FMs) are a supervised learning approach that can use<lb>second-order feature combinations even when the data is very high-dimensional.<lb>Unfortunately, despite increasing interest in FMs, there exists to date no efficient<lb>training algorithm for higher-order FMs (HOFMs). In this paper, we present<lb>the first generic yet efficient algorithms for training arbitrary-order HOFMs. We<lb>also present new variants of HOFMs with shared parameters, which greatly re-<lb>duce model size and prediction times while maintaining similar accuracy. We<lb>demonstrate the proposed approaches on four different link prediction tasks.",
    "creator" : "LaTeX with hyperref package"
  }
}