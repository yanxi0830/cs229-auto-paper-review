{
  "name" : "1705.03967.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "(adam.white@ualberta.ca)." ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 5.\n03 96\n7v 1\n[ cs\n.L G\n] 1\n0 M\nay 2\n01 7 GQ(λ) Quick Reference and Implementation Guide\nAdam White and Richard S. Sutton\nRevised July 29, 2014\nThis document should serve as a quick reference for and guide to the implementation of linear GQ(λ), a gradient-based off-policy temporal-difference learning algorithm. Explanation of the intuition and theory behind the algorithm are provided elsewhere (e.g., Maei & Sutton 2010, Maei 2011). If you questions or concerns about the content in this document or the attached java code please email Adam White (adam.white@ualberta.ca)."
    }, {
      "heading" : "1 Requirements and Setting",
      "text" : "For each use of GQ(λ) you will need to provide three question functions specifying the quantity to be predicted, and four answer functions characterizing the approximation that will be found. Let S and A denote the sets of states and actions. Then the question functions are:\n• π : S × A → [0, 1]; target policy to be learned. Incidently, if π is chosen as the greedy policy with respect to the learned value function, then the algorithm will implement a generalization of the Greedy-GQ algorithm (Maei, Szepesvari, Bhatnagar & Sutton 2010).\n• γ : S→ [0, 1]; termination or discounting function (γ(s) = 1− β(s) in GQ paper)\n• r : S×A× S→ R; reward function\nIn many publications there is also specified a fourth question function, the terminal reward function z : S → R used to specify a final reward at termination. More recently its has been recognized that this functionality can be included in the reward function, making use of the discounting function (Modayil, White & Sutton 2014). For example, if one wanted only a terminal reward function z(s) upon termination in state s, one would use a\nreward function of r(s, a, s′) = (1 − γ(s′))z(s′). This completes the specification of the predictive question that you are seeking to answer using the GQ(λ) algorithm.\nThe answer functions are:\n• b : S×A→ [0, 1]; behavior policy\n• I : S × A → [0, 1]; interest function (can set to 1 for all state-action pairs or indicate selected state-action pairs to be best approximated)\n• φ : S×A→ Rn; feature-vector function\n• λ : S→ [0, 1]; bootstrapping or eligibility-trace decay-rate function\nThe following data structures are internal to GQ:\n• θ ∈ Rn; the learned weights of the linear approximation: Qπ(s, a) = θ⊤φ(s, a) =\n∑n i=1 θiφi(s, a)\n• w ∈ Rn; secondary set of learned weights\n• e ∈ Rn; eligibility trace vector\nParameters internal to GQ:\n• α; step-size parameter for learning θ\n• η ∈ [0, 1]; relative step-size parameter for learning w (αη)"
    }, {
      "heading" : "2 Algorithm Specification",
      "text" : "We can now specify GQ(λ). Let w and e be initialized to zero and θ be initialized arbitrarily. Let the subscript t denote the current time step. Let ρt denote the “importance sampling” ratio:\nρt = π(St, At)\nb(St, At) , (1)\nwhere St and At are the state and action occuring on time step t. Let φ̄t denote the expected next feature vector, defined by:\nφ̄t = ∑\na∈A\nπ(St, a)φ(St, a) (2)\nThen the following equations fully specify GQ(λ):\nδt = r(St, At, St+1) + γ(St+1)θ ⊤ t φ̄t+1 − θ ⊤ t φ(St, At) (3)\nθt+1 = θt + α [ δtet − γ(St+1)(1 − λ(St+1))(w ⊤ t et)φ̄t+1 ]\n(4)\nwt+1 = wt + αη[δtet − (w ⊤ t φ(St, At))φ(St, At)] (5)\net = I(St)φ(St, At) + γ(St)λ(St)ρtet−1 (6)"
    }, {
      "heading" : "3 Pseudocode",
      "text" : "The following pseudocode characterizes the algorithm and its use.\nInitialize θ arbitrarily and w = 0 Repeat (for each episode):\nInitialize e = 0 S ← initial state of episode Repeat (for each step of episode):\nA← action selected by policy b in state S Take action A, observe next state, S′ φ̄← 0 For all a ∈ A(s):\nφ̄← φ̄+ π(S′, a)φ(S′, a)\nρ = π(S,A) b(S,A) GQlearn(φ(S,A), φ̄, λ(S′), γ(S′), r(S,A, S′), ρ, I(S)) S ← S′\nuntil S′ is terminal\nGQLearn(φ, φ̄, λ, γ,R, ρ, I) δ ← R+ γθ⊤φ̄− θ⊤φ e← ρe+ Iφ θ ← θ + α(δe − γ(1− λ)(w⊤e)φ̄) w ← w + αη(δe − (w⊤φ)φ) e← γλe"
    }, {
      "heading" : "4 Code",
      "text" : "The files GQlambda.java and GQlambda.cpp (in the arXiv source archive) contain implementations of the GQlearn function described in the pseudocode. We have excluded optimizations (e.g., binary features or efficient\ntrace implementation) to ensure the code is simple and easy to understand. We leave it to the reader to provide environment code for interfacing to GQ(λ) (e.g., using RL-Glue)."
    }, {
      "heading" : "5 References",
      "text" : "Maei, H. R., Szepesvári, Cs., Bhatnagar, S., Sutton, R. S. (2010). Toward off-policy learning control with function approximation. In Proceedings of the 27th International Conference on Machine Learning, Haifa, Israel.\nMaei, H. R. and Sutton, R. S. (2010). GQ(λ): A general gradient algorithm for temporal-difference prediction learning with eligibility traces. In Proceedings of the Third Conference on Artificial General Intelligence, pp. 91–96.\nModayil, J., White, A., Sutton, R. S. (2014). Multi-timescale nexting in a reinforcement learning robot. Adaptive Behavior 22 (2):146–160.\nSutton, R. S., Barto, A. G. (1998). Reinforcement Learning: An Introduction. MIT Press."
    } ],
    "references" : [ {
      "title" : "Toward off-policy learning control with function approximation",
      "author" : [ "H.R. Maei", "Szepesvári", "Cs", "S. Bhatnagar", "R.S. Sutton" ],
      "venue" : "In Proceedings of the 27th International Conference on Machine",
      "citeRegEx" : "Maei et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Maei et al\\.",
      "year" : 2010
    }, {
      "title" : "GQ(λ): A general gradient algorithm for temporal-difference prediction learning with eligibility traces",
      "author" : [ "H.R. Maei", "R.S. Sutton" ],
      "venue" : "In Proceedings of the Third Conference on Artificial General Intelligence,",
      "citeRegEx" : "Maei and Sutton,? \\Q2010\\E",
      "shortCiteRegEx" : "Maei and Sutton",
      "year" : 2010
    }, {
      "title" : "Multi-timescale nexting in a reinforcement learning robot",
      "author" : [ "J. Modayil", "A. White", "R.S. Sutton" ],
      "venue" : "Adaptive Behavior",
      "citeRegEx" : "Modayil et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Modayil et al\\.",
      "year" : 2014
    }, {
      "title" : "Reinforcement Learning: An Introduction",
      "author" : [ "R.S. Sutton", "A.G. Barto" ],
      "venue" : null,
      "citeRegEx" : "Sutton and Barto,? \\Q1998\\E",
      "shortCiteRegEx" : "Sutton and Barto",
      "year" : 1998
    } ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "This document should serve as a quick reference for and guide to the implementation of linear GQ(λ), a gradient-based off-policy temporal-difference learning algorithm. Explanation of the intuition and theory behind the algorithm are provided elsewhere (e.g., Maei & Sutton 2010, Maei 2011). If you questions or concerns about the content in this document or the attached java code please email Adam White (adam.white@ualberta.ca).",
    "creator" : "dvips(k) 5.996 Copyright 2016 Radical Eye Software"
  }
}