{
  "name" : "1503.08873.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "pmineiro@microsoft.com", "nikosk@microsoft.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n50 3.\n08 87\n3v 1\n[ cs\n.L G\n] 3\n0 M\nar 2\n01 5"
    }, {
      "heading" : "1 CONTRIBUTIONS",
      "text" : "We provide a statistical motivation for label embedding by demonstrating that the optimal rankconstrained least squares estimator can be constructed from an optimal unconstrained estimator of an embedding of the labels. In other words, embedding can provide beneficial sample complexity reduction even if computational constraints are not binding.\nWe identify a natural object to define label similarity: the expected outer product of the conditional label probabilities. In particular, in conjunction with a low-rank constraint, this indicates two label embeddings are similar when their conditional probabilities are linearly dependent across the dataset. This unifies prior work utilizing the confusion matrix for multiclass (Bengio et al., 2010) and the empirical label covariance for multilabel (Tai & Lin, 2012).\nWe apply techniques from randomized linear algebra (Halko et al., 2011) to develop an efficient and scalable algorithm for constructing the embeddings, essentially via a novel randomized algorithm for partial least squares (Geladi & Kowalski, 1986). Intuitively, this technique implicitly decomposes the prediction matrix of a model which would be prohibitively expensive to form explicitly."
    }, {
      "heading" : "2 DESCRIPTION",
      "text" : ""
    }, {
      "heading" : "2.1 NOTATION",
      "text" : "We denote vectors by lowercase letters x, y etc. and matrices by uppercase letters W , Z etc. The input dimension is denoted by d, the output dimension by c and the embedding dimension by k. For an m×n matrix X ∈ Rm×n we use ||X ||F for its Frobenius norm, X† for the pseudoinverse,ΠX,L for the projection onto the left singular subspace of X ."
    }, {
      "heading" : "2.2 PROPOSED ALGORITHM",
      "text" : "Our proposal is Rembrandt, described in Algorithm 1. We use the top right singular space of ΠX,LY as a label embedding, or equivalently, the top principal components of Y ⊤ΠX,LY (leveraging the fact that the projection is idempotent). Using randomized techniques, we can decompose this matrix without explicitly forming it, because we can compute the product of ΠX,LY with another matrix Q via Y ⊤ΠX,LY Q = Y ⊤XZ∗ where Z∗ = argminZ∈Rd×(k+p) ‖Y Q − XZ‖ 2 F . Algorithm 1\nAlgorithm 1 Rembrandt: Response EMBedding via RANDomized Techniques\n1: function REMBRANDT(k,X ∈ Rn×d, Y ∈ Rn×c) 2: (p, q) ← (20, 1) ⊲ These hyperparameters rarely need adjustment. 3: Q ← randn(c, k + p) 4: for i ∈ {1, . . . , q} do ⊲ Randomized range finder for Y ⊤ΠX,LY 5: Z ← argmin ‖Y Q−XZ‖2F 6: Q ← orthogonalize(Y ⊤XZ) 7: end for ⊲ NB: total of (q + 1) data passes, including next line 8: F ← (Y ⊤XQ)⊤(Y ⊤XQ) ⊲ F ∈ R(k+p)×(k+p) is “small” 9: (V,Σ2) ← eig(F, k) 10: V ← QV ⊲ V ∈ Rc×k is the embedding 11: return (V,Σ) 12: end function\nis a specialization of randomized PCA to this particular form of the matrix multiplication operator. Fortunately, because X†(ΠX,LY )k is the optimal rank-constrained least squares weight matrix, Rembrandt is a randomized algorithm for partial least squares (Geladi & Kowalski, 1986).\nAlgorithm 1 is inexpensive to compute. The matrix vector product Y Q is a sparse matrix-vector product so complexity O(nsk) depends only on the average (label) sparsity per example s and the embedding dimension k, and is independent of the number of classes c. The fit is done in the embedding space and therefore is independent of the number of classes c, and the outer product with the predicted embedding is again a sparse product with complexity O(nsk). The orthogonalization step is O(ck2), but this is amortized over the data set and essentially irrelevant as long as n > c. Furthermore random projection theory suggests k should grow only logarithmically with c."
    }, {
      "heading" : "2.3 EXPERIMENTS",
      "text" : ""
    } ],
    "references" : [ {
      "title" : "Logarithmic time online multiclass prediction",
      "author" : [ "Choromanska", "Anna", "Langford", "John" ],
      "venue" : "Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Choromanska et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Choromanska et al\\.",
      "year" : 2010
    }, {
      "title" : "Partial least-squares regression: a tutorial",
      "author" : [ "Geladi", "Paul", "Kowalski", "Bruce R" ],
      "venue" : "Analytica chimica acta,",
      "citeRegEx" : "Geladi et al\\.,? \\Q1986\\E",
      "shortCiteRegEx" : "Geladi et al\\.",
      "year" : 1986
    }, {
      "title" : "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions",
      "author" : [ "Halko", "Nathan", "Martinsson", "Per-Gunnar", "Tropp", "Joel A" ],
      "venue" : "SIAM review,",
      "citeRegEx" : "Halko et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Halko et al\\.",
      "year" : 2011
    }, {
      "title" : "Fastxml: a fast, accurate and stable tree-classifier for extreme multi-label learning",
      "author" : [ "Prabhu", "Yashoteja", "Varma", "Manik" ],
      "venue" : "In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining,",
      "citeRegEx" : "Prabhu et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Prabhu et al\\.",
      "year" : 2014
    }, {
      "title" : "Multilabel classification with principal label space transformation",
      "author" : [ "Tai", "Farbound", "Lin", "Hsuan-Tien" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Tai et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Tai et al\\.",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "We apply techniques from randomized linear algebra (Halko et al., 2011) to develop an efficient and scalable algorithm for constructing the embeddings, essentially via a novel randomized algorithm for partial least squares (Geladi & Kowalski, 1986).",
      "startOffset" : 51,
      "endOffset" : 71
    } ],
    "year" : 2015,
    "abstractText" : "Many modern multiclass and multilabel problems are characterized by increasingly large output spaces. For these problems, label embeddings have been shown to be a useful primitive that can improve computational and statistical efficiency. In this work we utilize a correspondence between rank constrained estimation and low dimensional label embeddings that uncovers a fast label embedding algorithm which works in both the multiclass and multilabel settings. The result is a randomized algorithm for partial least squares, whose running time is exponentially faster than naive algorithms. We demonstrate our techniques on two large-scale public datasets, from the Large Scale Hierarchical Text Challenge and the Open Directory Project, where we obtain state of the art results.",
    "creator" : "LaTeX with hyperref package"
  }
}