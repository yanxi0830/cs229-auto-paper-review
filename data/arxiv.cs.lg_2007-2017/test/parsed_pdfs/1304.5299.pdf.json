{
  "name" : "1304.5299.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Austerity in MCMC Land: Cutting the Metropolis-Hastings Budget",
    "authors" : [ "Anoop Korattikara", "Yutian Chen" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Markov chain Monte Carlo (MCMC) sampling has been the main workhorse of Bayesian computation since the 1990s. A canonical MCMC algorithm proposes samples from a distribution q and then accepts or rejects these proposals with a certain probability given by the Metropolis-Hastings (MH) formula [Metropolis et al., 1953, Hastings, 1970]. For each proposed sample, the MH rule needs to examine the likelihood of all dataitems. When the number of data-cases is large this is an awful lot of computation for one bit of information, namely whether to accept or reject a proposal.\nIn today’s Big Data world, we need to rethink our Bayesian inference algorithms. Standard MCMC methods do not meet the Big Data challenge for the reason described above. Researchers have made some progress in terms of making MCMC more efficient, mostly by focusing on parallelization. Very few question the algorithm itself: is the standard MCMC paradigm really optimally efficient in achieving its goals? We claim it is not.\nAny method that includes computation as an essential ingredient should acknowledge that there is a finite amount of time, T , to finish a calculation. An efficient MCMC algorithm should therefore decrease the “error” (properly defined) maximally in the given time T . For MCMC algorithms, there are two contributions to this error: bias and variance. Bias occurs because the chain needs to burn in during which it is sampling from the wrong distribution. Bias usually decreases fast, as evidenced by the fact that practitioners are willing to wait until the bias has (almost) completely vanished after which they discard these “burn-in samples”. The second cause of error is sampling variance, which occurs because of the random nature of the sampling process. The retained samples after burn-in will reduce the variance as O(1/T ).\nHowever, given a finite amount of computational time, it is not at all clear whether the strategy of retaining few unbiased samples and accepting an error dominated by variance is optimal. Perhaps, by decreasing the bias more slowly we could sample faster and thus reduce variance faster? In this paper we illustrate this effect by cutting the computational budget of the MH accept/reject step. To achieve that, we conduct sequential hypothesis tests to decide whether to accept or reject a given sample and find that the majority of these decisions can be made based on a small fraction of the data with high confidence. A\n∗akoratti@ics.uci.edu †yutian.chen@eng.cam.edu ‡welling@ics.uci.edu\nar X\niv :1\n30 4.\n52 99\nv4 [\ncs .L\nG ]\n1 4\nFe b\n20 14\nrelated method was used in Singh et al. [2012], where the factors of a graphical model are sub-sampled to compute fixed-width confidence intervals for the log-likelihood in the MH test.\nOur “philosophy” runs deeper than the algorithm proposed here. We advocate MCMC algorithms with a “bias-knob”, allowing one to dial down the bias at a rate that optimally balances error due to bias and variance. We only know of one algorithm that would also adhere to this strategy: stochastic gradient Langevin dynamics [Welling and Teh, 2011] and its successor stochastic gradient Fisher scoring [Ahn et al., 2012]. In their case the bias-knob was the stepsize. These algorithms do not have an MH step which resulted in occasional samples with extremely low probability. We show that our approximate MH step largely resolves this, still avoiding O(N) computations per iteration.\nIn the next section we introduce the MH algorithm and discuss its drawbacks. Then in Section 3, we introduce the idea of approximate MCMC methods and the bias variance trade-off involved. We develop approximate MH tests for Bayesian posterior sampling in Section 4 and present a theoretical analysis in Section 5. Finally, we show our experimental results in Section 6 and conclude in Section 7."
    }, {
      "heading" : "2 The Metropolis-Hastings algorithm",
      "text" : "MCMC methods generate samples from a distribution S0(θ) by simulating a Markov chain designed to have stationary distribution S0(θ). A Markov chain with a given stationary distribution can be constructed using the Metropolis-Hastings algorithm [Metropolis et al., 1953, Hastings, 1970], which uses the following rule for transitioning from the current state θt to the next state θt+1:\n1. Draw a candidate state θ′ from a proposal distribution q(θ′|θt)\n2. Compute the acceptance probability:\nPa = min [ 1, S0(θ′)q(θt|θ′) S0(θt)q(θ′|θt) ] (1)\n3. Draw u ∼ Uniform[0, 1]. If u < Pa set θt+1 ← θ′, otherwise set θt+1 ← θt.\nFollowing this transition rule ensures that the stationary distribution of the Markov chain is S0(θ). The samples from the Markov chain are usually used to estimate the expectation of a function f(θ) with respect\nto S0(θ). To do this we collect T samples and approximate the expectation I = 〈f〉S0 as Î = 1T ∑T t=1 f(θt). Since the stationary distribution of the Markov chain is S0, Î is an unbiased estimator of I (if we ignore burn-in).\nThe variance of Î is V = E[(〈f〉S0 − 1T ∑T t=1 f(θt))\n2], where the expectation is over multiple simulations of the Markov chain. It is well known that V ≈ σ2f,S0τ/T , where σ 2 f,S0 is the variance of f with respect to S0 and τ is the integrated auto-correlation time, which is a measure of the interval between independent samples [Gamerman and Lopes, 2006]. Usually, it is quite difficult to design a chain that mixes fast and therefore, the auto-correlation time will be quite high. Also, for many important problems, evaluating S0(θ) to compute the acceptance probability Pa in every step is so expensive that we can collect only a very small number of samples (T ) in a realistic amount of computational time. Thus the variance of Î can be prohibitively high, even though it is unbiased."
    }, {
      "heading" : "3 Approximate MCMC and the Bias-Variance Tradeoff",
      "text" : "Ironically, the reason MCMC methods are so slow is that they are designed to be unbiased. If we were to allow a small bias in the stationary distribution, it is possible to design a Markov chain that can be simulated cheaply [Welling and Teh, 2011, Ahn et al., 2012]. That is, to estimate I = 〈f〉S0 , we can use a Markov chain with stationary distribution S where is a parameter that can be used to control the bias in the algorithm. Then I can be estimated as Î = 1T ∑T t=1 f(θt), computed using samples from S instead of S0.\nAs → 0, S approaches S0 (the distribution of interest) but it becomes expensive to simulate the Markov chain. Therefore, the bias in Î is low, but the variance is high because we can collect only a small number of samples in a given amount of computational time. As moves away from 0, it becomes cheap to simulate\nthe Markov chain but the difference between S and S0 grows. Therefore, Î will have higher bias, but lower variance because we can collect a larger number of samples in the same amount of computational time. This is a classical bias-variance trade-off and can be studied using the risk of the estimator.\nThe risk can be defined as the mean squared error in Î, i.e. R = E[(I − Î)2], where the expectation is taken over multiple simulations of the Markov chain. It is easy to show that the risk can be decomposed as R = B2 + V , where B is the bias and V is the variance. If we ignore burn-in, it can be shown that B = 〈f〉S − 〈f〉S0 and V = E[(〈f〉S − 1T f(θt))\n2] ≈ σ2f,S τ/T . The optimal setting of that minimizes the risk depends on the amount of computational time available. If we have an infinite amount of computational time, we should set to 0. Then there is no bias, and the variance can be brought down to 0 by drawing an infinite number of samples. This is the traditional MCMC setting. However, given a finite amount of computational time, this setting may not be optimal. It might be better to tolerate a small amount of bias in the stationary distribution if it allows us to reduce the variance quickly, either by making it cheaper to collect a large number of samples or by mixing faster.\nIt is interesting to note that two recently proposed algorithms follow this paradigm: Stochastic Gradient Langevin Dynamics (SGLD) [Welling and Teh, 2011] and Stochastic Gradient Fisher Scoring (SGFS) [Ahn et al., 2012]. These algorithms are biased because they omit the required Metropolis-Hastings tests. However, in both cases, a knob (the step-size of the proposal distribution) is available to control the bias. As → 0, the acceptance probability Pa → 1 and the bias from not conducting MH tests disappears. However, when → 0 the chain mixes very slowly and the variance increases because the auto-correlation time τ →∞. As is increased from 0, the auto-correlation, and therefore the variance, reduces. But, at the same time, the acceptance probability reduces and the bias from not conducting MH tests increases as well.\nIn the next section, we will develop another class of approximate MCMC algorithms for the case where the target S0 is a Bayesian posterior distribution given a very large dataset. We achieve this by developing an approximate Metropolis-Hastings test, equipped with a knob for controlling the bias. Moreover, our algorithm has the advantage that it can be used with any proposal distribution. For example, our method allows approximate MCMC methods to be applied to problems where it is impossible to compute gradients (which is necessary to apply SGLD/SGFS). Or, we can even combine our method with SGLD/SGFS, to obtain the best of both worlds."
    }, {
      "heading" : "4 Approximate Metropolis-Hastings Test for Bayesian Posterior",
      "text" : "Sampling\nAn important method in the toolbox of Bayesian inference is posterior sampling. Given a dataset of N independent observations XN = {x1, . . . , xN}, which we model using a distribution p(x; θ) parameterized by θ, defined on a space Θ with measure Ω, and a prior distribution ρ(θ), the task is to sample from the\nposterior distribution S0(θ) ∝ ρ(θ) ∏N i=1 p(xi; θ).\nIf the dataset has a billion datapoints, it becomes very painful to compute S0(.) in the MH test, which has to be done for each posterior sample we generate. Spending O(N) computation to get just 1 bit of information, i.e. whether to accept or reject a sample, is likely not the best use of computational resources.\nBut, if we try to develop accept/reject tests that satisfy detailed balance exactly with respect to the posterior distribution using only sub-samples of data, we will quickly see the no free lunch theorem kicking in. For example, the pseudo marginal MCMC method [Andrieu and Roberts, 2009] and the method developed by Lin et al. [2000] provide a way to conduct exact accept/reject tests using unbiased estimators of the likelihood. However, unbiased estimators of the likelihood that can be computed from mini-batches of data, such as the Poisson estimator [Fearnhead et al., 2008] or the Kennedy-Bhanot estimator [Lin et al., 2000] have very high variance for large datasets. Because of this, once we get a very high estimate of the likelihood, almost all proposed moves are rejected and the algorithm gets stuck.\nThus, we should be willing to tolerate some error in the stationary distribution if we want faster accept/reject tests. If we can offset this small bias by drawing a large number of samples cheaply and reducing the variance faster, we can establish a potentially large reduction in the risk.\nWe will now show how to develop such approximate tests by reformulating the MH test as a statistical decision problem. It is easy to see that the original MH test (Eqn. 1) is equivalent to the following procedure: Draw u ∼ Uniform[0, 1] and accept the proposal θ′ if the average difference µ in the log-likelihoods of θ′ and\nθt is greater than a threshold µ0, i.e. compute\nµ0 = 1\nN log\n[ u ρ(θt)q(θ\n′|θt) ρ(θ′)q(θt|θ′)\n] , and (2)\nµ = 1\nN N∑ i=1 li where li = log p(xi; θ ′)− log p(xi; θt) (3)\nThen if µ > µ0, accept the proposal and set θt+1 ← θ′. If µ ≤ µ0, reject the proposal and set θt+1 ← θt. This reformulation of the MH test makes it very easy to frame it as a statistical hypothesis test. Given µ0 and a random sample {li1 , . . . , lin} drawn without replacement from the population {l1, . . . , lN}, can we decide whether the population mean µ is greater than or less than the threshold µ0? The answer to this depends on the precision in the random sample. If the difference between the sample mean l̄ and µ0 is significantly greater than the standard deviation s of l̄, we can make the decision to accept or reject the proposal confidently. If not, we should draw more data to increase the precision of l̄ (reduce s) until we have enough evidence to make a decision.\nMore formally, we test the hypotheses H1 : µ > µ0 vs H2 : µ < µ0. To do this, we proceed as follows: We compute the sample mean l̄ and the sample standard deviation sl = √\n(l2 − (l̄)2) nn−1 . Then the standard deviation of l̄ can be estimated as:\ns = sl√ n\n√ 1− n− 1\nN − 1 (4)\nwhere √\n1− n−1N−1 , the finite population correction term, is applied because we are drawing the subsample without replacement from a finite-sized population. Then, we compute the test statistic:\nt = l̄ − µ0 s\n(5)\nApproximate MH test Input: θt, θ ′, , µ0, XN , m Output: accept 1: Initialize estimated means l̄← 0 and l2 ← 0 2: Initialize n← 0, done← false 3: Draw u ∼ Uniform[0,1] 4: while not done do 5: Draw mini-batch X of size min (m, N − n) without replacement from XN and set XN ← XN \\ X 6: Update l̄ and l2 using X , and n← n+ |X | 7: Estimate std s using Eqn. 4\n8: Compute δ ← 1− φn−1 (∣∣∣∣ l̄ − µ0s ∣∣∣∣) 9: if δ < then\n10: accept← true if l̄ > µ0 and false otherwise 11: done← true 12: end if 13: end while\nIf n is large enough for the central limit theorem (CLT) to hold, the test statistic t follows a standard Student-t distribution with n − 1 degrees of freedom, when µ = µ0 (see Fig. 7 in supplementary for an empirical verification). Then, we compute δ = 1 − φn−1(|t|) where φn−1(.) is the cdf of the standard Student-t distribution with n− 1 degrees of freedom. If δ < (a fixed threshold) we can confidently say that µ is significantly different from µ0. In this case, if l̄ > µ0, we decide µ > µ0, otherwise we decide µ < µ0. If δ ≥ , we do not have enough evidence to make a decision. In this case, we draw more data to reduce the uncertainty, s, in the sample mean l̄. We keep drawing more data until we have the required confidence (i.e. until δ < ). Note, that this procedure will terminate because when we have used all the available data, i.e.\nn = N , the standard deviation s is 0, the sample mean l̄ = µ and δ = 0 < . So, we will make the same decision as the original MH test would make. Pseudo-code for our test is shown in Algorithm . Here, we start with a mini-batch of size m for the first test and increase it by m datapoints when required.\nThe advantage of our method is that often we can make confident decisions with n < N datapoints and save on computation, although we introduce a small bias in the stationary distribution. But, we can use the computational time we save to draw more samples and reduce the variance. The bias-variance trade-off can be controlled by adjusting the knob . When is high, we make decisions without sufficient evidence and introduce a high bias. As → 0, we make more accurate decisions but are forced to examine more data which results in high variance.\nOur algorithm will behave erratically if the CLT does not hold, e.g. with very sparse datasets or datasets with extreme outliers. The CLT assumption can be easily tested empirically before running the algorithm to avoid such pathological situations. The sequential hypothesis testing method can also be used to speed-up Gibbs sampling in densely connected Markov Random Fields. We explore this idea briefly in Section F of the supplementary."
    }, {
      "heading" : "5 Error Analysis and Test Design",
      "text" : "In 5.1, we study the relation between the parameter , the error E of the complete sequential test, the error ∆ in the acceptance probability and the error in the stationary distribution. In 5.2, we describe how to design an optimal test that minimizes data usage given a bound on the error."
    }, {
      "heading" : "5.1 Error Analysis and Estimation",
      "text" : "The parameter is an upper-bound on the error of a single test and not the error of the complete sequential test. To compute this error, we assume a) n is large enough that the t statistics can be approximated with z statistics, and b) the joint distribution of the l̄’s corresponding to different mini-batches used in the test is multivariate normal. Under these assumptions, we can show that the test statistic at different stages of the sequential test follows a Gaussian Random Walk process. This allows us to compute the error of the sequential test E(µstd,m, ), and the expected proportion of the data required to reach a decision π̄(µstd,m, ), using an efficient dynamic programming algorithm. Note that E and π̄ depend on θ, θ′ and u\nonly through the ‘standardized mean’ defined as µstd(u, θ, θ ′) def =\n(µ(θ, θ′)− µ0(θ, θ′, u)) √ N − 1\nσl(θ, θ′) where σl is\nthe true standard deviation of the li’s. See Section A of the supplementary for a detailed derivation and an empirical validation of the assumptions.\nFig. 1 shows the theoretical and actual error of 1000 sequential tests for the logistic regression model described in Section 6.1. The error E(µstd,m, ) is highest in the worst case when µ = µ0. Therefore, E(0,m, ) is an upper-bound on E . Since the error decreases sharply as µ moves away from µ0, we can get a more useful estimate of E if we have some knowledge about the distribution of µstd’s that will be encountered during the Markov chain simulation.\nNow, let Pa, (θ, θ ′) be the actual acceptance probability of our algorithm and let ∆(θ, θ′) def = Pa, (θ, θ ′)− Pa(θ, θ ′) be the error in Pa, . In Section B of the supplementary, we show that for any (θ, θ ′):\n∆ = ∫ 1 Pa E(µstd(u))du− ∫ Pa 0 E(µstd(u))du (6)\nThus, the errors corresponding to different u’s partly cancel each other. As a result, although |∆(θ, θ′)| is upper-bounded by the worst-case error E(0,m, ) of the sequential test, the actual error is usually much smaller. For any given (θ, θ′), ∆ can be computed easily using 1-dimensional quadrature.\nFinally, we show that the error in the stationary distribution is bounded linearly by ∆max = supθ,θ′ |∆(θ, θ′)|. As noted above, ∆max ≤ E(0,m, ) but is usually much smaller. Let dv(P,Q) denote the total variation distance1 between two distributions, P and Q. If the transition kernel T0 of the exact Markov chain satisfies the\n1The total variation distance between two distributions P and Q, that are absolutely continuous w.r.t. measure Ω, is defined\nas dv(P,Q) def = 1\n2 ∫ θ∈Θ |fP (θ)− fQ(θ)|dΩ(θ) where fP and fQ are their respective densities (or Radon-Nikodym derivatives to\nbe more precise).\ncontraction condition dv(PT0,S0) ≤ ηdv(P,S0) for all probability distributions P with a constant η ∈ [0, 1), we can prove (see supplementary Section C) the following upper bound on the error in the stationary distribution:\nTheorem 1. The distance between the posterior distribution S0 and the stationary distribution of our approximate Markov chain S is upper bounded as:\ndv(S0,S ) ≤ ∆max 1− η"
    }, {
      "heading" : "5.2 Optimal Sequential Test Design",
      "text" : "We now briefly describe how to choose the parameters of the algorithm: , the error of a single test and m, the mini-batch size. A very simple strategy we recommend is to choose m ≈ 500 so that the Central Limit Theorem holds and keep as small as possible while maintaining a low average data usage. This rule works well in practice and is used in Experiments 6.1 - 6.4.\nThe more discerning practitioner can design an optimal test that minimizes the data used while keeping the error below a given tolerance. Ideally, we want to do this based on a tolerance on the error in the stationary distribution S . Unfortunately, this error depends on the contraction parameter, η, of the exact transition kernel, which is difficult to compute. A more practical choice is a bound on the error ∆ in the acceptance probability, since the error in S increases linearly with ∆. Since ∆ is a function of (θ, θ′), we can try to control the average value of ∆ over the empirical distribution of (θ, θ′) that would be encountered while simulating the Markov chain. Given a tolerance ∆∗ on this average error, we can find the optimal m and by solving the following optimization problem (e.g. using grid search) to minimize the average data usage :\nmin m,\nEθ,θ′ [Euπ̄(µstd(u, θ, θ′),m, )]\ns.t. Eθ,θ′ |∆(m, , θ, θ′)| ≤ ∆∗ (7)\nIn the above equation, we estimate the average data usage, Eu[π̄], and the error in the acceptance probability, ∆, using dynamic programming with one dimensional numerical quadrature on u. The empirical distribution for computing the expectation with respect to (θ, θ′) can be obtained using a trial run of the Markov chain.\nWithout a trial run the best we can do is to control the worst case error E(0,m, ) (which is also an upperbound on ∆) in each sequential test by solving the following minimization problem:\nmin m,\nπ̄(0,m, ) s.t. E(0,m, ) ≤ ∆∗ (8)\nBut this leads to a very conservative design as the worst case error is usually much higher than the average case error. We illustrate the sequential design in Experiment 6.5. More details and a generalization of this method is given in supplementary Section D."
    }, {
      "heading" : "6 Experiments",
      "text" : ""
    }, {
      "heading" : "6.1 Random Walk - Logistic Regression",
      "text" : "We first test our method using a random walk proposal q(θ′|θt) = N (θt, σ2RW ). Although the random walk proposal is not efficient, it is very useful for illustrating our algorithm because the proposal does not contain any information about the target distribution, unlike Langevin or Hamiltonian methods. So, the responsibility of converging to the correct distribution lies solely with the MH test. Also since q is symmetric, it does not appear in the MH test and we can use µ0 = 1 N log [uρ(θt)/ρ(θ\n′)]. The target distribution in this experiment was the posterior for a logistic regression model trained on the MNIST dataset for classifying digits 7 vs 9. The dataset consisted of 12214 datapoints and we reduced the dimensionality from 784 to 50 using PCA. We chose a zero mean spherical Gaussian prior with precision = 10, and set σRW = 0.01.\nIn Fig. 2, we show how the logarithm of the risk in estimating the predictive mean, decreases as a function of wall clock time. The predictive mean of a test point x∗ is defined as Ep(θ|XN )[p(x∗|θ)]. To calculate the risk, we first estimate the true predictive mean using a long run of Hybrid Monte Carlo. Then, we compute multiple estimates of the predictive mean from our approximate algorithm and obtain the risk as the mean squared error in these estimates. We plot the average risk of 2037 datapoints in the test set. Since the risk R = B2 +V = B2 + σ 2f T , we expect it to decrease as a function of time until the bias dominates the variance. The figure shows that even after collecting a lot of samples, the risk is still dominated by the variance and the minimum risk is obtained with > 0."
    }, {
      "heading" : "6.2 Independent Component Analysis",
      "text" : "Next, we use our algorithm to sample from the posterior distribution of the unmixing matrix in Independent Component Analysis (ICA) [Hyvärinen and Oja, 2000]. When using prewhitened data, the unmixing matrix W ∈ RD×D is constrained to lie on the Stiefel manifold of orthonormal matrices. We choose a prior that is uniform over the manifold and zero elsewhere. We model the data as p(x|W ) =\n|det(W )| ∏D j=1 [ 4 cosh2( 12w T j x) ]−1 where wj are the rows of W . Since the prior is zero outside the manifold, the same is true for the posterior. Therefore we use a random walk on the Stiefel manifold as a proposal distribution [Ouyang, 2008]. Since this is a symmetric proposal distribution, it does not appear in the MH test and we can use µ0 = 1 N log [u].\nTo perform a large scale experiment, we created a synthetic dataset by mixing 1.95 million samples of 4 sources: (a) a Classical music recording (b) street / traffic noise (c) & (d) 2 independent Gaussian sources. To measure the correctness of the sampler, we measure the risk in estimating I = Ep(W |X) [dA(W,W0)] where the test function dA is the Amari distance [Amari et al., 1996] and W0 is the true unmixing matrix. We computed the ground truth using a long run (T = 100K samples) of the exact MH algorithm. Then we ran each algorithm 10 times, each time for ≈ 6400 secs. We calculated the risk by averaging the squared error in the estimate from each Markov chain, over the 10 chains. This is shown in Fig. 3. Note that even after 6400 secs the variance dominates the bias, as evidenced by the still decreasing risk, except for the most biased algorithm with = 0.2. Also, the lowest risk at 6400 secs is obtained with = 0.1 and not the exact MH algorithm ( = 0). But we expect the exact algorithm to outperform all the approximate algorithms if we were to run for an infinite time."
    }, {
      "heading" : "6.3 Variable selection in Logistic Regression",
      "text" : "Now, we apply our MH test to variable selection in a logistic regression model using the reversible jump MCMC algorithm of Green [1995]. We use a model that is similar to the Bayesian LASSO model for linear regression described in Chen et al. [2011]. Specifically, given D input features, our parameter θ = {β, γ} where β is a vector of D regression coefficients and γ is a D dimensional binary vector that indicates whether\na particular feature is included in the model or not. The prior we choose for β is p(βj |γ, ν) = 12ν exp { − |βj |ν } if γj = 1. If γj = 0, βj does not appear in the model. Here ν is a shrinkage parameter that pushes βj towards 0, and we choose a prior p(ν) ∝ 1/ν. We also place a right truncated Poisson prior p(γ|λ) ∝ λ k(\nD k\n) k!\non γ to control the size of the model, k = ∑D j=1 γj We set λ = 10\n−10 in this experiment. Denoting the likelihood of the data by lN (β, γ), the posterior distribution after integrating out ν is p(β, γ|XN , yN , λ) ∝ lN (β, γ)‖β‖−k1 λkB(k,D−k+1) where B(., .) is the beta function. Instead of integrating out λ, we use it as a parameter to control the size of the model. We use the same proposal distribution as in Chen et al. [2011] which is a mixture of 3 type of moves that are picked randomly in each iteration: an update move, a birth move and a death move. A detailed description is given in Supplementary Section E.\nWe applied this to the MiniBooNE dataset from the UCI machine learning repository[Bache and Lichman,\n2013]. Here the task is to classify electron neutrinos (signal) from muon neutrinos (background). There are 130,065 datapoints (28% in +ve class) with 50 features to which we add a constant feature of 1’s. We randomly split the data into a training (80%) and testing (20%) set. To compute ground truth, we collected T=400K samples using the exact reversible jump algorithm ( = 0). Then, we ran the approximate MH algorithm with different values of for around 3500 seconds. We plot the risk in predictive mean of test data (estimated from 10 Markov chains) in Fig. 4. Again we see that the lowest risk is obtained with > 0.\nThe acceptance rates for the birth/death moves starts off at ≈ 20% but dies down to ≈ 2% once a good model is found. The acceptance rate for update moves is kept at ≈ 50%. The model also suffers from local minima. For the plot in Fig. 4, we started with only one variable and we ended up learning models with around 12 features, giving a classification error ≈ 15%. But, if we initialize the sampler with all features included and initialize β to the MAP value, we learn models with around 45 features, but with a lower classification error ≈ 10%. Both the exact reversible jump algorithm and our approximate version suffer from this problem. We should bear this in mind when interpreting “ground truth”. However, we have observed that when initialized with the same values, we obtain similar results with the approximate algorithm and the exact algorithm (see e.g. Fig. 13 in supplementary)."
    }, {
      "heading" : "6.4 Stochastic Gradient Langevin Dynamics",
      "text" : "Finally, we apply our method to Stochastic Gradient Langevin Dynamics[Welling and Teh, 2011]. In each iteration, we randomly draw a mini-batch Xn of size n, and propose:\nθ′ ∼ q(.|θ,Xn) = N ( θ + α\n2 ∇θ\n{ N\nn ∑ x∈Xn log p(x|θ) + log ρ(θ)\n} , α ) (9)\nThe proposed state θ′ is always accepted (without conducting any MH test). Since the acceptance probability approaches 1 as we reduce α, the bias from not conducting the MH test can be kept under control by using α ≈ 0. However, we have to use a reasonably large α to keep the mixing rate high. This can be problematic for some distributions, because SGLD relies solely on gradients of the log density and it can be easily thrown off track by large gradients in low density regions, unless α ≈ 0.\nAs an example, consider an L1-regularized linear regression model. Given a dataset {xi, yi}Ni=1 where xi are predictors and yi are targets, we use a Gaussian error model p(y|x, θ) ∝ exp { −λ2 (y − θ Tx)2 }\nand choose a Laplacian prior for the parameters p(θ) ∝ exp(−λ0‖θ‖1). For pedagogical reasons, we will restrict ourselves to a toy version of the problem where θ and x are one dimensional. We use a synthetic dataset with N = 10000 datapoints generated as yi = 0.5xi + ξ where ξ ∼ N (0, 1/3). We choose λ = 3 and λ0 = 4950, so that the prior is not washed out by the likelihood. The posterior density and the gradient of the log posterior are shown in figures 5(a) and 5(b) respectively.\nAn empirical histogram of samples obtained by running SGLD with α = 5×10−6 is shown in Fig. 5(c). The effect of omitting the MH test is quite severe here. When the sampler reaches the mode of the distribution, the Langevin noise occasionally throws it into the valley to the left, where the gradient is very high. This propels the sampler far off to the right, after which it takes a long time to find its way back to the mode. However, if we had used an MH accept-reject test, most of these troublesome jumps into the valley would be rejected because the density in the valley is much lower than that at the mode.\nTo apply an MH test, note that the SGLD proposal q(θ′|θ) can be considered a mixture of component kernels q(θ′|θ,Xn) corresponding to different mini-batches. The mixture kernel will satisfy detailed balance with respect to the posterior distribution if the MH test enforces detailed balance between the posterior and each of the component kernels q(θ′|θ,Xn). Thus, we can use an MH test with µ0 = 1\nN log\n[ u ρ(θt)q(θ\n′|θt,Xn) ρ(θ′)q(θt|θ′,Xn)\n] .\nThe result of running SGLD (keeping α = 5× 10−6 as before) corrected using our approximate MH test, with = 0.5, is shown in Fig. 5(d). As expected, the MH test rejects most troublesome jumps into the valley because the density in the valley is much lower than that at the mode. The stationary distribution is almost indistinguishable from the true posterior. Note that when = 0.5, a decision is always made in the first step (using just m = 500 datapoints) without querying additional data sequentially."
    }, {
      "heading" : "6.5 Optimal Design of Sequential Tests",
      "text" : "We illustrate the advantages of the optimal test design proposed in Section 5.2 by applying it to the ICA experiment described in Section 6.2. We consider two design methods: the ‘average design’ (Eqn. 7) and the ‘worst-case design’ (Eqn. 8). For the average design, we collected 100 samples of the Markov chain to approximate the expectation of the error over (θ, θ′). We will call these samples the training set. The worst case design does not need the training set as it does not involve the distribution of (θ, θ′). We compute the optimal m and using grid search, for different values of the target training error, for both designs. We then collect a new set of 100 samples (θ, θ′) and measure the average error and data usage on this test set\n(Fig. 6). For the same target error on the training set, the worst-case design gives a conservative parameter setting that achieves a much smaller error on the test set. In contrast, the average design achieves a test error that is almost the same as the target error (Fig. 6(a)). Therefore, it uses much less data than the worst-case design (Fig. 6(b)).\nWe also analyze the performance in the case where we fix m = 600 and only change . This is a simple heuristic we recommended at the beginning of Section 5.2. Although this usually works well, using the optimal test design ensures the best possible performance. In this experiment, we see that when the error is large, the optimal design uses only half the data (Fig. 6(b)) used by the heuristic and is therefore twice as fast."
    }, {
      "heading" : "7 Conclusions and Future Work",
      "text" : "We have taken a first step towards cutting the computational budget of the Metropolis-Hastings MCMC algorithm, which takes O(N) likelihood evaluations to make the binary decision of accepting or rejecting a proposed sample. In our approach, we compute the probability that a new sample will be accepted based on a subset of the data. We increase the cardinality of the subset until a prescribed confidence level is reached. In the process we create a bias, which is more than compensated for by a reduction in variance due to the fact that we can draw more samples per unit time. Current MCMC procedures do not take these trade-offs into account. In this work we use a fixed decision threshold for accepting or rejecting a sample, but in theory a better algorithm can be obtained by adapting this threshold over time. An adaptive algorithm can tune bias and variance contributions in such a way that at every moment our risk (the sum of squared bias and variance) is as low as possible. We leave these extensions for future work."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank Alex Ihler, Daniel Gillen, Sungjin Ahn and Babak Shahbaba for their valuable suggestions. This material is based upon work supported by the National Science Foundation under Grant No. 1216045."
    }, {
      "heading" : "A Distribution of the test statistic",
      "text" : "In the sequential test, we first compute the test statistic from a mini-batch of size m. If a decision cannot be made with this statistic, we keep increasing the mini-batch size by m datapoints until we reach a decision. This procedure is guaranteed to terminate as explained in Section 4.\nThe parameter controls the probability of making an error in a single test and not the complete sequential test. As the statistics across multiple tests are correlated with each other, we should first obtain the joint distribution of these statistics in order to estimate the error of the complete sequential test. Let l̄j and sl,j be the sample mean and standard deviation respectively, computed using the first j mini-batches. Notice that when the size of a mini-batch is large enough, e.g. n > 100, the central limit theorem applies, and also sl,j is an accurate estimate of the population standard deviation. Additionally, since the degrees of freedom is high, the t-statistic in Eqn. 5 reduces to a z-statistic. Therefore, it is reasonable to make the following assumptions:\nAssumption 1. The joint distribution of the sequence (l̄1, l̄2, . . . ) follows a multivariate normal distribution.\nAssumption 2. sl = σl, where σl = std({li})\nFig. 7 shows that when µ = µ0 the empirical marginal distribution of tj (or zj) is well fitted by both a standard student-t and a standard normal distribution.\nUnder these assumptions, we state and prove the following proposition about the joint distribution of\nthe z-statistic z = (z1, z2, . . . ), where zj def = (l̄j − µ0)/σl ≈ tj , from different tests.\nProposition 2. Given Assumption 1 and 2, the sequence z follows a Gaussian random walk process:\nP (zj |z1, . . . , zj−1) = N (mj(zj−1), σ2z,j) (10)\nwhere\nmj(zj−1) = µstd πj − πj−1 1− πj−1 1√ πj(1− πj)\n+ zj−1 √ πj−1 πj 1− πj 1− πj−1\n(11)\nσ2z,j = πj − πj−1 πj(1− πj−1)\n(12)\nwith µstd = (µ−µ0)\n√ N−1\nσl being the standardized mean, and πj = jm/N the proportion of data in the first j\nmini-batches.\nProof of Proposition 2. Denote by xj the average of m l’s in the j-th mini-batch. Taking into account the fact that the l’s are drawn without replacement, we can compute the mean and covariance of the xj ’s as:\nE[xj ] = µ (13)\nCov(xi, xj) =  σ2l m ( 1− m− 1 N − 1 ) , i = j − σ 2 l\nN − 1 , i 6= j\n(14)\nIt is trivial to derive the expression for the mean. For the covariance, we first derive the covariance matrix\nof single data points as\nCov(lk, lk′) = Ek,k′ [lklk′ ]− Ek[lk]Ek′ [lk′ ] if k = k′\n= l2k − µ 2 def= σ2l\nif k 6= k′\n= Ek 6=k′ [lklk′ ]− µ2\n= 1 N(N − 1) ( ∑ k,k′ lkl ′ k − ∑ k l2k)− µ2\n= N\nN − 1 µ2 − l2k N − 1 − µ2\n= − σ 2 l\nN − 1 (15)\nNow, as xj can be written as a linear combination of the elements in j-th mini-batch as xj = 1 m1 T lj , the expression for covariance in Eqn. 14 follows immediately from:\nCov(xi, xj) = E[xixj ]− E[xi]E[xj ] = 1\nm2 1TCov(lil T j )1 (16)\nAccording to Assumption 1, the joint distribution of zj ’s is Gaussian because zj is a linear combination of l̄j ’s. It is however easier to derive the mean and covariance matrix of zj ’s by considering the vector z as a linear function of x: z = Q(x− µ01) with\nQ = ∣∣∣∣∣∣∣∣∣ d1 d2 . . .\ndj\n∣∣∣∣∣∣∣∣∣ ∣∣∣∣∣∣∣∣∣ 1 1 1 ... ... . . .\n1 1 . . . 1 ∣∣∣∣∣∣∣∣∣ (17) where\ndj =\n√ N − 1\njσx √ N−jm jm\n(18)\nThe mean and covariance can be computed as E[z] = Q1(µ − µ0) and Cov(z) = QCov(x)QT and the conditional distribution P (zj |z1, . . . , zj−1) follows straightforwardly. We conclude the proof by plugging the definition of µstd and πj into the distribution.\nFig. 8 shows the mean and 95% confidence interval of the random walk as a function of π with a few realizations of the z sequence. Notice that as the proportion of observed data πj approaches 1, the mean of zj approaches infinity with a constant variance of 1. This is consistent with the fact that when we observe all the data, we will always make a correct decision.\nIt is also worth noting that given the standardized mean µstd and πj , the process is independent of the actual size of a mini-batch m, population size N , or the variance of l’s σ2l . Thus, Eqns. 11 and 12 apply even if we use a different size for each mini-batch. This formulation allows us to study general properties of the sequential test, independent of any particular dataset.\nApplying the individual tests δ ≷ ⇔ |zj | ≷ Φ(1 − ) def = G at the j-th mini-batch corresponds to thresholding the absolute value of zj at πj with a bound G as shown in Fig. 9. Instead of m and , we will use π1 = m/N and G as the parameters of the sequential test in the supplementary. The probability of incorrectly deciding µ < µ0 when µ ≥ µ0 over the whole sequential test is computed as:\nE(µstd, π1, G) = J∑ j=1 P (zj < −G, |zi| ≤ G,∀i < j) (19)\nwhere J = d1/π1e is the maximum number of tests. Similarly the probability of incorrectly deciding µ ≥ µ0 when µ < µ0 can be computed similarly by replacing zj < −G with zj > G in Eqn. 19. We can also compute the expected proportion of data that will be used in the sequential test as:\nπ̄(µstd, π1, G) = Ez[πj′ ]\n= J∑ j=1 πjP (|zj | > G, |zi| ≤ G,∀i < j) (20)\nwhere j′ denotes the time when the sequential test terminates. Eqn. 19 and 20 can be efficiently approximated together using a dynamic programming algorithm by discretizing the value of zj between [−G,G]. The time complexity of this algorithm is O(L2J) where L is the number of discretized values.\nThe error and data usage as functions of µstd are maximum in the worst case scenario when µstd → 0⇔ µ→ µ0. In this case we have:\nE(0, π1, G) = lim µstd→0 E(µstd, π1, G) = (1− P (j′ = J))/2\ndef = Eworst(π1, G) (21)\nFigs. 1 and 10 show respectively that the theoretical value of the error (E) and the average data usage (π̄) estimated using our dynamic programming algorithm match the simulated values. Also, note that both error and data usage drop off very fast as µ moves away from µ0."
    }, {
      "heading" : "B Error in One Metropolis-Hastings Step",
      "text" : "In the approximate Metropolis-Hasting test, one first draws a uniform random variable u, and then conducts the sequential test. As µstd is a function of u (and µ, σl, both of which depend on θ and θ\n′), E measures the probability that one will make a wrong decision conditioned on u. One might expect that the average error in the accept/reject step of M-H using sequential test is the expected value of E w.r.t. to the distribution of u. But in fact, we can usually achieve a significantly smaller error than a typical value of E . This is because with a varying u, there is some probability that µ > µ0(u) and also some probability that µ < µ0(u). Part of the error one will make given a fixed u can be canceled when we marginalize out the distribution of u. Following the definition of µ0(u) for M-H in Eqn. 2, we can compute the actual error in the acceptance probability as:\n∆(µ(θ, θ′), σl(θ, θ ′), π1, G) = Pa, − Pa\n= ∫ 1 0 P (µ > µ0(u))du− ∫ Pa 0 du\n= ∫ 1 Pa P (µ > µ0(u))du− ∫ Pa 0 (1− P (µ > µ0(u)))du\n= ∫ 1 Pa E(µ− µ0(u))du− ∫ Pa 0 E(µ− µ0(u))du (22)\nTherefore, it is often observed in experiments (see Fig. 11 for example) that when Pa ≈ 0.5, a typical value of µstd(u) is close to 0, and the average value of the absolute error |E| can be large. But due to the cancellation of errors, the actual acceptance probability Pa, can approximate Pa very well. Fig. 12 shows the approximate Pa in one step of M-H. This result also suggests that making use of some (approximate) knowledge about µ and σl will help us obtain a much better estimate of the error than the worst case analysis in Eqn. 21."
    }, {
      "heading" : "C Proof of Theorem 1",
      "text" : "C.1 Upper Bound Based on One Step Error\nWe first prove a lemma that will be used for the proof of Theorem 1.\nLemma 3. Given two transition kernels, T0 and T , with respective stationary distributions, S0 and S , if T0 satisfies the following contraction condition with a constant η ∈ [0, 1) for all probability distributions P :\ndv(PT0,S0) ≤ ηdv(P,S0) (23)\nand the one step error between T0 and T is upper bounded uniformly with a constant ∆ > 0 as:\ndv(PT0, PT ) ≤ ∆,∀P (24)\nthen the distance between S0 and S is bounded as:\ndv(S0,S ) ≤ ∆\n1− η (25)\nProof. Consider a Markov chain with transition kernel T initialized from an arbitrary distribution P . Denote the distribution after t steps by P (t) def = PT t . At every time step, t ≥ 0, we apply the transition kernel T on P (t). According to the one step error bound in Eqn. 24, the distance between P (t+1) and the distribution obtained by applying T0 to P (t) is upper bounded as:\ndv(P (t+1), P (t)T0) = dv(P (t)T , P (t)T0) ≤ ∆ (26)\nFollowing the contraction condition of T0 in Eqn. 23, the distance of P (t)T0 from its stationary distribution S0 is less than P (t) as\ndv(P (t)T0,S0) ≤ ηdv(P (t),S0) (27)\nNow let us use the triangle inequality to combine Eqn. 26 and 27 to obtain an upper bounded for the distance between P (t+1) and S0:\ndv(P (t+1),S0) ≤ dv(P (t+1), P (t)T0) + dv(P (t)T0,S0)\n≤ ∆ + ηdv(P (t),S0) (28)\nLet r < 1− η be any positive constant and consider the ball B(S0, ∆r ) def = {P : dv(P,S0) < ∆r }. When P (t) is outside the ball, we have ∆ ≤ rdv(P (t), S). Plugging this into Eqn. 28, we can obtain a contraction condition for P (t) towards S0:\ndv(P (t+1),S0) ≤ (r + η)dv(P (t),S0) (29)\nSo if the initial distribution P is outside the ball, the Markov chain will move monotonically into the ball within a finite number of steps. Let us denote the first time it enters the ball as tr. If the initial distribution is already inside the ball, we simply let tr = 0. We then show by induction that P\n(t) will stay inside the ball for all t ≥ tr.\n1. At t = tr, P (t) ∈ B(S0, ∆r ) holds by the definition of tr.\n2. Assume P (t) ∈ B(S0, ∆r ) for some t ≥ tr. Then, following Eqn. 28, we have\ndv(P (t+1),S0) ≤ ∆ + η\n∆ r = r + η r ∆ < ∆ r\n=⇒ P (t+1) ∈ B(S0, ∆\nr ) (30)\nTherefore, P (t) ∈ B(S0, ∆r ) holds for all t ≥ tr. Since P (t) converges to S , it follows that:\ndv(S ,S0) < ∆\nr ,∀r < 1− η (31)\nTaking the limit r → 1− η, we prove the lemma:\ndv(S ,S) ≤ ∆\n1− η (32)\nC.2 Proof of Theorem 1\nWe first derive an upper bound for the one step error of the approximate Metropolis-Hastings algorithm, and then use Lemma 3 to prove Theorem 1. The transition kernel of the exact Metropolis-Hastings algorithm can be written as\nT0(θ, θ′) = Pa(θ, θ′)q(θ′|θ) + (1− Pa(θ, θ′))δD(θ′ − θ) (33)\nwhere δD is the Dirac delta function. For the approximate algorithm proposed in this paper, we use an approximate MH test with acceptance probability P̃a, (θ, θ ′) where the error, ∆Pa def = Pa, − Pa, is upper bounded as |∆Pa| ≤ ∆max. Now let us look at the distance between the distributions generated by one step of the exact kernel T0 and the approximate kernel T . For any P ,∫\nθ′ dΩ(θ′)|(PT )(θ′)− (PT0)(θ′)|\n= ∫ θ′ dΩ(θ′) ∣∣∣∣∫ θ dP (θ)∆Pa(θ, θ ′) (q(θ′|θ)− δD(θ′ − θ)) ∣∣∣∣ ≤ ∆max ∫ θ′ dΩ(θ′) ∣∣∣∣∫ θ dP (θ)(q(θ′|θ) + δD(θ′ − θ)) ∣∣∣∣\n= ∆max ∫ θ′ dΩ(θ′) (gQ(θ ′) + gP (θ ′)) = 2∆max (34)\nwhere gQ(θ ′) def = ∫ θ dP (θ)q(θ′|θ) is the density that would be obtained by applying one step of MetropolisHastings without rejection. So we get an upper bound for the total variation distance as\ndv(PT , PT0) = 1\n2 ∫ θ′ dΩ(θ′)|PT − PT0| ≤ ∆max (35)\nApply Lemma 3 with ∆ = ∆max and we prove Theorem 1."
    }, {
      "heading" : "D Optimal Sequential Test Design",
      "text" : "It is possible to design optimal tests that minimize the amount of data used while keeping the error below a given tolerance. Ideally, we want to do this based on a tolerance on the error in the stationary distribution S . Unfortunately, this error depends on the contraction parameter, η, of the exact transition kernel, which is difficult to compute. A more practical choice is a bound ∆max on the error in the acceptance probability, since the error in S increases linearly with ∆max.\nGiven ∆max, we want to minimize the average data usage π̄ over the parameters (or G) and/or m (or π1) of the sequential test. Unfortunately, the error is a function of µ and σl which depend on θ and θ\n′, and we cannot afford to change the test design at every iteration.\nOne solution is to base the design on the upper bound of the worst case error in Eqn. 21 which does not rely on µstd. But we have shown in Section B that this is a rather loose bound and will lead to a very conservative design that wastes the power of the sequential test. Therefore, we instead propose to design the\ntest by bounding the expectation of the error w.r.t. the distribution P (µ, σl). This leads to the following optimization problem:\nmin π1,G Eµ,σlEuπ̄(µ, σl, µ0(u), π1, G)\ns.t. Eµ,σl |∆(µ, σl, π1, G)| ≤ ∆max (36)\nThe expectation w.r.t. u can be computed accurately using one dimensional quadrature. For the expectation w.r.t. µ and σl, we collect a set of parameter samples (θ, θ\n′) during burn-in, compute the corresponding µ and σl for each sample, and use them to empirically estimate the expectation. We can also consider collecting samples periodically and adapting the sequential design over time. Once we obtain a set of samples {(µ, σl)}, the optimization is carried out using grid search.\nWe have been using a constant bound G across all the individual tests. This is known as the Pocock design [Pocock, 1977]. A more flexible sequential design can be obtained by allowing G to change as a function of π. Wang and Tsiatis [1987] proposed a bound sequence Gj = G0π 0.5−α j where α ∈ [0.5, 1] is a free parameter. When α = 0, it reduces to the Pocock design, and when α = 1, it reduces to O’Brien-Fleming design [O’Brien and Fleming, 1979]. We can adopt this more general form in our optimization problem straightforwardly, and the grid search will now be conducted over three parameters, π1, G0, and α."
    }, {
      "heading" : "E Reversible Jump MCMC",
      "text" : "We give a more detailed description of the different transition moves used in experiment 6.3. The update move is the usual MCMC move which involves changing the parameter vector β without changing the model γ. Specifically, we randomly pick an active component j : γj = 1 and set βj = βj+η where η ∼ N (0, σupdate). The birth move involves (for k < D) randomly picking an inactive component j : γj = 0 and setting γj = 1. We also propose a new value for βj ∼ N (0, σbirth). The birth move is paired with a corresponding death move (for k > 1) which involves randomly picking an active component j : γj = 1 and setting γj = 0. The corresponding βj is discarded. The probabilities of picking these moves p(γ → γ′) is the same as in Chen et al. [2011]. The value of µ0 used in the MH test for different moves is given below. 1. Update move:\nµ0 = 1\nN log [ u ‖β‖−k1 ‖β′‖−k1 ] (37)\n2. Birth move:\nµ0 = 1\nN log\n[ u ‖β‖−k1 p(γ → γ′)N (βj |0, σbirth)(D − k)\n‖β′‖−(k+1)1 p(γ′ → γ)λk\n] (38)\n2. Death move:\nµ0 = 1\nN ×\nlog [ u ‖β‖−k1 p(γ → γ′) ‖β′‖−(k−1)1 p(γ′ → γ) λ(k − 1) N (βj |0, σbirth)(D − k + 1) ] (39)\nWe used σupdate = 0.01 and σbirth = 0.1 in this experiment. As mentioned in the main text, both the exact reversible jump algorithm and our approximate version suffer from local minima. But, when initialized with the same values, we obtain similar results with both algorithms. For example, we plot the marginal posterior probability of including a feature in the model, i.e. p(γj = 1|XN , yN , λ) in figure 13."
    }, {
      "heading" : "F Application to Gibbs Sampling",
      "text" : "The same sequential testing method can be applied to the Gibbs sampling algorithm for discrete models. We study a model with binary variables in this paper while the extension to multi-valued variables is also possible. Consider running a Gibbs sampler on a probability distribution over D binary variables P (X1, . . . , XD). At every iteration, it updates one variable Xi using the following procedure:\n1. Compute the conditional probability:\nP (Xi = 1|x−i) = P (Xi = 1, x−i)\nP (Xi = 1, x−i) + P (Xi = 0, x−i) (40)\nwhere x−i denotes the value of all variables other than the i th one.\n2. Draw u ∼ Uniform[0, 1]. If u < P (Xi = 1|x−i) set Xi = 1, otherwise set Xi = 0.\nThe condition in step 2 is equivalent to checking:\nlog u\nlog(1− u) <\nlogP (Xi = 1, x−i) logP (Xi = 0, x−i) (41)\nWhen the joint distribution is expensive to compute but can be represented as a product over multiple terms, P (X) = ∏N n=1 fn(X), we can apply our sequential test to speed up the Gibbs sampling algorithm. In this case the variable µ0 and µ is given by\nµ0 = 1\nN\nlog u\nlog(1− u) (42)\nµ = 1\nN N∑ n=1 log fn(Xi = 1, x−i) fn(Xi = 0, x−i) (43)\nSimilar to the Metropolis-Hastings algorithm, given an upper bound in the error of the approximate conditional probability\n∆max = max i,x−i\n|P (Xi is assigned 1|x−i)− P (Xi = 1|x−i)|\nwe can prove the following theorem:\nTheorem 4. For a Gibbs sampler with a Dobrushin coefficient η ∈ [0, 1) [Brémaud, 1999, §7.6.2], the distance between the stationary distribution and that of the approximate Gibbs sampler S is upper bounded by\ndv(S0, S ) ≤ ∆max 1− η\nProof. The proof is similar to that of Theorem 1. We first obtain an upper bound for the one step error and then plug it into Lemma 3.\nThe exact transition kernel of the Gibbs sampler for variable Xi can be represented by a matrix T0,i of size 2D × 2D: T0,i(x, y) = {\n0 if x−i 6= y−i P (Yi = yi|y−i) otherwise\n(44)\nwhere 1 ≤ i ≤ N, x, y ∈ {0, 1}D. The approximate transition kernel T ,i can be represented similarly as T ,i(x, y) = {\n0 if x−i 6= y−i P (Yi = yi|y−i) otherwise\n(45)\nwhere P is the approximate conditional distribution. Define the approximation error ∆Ti(x, y) def = T ,i(x, y)− T0,i(x, y). We know that ∆Ti(x, y) = 0 if y−i 6= x−i and it is upper bounded by ∆max from the premise of Theorem 4.\nNotice that the total variation distance reduces to a half of the L1 distance for discrete distributions. For any distribution P , the one step error is bounded as\ndv(PT ,i, PT0,i) = 1\n2 ‖PT ,i − PT0,i‖1\n= 1\n2 ∑ y ∣∣∣∣∣∑ x P (x)∆T (x, y) ∣∣∣∣∣ = 1\n2 ∑ y ∣∣∣∣∣∣ ∑\nxi∈{0,1}\nP (xi, y−i)∆P (xi|y−i) ∣∣∣∣∣∣ ≤ 1\n2 ∆max ∑ y |P (Y−i = y−i)|\n= ∆max (46)\nFor a Gibbs sampling algorithm, we have the contraction condition [Brémaud, 1999, §7.6.2]:\ndv(PT , S) ≤ ηdv(P, S) (47)\nPlug ∆ = ∆max and η into Lemma 3 and we obtain the conclusion.\nF.1 Experiments on Markov Random Fields\nWe illustrate the performance of our approximate Gibbs sampling algorithm on a synthetic Markov Random Field. The model under consideration has D = 100 binary variables and they are densely connected by potential functions of three variables ψi,j,k(Xi, Xj , Xk),∀i 6= j 6= k. There are D(D − 1)(D − 2)/6 potential functions in total (we assume potential functions with permuted indices in the argument are the same potential function), and every function has 23 = 8 values. The entries in the potential function tables are drawn randomly from a log-normal distribution, logψi,j,k(Xi, Xj , Xk) ∼ N (0, 0.02). To draw a Gibbs sample for one variable Xi we have to compute (D − 1)(D − 2)/2 = 4851 pairs of potential functions as\nP (Xi = 1|x−i) P (Xi = 0|x−i) = ∏ i 6=j 6=k ψi,j,k(Xi = 1, xj , xk)∏ i 6=j 6=k ψi,j,k(Xi = 0, xj , xk)\n(48)\nThe approximate methods use a mini-batches of 500 pairs of potential functions at a time. We compare the exact Gibbs sampling algorithm with approximate versions with ∈ {0.01, 0.05, 0.1, 0.15, 0.2, 0.25}.\nTo measure the performance in approximating P (X) with samples xt, the ideal metric would be a distance between the empirical joint distribution and P . Since it is impossible to store all the 2100 probabilities, we instead repeatedly draw M = 1600 subsets of 5 variables, {sm}Mm=1, sm ⊂ {1, . . . , D}, |sm| = 5, and compute the average L1 distance of the joint distribution on these subsets between the empirical distribution and P :\nError = 1\nM ∑ sm ‖P̂ (Xsm)− P (Xsm)‖1 (49)\nThe true P is estimated by running exact Gibbs chains for a long time. We show the empirical conditional probability obtained by our approximate algorithms (percentage of Xi being assigned 1) for different in Fig. 14. It tends to underestimate large probabilities and overestimate on the other end. When = 0.01, the observed maximum error is within 0.01.\nFig. 15 shows the error for different as a function of the running time. For small , we use fewer mini-batches per iteration and thus generate more samples in the same amount of time than the exact Gibbs sampler. So the error decays faster in the beginning. As more samples are collected the variance is reduced. We see that these plots converge towards their bias floor while the exact Gibbs sampler out-performs all the approximate methods at around 1000 seconds."
    } ],
    "references" : [ {
      "title" : "Bayesian posterior sampling via stochastic gradient Fisher scoring",
      "author" : [ "S. Ahn", "A. Korattikara", "M. Welling" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Ahn et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Ahn et al\\.",
      "year" : 2012
    }, {
      "title" : "A new learning algorithm for blind signal separation",
      "author" : [ "S.-i. Amari", "A. Cichocki", "H.H. Yang" ],
      "venue" : "Advances in neural information processing systems,",
      "citeRegEx" : "Amari et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "Amari et al\\.",
      "year" : 1996
    }, {
      "title" : "The pseudo-marginal approach for efficient Monte Carlo computations",
      "author" : [ "C. Andrieu", "G.O. Roberts" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "Andrieu and Roberts.,? \\Q2009\\E",
      "shortCiteRegEx" : "Andrieu and Roberts.",
      "year" : 2009
    }, {
      "title" : "Markov chains: Gibbs fields, Monte Carlo simulation, and queues, volume 31",
      "author" : [ "P. Brémaud" ],
      "venue" : null,
      "citeRegEx" : "Brémaud.,? \\Q1999\\E",
      "shortCiteRegEx" : "Brémaud.",
      "year" : 1999
    }, {
      "title" : "A Bayesian Lasso via reversible-jump MCMC",
      "author" : [ "X. Chen", "Z. Jane Wang", "M.J. McKeown" ],
      "venue" : "Signal Processing,",
      "citeRegEx" : "Chen et al\\.,? \\Q1920\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 1920
    }, {
      "title" : "Particle filters for partially observed diffusions",
      "author" : [ "P. Fearnhead", "O. Papaspiliopoulos", "G.O. Roberts" ],
      "venue" : "Journal of the Royal Statistical Society: Series B (Statistical Methodology),",
      "citeRegEx" : "Fearnhead et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Fearnhead et al\\.",
      "year" : 2008
    }, {
      "title" : "Markov chain Monte Carlo: stochastic simulation for Bayesian inference, volume 68",
      "author" : [ "D. Gamerman", "H.F. Lopes" ],
      "venue" : null,
      "citeRegEx" : "Gamerman and Lopes.,? \\Q2006\\E",
      "shortCiteRegEx" : "Gamerman and Lopes.",
      "year" : 2006
    }, {
      "title" : "Reversible jump Markov chain Monte Carlo computation and Bayesian model determination",
      "author" : [ "P.J. Green" ],
      "venue" : null,
      "citeRegEx" : "Green.,? \\Q1995\\E",
      "shortCiteRegEx" : "Green.",
      "year" : 1995
    }, {
      "title" : "Monte Carlo sampling methods using Markov chains and their applications",
      "author" : [ "W.K. Hastings" ],
      "venue" : null,
      "citeRegEx" : "Hastings.,? \\Q1970\\E",
      "shortCiteRegEx" : "Hastings.",
      "year" : 1970
    }, {
      "title" : "Independent component analysis: algorithms and applications",
      "author" : [ "A. Hyvärinen", "E. Oja" ],
      "venue" : "Neural networks,",
      "citeRegEx" : "Hyvärinen and Oja.,? \\Q2000\\E",
      "shortCiteRegEx" : "Hyvärinen and Oja.",
      "year" : 2000
    }, {
      "title" : "A noisy Monte Carlo algorithm",
      "author" : [ "L. Lin", "K. Liu", "J. Sloan" ],
      "venue" : "Physical Review D,",
      "citeRegEx" : "Lin et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2000
    }, {
      "title" : "Equation of state calculations by fast computing machines",
      "author" : [ "N. Metropolis", "A.W. Rosenbluth", "M.N. Rosenbluth", "A.H. Teller", "E. Teller" ],
      "venue" : "The journal of chemical physics,",
      "citeRegEx" : "Metropolis et al\\.,? \\Q1953\\E",
      "shortCiteRegEx" : "Metropolis et al\\.",
      "year" : 1953
    }, {
      "title" : "A multiple testing procedure for clinical trials",
      "author" : [ "P.C. O’Brien", "T.R. Fleming" ],
      "venue" : null,
      "citeRegEx" : "O.Brien and Fleming.,? \\Q1979\\E",
      "shortCiteRegEx" : "O.Brien and Fleming.",
      "year" : 1979
    }, {
      "title" : "Bayesian Additive Regression Kernels",
      "author" : [ "Z. Ouyang" ],
      "venue" : "PhD thesis, Duke University,",
      "citeRegEx" : "Ouyang.,? \\Q2008\\E",
      "shortCiteRegEx" : "Ouyang.",
      "year" : 2008
    }, {
      "title" : "Group sequential methods in the design and analysis of clinical trials",
      "author" : [ "S.J. Pocock" ],
      "venue" : null,
      "citeRegEx" : "Pocock.,? \\Q1977\\E",
      "shortCiteRegEx" : "Pocock.",
      "year" : 1977
    }, {
      "title" : "Monte Carlo MCMC: efficient inference by approximate sampling",
      "author" : [ "S. Singh", "M. Wick", "A. McCallum" ],
      "venue" : "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,",
      "citeRegEx" : "Singh et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Singh et al\\.",
      "year" : 2012
    }, {
      "title" : "Approximately optimal one-parameter boundaries for group sequential trials",
      "author" : [ "S.K. Wang", "A.A. Tsiatis" ],
      "venue" : null,
      "citeRegEx" : "Wang and Tsiatis.,? \\Q1987\\E",
      "shortCiteRegEx" : "Wang and Tsiatis.",
      "year" : 1987
    }, {
      "title" : "Bayesian learning via stochastic gradient Langevin dynamics",
      "author" : [ "M. Welling", "Y. Teh" ],
      "venue" : "In Proceedings of the 28th International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Welling and Teh.,? \\Q2011\\E",
      "shortCiteRegEx" : "Welling and Teh.",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 17,
      "context" : "We only know of one algorithm that would also adhere to this strategy: stochastic gradient Langevin dynamics [Welling and Teh, 2011] and its successor stochastic gradient Fisher scoring [Ahn et al.",
      "startOffset" : 109,
      "endOffset" : 132
    }, {
      "referenceID" : 0,
      "context" : "We only know of one algorithm that would also adhere to this strategy: stochastic gradient Langevin dynamics [Welling and Teh, 2011] and its successor stochastic gradient Fisher scoring [Ahn et al., 2012].",
      "startOffset" : 186,
      "endOffset" : 204
    }, {
      "referenceID" : 14,
      "context" : "related method was used in Singh et al. [2012], where the factors of a graphical model are sub-sampled to compute fixed-width confidence intervals for the log-likelihood in the MH test.",
      "startOffset" : 27,
      "endOffset" : 47
    }, {
      "referenceID" : 6,
      "context" : "It is well known that V ≈ σ f,S0τ/T , where σ 2 f,S0 is the variance of f with respect to S0 and τ is the integrated auto-correlation time, which is a measure of the interval between independent samples [Gamerman and Lopes, 2006].",
      "startOffset" : 203,
      "endOffset" : 229
    }, {
      "referenceID" : 17,
      "context" : "It is interesting to note that two recently proposed algorithms follow this paradigm: Stochastic Gradient Langevin Dynamics (SGLD) [Welling and Teh, 2011] and Stochastic Gradient Fisher Scoring (SGFS) [Ahn et al.",
      "startOffset" : 131,
      "endOffset" : 154
    }, {
      "referenceID" : 0,
      "context" : "It is interesting to note that two recently proposed algorithms follow this paradigm: Stochastic Gradient Langevin Dynamics (SGLD) [Welling and Teh, 2011] and Stochastic Gradient Fisher Scoring (SGFS) [Ahn et al., 2012].",
      "startOffset" : 201,
      "endOffset" : 219
    }, {
      "referenceID" : 2,
      "context" : "For example, the pseudo marginal MCMC method [Andrieu and Roberts, 2009] and the method developed by Lin et al.",
      "startOffset" : 45,
      "endOffset" : 72
    }, {
      "referenceID" : 5,
      "context" : "However, unbiased estimators of the likelihood that can be computed from mini-batches of data, such as the Poisson estimator [Fearnhead et al., 2008] or the Kennedy-Bhanot estimator [Lin et al.",
      "startOffset" : 125,
      "endOffset" : 149
    }, {
      "referenceID" : 10,
      "context" : ", 2008] or the Kennedy-Bhanot estimator [Lin et al., 2000] have very high variance for large datasets.",
      "startOffset" : 40,
      "endOffset" : 58
    }, {
      "referenceID" : 2,
      "context" : "For example, the pseudo marginal MCMC method [Andrieu and Roberts, 2009] and the method developed by Lin et al. [2000] provide a way to conduct exact accept/reject tests using unbiased estimators of the likelihood.",
      "startOffset" : 46,
      "endOffset" : 119
    }, {
      "referenceID" : 9,
      "context" : "2 Independent Component Analysis Next, we use our algorithm to sample from the posterior distribution of the unmixing matrix in Independent Component Analysis (ICA) [Hyvärinen and Oja, 2000].",
      "startOffset" : 165,
      "endOffset" : 190
    }, {
      "referenceID" : 13,
      "context" : "Therefore we use a random walk on the Stiefel manifold as a proposal distribution [Ouyang, 2008].",
      "startOffset" : 82,
      "endOffset" : 96
    }, {
      "referenceID" : 1,
      "context" : "To measure the correctness of the sampler, we measure the risk in estimating I = Ep(W |X) [dA(W,W0)] where the test function dA is the Amari distance [Amari et al., 1996] and W0 is the true unmixing matrix.",
      "startOffset" : 150,
      "endOffset" : 170
    }, {
      "referenceID" : 6,
      "context" : "3 Variable selection in Logistic Regression Now, we apply our MH test to variable selection in a logistic regression model using the reversible jump MCMC algorithm of Green [1995]. We use a model that is similar to the Bayesian LASSO model for linear regression described in Chen et al.",
      "startOffset" : 167,
      "endOffset" : 180
    }, {
      "referenceID" : 4,
      "context" : "We use a model that is similar to the Bayesian LASSO model for linear regression described in Chen et al. [2011]. Specifically, given D input features, our parameter θ = {β, γ} where β is a vector of D regression coefficients and γ is a D dimensional binary vector that indicates whether a particular feature is included in the model or not.",
      "startOffset" : 94,
      "endOffset" : 113
    }, {
      "referenceID" : 4,
      "context" : "We use the same proposal distribution as in Chen et al. [2011] which is a mixture of 3 type of moves that are picked randomly in each iteration: an update move, a birth move and a death move.",
      "startOffset" : 44,
      "endOffset" : 63
    }, {
      "referenceID" : 17,
      "context" : "4 Stochastic Gradient Langevin Dynamics Finally, we apply our method to Stochastic Gradient Langevin Dynamics[Welling and Teh, 2011].",
      "startOffset" : 109,
      "endOffset" : 132
    }, {
      "referenceID" : 14,
      "context" : "This is known as the Pocock design [Pocock, 1977].",
      "startOffset" : 35,
      "endOffset" : 49
    }, {
      "referenceID" : 12,
      "context" : "When α = 0, it reduces to the Pocock design, and when α = 1, it reduces to O’Brien-Fleming design [O’Brien and Fleming, 1979].",
      "startOffset" : 98,
      "endOffset" : 125
    }, {
      "referenceID" : 13,
      "context" : "This is known as the Pocock design [Pocock, 1977]. A more flexible sequential design can be obtained by allowing G to change as a function of π. Wang and Tsiatis [1987] proposed a bound sequence Gj = G0π 0.",
      "startOffset" : 21,
      "endOffset" : 169
    }, {
      "referenceID" : 4,
      "context" : "The probabilities of picking these moves p(γ → γ′) is the same as in Chen et al. [2011]. The value of μ0 used in the MH test for different moves is given below.",
      "startOffset" : 69,
      "endOffset" : 88
    } ],
    "year" : 2014,
    "abstractText" : "Can we make Bayesian posterior MCMC sampling more efficient when faced with very large datasets? We argue that computing the likelihood for N datapoints in the Metropolis-Hastings (MH) test to reach a single binary decision is computationally inefficient. We introduce an approximate MH rule based on a sequential hypothesis test that allows us to accept or reject samples with high confidence using only a fraction of the data required for the exact MH rule. While this method introduces an asymptotic bias, we show that this bias can be controlled and is more than offset by a decrease in variance due to our ability to draw more samples per unit of time.",
    "creator" : "LaTeX with hyperref package"
  }
}