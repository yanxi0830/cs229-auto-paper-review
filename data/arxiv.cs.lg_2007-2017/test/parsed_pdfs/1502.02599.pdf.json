{
  "name" : "1502.02599.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Adaptive Random SubSpace Learning (RSSL) Algorithm for Prediction",
    "authors" : [ "Mohamed Elshrif", "Ernest Fokoué" ],
    "emails" : [ "MME4362@RIT.EDU", "EPFEQA@RIT.EDU" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Given a dataset D = {zi = (x>i ,yi)>, i = 1, · · · ,n}, where xi = (xi1, · · · ,xip)> ∈X ⊂ IRp and yi ∈Y are realizations of two random variables X and Y respectively, we seek to use the data D to build estimators f̂ of the underlying func-\ntion f for predicting the response Y given the vector X of explanatory variables. In keeping with the standard in statistical learning theory, we will measure the predictive performance of any given function f using the theoretical risk functional given by\nR( f ) = E[`(Y, f (X))] = ∫\nX ×Y `(x,y)dP(x,y), (1)\nwith the ideal scenario corresponding to the universally best function defined by\nf̂ ∗ = arginf f {R( f )}= arginf f {E[`(Y, f (X))]} . (2)\nFor classification tasks, the most commonly used loss function is the zero-one loss `(Y, f (X)) = 1{Y 6= f (X)}, for which the theoretical universal best defined in (2) is the Bayes classifier given by f ∗(x) = argmax\ny∈Y {Pr[Y = y|x]}. For re-\ngression tasks, the squared loss `(Y, f (X)) = (Y − f (X))2 is by far the most commonly used, mainly because of the wide variety of statistical, mathematical and computational benefits it offers. For regression under the squared loss, the universal best defined in (2) is also known theoretically known to be the conditional expectation of Y given X , specifically given by f ∗(x) = E[Y |X = x]. Unfortunately, these theoretically expressions of the best estimators cannot be realized in practice because the distribution function P(x,y) of (X ,Y ) defined on X ×Y is unknown. To circumvent this learning challenge, one has to do essentially two foundational thing, namely: (a) choose a certain function class F (approximation) from which to search for the estimator f̂ of the true but unknown underlying f , (b) specify the empirical version of (1) based on the given sample D , an use that empirical risk as the practical objective function. However, in this paper, we do not directly construct our estimating classification functions from the empirical risk. Instead, we build the estimators using other optimality criteria, and then compare their predictive performances using the average test error AVTE(·), namely\nAVTE( f̂ ) = 1 R\nR\n∑ r=1 { 1 m m ∑ t=1 `(y(r)it , f̂r(x (r) it )) } , (3)\nar X\niv :1\n50 2.\n02 59\n9v 1\n[ cs\n.L G\n] 9\nF eb\n2 01\nwhere f̂r(·) is the r-th realization of the estimator f̂ (·) built using the training portion of the split of D into training set and test set, and ( x(r)it ,y (r) it ) is the t-th observation from the test set at the r-th random replication of the split of D . In this paper, we consider both multiclass classification tasks with response space Y = {1,2, · · · ,G} and regression tasks with Y = IR, and we focus on learning machines from a function class F whose members are ensemble learners in the sense of Definition (1). In machine learning, in order to improve the accuracy of a regression function, or a classification function, scholars tend to combine multiple estimators because it has been proven both theoretically and empirically (Tumer & Ghosh, 1995; Tumer & Oza, 1999) that an appropriate combination of good base learners leads to a reduction in prediction error. This technique is known as ensemble learning (aggregation). In spite of the underlying algorithm used, the ensemble learning technique most of the time (on average) outperforms the single learning technique, especially for prediction purposes (van Wezel & Potharst, 2007). There are many approaches of performing ensemble learning. Among these, there are two popular ensemble learning techniques, bagging (Breiman, 1996) and boosting (Freund, 1995). Many variants of these two techniques have been studied previously such as random forest (Breiman, 2001) and AdaBoost (Freund & Schapire, 1997) and applied in a prediction problem. Our proposed method belongs to the subclass of ensemble learning methods known as random subspace learning.\nDefinition 1 Given an ensemble H = {h1,h2, · · · ,hL} of base learners hl : X −→ Y , with relative weight αl ∈ IR∗+ (usually αl ∈ (0,1) for convex aggregation), the ensemble representation f (L) of the underlying function f is given by the aggregation (weighted sum)\nf (L)(·) = L\n∑ l=1 αlhl(·). (4)\nA question naturally arises as to how the ensemble H is chosen, and how the weights are determined. Bootstrap Aggregating also known as bagging (Breiman, 1996), boosting (Freund & Schapire, 1996), random forests (Breiman, 2001), and bagging with subspaces (Panov & Dzeroski, 2007) are all predictive learning methods based on the ensemble learning principle for which the ensemble is built from the provided data set D and the weights are typically taken to be equal. In this paper, we focus on learning tasks involving high dimension low sample size (HDLSS) data, and we further zero-in on those data sets for which the number of explanatory variables p is substantially larger than the sample size n. As our main contribution in this paper, we introduce, develop and apply a new adaptation of the theme of random subspace learning (Ho, 1998) using the traditional multiple linear regres-\nsion (MLR) model as our base learner in regression and the generalized linear model (GLM) as a base learner in classification. Some applications by nature posses few instances (small n) with large number of features (p ≫ n) such as fMRI (Kuncheva et al., 2010) and DNA microarrays (Bertoni et al., 2005) data sets. It is hard for a traditional (conventional) algorithm to build a regression model, or to classify the data set when it possesses a very small instances to features ratio. The prediction problem becomes even more difficult when this huge number of features correlated are highly correlated, or irrelevant for the task of building such a model, as we will show later in this paper. Therefore, we harness the power of our proposed adaptive subspace learning technique to guide the choice/selection of good candidate features from the data set, and therefore select the best base learners, and ultimately the ensemble yielding the lowest possible prediction error. In most typical random subspace learning algorithms, the features are selected according to an equally likely scheme. The question then arises as to whether one can devise a better scheme to choose the candidate features for efficiently with some predictive benefits. On the other hand, it is interesting to assess the accuracy of our proposed algorithm under different levels of the correlation of the features. The answer to this question constitutes one of the central aspect of our proposed method, in the sense we explore a variety of weighting scheme for choosing the features, most of them (the schemes) based on statistical measures of relationship between the response variable and each explanatory variable. As the computational section will reveal, the weighting schemes proposed here lead to a substantially improvement in predictive performance of our method over random forest on all but one data set, arguably due to the fact that our method because it leverages the accuracy of the learning algorithm through selecting many good models (since the weighting scheme allows good variables to be selected more often and therefore leads to near optimal base learners)."
    }, {
      "heading" : "2. Related Work",
      "text" : "Traditionally, in a prediction problem, a single model is built based on the training set and the prediction is decided based solely on this single fitted model. However, in bagging, bootstrap samples are taken from the data set, then, for each instance, the model is fitted. Finally, the prediction is made based on the average of all bagged models. Mathematically, the prediction accuracy for the constructed model using bagging outperforms the traditional model and in the worst case it has the same performance. However, it must be said that it depends on the stability of the modeling procedure. It turns out that bagging reduces the variance without affecting the bias, thereby leading to an overall reduction in prediction error, and hence its great appeal. Any\nset of predictive models can be used as an ensemble in the sense defined earlier. There are many ensemble learning approaches. These approaches could be categorized into four classes: (1) algorithms that use heterogeneous predictive models such as stacking (Wolpert, 1992). (2) algorithms that manipulate the instances of the data sets such as bagging (Breiman, 1996), boosting (Freund & Schapire, 1996), random forests (Breiman, 2001), and bagging with subspaces (Panov & Dzeroski, 2007). (3) algorithms that maniplulate the features of the data sets such as random forests (Breiman, 2001), random subspaces (Ho, 1998), and bagging with subspaces (Panov & Dzeroski, 2007). (4) algorithms that manipulate the learning algorithm such as random forests (Breiman, 2001), neural networks ensemble (Hansen & Salamon, 1990), and extra-trees ensemble (Geurts et al., 2006). Since our proposed algorithm manipulates both the instances and features of the data sets, we will focus on the algorithms in the second and third categories (Breiman, 1996; 2001; Panov & Dzeroski, 2007; Ho, 1998).\nBagging (Breiman, 1996), or bootstrap aggregating is an ensemble learning method that generates multiple predictive models. These models are based on performing bootstrap replicates of the learning (training) data set and utilizing from each replicate to build a separate predictive model. The bootstrap sample is attained through randomly (uniformly) sampling with replacement from instances of the training data set. The decision is made based on averaging the predictor classifiers in regression task and taking the majority vote in classification task. Bagging tend to decrease the variance and keeps the bias as in the case of a single classifier. The bagging accuracy increases when the applied learner is unstable, which means that for any small fluctuation on the training data set causes large impact on the test data set such as trees (Breiman, 1996). Random forests (Breiman, 2001), is an ensemble learning method that averages the prediction results from multiple independent predictor (tree) models. It also performs bootstrap replicates, like bagging (Breiman, 1996), to construct different predictors. For each node of the tree, randomly selecting subset of the attributes. It is considered to improve over bagging through de-correlating the trees. Choose the best attribute from the selected subset. As (Denil et al., 2014) mentions that when building a random tree, there are three issues that should be decided in advance; (1) the leafs splitting method, (2) the type of predictor, and 3- the randomness method. Random subspace learning (Ho, 1998), is an ensemble learning method that constructs base models based on different features. It chooses a subset of features and then learns the base model depending only on these features. The random subspaces reaches the highest accuracy when the number of features is large as well as the number of instances. In addition, it performs good\nwhen there are redundant features on the data set. Bagging subspaces (Panov & Dzeroski, 2007), is an ensemble learning method that combines both the bagging (Breiman, 1996) and random subspaces (Ho, 1998) learning methods. It generates a bootstrap replicates of the training data set, in the same way as bagging. Then, it randomly chooses a subset from the features, in the same manner as random subspaces. It outperforms the bagging and random subspaces. Also, it is found to yield the same performance as random forests in case of using decision tree as a base learner. In the simulation part of this paper, we aim to answer the following research questions: (1) Is the performance of the adaptive random subspace learning (RSSL) better than the performance of single classifiers? (2) What is the performance of the adaptive RSSL compared to the most widely used classifier ensembles? (3) Is there a theoretical explanation as to why adaptive RSSL works well for most of the simulated and real-life data sets? (4) How does adaptive RSSL perform on different parameter settings and with various percentages of the instance-to-feature ratio (IFR)? (5) How does the correlation between features affect the predictive performance of adaptive RSSL?"
    }, {
      "heading" : "3. Adaptive RSSL",
      "text" : "In this section, we present an adaptive random subspace learning algorithm for the prediction problem. We start with the formulation of the problem, followed by our suggested solution (proposed algorithm) to tackle (handle) it. A crucial step of assessing the candidate features for building the models is explained in detail. Finally, we elucidate the strength of the new algorithm, from a theoretical perspective."
    }, {
      "heading" : "3.1. Problem formulation",
      "text" : "As we said earlier our proposed method belongs to the category of random subspace learning where each base learner is constructed using a bootstrap sample and a subset of the original p features. The main difference here is that we use base learners that are typically considered not to lead to any improvement when aggregated, and we also select features using weighting schemes that inspired for the strength of the relationship between each feature and the response (target). Each base learner is driven by the subset { j(l)1 , · · · , j (l) d } ⊂ {1,2, · · · , p} of d variables of predictors that are randomly select to build it, and the subsample D (l) drawn with replacement from D . For notational convenience, we use vectors of indicator variables to denote these two important quantities. The sample indicator δ (l) = (δ (l)1 , · · · ,δ (l) n ) ∈ {0,1}n, where\nδ (l)i = { 1 if zi ∈D (l) 0 otherwise\nThe variable indicator γ (l) = (γ(l)1 , · · · ,γ (l) p ) ∈ {0,1}p, where\nγ(l)j = { 1 if j ∈ { j(l)1 , · · · , j (l) d }\n0 otherwise\nThe estimator of the lth base learner can therefore fully and unambiguously denoted by ĝ(l)(·;γ (l),δ (l)) which we refer to as ĝ(l)(·) for notational simplicity. Each δ (`)j is chosen according to one of the weighting schemes. To increase the strength of the developed algorithm, we introduce a weighting scheme procedure to select the important features, which facilitates building a proper model and leverage the prediction accuracy. Our weighting schemes are\n• Correlation coefficient: We measure the strength of the association between each feature vector and the response (target), and take the square of the resulting sample correlation\n• F-statistic: For classification tasks especially, we use the observed F-statistic resulting from the analysis of variance with x j as the response and the class label the treatment group.\nUsing the ensemble { ĝ(l)(·), l = 1, · · · ,L }\nwe form the ensemble estimator of class Membership as\nf̂ (L)(x) = argmax y∈Y\n{ L\n∑ l=1\n( 1{y=ĝ(l)(x)} )} ,\nand the ensemble estimator of regression response as\nf̂ (L)(x∗) = 1 L\nL\n∑ l=1\nĝ(l)(x∗)."
    }, {
      "heading" : "4. Experiments",
      "text" : "We used a collection of simulated and real-world data sets for our experiments. In addition, we used real-world data sets from previous papers, which aim to solve the same problem, for comparison purpose. We report the mean square error (MSE) for each individual algorithm and task purposes, i.e., regression, or classification."
    }, {
      "heading" : "4.1. Simulated data sets",
      "text" : "We designed our artificial data sets to fit six scenarios based on the factors, which are the dimensionality of the data (number of features), the number of sample size ((number of instances), and the correlation of the data."
    }, {
      "heading" : "4.2. Real data sets",
      "text" : "We benefit from the public repository of the UCI University real-life data sets in our paper. For the purposes of\nconsistency and completeness, we choose the real data sets that carries different characteristics in terms of the number of instances and the number of features along with variety of applications. The real data sets can be represented based on the task as follows:\nTo elucidate the performance of our developed model, we compare the accuracy of the RSSL with random forest and ... on the same real data sets they used before."
    }, {
      "heading" : "5. Discussion",
      "text" : "As revealed (experienced) from our experiments on synthetic data sets that when the number of selected features is higher than 15-20 (for our particular dataset) yields ensemble classifiers that are highly accurate and stable. The reason for this is that only if the number of voters is Òlarge enoughÓ does the random process of attribute selection yield suKcient number of qualitatively different classifiers that ensure high accuracy and stability of the ensemble.\nhow many bootstrap replications are useful? The evidence both experimental and theoretical is that bagging can push a good but unstable procedure a significant step towards optimality. why the training set in real dataset was chosen to be large and in simulated dataset the test set used to be large? The bootstrap sample was repeated 50 times. The\nTable 1. Regression Analysis: Mean Square Error (MSE) for different machine learning algorithms on various scenarios of synthetic data sets.\nWEIGHTING N P ρ MLR UNIFORM MLR ADAPTIVE MLR RF BETTER?\nCORRELATION\n200 25 0.05 5.69±0.89 14.50±2.63 4.60±0.706 9.81±1.86 √ 200 25 0.5 4.78±0.81 11.67±2.55 4.77±0.94 8.46±1.97 √\n25 200 0.05 974.37±5.E3 18.35±6.92 8.10±3.86 18.56±7.24 √ 25 200 0.5 5.E3±5.E4 18.83±8.72 8.27±5.24 18.18±8.65 √ 50 1000 0.05 2.E4±1.E5 28.36±11.51 12.38±5.91 27.92±11.78 √ 1000 50 0.05 4.66±0.34 16.62±1.37 4.33±0.33 6.73±0.62 √\nF-STATISTICS\n200 25 0.05 5.04±0.79 14.42±2.67 4.48±0.74 8.75±1.76 √ 200 25 0.5 4.49±0.76 12.06±2.04 5.51±1.09 8.33±1.59 √\n25 200 0.05 3.E4±2.E5 17.77±9.15 5.81±4.10 15.81±8.55 √ 25 200 0.5 1.E4±1.E5 23.09±16.06 12.53±10.27 24.11±16.31 √ 50 1000 0.05 4.E5±3.E6 16.65±5.38 7.65±2.83 15.54±5.31 √ 1000 50 0.05 4.19±0.33 15.97±1.15 3.90±0.30 6.24±0.55 √\nrandom division of the data is repeated 100 times. Choosing between these two strategies is not an easy task since it involves a trade-off between bias and estimation variance over the forecast horizon.\nEven though that our developed adaptive RSSL algorithm outperforms many classifier ensembles. It has limitations where this new algorithm can not deal with data set that has categorical features. Instead it necessities to encode these features numerically. Also, the algorithm is not designed\nMLR U−MLR A−MLR Random Forest\n2 4\n6 8\n10 12 Average Test Error: Logarithmic Scale P re di ct iv e M ea n S qu ar ed E rr or\nFigure 3. A representative results of synthetic dataset of scenario with number of instances n=50, number of features p=1000, correlation coefficient ρ=0.05, number of learners=450, and number of replications=100. We used the correlation weighting scheme for regression analysis on logarithmic scale.\nto classify data sets with multiple classes. Moreover, the adaptive RSSL algorithms sometimes fails to select the optimal feature subsets?"
    }, {
      "heading" : "6. Conclusion and Future Work",
      "text" : "We presented a detailed quantitative analysis of the adaptive RSSL algorithm for an ensemble prediction problem. We support this analysis with deep theoretical (mathematical) explanation (formulation). The key important issues for the developed algorithm resides on four fundamental factors: generalization, flexibility, speed, and accuracy. We will explain each of these four factors. We present a rigorous theoretical justification of our propose algorithm. For now, we choose fixed number of attribute subset. However, the algorithm should evaluated based on the performance\n(accuracy) to determine the appropriate number (dimension) for single classifiers used in the ensemble learning. In addition, the adaptive RSSL algorithm is tested on a relatively small data sets. Our next step will be applying the developed algorithm on a big data sets.\nAlso, we show that the adaptive RSSL performs better than widely used ensemble algorithms even with the dependence of feature subsets.\nComputational issues."
    } ],
    "references" : [ {
      "title" : "Bio-molecular cancer prediction with random subspace ensembles of support vector machines",
      "author" : [ "Bertoni", "Alberto", "Folgieri", "Raffaella", "Valentini", "Giorgio" ],
      "venue" : null,
      "citeRegEx" : "Bertoni et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Bertoni et al\\.",
      "year" : 2005
    }, {
      "title" : "ISSN 0885-6125. doi: 10.1023/A: 1018054314350",
      "author" : [ "Breiman", "Leo" ],
      "venue" : "Bagging predictors. Machine Learning,",
      "citeRegEx" : "Breiman and Leo.,? \\Q1996\\E",
      "shortCiteRegEx" : "Breiman and Leo.",
      "year" : 1996
    }, {
      "title" : "Narrowing the gap: Random forests in theory and in practice",
      "author" : [ "Denil", "Misha", "Matheson", "David", "de Freitas", "Nando" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Denil et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Denil et al\\.",
      "year" : 2014
    }, {
      "title" : "Experiments with a new boosting",
      "author" : [ "Freund", "Yoav", "Schapire", "Robert E" ],
      "venue" : null,
      "citeRegEx" : "Freund et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "Freund et al\\.",
      "year" : 1996
    }, {
      "title" : "A decisiontheoretic generalization of on-line learning and an application to boosting",
      "author" : [ "Freund", "Yoav", "Schapire", "Robert E" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "Freund et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Freund et al\\.",
      "year" : 1997
    }, {
      "title" : "Extremely randomized trees",
      "author" : [ "Geurts", "Pierre", "Ernst", "Damien", "Wehenkel", "Louis" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Geurts et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Geurts et al\\.",
      "year" : 2006
    }, {
      "title" : "Neural network ensembles",
      "author" : [ "L.K. Hansen", "P. Salamon" ],
      "venue" : "IEEE Trans. Pattern Anal. Mach. Intell.,",
      "citeRegEx" : "Hansen and Salamon,? \\Q1990\\E",
      "shortCiteRegEx" : "Hansen and Salamon",
      "year" : 1990
    }, {
      "title" : "Classifier combining: Analytical results and implications",
      "author" : [ "Tumer", "Kagan", "Ghosh", "Joydeep" ],
      "venue" : "Proceedings of the AAAI-96 Workshop on Integrating Multiple Learned Models for Improving and Scaling Machine Learning Algorithms,",
      "citeRegEx" : "Tumer et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "Tumer et al\\.",
      "year" : 1995
    }, {
      "title" : "Decimated input ensembles for improved generalization",
      "author" : [ "Tumer", "Kagan", "Oza", "Nikunj C" ],
      "venue" : null,
      "citeRegEx" : "Tumer et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Tumer et al\\.",
      "year" : 1999
    }, {
      "title" : "Improved customer choice predictions using ensemble methods",
      "author" : [ "van Wezel", "Michiel", "Potharst", "Rob" ],
      "venue" : "European Journal of Operational Research,",
      "citeRegEx" : "Wezel et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Wezel et al\\.",
      "year" : 2007
    }, {
      "title" : "Stacked generalization",
      "author" : [ "Wolpert", "David H" ],
      "venue" : "Neural Networks,",
      "citeRegEx" : "Wolpert and H.,? \\Q1992\\E",
      "shortCiteRegEx" : "Wolpert and H.",
      "year" : 1992
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : ", 2010) and DNA microarrays (Bertoni et al., 2005) data sets.",
      "startOffset" : 28,
      "endOffset" : 50
    }, {
      "referenceID" : 5,
      "context" : "(4) algorithms that manipulate the learning algorithm such as random forests (Breiman, 2001), neural networks ensemble (Hansen & Salamon, 1990), and extra-trees ensemble (Geurts et al., 2006).",
      "startOffset" : 170,
      "endOffset" : 191
    }, {
      "referenceID" : 2,
      "context" : "As (Denil et al., 2014) mentions that when building a random tree, there are three issues that should be decided in advance; (1) the leafs splitting method, (2) the type of predictor, and 3- the randomness method.",
      "startOffset" : 3,
      "endOffset" : 23
    } ],
    "year" : 2015,
    "abstractText" : "We present a novel adaptive random subspace learning algorithm (RSSL) for prediction purpose. This new framework is flexible where it can be adapted with any learning technique. In this paper, we tested the algorithm for regression and classification problems. In addition, we provide a variety of weighting schemes to increase the robustness of the developed algorithm. These different wighting flavors were evaluated on simulated as well as on real-world data sets considering the cases where the ratio between features (attributes) and instances (samples) is large and vice versa. The framework of the new algorithm consists of many stages: first, calculate the weights of all features on the data set using the correlation coefficient and F-statistic statistical measurements. Second, randomly draw n samples with replacement from the data set. Third, perform regular bootstrap sampling (bagging). Fourth, draw without replacement the indices of the chosen variables. The decision was taken based on the heuristic subspacing scheme. Fifth, call base learners and build the model. Sixth, use the model for prediction purpose on test set of the data. The results show the advancement of the adaptive RSSL algorithm in most of the cases compared with the synonym (conventional) machine learning algorithms.",
    "creator" : "LaTeX with hyperref package"
  }
}