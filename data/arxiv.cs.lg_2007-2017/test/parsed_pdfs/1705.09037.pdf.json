{
  "name" : "1705.09037.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Deriving Neural Architectures from Sequence and Graph Kernels",
    "authors" : [ "Tao Lei", "Wengong Jin", "Regina Barzilay", "Tommi Jaakkola" ],
    "emails" : [ "<taolei@csail.mit.edu>,", "gong@csail.mit.edu>." ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Many recent studies focus on designing novel neural architectures for structured data such as sequences or annotated graphs. For instance, LSTM (Hochreiter & Schmidhuber, 1997), GRU (Chung et al., 2014) and other complex recurrent units (Zoph & Le, 2016) can be easily adapted to embed structured objects such as sentences (Tai et al., 2015) or molecules (Li et al., 2015; Dai et al., 2016) into vector spaces suitable for later processing by standard predictive methods. The embedding algorithms are typically integrated into an end-to-end trainable architecture so as to tailor the learnable embeddings directly to the task at hand.\nThe embedding process itself is characterized by a sequence operations summarized in a structure known as the computational graph. Each node in the computational\n*Equal contribution 1MIT Computer Science & Artificial Intelligence Laboratory. Correspondence to: Tao Lei <taolei@csail.mit.edu>, Wengong Jin <wengong@csail.mit.edu>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\ngraph identifies the unit/mapping applied while the arcs specify the relative arrangement/order of operations. The process of designing such computational graphs or associated operations for classes of objects is often guided by insights and expertise rather than a formal process.\nRecent work has substantially narrowed the gap between desirable computational operations associated with objects and how their representations are acquired. For example, value iteration calculations can be folded into convolutional architectures so as to optimize the representations to facilitate planning (Tamar et al., 2016). Similarly, inference calculations in graphical models about latent states of variables such as atom characteristics can be directly associated with embedding operations (Dai et al., 2016).\nWe appeal to kernels over combinatorial structures to define the appropriate computational operations. Kernels give rise to well-defined function spaces and possess rules of composition that guide how they can be built from simpler ones. The comparison of objects inherent in kernels is often broken down to elementary relations such as counting of common sub-structures as in\nK(χ, χ′) = ∑ s∈S 1[s ∈ χ]1[s ∈ χ′] (1)\nwhere S is the set of possible substructures. For example, in a string kernel (Lodhi et al., 2002), S may refer to all possible subsequences while a graph kernel (Vishwanathan et al., 2010) would deal with possible paths in the graph. Several studies have highlighted the relation between feed-forward neural architectures and kernels (Hazan & Jaakkola, 2015; Zhang et al., 2016) but we are unaware of any prior work pertaining to kernels associated with neural architectures for structured objects.\nIn this paper, we introduce a class of deep recurrent neural embedding operations and formally characterize their associated kernel spaces. The resulting kernels are parameterized in the sense that the neural operations relate objects of interest to virtual reference objects through kernels. These reference objects are parameterized and readily optimized for end-to-end performance.\nTo summarize, the proposed neural architectures, or Kernel Neural Networks 1 , enjoy the following advantages:\n1Code available at https://github.com/taolei87/icml17 knn\nar X\niv :1\n70 5.\n09 03\n7v 3\n[ cs\n.N E\n] 3\n0 O\nct 2\n01 7\nDeriving Neural Architectures from Sequence and Graph Kernels\n• The architecture design is grounded in kernel computations.\n• Our neural models remain end-to-end trainable to the task at hand.\n• Resulting architectures demonstrate state-of-the-art performance against strong baselines.\nIn the following sections, we will introduce these neural components derived from string and graph kernels, as well as their deep versions. Due to space limitations, we defer proofs to supplementary material."
    }, {
      "heading" : "2. From String Kernels to Sequence NNs",
      "text" : "Notations We define a sequence (or a string) of tokens (e.g. a sentence) as x1:L = {xi}Li=1 where xi ∈ Rd represents its ith element and |x| = L denotes the length. Whenever it is clear from the context, we will omit the subscript and directly use x (and y) to denote a sequence. For a pair of vectors (or matrices) u,v, we denote 〈u,v〉 = ∑ k ukvk as their inner product. For a kernel function Ki(·, ·) with subscript i, we use φi(·) to denote its underlying mapping, i.e. Ki(x,y) = 〈φi(x), φi(y)〉 = φi(x)>φi(y).\nString Kernel String kernel measures the similarity between two sequences by counting shared subsequences (see Lodhi et al. (2002)). For example, let x and y be two strings, a bi-gram string kernel K2(x,y) counts the number of bi-grams (xi,xj) and (yk,yl) such that (xi,xj) = (yk,yl) 2,\nK2(x,y) = ∑\n1≤i<j≤|x| 1≤k<l≤|y|\nλx,i,j λy,k,l δ(xi,yk) · δ(xj ,yl)\n(2) where λx,i,j , λy,k,l ∈ [0, 1) are context-dependent weights and δ(x, y) is an indicator that returns 1 only when x = y. The weight factors can be realized in various ways. For instance, in temporal predictions such as language modeling, substrings (i.e. patterns) which appear later may have higher impact for prediction. Thus a realization λx,i,j = λ|x|−i−1 and λy,k,l = λ|y|−k−1 (penalizing substrings far from the end) can be used to determine weights given a constant decay factor λ ∈ (0, 1).\nIn our case, each token in the sequence is a vector (such as one-hot encoding of a word or a feature vector). We shall replace the exact match δ(u,v) by the inner product 〈u,v〉. To this end, the kernel function (2) can be rewritten as,∑ 1≤i<j≤|x| ∑ 1≤k<l≤|y| λx,i,j λy,k,l 〈xi,yk〉 · 〈xj ,yl〉\n2We define n-gram as a subsequence of original string (not necessarily consecutive).\n= ∑\n1≤i<j≤|x| ∑ 1≤k<l≤|y| λx,i,j λy,k,l 〈xi ⊗ xj ,yk ⊗ yl〉\n= 〈∑ i<j λ|x|−i−1 xi ⊗ xj , ∑ k<l λ|y|−k−1 yk ⊗ yl 〉 (3)\nwhere xi⊗xj ∈ Rd×d (and similarly yk⊗yl) is the outerproduct. In other words, the underlying mapping of kernel K2() defined above is φ2(x) = ∑ 1≤i<j≤|x| λ\n|x|−i−1xi ⊗ xj . Note we could alternatively use a partial additive scoring 〈xi,yk〉+ 〈xj ,yl〉, and the kernel function can be generalized to n-grams when n 6= 2. Again, we commit to one realization in this section.\nString Kernel NNs We introduce a class of recurrent modules whose internal feature states embed the computation of string kernels. The modules project kernel mapping φ(x) into multi-dimensional vector space (i.e. internal states of recurrent nets). Owing to the combinatorial structure of φ(x), such projection can be realized and factorized via efficient computation. For the example kernel discussed above, the corresponding neural component is realized as,\nc1[t] = λ · c1[t− 1] + ( W(1)xt ) cj [t] = λ · cj [t− 1] + ( cj−1[t− 1] W(j)xt\n) h[t] = σ(cn[t]), 1 < j ≤ n (4)\nwhere cj [t] are the pre-activation cell states at word xt, and h[t] is the (post-activation) hidden vector. cj [0] is initialized with a zero vector. W(1), ..,W(n) are weight matrices to be learned from training examples.\nThe network operates like other RNNs by processing each input token and updating the internal states. The elementwise multiplication can be replaced by addition + (corresponding to the partial additive scoring above). As a special case, the additive variant becomes a word-level convolutional neural net (Kim, 2014) when λ = 0.3\n3h[t] = σ(W(1)xt−n+1 + · · ·+W(n)xt) when λ = 0.\nDeriving Neural Architectures from Sequence and Graph Kernels"
    }, {
      "heading" : "2.1. Single Layer as Kernel Computation",
      "text" : "Now we state how the proposed class embeds string kernel computation. For j ∈ {1, .., n}, let cj [t][i] be the i-th entry of state vector cj [t], w (j) i represents the i-th row of matrix W(j). Define wi,j = {w(1)i ,w (2) i , ...,w (j) i } as a “reference sequence” constructed by taking the i-th row from each matrix W(1), ..,W(j).\nTheorem 1. Let x1:t be the prefix of x consisting of first t tokens, and Kj be the string kernel of j-gram shown in Eq.(3). Then cj [t][i] evaluates kernel function,\ncj [t][i] = Kj (x1:t,wi,j) = 〈φj(x1:t), φj(wi,j)〉\nfor any j ∈ {1, .., n}, t ∈ {1, .., |x|}.\nIn other words, the network embeds sequence similarity computation by assessing the similarity between the input sequence x1:t and the reference sequence wi,j . This interpretation is similar to that of CNNs, where each filter is a “reference pattern” to search in the input. String kernel NN further takes non-consecutive n-gram patterns into consideration (seen from the summation over all n-grams in Eq.(3)).\nApplying Non-linear Activation In practice, a non-linear activation function such as polynomial or sigmoid-like activation is added to the internal states to produce the final output state h[t]. It turns out that many activations are also functions in the reproducing kernel Hilbert space (RKHS) of certain kernel functions (see Shalev-Shwartz et al. (2011); Zhang et al. (2016)). When this is true, the underlying kernel of h[t] is the composition of string kernel and the kernel containing the activation. We give the formal statements below.\nLemma 1. Let x and w be multi-dimensional vectors with finite norm. Consider the function f(x) := σ(w>x) with non-linear activation σ(·). For functions such as polynomials and sigmoid function, there exists kernel functions Kσ(·, ·) and the underlying mapping φσ(·) such that f(x) is in the reproducing kernel Hilbert space of Kσ(·, ·), i.e.,\nf(x) = σ(w>x) = 〈φσ(x), ψ(w)〉\nfor some mapping ψ(w) constructed from w. In particular, Kσ(x,y) can be the inverse-polynomial kernel 12−〈x,y〉 for the above activations.\nProposition 1. For one layer string kernel NN with nonlinear activation σ(·) discussed in Lemma 1, h[t][i] as a function of input x belongs to the RKHS introduced by the composition of Kσ(·, ·) and string kernel Kn(·, ·). Here a kernel composition Kσ,n(x,y) is defined with the underlying mapping x 7→ φσ(φn(x)), and hence Kσ,n(x,y) = φσ(φn(x))>φσ(φn(y)).\nProposition 1 is the corollary of Lemma 1 and Theorem 1, since h[t][i] = σ(cn[t][i]) = σ(Kn(x1:t,wi,j)) = 〈φσ(φn(x1:t)), w̃i,j〉 and φσ(φn(·)) is the mapping for the composed kernel. The same proof applies when h[t] is a linear combination of all ci[t] since kernel functions are closed under addition."
    }, {
      "heading" : "2.2. Deep Networks as Deep Kernel Construction",
      "text" : "We now address the case when multiple layers of the same module are stacked to construct deeper networks. That is, the output states h(l)[t] of the l-th layer are fed to the (l+1)th layer as the input sequence. We show that layer stacking corresponds to recursive kernel construction (i.e. (l + 1)- th kernel is defined on top of l-th kernel), which has been proven for feed-forward networks (Zhang et al., 2016).\nWe first generalize the sequence kernel definition to enable recursive construction. Notice that the definition in Eq.(3) uses the linear kernel (inner product) 〈xi,yk〉 as a “subroutine” to measure the similarity between substructures (e.g. tokens) within the sequences. We can therefore replace it with other similarity measures introduced by other “base kernels”. In particular, let K(1)(·, ·) be the string kernel (associated with a single layer). The generalized sequence kernel K(l+1)(x,y) can be recursively defined as,∑ i<j k<l λx,i,j λy,k,l K(l)σ (x1:i,y1:k) K(l)σ (x1:j ,y1:l) =\n〈∑ i<j φ(l)σ (x1:i)⊗ φ(l)σ (x1:j), ∑ k<l φ(l)σ (y1:k)⊗ φ(l)σ (y1:l) 〉\nwhere φ(l)(·) denotes the pre-activation mapping of the lth kernel, φ(l)σ (·) = φσ(φ(l)(·)) denotes the underlying (post-activation) mapping for non-linear activation σ(·), and K(l)σ (·, ·) is the l-th post-activation kernel. Based on this definition, a deeper model can also be interpreted as a kernel computation.\nTheorem 2. Consider a deep string kernel NN with L layers and activation function σ(·). Let the final output state h(L)[t] = σ(c(L)n [t]) (or any linear combination of {c(l)i [t]}, i = 1, .., n). For l = 1, · · · , L,\n(i) c(l)n [t][i] as a function of input x belongs to the RKHS of kernel K(l)(·, ·);\n(ii) h(l)[t][i] belongs to the RKHS of kernel K(l)σ (·, ·)."
    }, {
      "heading" : "3. From Graph Kernels to Graph NNs",
      "text" : "In the previous section, we encode sequence kernel computation into neural modules and demonstrate possible extensions using different base kernels. The same ideas apply\nDeriving Neural Architectures from Sequence and Graph Kernels\nto other types of kernels and data. Specifically, we derive neural components for graphs in this section.\nNotations A graph is defined as G = (V,E), with each vertex v ∈ V associated with feature vector fv . The neighbor of node v is denoted as N(v). Following previous notations, for any kernel function K∗(·, ·) with underlying mapping φ∗(·), we use K∗,σ(·, ·) to denote the postactivation kernel induced from the composed underlying mapping φ∗,σ = φσ(φ∗(·))."
    }, {
      "heading" : "3.1. Random Walk Kernel NNs",
      "text" : "We start from random walk graph kernels (Gärtner et al., 2003), which count common walks in two graphs. Formally, let Pn(G) be the set of walks x = x1 · · ·xn, where ∀i : (xi, xi+1) ∈ E.4 Given two graphs G and G′, an n-th order random walk graph kernel is defined as:\nKnW (G,G′) = λn−1 ∑\nx∈Pn(G) ∑ y∈Pn(G′) n∏ i=1 〈fxi , fyi〉 (5)\nwhere fxi ∈ Rd is the feature vector of node xi in the walk.\nNow we show how to realize the above graph kernel with a neural module. Given a graph G, the proposed neural module is:\nc1[v] = W (1)fv cj [v] = λ ∑\nu∈N(v)\ncj−1[u] W(j)fv (6)\nhG = σ( ∑ v cn[v]) 1 < j ≤ n\nwhere again c∗[v] is the cell state vector of node v, and hG is the representation of graph G aggregated from node vectors. hG could then be used for classification or regression.\nNow we show the proposed model embeds the random walk kernel. To show this, construct Ln,k as a “reference walk” consisting of the row vectors {w(1)k , · · · ,w (n) k } from the parameter matrices. Here Ln,k = (LV , LE), where LV = {v0, v1, · · · , vn}, LE = {(vi, vi+1)} and vi’s feature vector is w(i)k . We have the following theorem:\nTheorem 3. For any n ≥ 1, the state value cn[v][k] (the k-th coordinate of cn[v]) satisfies:∑\nv\ncn[v][k] = KnW (G,Ln,k)\nthus ∑ v cn[v] lies in the RKHS of kernel KnW . As a corollary, hG lies in the RKHS of kernel KnW,σ(). 4A single node could appear multiple times in a walk."
    }, {
      "heading" : "3.2. Unified View of Graph Kernels",
      "text" : "The derivation of the above neural module could be extended to other classes of graph kernels, such as subtree kernels (cf. (Ramon & Gärtner, 2003; Vishwanathan et al., 2010)). Generally speaking, most of these kernel functions factorize graphs into local sub-structures, i.e.\nK(G,G′) = ∑ v ∑ v′ Kloc(v, v′) (7)\nwhere Kloc(v, v′) measures the similarity between local sub-structures centered at node v and v′.\nFor example, the random walk kernel KnW can be equivalently defined with Knloc(v, v′) = 〈fv, fv ′〉 if n = 1 〈fv, fv′〉 · λ\n∑ u∈N(v) ∑ u′∈N(v′) Kn−1loc (u, u′) if n > 1\nOther kernels like subtree kernels could be recursively defined similarly. Therefore, we adopt this unified view of graph kernels for the rest of this paper.\nIn addition, this definition of random walk kernel could be further generalized and enhanced by aggregating neighbor features non-linearly:\nKnloc(v, v′) = 〈fv, fv′〉 ◦ λ ∑\nu∈N(v) ∑ u′∈N(v′) Kn−1loc,σ(u, u ′)\nwhere ◦ could be either multiplication or addition. σ(·) denotes a non-linear activation and Kn−1loc,σ(·, ·) denotes the post-activation kernel when σ(·) is involved. The generalized kernel could be realized by modifying Eq.(6) into:\ncj [v] = W (j)fv ◦ λ ∑ u∈N(v) σ(cj−1[u]) (8)\nwhere ◦ could be either + or operation."
    }, {
      "heading" : "3.3. Deep Graph Kernels and NNs",
      "text" : "Following Section 2, we could stack multiple graph kernel NNs to form a deep network. That is:\nc (l) 1 [v] = W (l,1)h(l−1)[v]\nc (l) j [v] = W\n(l,j)h(l−1)[v] ◦ λ ∑\nu∈N(v)\nσ ( c (l) j−1[u] ) h(l)[v] = σ(U(l)c(l)n [v]) 1 ≤ l ≤ L, 1 < j ≤ n\nThe local kernel function is recursively defined in two dimensions: depth (term h(l)) and width (term cj). Let the pre-activation kernel in the l-th layer be K(l)loc(v, v′) = K (l,n) loc (v, v\n′), and the post-activation kernel be K(l)loc,σ(v, v′) = K (l,n) loc,σ(v, v ′). We recursively define\nDeriving Neural Architectures from Sequence and Graph Kernels\nK(l,j)loc (v, v′) = K (l−1) loc,σ (v, v ′) if j = 1 K(l−1)loc,σ (v, v′) ◦ λ ∑\nu∈N(v) ∑ u′∈N(v′) K(l,j−1)loc,σ (u, u′) if j > 1\nfor j = 1, · · · , n. Finally, the graph kernel is K(L,n)(G,G′) = ∑ v,v′ K (L,n) loc (v, v\n′). Similar to Theorem 2, we have Theorem 4. Consider a deep graph kernel NN with L layers and activation function σ(·). Let the final output state hG = ∑ v h (L)[v]. For l = 1, · · · , L; j = 1, · · · , n:\n(i) c(l)j [v][i] as a function of input v and graph G belongs\nto the RKHS of kernel K(l,j)loc (·, ·);\n(ii) h(l)[v][i] belongs to the RKHS of kernel K(l,n)loc,σ(·, ·).\n(iii) hG[i] belongs to the RKHS of kernel K(L,n)(·, ·)."
    }, {
      "heading" : "3.4. Connection to Weisfeiler-Lehman Kernel",
      "text" : "We derived the above deep kernel NN for the purpose of generality. This model could be simplified by setting n = 2, without losing representational power (as non-linearity is already involved in depth dimension). In this case, we rewrite the network by reparametrization:\nh(l)v = σ U(l)1 h(l−1)v ◦U(l)2 ∑ u∈N(v) σ ( V(l)h(l−1)u ) (9) In this section, we further show that this model could be enhanced by sharing weight matrices U and V across layers. This parameter tying mechanism allows our model to embed Weisfeiler-Lehman kernel (Shervashidze et al., 2011). For clarity, we briefly review basic concepts of WeisfeilerLehman kernel below.\nWeisfeiler-Lehman Graph Relabeling WeisfeilerLehman kernel borrows concepts from the WeisfeilerLehman isomorphism test for labeled graphs. The key idea of the algorithm is to augment the node labels by the sorted set of node labels of neighbor nodes, and compress these augmented labels into new, short labels (Figure 2). Such relabeling process is repeated T times. In the i-th iteration, it generates a new labeling li(v) for all nodes v in graph G, with initial labeling l0.\nGeneralized Graph Relabeling The key observation here is that graph relabeling operation could be viewed as neighbor feature aggregation. As a result, the relabeling process naturally generalizes to the case where nodes are associated with continuous feature vectors. In particular, let r be the relabeling function. For a node v ∈ G:\nr(v) = σ(U1fv +U2 ∑\nu∈N(v)\nσ(Vfu)) (10)\nFigure 2. Node relabeling in Weisfeiler-Lehman isomorphism test. Figure taken from Shervashidze et al. (2011)\nNote that our definition of r(v) is exactly the same as hv in Equation 9, with ◦ being additive composition.\nWeisfeiler-Lehman Kernel Let K be any graph kernel (called base kernel). Given a relabeling function r, Weisfeiler-Lehman kernel with base kernel K and depth L is defined as\nK(L)WL(G,G ′) = L∑ i=0 K(ri(G), ri(G′)) (11)\nwhere r0(G) = G and ri(G), ri(G′) are the i-th relabeled graph of G and G′ respectively.\nWeisfeiler-Lehman Kernel NN Now with the above kernel definition, and random walk kernel as the base kernel, we propose the following recurrent module:\nc (l) 0 [v] = W (l,0)h(l−1)v c (l) j [v] = λ ∑ u∈N(v) c (l) j−1[u] W (l,j)h(l−1)v\nh(l)v = σ U1h(l−1)v +U2 ∑ u∈N(v) σ(Vh(l−1)u )  h (l) G =\n∑ v c(l)n [v] 1 ≤ l ≤ L, 1 < j ≤ n\nwhere h(0)v = fv and U1,U2,V are shared across layers. The final output of this network is hG = ∑L l=1 h (l) G .\nThe above recurrent module is still an instance of deep kernel, even though some parameters are shared. A minor difference here is that there is an additional random walk kernel NN that connects i-th layer and the output layer. But this is just a linear combination of L deep random walk kernels (of different depth). Therefore, as an corollary of Theorem 4, we have: Proposition 2. For a Weisfeiler-Lehman Kernel NN with L iterations and random walk kernel KnW as base kernel, the final output state hG = ∑ l h (l) G belongs to the RKHS of kernel K(L)WL(·, ·).\nDeriving Neural Architectures from Sequence and Graph Kernels"
    }, {
      "heading" : "4. Adaptive Decay with Neural Gates",
      "text" : "The sequence and graph kernel (and their neural components) discussed so far use a constant decay value λ regardless of the current input. However, this is often not the case since the importance of the input can vary across the context or the applications. One extension is to make use of neural gates that adaptively control the decay factor. Here we give two illustrative examples:\nGated String Kernel NN By replacing constant decay λ with a sigmoid gate, we modify our single-layer sequence module as:\nλt = σ(U[xt,ht−1] + b) c1[t] = λt c1[t− 1] + ( W(1)xt ) cj [t] = λt cj [t− 1] + ( cj−1[t− 1] W(j)xt\n) h[t] = σ(cn[t]) 1 < j ≤ n\nAs compared with the original string kernel, now the decay factor λx,i,j is no longer λ|x|−i−1, but rather an adaptive value based on current context.\nGated Random Walk Kernel NN Similarly, we could introduce gates so that different walks have different weights:\nλu,v = σ(U[fu, fv] + b)\nc0[v] = W (0)fv cj [v] = ∑\nu∈N(v)\nλu,v cj−1[u] W(j)fv\nhG = σ( ∑ v cn[v]) 1 < j ≤ n\nThe underlying kernel of the above gated network becomes\nKnW (G,G′) = ∑\nx∈Pn(G) ∑ y∈Pn(G′) n∏ i=1 λxi,yi 〈fxi , fyi〉\nwhere each path is weighted by different decay weights, determined by network itself."
    }, {
      "heading" : "5. Related Work",
      "text" : "Sequence Networks Considerable effort has gone into designing effective networks for sequence processing. This includes recurrent modules with the ability to carry persistent memories such as LSTM (Hochreiter & Schmidhuber, 1997) and GRU (Chung et al., 2014), as well as non-consecutive convolutional modules (RCNNs, Lei et al. (2015)), and others. More recently, Zoph & Le (2016) exemplified a reinforcement learning-based search algorithm to further optimize the design of such recurrent architectures. Our proposed neural networks offer similar state\nevolution and feature aggregation functionalities but derive the motivation for the operations involved from wellestablished kernel computations over sequences.\nRecursive neural networks are alternative architectures to model hierarchical structures such as syntax trees and logic forms. For instance, Socher et al. (2013) employs recursive networks for sentence classification, where each node in the dependency tree of the sentence is transformed into a vector representation. Tai et al. (2015) further proposed tree-LSTM, which incorporates LSTM-style architectures as the transformation unit. Dyer et al. (2015; 2016) recently introduced a recursive neural model for transitionbased language modeling and parsing. While not specifically discussed in the paper, our ideas do extend to similar neural components for hierarchical objects (e.g. trees).\nGraph Networks Most of the current graph neural architectures perform either convolutional or recurrent operations on graphs. Duvenaud et al. (2015) developed Neural Fingerprint for chemical compounds, where each convolution operation is a sum of neighbor node features, followed by a linear transformation. Our model differs from theirs in that our generalized kernels and networks can aggregate neighboring features in a non-linear way. Other approaches, e.g., Bruna et al. (2013) and Henaff et al. (2015), rely on graph Laplacian or Fourier transform.\nFor recurrent architectures, Li et al. (2015) proposed gated graph neural networks, where neighbor features are aggregated by GRU function. Dai et al. (2016) considers a different architecture where a graph is viewed as a latent variable graphical model. Their recurrent model is derived from Belief Propagation-like algorithms. Our approach is most closely related to Dai et al. (2016), in terms of neighbor feature aggregation and resulting recurrent architecture. Nonetheless, the focus of this paper is on providing a framework for how such recurrent networks could be derived from deep graph kernels.\nKernels and Neural Nets Our work follows recent work demonstrating the connection between neural networks and kernels (Cho & Saul, 2009; Hazan & Jaakkola, 2015). For example, Zhang et al. (2016) showed that standard feedforward neural nets belong to a larger space of recursively constructed kernels (given certain activation functions). Similar results have been made for convolutional neural nets (Anselmi et al., 2015), and general computational graphs (Daniely et al., 2016). We extend prior work to kernels and neural architectures over structured inputs, in particular, sequences and graphs. Another difference is how we train the model. While some prior work appeals to convex optimization through improper learning (Zhang et al., 2016; Heinemann et al., 2016) (since kernel space is larger), we use the proposed networks as building blocks in typical non-convex but flexible neural network training.\nDeriving Neural Architectures from Sequence and Graph Kernels"
    }, {
      "heading" : "6. Experiments",
      "text" : "The left-over question is whether the proposed class of operations, despite its formal characteristics, leads to more effective architecture exploration and hence improved performance. In this section, we apply the proposed sequence and graph modules to various tasks and empirically evaluate their performance against other neural network models. These tasks include language modeling, sentiment classification and molecule regression."
    }, {
      "heading" : "6.1. Language Modeling on PTB",
      "text" : "Dataset and Setup We use the Penn Tree Bank (PTB) corpus as the benchmark. The dataset contains about 1 million tokens in total. We use the standard train/development/test split of this dataset with vocabulary of size 10,000.\nModel Configuration Following standard practice, we use SGD with an initial learning rate of 1.0 and decrease the learning rate by a constant factor after a certain epoch. We back-propagate the gradient with an unroll size of 35 and use dropout (Hinton et al., 2012) as the regularization. Unless otherwise specified, we train 3-layer networks with n = 1 and normalized adaptive decay.5 Following (Zilly et al., 2016), we add highway connections (Srivastava et al., 2015) within each layer:\nc(l)[t] = λt c(l)[t− 1] + (1− λt) (W(l)h(l−1)[t]) h(l)[t] = ft c(l)[t] + (1− ft) h(l−1)[t]\nwhere h(0)[t] = xt, λt is the gated decay factor and ft is the transformation gate of highway connections.6\nResults Table 1 compares our model with various state-ofthe-art models. Our small model with 5 million parameters achieves a test perplexity of 73.6, already outperforming many results achieved using much larger network. By increasing the network size to 20 million, we obtain a test perplexity of 69.2, with standard dropout. Adding variational dropout (Gal & Ghahramani, 2016) within the recurrent cells further improves the perplexity to 65.5. Finally, the model achieves 63.8 perplexity when the recurrence depth is increased to 4, being state-of-the-art and on par with the results reported in (Zilly et al., 2016; Zoph & Le, 2016). Note that Zilly et al. (2016) uses 10 neural layers and Zoph & Le (2016) adopts a complex recurrent cell found by reinforcement learning based search. Our network is architecturally much simpler.\nFigure 3 analyzes several variants of our model. Wordlevel CNNs are degraded cases (λ = 0) that ignore non-\n5See the supplementary sections for a discussion of network variants.\n6We found non-linear activation is no longer necessary when the highway connection is added.\ncontiguous n-gram patterns. Clearly, this variant performs worse compared to other recurrent variants with λ > 0. Moreover, the test perplexity improves from 84.3 to 76.8 when we train the constant decay vector as part of the model parameters. Finally, the last two variants utilize neural gates (depending on input xt only or both input xt and previous state h[t−1]), further improving the performance."
    }, {
      "heading" : "6.2. Sentiment Classification",
      "text" : "Dataset and Setup We evaluate our model on the sentence classification task. We use the Stanford Sentiment Treebank benchmark (Socher et al., 2013). The dataset consists of 11855 parsed English sentences annotated at both the root (i.e. sentence) level and the phrase level using 5- class fine-grained labels. We use the standard split for training, development and testing. Following previous work, we also evaluate our model on the binary classification variant of this benchmark, ignoring all neutral sentences.\nDeriving Neural Architectures from Sequence and Graph Kernels\nFollowing the recent work of DAN (Iyyer et al., 2015) and RLSTM (Tai et al., 2015), we use the publicly available 300-dimensional GloVe word vectors (Pennington et al., 2014). Unlike prior work which fine tunes the word vectors, we normalize the vectors (i.e. ‖w‖22 = 1) and fixed them for simplicity.\nModel Configuration Our best model is a 3-layer network with n = 2 and hidden dimension d = 200. We average the hidden states h[t] across t, and concatenate the averaged vectors from the 3 layers as the input of the final softmax layer. The model is optimized with Adam (Kingma & Ba, 2015), and dropout probability of 0.35.\nResults Table 2 presents the performance of our model and other networks. We report the best results achieved across 5 independent runs. Our best model obtains 53.2% and 89.9% test accuracies on fine-grained and binary tasks respectively. Our model with only a constant decay factor also obtains quite high accuracy, outperforming other baseline methods shown in the table."
    }, {
      "heading" : "6.3. Molecular Graph Regression",
      "text" : "Dataset and Setup We further evaluate our graph NN models on the Harvard Clean Energy Project benchmark, which has been used in Dai et al. (2016); Duvenaud et al. (2015) as their evaluation dataset. This dataset contains 2.3 million candidate molecules, with each molecule labeled with its power conversion efficiency (PCE) value.\nWe follow exactly the same train-test split as Dai et al. (2016), and the same re-sampling procedure on the training data (but not the test data) to make the algorithm put more\nemphasis on molecules with higher PCE values, since the data is distributed unevenly.\nWe use the same feature set as in Duvenaud et al. (2015) for atoms and bonds. Initial atom features include the atoms element, its degree, the number of attached hydrogens, its implicit valence, and an aromaticity indicator. The bond feature is a concatenation of bond type indicator, whether the bond is conjugated, and whether the bond is in a ring.\nModel Configuration Our model is a Weisfeiler-Lehman NN, with 4 recurrent iterations and n = 2. All models (including baseline) are optimized with Adam (Kingma & Ba, 2015), with learning rate decay factor 0.9.\nResults In Table 3, we report the performance of our model against other baseline methods. Neural Fingerprint (Duvenaud et al., 2015) is a 4-layer convolutional neural network. Convolution is applied to each atom, which sums over its neighbors’ hidden state, followed by a linear transformation and non-linear activation. Embedded Loopy BP (Dai et al., 2016) is a recurrent architecture, with 4 recurrent iterations. It maintains message vectors for each atom and bond, and propagates those vectors in a message passing fashion. Table 3 shows our model achieves stateof-the-art against various baselines."
    }, {
      "heading" : "7. Conclusion",
      "text" : "We proposed a class of deep recurrent neural architectures and formally characterized its underlying computation using kernels. By linking kernel and neural operations, we have a “template” for deriving new families of neural architectures for sequences and graphs. We hope the theoretical view of kernel neural networks can be helpful for future model exploration.\nDeriving Neural Architectures from Sequence and Graph Kernels"
    }, {
      "heading" : "Acknowledgement",
      "text" : "We thank Prof. Le Song for sharing Harvard Clean Energy Project dataset. We also thank Yu Zhang, Vikas Garg, David Alvarez, Tianxiao Shen, Karthik Narasimhan and the reviewers for their helpful comments. This work was supported by the DARPA Make-It program under contract ARO W911NF-16-2-0023."
    }, {
      "heading" : "A. Examples of kernel / neural variants",
      "text" : "Our theoretical results apply to some other variants of sequence kernels and the associated neural components. We give some examples in the this section. Table 4 shows three network variants, corresponding to three realizations of string kernels provided in Table 5.\nConnection to LSTMs Interestingly, many recent work has reached similar RNN architectures through empirical exploration. For example, Greff et al. (2015) found that simplifying LSTMs, by removing the input gate or coupling it with the forget gate does not significantly change the performance. However, the forget gate (corresponding to the decay factor λ in our notation) is crucial for performance. This is consistent with our theoretical analysis and the empirical results in Figure 3. Moreover, Balduzzi & Ghifary (2016) and Lee et al. (2017) both suggest that a linear additive state computation suffices to provide competitive performance compared to LSTMs: 7\nc[t] = λf c1[t− 1] + λi (Wxt) h[t] = σ(c[t])\nIn fact, this variant becomes an instance of the kernel NN presented in this work (with n = 1 and adaptive gating), when λf = λt and λi = 1− λt or 1.\n7Balduzzi & Ghifary (2016) also includes the previous token , i.e. Wxt + W′xt−1, which doesn’t affect the discussion here."
    }, {
      "heading" : "B. Proof of Theorem 1",
      "text" : "We first generalize the kernel definition in Eq.(3) to the case of any n-gram. For any integer n > 0, the underlying mapping of the n-th order string kernel is defined as,\nφn(x) = ∑\n1≤i1<···<in≤|x|\nλ|x|−i1−n+1 xi1 ⊗ xi2 ⊗ · · · ⊗ xin\nWe now show that String Kernel NN states cn[t] defined in (4),\nn = 1 : c1[t] = λ · c1[t− 1] +W(1)xt n > 1 : cn[t] = λ · cn[t− 1] + ( cn−1[t− 1] W(n)xt ) is equivalent to summing over all n-grams within the first t tokens x1:t = {x1, · · · ,xt},\ncn[t] = ∑\n1≤i1<i2<···<in≤t\n( W(1)xi1 · · · W(n)xin ) · λt−i1−n+1\nProof: We prove the equivalence by induction. When n = 1, for c1[t] we have,\nc1[t] = ∑\n1≤i1≤t\n( W(1)xi1 ) · λt−i1\n=  ∑ 1≤i1≤t−1 ( W(1)xi1 ) · λt−1−i1  ︸ ︷︷ ︸\nc1[t−1]\n·λ + W(1)xt\n= c1[t− 1] · λ + W(1)xt\nWhen n = k > 1, suppose the state iteration holds for 1, · · · , k − 1, we have,\ncn[t] = ∑\n1≤i1<i2<···<in≤t\n( W(1)xi1 · · · W(n)xin ) · λt−i1−n+1\n=  ∑ 1≤i1<i2<···<in≤t−1 ( W(1)xi1 · · · W(n)xin ) · λt−1−i1−n+1  ︸ ︷︷ ︸\nwhen in<t: cn[t−1]\n·λ\n+  ∑ 1≤i1<i2<···<in=t ( W(1)xi1 · · · W(n)xin ) · λt−1−i1−n+1  ︸ ︷︷ ︸\nwhen in=t = cn[t− 1] · λ + ( cn−1[t− 1] W(n)xt ) Now we proceed to prove Theorem 1.\nProof: From the above results, we know that for cn[t],\ncn[t] = ∑\n1≤i1<i2<···<in≤t\n( W(1)xi1 · · · W(n)xin ) · λt−i1−n+1\nIn particular, the value of the i-th entry, cn[t][i], is equal to,\ncn[t][i] = ∑\n1≤i1<i2<···<in≤t\n〈 w\n(1) i ,xi1\n〉 · · · 〈 w\n(n) i ,xin 〉 ︸ ︷︷ ︸〈\nxi1⊗···⊗xin ,w (1) i ⊗···⊗w (n) i\n〉 · λt−i1−n+1\n= 〈 ∑ 1≤i1<i2<···<in≤t λt−i1−n+1 xi1 ⊗ · · · ⊗ xin , w (1) i ⊗ · · · ⊗w (n) i 〉\n= 〈φn(x1:t), φn(wi,n)〉\nwhere w(k)i represents the i-th row of matrix W (k) and wi,n = {w(1)i , · · · ,w (n) i }. This completes the proof since Kn(x1:t,wi,n) = 〈φn(x1:t), φn(wi,n)〉 by definition.\nRemarks This theorem demonstrates that the NN state vectors cn can be seen as projecting the kernel mapping φn(x) into a low-dimensional space using the parameterized matrices {W(i)}i = 1n. This is similar to word embeddings and CNNs, where the input (a single word or an n-gram) is projected to obtain a low-dimensional representation."
    }, {
      "heading" : "C. Proof of Theorem 2",
      "text" : "We first review necessary concepts and notations for the ease of reading. Similar to the proof in Appendix B, the generalized string kernel K(l) and K(l)σ in Eq.(??) can be defined with the underlying mappings,\nφ(l)(x) = ∑\n1≤i1<···<in≤|x|\nλ|x|−i1−n+1 φ(l−1)σ (x1:i1)⊗ φ(l−1)σ (x1:i2)⊗ · · · ⊗ φ(l−1)σ (x1:i1)\nφ(l)σ (x) = φσ(φ (l)(x))\nwhere φσ() is the underlying mapping of a kernel function whose reproducing kernel Hilbert space (RKHS) contains the non-linear activation σ() used in the String Kernel NN layer. Here K(l)() is the “pre-activation kernel” and K(l)σ () is the “post-activation kernel”. To show that the values of String Kernel NN states c(l)[t] is contained in the RKHS of K(l)() and that of h(l)[t] is contained in the RKHS of K(l)σ (), we re-state the claim in the following way, Theorem 5. Given a deep n-gram String Kernel NN with non-linear activation σ(). Assuming σ() lies in the RKHS of a kernel function with underlying mapping φσ(), then\n(i) c(l)[t] lies in the RKHS of kernel K(l)() in the sense that\nc (l) j [t][i] = 〈 φ(l)(x1:t), ψ (l) i,j 〉 for any internal state c(l)j [t] (1 ≤ j ≤ n) of the l-th layer, where ψ (l) i,j is a mapping constructed from model parameters;\n(ii) h(l)[t] lies in the RKHS of kernel K(l)σ () as a corollary in the sense that\nh(l)[t][i] = σ(c(l)n [t][i]) = σ (〈 φ(l)(x1:t), ψ (l) i,n 〉) (based on (i))\n= 〈 φσ(φ (l)(x1:t)), ψσ(ψ (l) i,n) 〉\n(Lemma 1)\nand we denote ψσ(ψ (l) i,n) as ψ (l) σ,i,n for short.\nProof: We prove by induction on l. When l = 1, the proof of Theorem 1 already shows that c(1)j [t][i] =〈 φ (1) j (x1:t), φ (1) j (wi,j) 〉 in a one-layer String Kernel NN. Simply let ψ(1)i,j = φ (1) j (wi,j) completes the proof for the case of l = 1.\nSuppose the lemma holds for l = 1, · · · , k, we now prove the case of l = k + 1. Similar to the proof of Theorem 1, the value of c(k+1)j [t][i] equals to\nc (k+1) j [t][i] = ∑ 1≤i1<···<ij≤t 〈 w (1) i ,h (k)[i1] 〉 · · · 〈 w (j) i ,h (k)[ij ] 〉 · λt−i1−j+1 (12)\nwhere w(j)i is the i-th row of the parameter matrix W (j) of the l-th layer. Note h(k)[t][i] =〈\nφ (k) σ (x1:t), ψ (k) σ,i,n 〉 , we construct a matrix M by stacking all {ψ(k)σ,i,n}i as the row vectors.8 We then\ncan rewrite h(k)[t] = Mφ(k)σ (x1:t). Plugging this into Eq (12), we get\nc (k+1) j [t][i] = ∑ 1≤i1<···<ij≤t 〈 w (1) i ,Mφ (k) σ (x1:i1) 〉 · · · 〈 w (j) i ,Mφ (k) σ (x1:ij ) 〉 · λt−i1−j+1\n= ∑\n1≤i1<···<ij≤t\n〈 M>w\n(1) i︸ ︷︷ ︸\nui,1\n, φ(k)σ (x1:i1)\n〉 · · · 〈 M>w\n(j) i︸ ︷︷ ︸\nui,j\n, φ(k)σ (x1:ij )\n〉 · λt−i1−j+1\n= ∑\n1≤i1<···<ij≤t\n〈 ui,1, φ (k) σ (x1:i1) 〉 · · · 〈 ui,j , φ (k) σ (x1:ij ) 〉 · λt−i1−j+1\n= 〈 ∑ 1≤i1<···<ij≤t\nλt−i1−j+1 φ(k)σ (xi1)⊗ · · · ⊗ φ(k)σ (xij )︸ ︷︷ ︸ φ(k+1)(x1:t) , ui,1 ⊗ · · · ⊗ ui,j\n〉\n= 〈 φ(k+1)(x1:t), ui,1 ⊗ · · · ⊗ ui,j 〉 Define ψ(k+1)i,j = ui,1 ⊗ · · · ⊗ ui,j completes the proof for the case of l = k + 1.\n8Note in practice the mappings φ(k)σ and ψ (k) σ may have infinite dimension because the underlying mapping for the non-linear activation φσ() can have infinite dimension. The proof still apply since the dimensions are still countable and the vectors have finite norm (easy to show this by assuming the input xi and parameter W are bounded."
    }, {
      "heading" : "D. Proof for Theorem 3",
      "text" : "Recall that random walk graph kernel is defined as:\nKn(G,G′) = λn ∑\nx∈Pn(G) ∑ y∈Pn(G′) n∏ i=1 〈fxi , fyi〉\nφn(G) = λ n ∑ x∈Pn(G) fx0 ⊗ fx1 ⊗ · · · ⊗ fxn\nand single-layer graph NN computes its states as:\ncn[v] = λ ∑\nu∈N(v)\ncn−1[u] W(n)fv\n= λn ∑\nun−1∈N(v) ∑ un−2∈N(un−1) · · · ∑ u0∈N(u1) W(0)fu0 W(1)fu1 · · · W(n)fun\n= λn ∑\nu=u0···un∈Pn(G,v)\nW(0)fu0 W(1)fu1 · · · W(n)fun\nwhere we define Pn(G, v) be the set of walks that end with node v. For the i-th coordinate of cn[v], we have\ncn[v][i] = λ n ∑ u∈Pn(G,v) ∏ i 〈 w (i) i , fui 〉 ∑ v cn[v][i] = λ n ∑ u∈Pn(G) ∏ i 〈 w (i) i , fui\n〉 = Kn(G,Ln,k)\nThe last step relies on the fact that Pn(Ln,k) has only one element {w(0)i ,w (1) i , · · · ,w (n) i }."
    }, {
      "heading" : "E. Proof for Theorem 4",
      "text" : "For clarity, we re-state the kernel definition and theorem in the following:\nK(L,n)(G,G′) = ∑ v ∑ v′ K(L,n)loc,σ (v, v ′)\nWhen l = 1:\nK(l,j)loc (v, v ′) =  〈fv, fv′〉 if j = 1 〈fv, fv′〉+ λ ∑ u∈N(v) ∑ u′∈N(v′) K(l,j−1)loc,σ (u, u′) if j > 1\n0 if Pj(G, v) = ∅ or Pj(G′, v′) = ∅\nWhen l > 1:\nK(l,j)loc (v, v ′) =  K(l−1)loc,σ (v, v ′) if j = 1 K(l−1)loc,σ (v, v′) + λ ∑ u∈N(v) ∑ u′∈N(v′) K(l,j−1)loc,σ (u, u′) if j > 1\n0 if Pj(G, v) = ∅ or Pj(G′, v′) = ∅\nwhere Pj(G, v) is the set of walks of length j starting from v. Note that we force K(l,j)loc = 0 when there is no valid walk starting from v or v′, so that it only considers walks of length j. We use additive version just for illustration (multiplicative version follows the same argument).\nTheorem 6. For a deep random walk kernel NN with L layers and activation function σ(·), assuming the final output state hG = ∑ v h (l)[v], for l = 1, · · · , L; j = 1, · · · , n:\n(i) c(l)j [v][i] as a function of input v and graph G belongs to the RKHS of kernel K (l,j) loc (·, ·) in that\nc (l) j [v][i] = 〈 φ (l,j) G (v), ψ (l) i,j 〉 where φ(l)G (v) is the mapping of node v in graph G, and ψ (l) i,j is a mapping constructed from model parameters.\n(ii) h(l)[v][i] belongs to the RKHS of kernel K(l,n)loc,σ(·, ·) as a corollary:\nh(l)[v][i] = σ (∑ k uikc (l) n [v][k] ) = σ (∑ k uik 〈 φ (l,n) G (v), ψ (l) k,n 〉)\n= σ (〈 φ (l,n) G (v), ∑ k uikψ (l) k,n 〉)\n= 〈 φσ(φ (l,n) G (v)), ψσ (∑ k uikψ (l) k,n )〉\nWe denote ψσ (∑ k uikψ (l) k,n ) as ψ(l)σ,i,n, and φσ(φ (l,n) G (v)) as φ (l) G,σ(v) for short.\n(iii) hG[i] belongs to the RKHS of kernel K(L,n)(·, ·).\nProof of (i), (ii): We prove by induction on l. When l = 1, from kernel definition, the kernel mapping is recursively defined as:\nφ (1,j) G (v) =  fv if j = 1[ fv, √ λ ∑ u∈N(v) φσ(φ (1,j−1) G (u)) ] if j > 1\n0 if Pj(G, v) = ∅ or Pj(G′, v′) = ∅\nWe prove c(1)j [v][i] = 〈 φ (1,j) G (v), ψ (1) i,j 〉 by induction on j. When j = 1, c(1)1 [v][i] = 〈 w (1,1) i , fv 〉 . Our hypothesis holds by ψ(1)i,1 = w (1,1) i . Suppose induction hypothesis holds for 1, 2, · · · , j − 1. If Pj(G, v) = ∅, we could always set c(1)j [v][i] = 0 in neural network computation. Otherwise:\nc (1) j [v][i] =\n〈 w\n(1,j) i , fv\n〉 + λ ∑ u∈N(v) σ ( c (1) j−1[u][i] ) = 〈 w\n(1,j) i , fv\n〉 + λ ∑ u∈N(v) σ (〈 φ (1,j−1) G (u), ψ (1) i,j−1 〉) = 〈 w\n(1,j) i , fv\n〉 + √ λ ∑\nu∈N(v)\n〈 φσ(φ (1,j−1) G (u)), √ λψσ(ψ (1) i,j−1) 〉\n= 〈fv,√λ ∑ u∈N(v) φσ(φ (1,j−1) G (u))  , [w(1,j)i ,√λψσ(ψ(1)i,j−1)] 〉\nLet ψ(1)i,j = [ w (1,j) i , √ λψσ(ψ (1) i,j−1) ] concludes the base case l = 1. Suppose induction hypothesis holds for 1, 2, · · · , l − 1. Note that when l > 1:\nφ (l,j) G (v) =  φσ(φ (l−1) G (v)) if j = 1[ φσ(φ (l−1) G (v)), √ λ ∑ u∈N(v) φσ(φ (l,j−1) G (u)) ] if j > 1\n0 if Pj(G, v) = ∅ or Pj(G′, v′) = ∅\nNow we prove c(l)j [v][i] = 〈 φ (l,j) G (v), ψ (l) i,j 〉 by induction on j. When j = 1,\nc (l) 1 [v][i] =\n〈 w\n(l,1) i ,h\n(l−1)[v] 〉\n= ∑ k w (l,1) ik 〈 φ (l−1) G,σ (v), ψ (l−1) σ,k,n 〉 = 〈 φ (l,1) G (v),\n∑ k w (l,1) ik ψ (l−1) σ,k,n\n〉\nLet ψ(l)i,j = ∑ kw (l,1) ik ψ (l−1) σ,k,n completes the proof for j = 1. Now suppose this holds for 1, 2, · · · , j − 1. Same as before, we only consider the case when Pj(G, v) 6= ∅:\nc (l) j [v][i] =\n〈 w\n(l,j) i ,h\n(l−1)[v] 〉 + λ ∑ u∈N(v) σ ( c (l) j−1[u][i] ) =\n∑ k w (l,j) ik 〈 φ (l−1) G,σ (v), ψ (l−1) σ,k,n 〉 + λ ∑ u∈N(v) σ (〈 φ (l,j−1) G (u), ψ (l) i,j−1 〉)\n= 〈 φ (l−1) G,σ (v), ∑ k w (l,j) ik ψ (l−1) σ,k,n 〉 + λ ∑ u∈N(v) 〈 φ (l,j−1) G,σ (u), ψσ(ψ (l) i,j−1) 〉\n= 〈 φ (l−1) G,σ (v), ∑ k w (l,j) ik ψ (l−1) σ,k,n 〉 + 〈 √ λ ∑ u∈N(v) φ (l,j−1) G,σ (u), √ λψσ(ψ (l) i,j−1) 〉\n= 〈φ(l−1)G,σ (v),√λ ∑ u∈N(v) φ (l,j−1) G,σ (u)  ,[∑ k w (l,j) ik ψ (l−1) σ,k,n , √ λψσ(ψ (l) i,j−1) ]〉\n= 〈 φ (l,j) G (v), ψ (l) i,j 〉 Let ψ(l)i,j = [∑ kw (l,j) ik ψ (l−1) σ,k,n , √ λψσ(ψ (l) i,j−1) ] concludes the proof.\nProof for (iii): We construct a directed chain Ln,k = (V,E) from model parameters, with nodes V = {ln, ln−1, · · · , l0} and E = {(vi+1, vi)}. lj’s underlying mapping is ψ(L)σ,i,j . Now we have\nhG[i] = ∑ v h(L)[v][i] = ∑ v∈G 〈 φσ(φ (L) G (v)), ψ (L) σ,i,n 〉 = ∑ v∈G ∑ v∈Ln,k K(L,n)loc (v, ln) = K (L,n)(G,Ln,k)\nNote that we are utilizing the fact that K(L,n)loc (v, lj) = 0 for all j 6= n (because Pn(Ln,j , lj) = ∅)."
    } ],
    "references" : [ {
      "title" : "Deep convolutional networks are hierarchical kernel machines",
      "author" : [ "Anselmi", "Fabio", "Rosasco", "Lorenzo", "Tan", "Cheston", "Poggio", "Tomaso" ],
      "venue" : null,
      "citeRegEx" : "Anselmi et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Anselmi et al\\.",
      "year" : 2015
    }, {
      "title" : "Strongly-typed recurrent neural networks",
      "author" : [ "Balduzzi", "David", "Ghifary", "Muhammad" ],
      "venue" : "In Proceedings of 33th International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Balduzzi et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Balduzzi et al\\.",
      "year" : 2016
    }, {
      "title" : "Spectral networks and locally connected networks on graphs",
      "author" : [ "Bruna", "Joan", "Zaremba", "Wojciech", "Szlam", "Arthur", "LeCun", "Yann" ],
      "venue" : "arXiv preprint arXiv:1312.6203,",
      "citeRegEx" : "Bruna et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bruna et al\\.",
      "year" : 2013
    }, {
      "title" : "Long short-term memory networks for machine reading",
      "author" : [ "Cheng", "Jianpeng", "Dong", "Li", "Lapata", "Mirella" ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Cheng et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Cheng et al\\.",
      "year" : 2016
    }, {
      "title" : "Kernel methods for deep learning",
      "author" : [ "Cho", "Youngmin", "Saul", "Lawrence K" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Cho et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2009
    }, {
      "title" : "Empirical evaluation of gated recurrent neural networks on sequence modeling",
      "author" : [ "Chung", "Junyoung", "Gulcehre", "Caglar", "Cho", "KyungHyun", "Bengio", "Yoshua" ],
      "venue" : "arXiv preprint arXiv:1412.3555,",
      "citeRegEx" : "Chung et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Chung et al\\.",
      "year" : 2014
    }, {
      "title" : "Discriminative embeddings of latent variable models for structured data",
      "author" : [ "Dai", "Hanjun", "Bo", "Song", "Le" ],
      "venue" : "arXiv preprint arXiv:1603.05629,",
      "citeRegEx" : "Dai et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Dai et al\\.",
      "year" : 2016
    }, {
      "title" : "Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity",
      "author" : [ "Daniely", "Amit", "Frostig", "Roy", "Singer", "Yoram" ],
      "venue" : null,
      "citeRegEx" : "Daniely et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Daniely et al\\.",
      "year" : 2016
    }, {
      "title" : "Recurrent neural network grammars",
      "author" : [ "Dyer", "Chris", "Kuncoro", "Adhiguna", "Ballesteros", "Miguel", "Smith", "Noah A" ],
      "venue" : "In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics,",
      "citeRegEx" : "Dyer et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Dyer et al\\.",
      "year" : 2016
    }, {
      "title" : "A theoretically grounded application of dropout in recurrent neural networks",
      "author" : [ "Gal", "Yarin", "Ghahramani", "Zoubin" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Gal et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Gal et al\\.",
      "year" : 2016
    }, {
      "title" : "On graph kernels: Hardness results and efficient alternatives",
      "author" : [ "Gärtner", "Thomas", "Flach", "Peter", "Wrobel", "Stefan" ],
      "venue" : "In Learning Theory and Kernel Machines,",
      "citeRegEx" : "Gärtner et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Gärtner et al\\.",
      "year" : 2003
    }, {
      "title" : "Lstm: A search space odyssey",
      "author" : [ "Greff", "Klaus", "Srivastava", "Rupesh Kumar", "Koutnı́k", "Jan", "Steunebrink", "Bas R", "Schmidhuber", "Jürgen" ],
      "venue" : "arXiv preprint arXiv:1503.04069,",
      "citeRegEx" : "Greff et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Greff et al\\.",
      "year" : 2015
    }, {
      "title" : "Steps toward deep kernel methods from infinite neural networks",
      "author" : [ "Hazan", "Tamir", "Jaakkola", "Tommi" ],
      "venue" : "arXiv preprint arXiv:1508.05133,",
      "citeRegEx" : "Hazan et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Hazan et al\\.",
      "year" : 2015
    }, {
      "title" : "Improper deep kernels",
      "author" : [ "Heinemann", "Uri", "Livni", "Roi", "Eban", "Elad", "Elidan", "Gal", "Globerson", "Amir" ],
      "venue" : "In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "Heinemann et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Heinemann et al\\.",
      "year" : 2016
    }, {
      "title" : "Deep convolutional networks on graph-structured data",
      "author" : [ "Henaff", "Mikael", "Bruna", "Joan", "LeCun", "Yann" ],
      "venue" : "arXiv preprint arXiv:1506.05163,",
      "citeRegEx" : "Henaff et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Henaff et al\\.",
      "year" : 2015
    }, {
      "title" : "Improving neural networks by preventing co-adaptation of feature detectors",
      "author" : [ "Hinton", "Geoffrey E", "Srivastava", "Nitish", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan R" ],
      "venue" : "arXiv preprint arXiv:1207.0580,",
      "citeRegEx" : "Hinton et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2012
    }, {
      "title" : "Long shortterm memory",
      "author" : [ "Hochreiter", "Sepp", "Schmidhuber", "Jürgen" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "Hochreiter et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Hochreiter et al\\.",
      "year" : 1997
    }, {
      "title" : "Deep recursive neural networks for compositionality in language",
      "author" : [ "Irsoy", "Ozan", "Cardie", "Claire" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Irsoy et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Irsoy et al\\.",
      "year" : 2014
    }, {
      "title" : "Deep unordered composition rivals syntactic methods for text classification",
      "author" : [ "Iyyer", "Mohit", "Manjunatha", "Varun", "Boyd-Graber", "Jordan", "Daumé III", "Hal" ],
      "venue" : "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics",
      "citeRegEx" : "Iyyer et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Iyyer et al\\.",
      "year" : 2015
    }, {
      "title" : "A convolutional neural network for modelling sentences",
      "author" : [ "Kalchbrenner", "Nal", "Grefenstette", "Edward", "Blunsom", "Phil" ],
      "venue" : "In Proceedings of the 52th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Kalchbrenner et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kalchbrenner et al\\.",
      "year" : 2014
    }, {
      "title" : "Convolutional neural networks for sentence classification",
      "author" : [ "Kim", "Yoon" ],
      "venue" : "In Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP",
      "citeRegEx" : "Kim and Yoon.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kim and Yoon.",
      "year" : 2014
    }, {
      "title" : "Character-aware neural language models",
      "author" : [ "Kim", "Yoon", "Jernite", "Yacine", "Sontag", "David", "Rush", "Alexander M" ],
      "venue" : "Twenty-Ninth AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "Kim et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2015
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Kingma", "Diederik P", "Ba", "Jimmy Lei" ],
      "venue" : "In International Conference on Learning Representation,",
      "citeRegEx" : "Kingma et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kingma et al\\.",
      "year" : 2015
    }, {
      "title" : "Ask me anything: Dynamic memory networks for natural language processing",
      "author" : [ "Kumar", "Ankit", "Irsoy", "Ozan", "Ondruska", "Peter", "Iyyer", "Mohit", "James Bradbury", "Ishaan Gulrajani", "Zhong", "Victor", "Paulus", "Romain", "Socher", "Richard" ],
      "venue" : null,
      "citeRegEx" : "Kumar et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kumar et al\\.",
      "year" : 2016
    }, {
      "title" : "Distributed representations of sentences and documents",
      "author" : [ "Le", "Quoc", "Mikolov", "Tomas" ],
      "venue" : "In Proceedings of the 31st International Conference on Machine Learning",
      "citeRegEx" : "Le et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Le et al\\.",
      "year" : 2014
    }, {
      "title" : "Recurrent additive networks",
      "author" : [ "Lee", "Kenton", "Levy", "Omer", "Zettlemoyer", "Luke" ],
      "venue" : null,
      "citeRegEx" : "Lee et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2017
    }, {
      "title" : "Semi-supervised question retrieval with gated convolutions",
      "author" : [ "Lei", "Tao", "Joshi", "Hrishikesh", "Barzilay", "Regina", "Jaakkola", "Tommi", "Tymoshenko", "Katerina", "Moschitti", "Alessandro", "Marquez", "Lluis" ],
      "venue" : "arXiv preprint arXiv:1512.05726,",
      "citeRegEx" : "Lei et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Lei et al\\.",
      "year" : 2015
    }, {
      "title" : "Gated graph sequence neural networks",
      "author" : [ "Li", "Yujia", "Tarlow", "Daniel", "Brockschmidt", "Marc", "Zemel", "Richard" ],
      "venue" : "arXiv preprint arXiv:1511.05493,",
      "citeRegEx" : "Li et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2015
    }, {
      "title" : "Text classification using string kernels",
      "author" : [ "Lodhi", "Huma", "Saunders", "Craig", "Shawe-Taylor", "John", "Cristianini", "Nello", "Watkins", "Chris" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Lodhi et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Lodhi et al\\.",
      "year" : 2002
    }, {
      "title" : "Pointer sentinel mixture models",
      "author" : [ "Merity", "Stephen", "Xiong", "Caiming", "Bradbury", "James", "Socher", "Richard" ],
      "venue" : "arXiv preprint arXiv:1609.07843,",
      "citeRegEx" : "Merity et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Merity et al\\.",
      "year" : 2016
    }, {
      "title" : "Glove: Global vectors for word representation. volume",
      "author" : [ "Pennington", "Jeffrey", "Socher", "Richard", "Manning", "Christopher D" ],
      "venue" : null,
      "citeRegEx" : "Pennington et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Using the output embedding to improve language models",
      "author" : [ "Press", "Ofir", "Wolf", "Lior" ],
      "venue" : "arXiv preprint arXiv:1608.05859,",
      "citeRegEx" : "Press et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Press et al\\.",
      "year" : 2016
    }, {
      "title" : "Expressivity versus efficiency of graph kernels",
      "author" : [ "Ramon", "Jan", "Gärtner", "Thomas" ],
      "venue" : "In First international workshop on mining graphs, trees and sequences,",
      "citeRegEx" : "Ramon et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Ramon et al\\.",
      "year" : 2003
    }, {
      "title" : "Learning kernel-based halfspaces with the 01 loss",
      "author" : [ "Shalev-Shwartz", "Shai", "Shamir", "Ohad", "Sridharan", "Karthik" ],
      "venue" : "SIAM Journal on Computing,",
      "citeRegEx" : "Shalev.Shwartz et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Shalev.Shwartz et al\\.",
      "year" : 2011
    }, {
      "title" : "Weisfeiler-lehman graph kernels",
      "author" : [ "Shervashidze", "Nino", "Schweitzer", "Pascal", "Leeuwen", "Erik Jan van", "Mehlhorn", "Kurt", "Borgwardt", "Karsten M" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Shervashidze et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Shervashidze et al\\.",
      "year" : 2011
    }, {
      "title" : "Recursive deep models for semantic compositionality over a sentiment treebank",
      "author" : [ "Socher", "Richard", "Perelygin", "Alex", "Wu", "Jean", "Chuang", "Jason", "Manning", "Christopher D", "Ng", "Andrew Y", "Potts", "Christopher" ],
      "venue" : "In Proceedings of the 2013 Conference on Empirical Methods in Natural",
      "citeRegEx" : "Socher et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "Training very deep networks",
      "author" : [ "Srivastava", "Rupesh K", "Greff", "Klaus", "Schmidhuber", "Jürgen" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Srivastava et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 2015
    }, {
      "title" : "Improved semantic representations from treestructured long short-term memory networks",
      "author" : [ "Tai", "Kai Sheng", "Socher", "Richard", "Manning", "Christopher D" ],
      "venue" : "In Proceedings of the 53th Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "Tai et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Tai et al\\.",
      "year" : 2015
    }, {
      "title" : "Value iteration networks",
      "author" : [ "Tamar", "Aviv", "Levine", "Sergey", "Abbeel", "Pieter", "Wu", "Yi", "Thomas", "Garrett" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Tamar et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Tamar et al\\.",
      "year" : 2016
    }, {
      "title" : "Recurrent neural network regularization",
      "author" : [ "Zaremba", "Wojciech", "Sutskever", "Ilya", "Vinyals", "Oriol" ],
      "venue" : "arXiv preprint arXiv:1409.2329,",
      "citeRegEx" : "Zaremba et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Zaremba et al\\.",
      "year" : 2014
    }, {
      "title" : "`1regularized neural networks are improperly learnable in polynomial time",
      "author" : [ "Zhang", "Yuchen", "Lee", "Jason D", "Jordan", "Michael I" ],
      "venue" : "In Proceedings of the 33nd International Conference on Machine Learning,",
      "citeRegEx" : "Zhang et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2016
    }, {
      "title" : "Recurrent Highway Networks",
      "author" : [ "Zilly", "Julian Georg", "Srivastava", "Rupesh Kumar", "Koutnı́k", "Jan", "Schmidhuber", "Jürgen" ],
      "venue" : "arXiv preprint arXiv:1607.03474,",
      "citeRegEx" : "Zilly et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Zilly et al\\.",
      "year" : 2016
    }, {
      "title" : "Neural architecture search with reinforcement learning",
      "author" : [ "Zoph", "Barret", "Le", "Quoc V" ],
      "venue" : "arXiv preprint arXiv:1611.01578,",
      "citeRegEx" : "Zoph et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Zoph et al\\.",
      "year" : 2016
    }, {
      "title" : "2017) both suggest that a linear additive state computation suffices to provide competitive performance compared to LSTMs",
      "author" : [ "Lee" ],
      "venue" : "Ghifary",
      "citeRegEx" : "Lee,? \\Q2017\\E",
      "shortCiteRegEx" : "Lee",
      "year" : 2017
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "For instance, LSTM (Hochreiter & Schmidhuber, 1997), GRU (Chung et al., 2014) and other complex recurrent units (Zoph & Le, 2016) can be easily adapted to embed structured objects such as sentences (Tai et al.",
      "startOffset" : 57,
      "endOffset" : 77
    }, {
      "referenceID" : 37,
      "context" : ", 2014) and other complex recurrent units (Zoph & Le, 2016) can be easily adapted to embed structured objects such as sentences (Tai et al., 2015) or molecules (Li et al.",
      "startOffset" : 128,
      "endOffset" : 146
    }, {
      "referenceID" : 27,
      "context" : ", 2015) or molecules (Li et al., 2015; Dai et al., 2016) into vector spaces suitable for later processing by standard predictive methods.",
      "startOffset" : 21,
      "endOffset" : 56
    }, {
      "referenceID" : 6,
      "context" : ", 2015) or molecules (Li et al., 2015; Dai et al., 2016) into vector spaces suitable for later processing by standard predictive methods.",
      "startOffset" : 21,
      "endOffset" : 56
    }, {
      "referenceID" : 38,
      "context" : "For example, value iteration calculations can be folded into convolutional architectures so as to optimize the representations to facilitate planning (Tamar et al., 2016).",
      "startOffset" : 150,
      "endOffset" : 170
    }, {
      "referenceID" : 6,
      "context" : "Similarly, inference calculations in graphical models about latent states of variables such as atom characteristics can be directly associated with embedding operations (Dai et al., 2016).",
      "startOffset" : 169,
      "endOffset" : 187
    }, {
      "referenceID" : 28,
      "context" : "For example, in a string kernel (Lodhi et al., 2002), S may refer to all possible subsequences while a graph kernel (Vishwanathan et al.",
      "startOffset" : 32,
      "endOffset" : 52
    }, {
      "referenceID" : 40,
      "context" : "Several studies have highlighted the relation between feed-forward neural architectures and kernels (Hazan & Jaakkola, 2015; Zhang et al., 2016) but we are unaware of any prior work pertaining to kernels associated with neural architectures for structured objects.",
      "startOffset" : 100,
      "endOffset" : 144
    }, {
      "referenceID" : 28,
      "context" : "String Kernel String kernel measures the similarity between two sequences by counting shared subsequences (see Lodhi et al. (2002)).",
      "startOffset" : 111,
      "endOffset" : 131
    }, {
      "referenceID" : 33,
      "context" : "It turns out that many activations are also functions in the reproducing kernel Hilbert space (RKHS) of certain kernel functions (see Shalev-Shwartz et al. (2011); Zhang et al.",
      "startOffset" : 134,
      "endOffset" : 163
    }, {
      "referenceID" : 33,
      "context" : "It turns out that many activations are also functions in the reproducing kernel Hilbert space (RKHS) of certain kernel functions (see Shalev-Shwartz et al. (2011); Zhang et al. (2016)).",
      "startOffset" : 134,
      "endOffset" : 184
    }, {
      "referenceID" : 40,
      "context" : "(l + 1)th kernel is defined on top of l-th kernel), which has been proven for feed-forward networks (Zhang et al., 2016).",
      "startOffset" : 100,
      "endOffset" : 120
    }, {
      "referenceID" : 10,
      "context" : "Random Walk Kernel NNs We start from random walk graph kernels (Gärtner et al., 2003), which count common walks in two graphs.",
      "startOffset" : 63,
      "endOffset" : 85
    }, {
      "referenceID" : 34,
      "context" : "This parameter tying mechanism allows our model to embed Weisfeiler-Lehman kernel (Shervashidze et al., 2011).",
      "startOffset" : 82,
      "endOffset" : 109
    }, {
      "referenceID" : 34,
      "context" : "Figure taken from Shervashidze et al. (2011) Note that our definition of r(v) is exactly the same as hv in Equation 9, with ◦ being additive composition.",
      "startOffset" : 18,
      "endOffset" : 45
    }, {
      "referenceID" : 5,
      "context" : "This includes recurrent modules with the ability to carry persistent memories such as LSTM (Hochreiter & Schmidhuber, 1997) and GRU (Chung et al., 2014), as well as non-consecutive convolutional modules (RCNNs, Lei et al.",
      "startOffset" : 132,
      "endOffset" : 152
    }, {
      "referenceID" : 0,
      "context" : "Similar results have been made for convolutional neural nets (Anselmi et al., 2015), and general computational graphs (Daniely et al.",
      "startOffset" : 61,
      "endOffset" : 83
    }, {
      "referenceID" : 7,
      "context" : ", 2015), and general computational graphs (Daniely et al., 2016).",
      "startOffset" : 42,
      "endOffset" : 64
    }, {
      "referenceID" : 40,
      "context" : "While some prior work appeals to convex optimization through improper learning (Zhang et al., 2016; Heinemann et al., 2016) (since kernel space is larger), we use the proposed networks as building blocks in typical non-convex but flexible neural network training.",
      "startOffset" : 79,
      "endOffset" : 123
    }, {
      "referenceID" : 13,
      "context" : "While some prior work appeals to convex optimization through improper learning (Zhang et al., 2016; Heinemann et al., 2016) (since kernel space is larger), we use the proposed networks as building blocks in typical non-convex but flexible neural network training.",
      "startOffset" : 79,
      "endOffset" : 123
    }, {
      "referenceID" : 3,
      "context" : "This includes recurrent modules with the ability to carry persistent memories such as LSTM (Hochreiter & Schmidhuber, 1997) and GRU (Chung et al., 2014), as well as non-consecutive convolutional modules (RCNNs, Lei et al. (2015)), and others.",
      "startOffset" : 133,
      "endOffset" : 229
    }, {
      "referenceID" : 3,
      "context" : "This includes recurrent modules with the ability to carry persistent memories such as LSTM (Hochreiter & Schmidhuber, 1997) and GRU (Chung et al., 2014), as well as non-consecutive convolutional modules (RCNNs, Lei et al. (2015)), and others. More recently, Zoph & Le (2016) exemplified a reinforcement learning-based search algorithm to further optimize the design of such recurrent architectures.",
      "startOffset" : 133,
      "endOffset" : 275
    }, {
      "referenceID" : 3,
      "context" : "This includes recurrent modules with the ability to carry persistent memories such as LSTM (Hochreiter & Schmidhuber, 1997) and GRU (Chung et al., 2014), as well as non-consecutive convolutional modules (RCNNs, Lei et al. (2015)), and others. More recently, Zoph & Le (2016) exemplified a reinforcement learning-based search algorithm to further optimize the design of such recurrent architectures. Our proposed neural networks offer similar state evolution and feature aggregation functionalities but derive the motivation for the operations involved from wellestablished kernel computations over sequences. Recursive neural networks are alternative architectures to model hierarchical structures such as syntax trees and logic forms. For instance, Socher et al. (2013) employs recursive networks for sentence classification, where each node in the dependency tree of the sentence is transformed into a vector representation.",
      "startOffset" : 133,
      "endOffset" : 771
    }, {
      "referenceID" : 3,
      "context" : "This includes recurrent modules with the ability to carry persistent memories such as LSTM (Hochreiter & Schmidhuber, 1997) and GRU (Chung et al., 2014), as well as non-consecutive convolutional modules (RCNNs, Lei et al. (2015)), and others. More recently, Zoph & Le (2016) exemplified a reinforcement learning-based search algorithm to further optimize the design of such recurrent architectures. Our proposed neural networks offer similar state evolution and feature aggregation functionalities but derive the motivation for the operations involved from wellestablished kernel computations over sequences. Recursive neural networks are alternative architectures to model hierarchical structures such as syntax trees and logic forms. For instance, Socher et al. (2013) employs recursive networks for sentence classification, where each node in the dependency tree of the sentence is transformed into a vector representation. Tai et al. (2015) further proposed tree-LSTM, which incorporates LSTM-style architectures as the transformation unit.",
      "startOffset" : 133,
      "endOffset" : 945
    }, {
      "referenceID" : 3,
      "context" : "This includes recurrent modules with the ability to carry persistent memories such as LSTM (Hochreiter & Schmidhuber, 1997) and GRU (Chung et al., 2014), as well as non-consecutive convolutional modules (RCNNs, Lei et al. (2015)), and others. More recently, Zoph & Le (2016) exemplified a reinforcement learning-based search algorithm to further optimize the design of such recurrent architectures. Our proposed neural networks offer similar state evolution and feature aggregation functionalities but derive the motivation for the operations involved from wellestablished kernel computations over sequences. Recursive neural networks are alternative architectures to model hierarchical structures such as syntax trees and logic forms. For instance, Socher et al. (2013) employs recursive networks for sentence classification, where each node in the dependency tree of the sentence is transformed into a vector representation. Tai et al. (2015) further proposed tree-LSTM, which incorporates LSTM-style architectures as the transformation unit. Dyer et al. (2015; 2016) recently introduced a recursive neural model for transitionbased language modeling and parsing. While not specifically discussed in the paper, our ideas do extend to similar neural components for hierarchical objects (e.g. trees). Graph Networks Most of the current graph neural architectures perform either convolutional or recurrent operations on graphs. Duvenaud et al. (2015) developed Neural Fingerprint for chemical compounds, where each convolution operation is a sum of neighbor node features, followed by a linear transformation.",
      "startOffset" : 133,
      "endOffset" : 1450
    }, {
      "referenceID" : 1,
      "context" : ", Bruna et al. (2013) and Henaff et al.",
      "startOffset" : 2,
      "endOffset" : 22
    }, {
      "referenceID" : 1,
      "context" : ", Bruna et al. (2013) and Henaff et al. (2015), rely on graph Laplacian or Fourier transform.",
      "startOffset" : 2,
      "endOffset" : 47
    }, {
      "referenceID" : 1,
      "context" : ", Bruna et al. (2013) and Henaff et al. (2015), rely on graph Laplacian or Fourier transform. For recurrent architectures, Li et al. (2015) proposed gated graph neural networks, where neighbor features are aggregated by GRU function.",
      "startOffset" : 2,
      "endOffset" : 140
    }, {
      "referenceID" : 1,
      "context" : ", Bruna et al. (2013) and Henaff et al. (2015), rely on graph Laplacian or Fourier transform. For recurrent architectures, Li et al. (2015) proposed gated graph neural networks, where neighbor features are aggregated by GRU function. Dai et al. (2016) considers a different architecture where a graph is viewed as a latent variable graphical model.",
      "startOffset" : 2,
      "endOffset" : 252
    }, {
      "referenceID" : 1,
      "context" : ", Bruna et al. (2013) and Henaff et al. (2015), rely on graph Laplacian or Fourier transform. For recurrent architectures, Li et al. (2015) proposed gated graph neural networks, where neighbor features are aggregated by GRU function. Dai et al. (2016) considers a different architecture where a graph is viewed as a latent variable graphical model. Their recurrent model is derived from Belief Propagation-like algorithms. Our approach is most closely related to Dai et al. (2016), in terms of neighbor feature aggregation and resulting recurrent architecture.",
      "startOffset" : 2,
      "endOffset" : 481
    }, {
      "referenceID" : 1,
      "context" : ", Bruna et al. (2013) and Henaff et al. (2015), rely on graph Laplacian or Fourier transform. For recurrent architectures, Li et al. (2015) proposed gated graph neural networks, where neighbor features are aggregated by GRU function. Dai et al. (2016) considers a different architecture where a graph is viewed as a latent variable graphical model. Their recurrent model is derived from Belief Propagation-like algorithms. Our approach is most closely related to Dai et al. (2016), in terms of neighbor feature aggregation and resulting recurrent architecture. Nonetheless, the focus of this paper is on providing a framework for how such recurrent networks could be derived from deep graph kernels. Kernels and Neural Nets Our work follows recent work demonstrating the connection between neural networks and kernels (Cho & Saul, 2009; Hazan & Jaakkola, 2015). For example, Zhang et al. (2016) showed that standard feedforward neural nets belong to a larger space of recursively constructed kernels (given certain activation functions).",
      "startOffset" : 2,
      "endOffset" : 895
    }, {
      "referenceID" : 15,
      "context" : "We back-propagate the gradient with an unroll size of 35 and use dropout (Hinton et al., 2012) as the regularization.",
      "startOffset" : 73,
      "endOffset" : 94
    }, {
      "referenceID" : 41,
      "context" : "5 Following (Zilly et al., 2016), we add highway connections (Srivastava et al.",
      "startOffset" : 12,
      "endOffset" : 32
    }, {
      "referenceID" : 36,
      "context" : ", 2016), we add highway connections (Srivastava et al., 2015) within each layer: c[t] = λt c[t− 1] + (1− λt) (W(l)h(l−1)[t]) h[t] = ft c[t] + (1− ft) h(l−1)[t] where h[t] = xt, λt is the gated decay factor and ft is the transformation gate of highway connections.",
      "startOffset" : 36,
      "endOffset" : 61
    }, {
      "referenceID" : 41,
      "context" : "8 perplexity when the recurrence depth is increased to 4, being state-of-the-art and on par with the results reported in (Zilly et al., 2016; Zoph & Le, 2016).",
      "startOffset" : 121,
      "endOffset" : 158
    }, {
      "referenceID" : 15,
      "context" : "We back-propagate the gradient with an unroll size of 35 and use dropout (Hinton et al., 2012) as the regularization. Unless otherwise specified, we train 3-layer networks with n = 1 and normalized adaptive decay.5 Following (Zilly et al., 2016), we add highway connections (Srivastava et al., 2015) within each layer: c[t] = λt c[t− 1] + (1− λt) (W(l)h(l−1)[t]) h[t] = ft c[t] + (1− ft) h(l−1)[t] where h[t] = xt, λt is the gated decay factor and ft is the transformation gate of highway connections.6 Results Table 1 compares our model with various state-ofthe-art models. Our small model with 5 million parameters achieves a test perplexity of 73.6, already outperforming many results achieved using much larger network. By increasing the network size to 20 million, we obtain a test perplexity of 69.2, with standard dropout. Adding variational dropout (Gal & Ghahramani, 2016) within the recurrent cells further improves the perplexity to 65.5. Finally, the model achieves 63.8 perplexity when the recurrence depth is increased to 4, being state-of-the-art and on par with the results reported in (Zilly et al., 2016; Zoph & Le, 2016). Note that Zilly et al. (2016) uses 10 neural layers and Zoph & Le (2016) adopts a complex recurrent cell found by reinforcement learning based search.",
      "startOffset" : 74,
      "endOffset" : 1171
    }, {
      "referenceID" : 15,
      "context" : "We back-propagate the gradient with an unroll size of 35 and use dropout (Hinton et al., 2012) as the regularization. Unless otherwise specified, we train 3-layer networks with n = 1 and normalized adaptive decay.5 Following (Zilly et al., 2016), we add highway connections (Srivastava et al., 2015) within each layer: c[t] = λt c[t− 1] + (1− λt) (W(l)h(l−1)[t]) h[t] = ft c[t] + (1− ft) h(l−1)[t] where h[t] = xt, λt is the gated decay factor and ft is the transformation gate of highway connections.6 Results Table 1 compares our model with various state-ofthe-art models. Our small model with 5 million parameters achieves a test perplexity of 73.6, already outperforming many results achieved using much larger network. By increasing the network size to 20 million, we obtain a test perplexity of 69.2, with standard dropout. Adding variational dropout (Gal & Ghahramani, 2016) within the recurrent cells further improves the perplexity to 65.5. Finally, the model achieves 63.8 perplexity when the recurrence depth is increased to 4, being state-of-the-art and on par with the results reported in (Zilly et al., 2016; Zoph & Le, 2016). Note that Zilly et al. (2016) uses 10 neural layers and Zoph & Le (2016) adopts a complex recurrent cell found by reinforcement learning based search.",
      "startOffset" : 74,
      "endOffset" : 1214
    }, {
      "referenceID" : 39,
      "context" : "Model |θ| PPL LSTM (large) (Zaremba et al., 2014) 66m 78.",
      "startOffset" : 27,
      "endOffset" : 49
    }, {
      "referenceID" : 21,
      "context" : "4 Character CNN (Kim et al., 2015) 19m 78.",
      "startOffset" : 16,
      "endOffset" : 34
    }, {
      "referenceID" : 41,
      "context" : "9 Variational RHN (Zilly et al., 2016) 23m 65.",
      "startOffset" : 18,
      "endOffset" : 38
    }, {
      "referenceID" : 35,
      "context" : "We use the Stanford Sentiment Treebank benchmark (Socher et al., 2013).",
      "startOffset" : 49,
      "endOffset" : 70
    }, {
      "referenceID" : 31,
      "context" : "Model Fine Binary RNN (Socher et al. (2011)) 43.",
      "startOffset" : 23,
      "endOffset" : 44
    }, {
      "referenceID" : 31,
      "context" : "Model Fine Binary RNN (Socher et al. (2011)) 43.2 82.4 RNTN (Socher et al. (2013)) 45.",
      "startOffset" : 23,
      "endOffset" : 82
    }, {
      "referenceID" : 31,
      "context" : "Model Fine Binary RNN (Socher et al. (2011)) 43.2 82.4 RNTN (Socher et al. (2013)) 45.7 85.4 DRNN (Irsoy & Cardie (2014)) 49.",
      "startOffset" : 23,
      "endOffset" : 121
    }, {
      "referenceID" : 31,
      "context" : "Model Fine Binary RNN (Socher et al. (2011)) 43.2 82.4 RNTN (Socher et al. (2013)) 45.7 85.4 DRNN (Irsoy & Cardie (2014)) 49.8 86.8 RLSTM (Tai et al. (2015)) 51.",
      "startOffset" : 23,
      "endOffset" : 157
    }, {
      "referenceID" : 17,
      "context" : "0 DCNN (Kalchbrenner et al. (2014)) 48.",
      "startOffset" : 8,
      "endOffset" : 35
    }, {
      "referenceID" : 17,
      "context" : "0 DCNN (Kalchbrenner et al. (2014)) 48.5 86.9 CNN-MC (Kim (2014)) 47.",
      "startOffset" : 8,
      "endOffset" : 65
    }, {
      "referenceID" : 17,
      "context" : "0 DCNN (Kalchbrenner et al. (2014)) 48.5 86.9 CNN-MC (Kim (2014)) 47.4 88.1 Bi-LSTM (Tai et al. (2015)) 49.",
      "startOffset" : 8,
      "endOffset" : 103
    }, {
      "referenceID" : 3,
      "context" : "5 LSTMN (Cheng et al. (2016)) 47.",
      "startOffset" : 9,
      "endOffset" : 29
    }, {
      "referenceID" : 3,
      "context" : "5 LSTMN (Cheng et al. (2016)) 47.9 87.0 PVEC (Le & Mikolov (2014)) 48.",
      "startOffset" : 9,
      "endOffset" : 66
    }, {
      "referenceID" : 3,
      "context" : "5 LSTMN (Cheng et al. (2016)) 47.9 87.0 PVEC (Le & Mikolov (2014)) 48.7 87.8 DAN (Iyyer et al. (2014)) 48.",
      "startOffset" : 9,
      "endOffset" : 102
    }, {
      "referenceID" : 3,
      "context" : "5 LSTMN (Cheng et al. (2016)) 47.9 87.0 PVEC (Le & Mikolov (2014)) 48.7 87.8 DAN (Iyyer et al. (2014)) 48.2 86.8 DMN (Kumar et al. (2016)) 52.",
      "startOffset" : 9,
      "endOffset" : 138
    }, {
      "referenceID" : 18,
      "context" : "Following the recent work of DAN (Iyyer et al., 2015) and RLSTM (Tai et al.",
      "startOffset" : 33,
      "endOffset" : 53
    }, {
      "referenceID" : 37,
      "context" : ", 2015) and RLSTM (Tai et al., 2015), we use the publicly available 300-dimensional GloVe word vectors (Pennington et al.",
      "startOffset" : 18,
      "endOffset" : 36
    }, {
      "referenceID" : 30,
      "context" : ", 2015), we use the publicly available 300-dimensional GloVe word vectors (Pennington et al., 2014).",
      "startOffset" : 74,
      "endOffset" : 99
    }, {
      "referenceID" : 6,
      "context" : "Model (Dai et al., 2016) |θ| RMSE Mean Predicator 1 2.",
      "startOffset" : 6,
      "endOffset" : 24
    }, {
      "referenceID" : 6,
      "context" : "Molecular Graph Regression Dataset and Setup We further evaluate our graph NN models on the Harvard Clean Energy Project benchmark, which has been used in Dai et al. (2016); Duvenaud et al.",
      "startOffset" : 155,
      "endOffset" : 173
    }, {
      "referenceID" : 6,
      "context" : "Molecular Graph Regression Dataset and Setup We further evaluate our graph NN models on the Harvard Clean Energy Project benchmark, which has been used in Dai et al. (2016); Duvenaud et al. (2015) as their evaluation dataset.",
      "startOffset" : 155,
      "endOffset" : 197
    }, {
      "referenceID" : 6,
      "context" : "Molecular Graph Regression Dataset and Setup We further evaluate our graph NN models on the Harvard Clean Energy Project benchmark, which has been used in Dai et al. (2016); Duvenaud et al. (2015) as their evaluation dataset. This dataset contains 2.3 million candidate molecules, with each molecule labeled with its power conversion efficiency (PCE) value. We follow exactly the same train-test split as Dai et al. (2016), and the same re-sampling procedure on the training data (but not the test data) to make the algorithm put more Table 3: Experiments on Harvard Clean Energy Project.",
      "startOffset" : 155,
      "endOffset" : 423
    }, {
      "referenceID" : 6,
      "context" : "Molecular Graph Regression Dataset and Setup We further evaluate our graph NN models on the Harvard Clean Energy Project benchmark, which has been used in Dai et al. (2016); Duvenaud et al. (2015) as their evaluation dataset. This dataset contains 2.3 million candidate molecules, with each molecule labeled with its power conversion efficiency (PCE) value. We follow exactly the same train-test split as Dai et al. (2016), and the same re-sampling procedure on the training data (but not the test data) to make the algorithm put more Table 3: Experiments on Harvard Clean Energy Project. We report Root Mean Square Error(RMSE) on test set. The first block lists the results reported in Dai et al. (2016) for reference.",
      "startOffset" : 155,
      "endOffset" : 705
    }, {
      "referenceID" : 6,
      "context" : "Embedded Loopy BP (Dai et al., 2016) is a recurrent architecture, with 4 recurrent iterations.",
      "startOffset" : 18,
      "endOffset" : 36
    } ],
    "year" : 2017,
    "abstractText" : "The design of neural architectures for structured objects is typically guided by experimental insights rather than a formal process. In this work, we appeal to kernels over combinatorial structures, such as sequences and graphs, to derive appropriate neural operations. We introduce a class of deep recurrent neural operations and formally characterize their associated kernel spaces. Our recurrent modules compare the input to virtual reference objects (cf. filters in CNN) via the kernels. Similar to traditional neural operations, these reference objects are parameterized and directly optimized in end-to-end training. We empirically evaluate the proposed class of neural architectures on standard applications such as language modeling and molecular graph regression, achieving state-of-the-art results across these applications.",
    "creator" : "LaTeX with hyperref package"
  }
}