{
  "name" : "1709.02535.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Megumi Miyashita", "Shiro Yano", "Toshiyuki Kondo" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 9.\n02 53\n5v 1\n[ cs\n.L G\n] 8\nS ep\n2 01\nIn recent years, attention has been focused on the relationship between black box optimization and reinforcement learning. Black box optimization is a framework for the problem of finding the input that optimizes the output represented by an unknown function. Reinforcement learning, by contrast, is a framework for finding a policy to optimize the expected cumulative reward from trial and error. In this research, we propose a reinforcement learning algorithm based on the mirror descent method, which is general optimization algorithm. The proposed method is called Mirror Descent Search. The contribution of this research is roughly twofold. First, an extension method for mirror descent can be applied to reinforcement learning and such a method is here considered. Second, the relationship between existing reinforcement learning algorithms is clarified. Based on these, we propose Mirror Descent Search and derivative methods. The experimental results show that learning with the proposed method progresses faster.\nKeywords: Reinforcement Learning, Mirror Descent"
    }, {
      "heading" : "1. Introduction",
      "text" : "In recent years, as stated in [1], attention has focused on the relationship between black box optimization and reinforcement learning. Black box optimization is a framework for the problem of finding the input x∗ ∈ X that optimizes the output f (x) : a → R represented by an unknown function. Because the objective function is unknown, we solve the black box optimization problem without gradient information. Reinforcement learning, by contrast, is a framework for finding a policy to optimize the expected cumulative reward from trial and error. Based on this, the solution to the black box optimization problem can be used as a solution for reinforcement learning.\nIn this research, we propose a reinforcement learning algorithm based on the mirror descent (MD) method [2]. MD is general optimization algorithm that employs a Bregman\n✩The research was partially supported by JSPS KAKENHI (Grant numbers JP26120005, JP16H03219, and JP17K12737).\nPreprint submitted to Robotics and Autonomous Systems September 11, 2017\ndivergence alternative to the Euclidean distance, which is the metric of gradient descent. The derivation for this is detailed in Section 2. We call our proposed method Mirror Descent Search (MDS). In addition, MDS is expected to generalize some existing reinforcement learning algorithms. This research shows (1) that the extension method in the MD can be applied to reinforcement learning, and (2) that the relationship between existing reinforcement learning algorithms can be clarified."
    }, {
      "heading" : "1.1. Related works",
      "text" : "In this section, we describe previous research and its relation to the research in this paper.\nRelative Entropy Policy Search (REPS) [? ] and its derivation method focuses on information loss during policy searches. This information loss is the relative entropy of the data distribution generated from the observation data distribution. The new policy and is set as the upper limit value. This is equivalent to defining the upper limit value of the Kullback–Librar (KL) divergence between each distribution. Episode-based REPS [4] is a derivation method formalized by considering the upper-level policy. Although the equations for episode-based REPS and the proposed method are similar, our method can naturally consider distance metrics other than the KL divergence. Consequently, we can apply extension methods in MD.\nIn [1, 5], the authors focus on the relationship between reinforcement learning and black box optimization. Specifically, [1] explains the history of black box optimization and reinforcement learning, and proposes PIBB. PIBB refers to Policy Improvement with Path Integrals (PI2) [6, 7]. It is considered a black box optimization method, where PIBB is derived on the basis of the Covariance Matrix Adaptation Evolutionary Strategy (CMA-ES) [8], a black box optimization algorithm. Through a comparison with PI2, the authors discuss the connection between reinforcement learning and black box optimization. We further discuss the connection between PI2 and the proposed method in the Appendix, below.\nPrevious studies proposed solutions to reinforcement learning based on MD. Indeed, [9] is strongly associated with our research insofar as the authors propose a method based on MD. However, the details are different. We adopted the exponentiated gradient method (EG) using KL divergence as a regularization term. By contrast, [9] argues that using the Minkowski distance with the Euclidean distance is preferable to KL divergence, because it offers flexibility when updating the gradient."
    }, {
      "heading" : "2. Methods: MDS, G-MDS, AMDS, G-AMDS",
      "text" : ""
    }, {
      "heading" : "2.1. Derivation of proposed algorithm: MDS and G-MDS",
      "text" : ""
    }, {
      "heading" : "2.1.1. MDS",
      "text" : "A reinforcement learning algorithm is an algorithm aimed at obtaining an optimal policy to maximize reward (i.e., by minimizing cost). Consider the problem of minimizing the objective function J (θ). Rather than dealing with policy parameters θ ∈ Θ directly, we\nconsider the probability function p (θ). Therefore, we search the following domain:\nP =\n{\np (θ) = [p (θ1) , · · · , p (θM)] ∈ R M\n∣ ∣ ∣ ∣ ∣ ∀jp (θj) ≥ 0, M ∑\nj=1\np (θj) = 1\n}\n. (1)\nThe decision variable is p (θ), and the objective function is the expectation of the cost J (θ).\nJ = M ∑\nj=1\np (θj)J (θj) (2)\nTherefore, the optimal generative probability is\np∗(θ) = arg min p(θ)∈P J . (3)\nNext, we consider obtaining the optimal policy by updating p (θ). As a means for updating p (θ), we use MD, given as follows:\nβt = arg min β∈B {〈gt, β〉+ ηtBφ (β||βt−1)} . (4)\nThe parameter βt in (4) is the probability distribution pt (θ) of the policy parameter θ at update step t. Thus,\npt (θ) := βt (5)\nSubstituting the above equation into Eq. (4), we obtain the following:\npt (θ) = arg min p(θ)∈P {〈gt, p (θ)〉+ ηtBφ (p (θ) ||pt−1 (θ))} (6)\nwhere Bφ is the Bregman divergence, which has an arbitrarily smooth convex function φ and is defined as Bφ (x, x\n′) = φ (x)− φ (x′)− 〈∇φ (x′) , x− x′〉. The domain of the decision variable is found on the simplex P. We can select the Bregman divergence as the KL divergence φ (xt) = ∑N\nj=1 xt,j log (xt,j), but we can also use the Euclidean distances assumed on the simplex [10]. Moreover, we can select a different Bregman divergence, as discussed in [10, 11]. Note that gt in Eq. (6) is the gradient of the objective function ∇p(θ)J . We derive this as follows:\n∇p(θ)J =\n[\n∂J\n∂p (θ1) , · · · ,\n∂J\n∂p (θj) , · · · ,\n∂J\n∂p (θM )\n]\n= [J (θ1) , · · · , J (θj) , · · · , J (θM)]\n= J (θ) . (7)\nThat is, ∇p(θ)J is a value obtained without using derivatives of J . From the above, pt (θ) can be updated using Eq. (6) and learning can proceed.\nIn a typical reinforcement learning problem, we employ the expected cumulative reward— derived with Eq. (8)—as the objective function J (θ):\nJ (θj) =\n∫\nT\np ( τ θj ) r ( τ θj ) dτ (8)\nwhere the trajectory generated from the policy parameter θj is τ θj ∈ T , the generating probability of the trajectory τ θj is p ( τ θj ) , and the reward in the trajectory τ θj is r (\nτ θj i\n)\n.\nWe can approximate this using a Monte Carlo integral, as follows:\nJ (θj) ≃ 1\nN\nN ∑\ni=1\nr (\nτ θj i\n)\n(9)\n∝ N ∑\ni=1\nr (\nτ θj i\n)\n(10)\nIn order to solve this problem, we must generate trajectories of N kinds for M-type policy parameters, and make M ×N attempts for one update. Here, we use the concept of online learning.\nConsidering this as online learning, the gradient of the objective function is derived as follows:.\n∇p(θ)J ≃ [ r ( τ θ1i ) , · · · , r ( τ θj i ) , · · · , r ( τ θMi )] = r ( τθi )\n(11)\nwhere r ( τθi )\nis a vector of the cumulative reward before calculating the expected value. Thus, ∇p(θ)J ≃ r ( τθi )\ncan be used as the gradient of MD gt. Because this derived algorithm is a policy search based on MD, it is called MDS."
    }, {
      "heading" : "2.2. G-MDS",
      "text" : "For the experiment, we considered a case where the Bregman divergence Bφ in Eq. (6)\nis the KL divergence. That is, in Bφ, φ is φ (xt) = ∑N j=1 xt,j log (xt,j) ( x ∈ RN , xt,j > 0 )\n. Then, it can be rewritten as follows:\npt (θi) = exp (−ηtgt,i) pt−1 (θi)\n∑N j=1 exp (−ηtgt,j) pt−1 (θj) (12)\nIn this paper, we considered pt (θi) as the Gaussian distribution of the average µt−1 and the variance Σǫt−1,i, where θi is generated accordingly:\npt (θi) = N ( θ | µt−1,Σǫt−1,i )\n(13)\nHere, we consider the average µt of the Gaussian distribution. From Eq. (12), µt can be calculated as follows:\nµt = N ∑\ni=1\nθipt (θi) = Ept−1 [θi exp (−ηtgt,i)]\nEpt−1 [exp (−ηtgt,j)] (14)\nBy using the Monte Carlo integral for Eq. (14), the average value µt can be estimated as µ̂t when N is sufficiently large:\nµ̂t = 1 N ∑N i=1 θi exp (−ηtgt,i) 1 N ∑N j=1 exp (−ηtgt,j) (15)\nFurthermore, from Eq. (13), θi for ǫt,i ∼ N ( 0,Σǫt,i )\nduring the update step t can be expressed as follows:\nθi = µt−1 + ǫt−1,i (16)\nSubstituting this into Eq. (15), we have the following:\nµ̂t =\n∑N\ni=1 (µ̂t−1 + ǫt−1,i) exp (−ηtgt,i) ∑N\nj=1 exp (−ηtgt,j)\n= µ̂t−1 +\nN ∑\ni=1\n(\nexp (−ηtgt,i) ǫt−1,i ∑N\ni=j exp (−ηtgt,j)\n)\n. (17)\nBecause this derived algorithm is an instance of MDS that assumes that the policy follows a Gaussian distribution, it is called G-MDS."
    }, {
      "heading" : "2.3. Derivation of the proposed algorithm: AMDS and G-AMDS",
      "text" : ""
    }, {
      "heading" : "2.3.1. AMDS",
      "text" : "Next, the accelerated mirror descent (AMD) method [11] is applied to the proposed method. AMD is an accelerated method that generalizes Nesterov’s accelerated gradient such that it can be applied to MD. Here, Eq. (6) with AMD yields the following equations:\npt (θ) = λt−1p z̃ t−1 (θ) + (1− λt−1) p x̃ t−1 (θ) ,withλt−1 =\nr\nr + t (18)\npz̃t (θ) = arg min pz̃(θ)∈P\nts\nr\n{\n〈gt, p z̃ (θ)〉+Bφ\n( pz̃ (θ) ||pz̃t−1 (θ) )}\n(19)\npx̃t (θ) = arg min px̃(θ)∈P γs { 〈gt, p x̃ (θ)〉+R ( px̃ (θ) ||px̃t−1 (θ) )}\n(20)\nwhere R (x, x′) = Bω (x, x ′), which represents the Bregman divergence of the arbitrarily smooth convex function ω (x). We here explain the parameters for AMD. First, Eqs. (18)–(20) consist of parallel MDs. Therefore, it seems that λ defines the mixture ratio of Eqs. (20) and (19). In addition, λ is initially close to 1, such that AMD applies Eq. (20). As λ comes close to 0, AMD applies Eq. (19).\nFurthermore, consider Eq. (19), where ts r corresponds to a reciprocal learning rate. This increases as the number of updates increases. Therefore, it can be said that the learning rate decreases as the number of updates increases. This is equivalent to a simulated annealing operation. Moreover, the existing method [6, 7] includes simulated annealing heuristically, yet AMD can include it naturally.\nNext, we describe the implementation of AMDS in detail. AMDS proceeds as follows. By repeating the following series of flows, AMD can be treated as reinforcement learning:\n1. Sample from the continuous distribution pt (θ); 2. Calculate the discrete distribution px̃t−1 (θ) and p z̃ t−1 (θ) from the continuous distribu-\ntion pt (θ), using the obtained samples;\n3. Evaluate objective values for each obtained samples as inputs; 4. Calculate the discrete distribution px̃t (θ) and p z̃ t (θ) based on Eqs. (19) and (20); 5. Perform fitting for the discrete distributions px̃t (θ) and p z̃ t as the continuous distribu-\ntion (e.g., with a Gaussian distribution);\n6. Calculate the continuous distribution pt (θ) for the next sampling."
    }, {
      "heading" : "2.3.2. G-AMDS",
      "text" : "We derive the same procedure as G-MDS when using KL divergence. Let the Bregman divergence Bφ from Eq. (19) be the KL distance, and let R = Bω in Eq. (20) be ω (x) = ǫ ∑n\ni=1 (xi + ǫ) log (xi + ǫ) ( x ∈ RN , xt,j > 0 )\n. Accordingly, this method is referred to as GAMDS. Furthermore, the result cannot be calculated analytically. Indeed, it is known that an efficient and numerical calculation is available.\nFinally, we approximate the distributions px̃ (θ) and pz̃ (θ) with a Gaussian distribution."
    }, {
      "heading" : "3. Results",
      "text" : ""
    }, {
      "heading" : "3.1. 2DOF Via-point task",
      "text" : "We performed a 2DOF Via-point task to evaluate the proposed method. The agent is represented as a point on the x–y plane. This agent learns to pass through the point (0.5,\n0.2) at 250 ms. Before learning this, an initial trajectory from (0, 0) to (1, 1) is generated. The reward function is as follows:\nrt = 5000f 2 t + 0.5θ Tθ (21)\n∆r250ms = 10000000000 ( (0.5− x250ms) 2 + (0.2− y250ms) 2) (22)\nHere, DMP [12] is used for the parameterization of the policy, and the agent is seeking a policy for each x-axis and y-axis.\nThe parameter settings are as follows: 1000 updates, 15 rollouts, and 10 basis functions."
    }, {
      "heading" : "3.2. Experimental Results",
      "text" : "In this section, we describe the experimental results. We summarize the results for GMDS and G-AMDS in Fig. Figure 2. In the figure, the thin line represents a standard deviation of 1. Table Table 1 shows the average and the variance at convergence. The variance Σǫ for each search noise shall be 1.0.\nFrom the above, we confirm that G-AMDS learns at a faster rate than G-MDS. Therefore, it is effective to apply the proposed extension for MD to reinforcement learning."
    }, {
      "heading" : "4. Conclusions",
      "text" : "In this study, we proposed MDS. We explained the theoretical derivations of MDS, G-MDS, AMDS, and G-AMDS. According to the experimental results, learning progressed faster with the proposed G-AMDS. Moreover, based on the fact that AMD is a generalization of Nesterov’s acceleration method, we expect that the acceleration will be effective for an objective function with a saddle point."
    }, {
      "heading" : "Acknowledgment",
      "text" : "The research was supported by JSPS KAKENHI (Grant numbers JP26120005, JP16H03219, and JP17K12737).\nAppendix A. Relationship between G-MDS and PI2 algorithm\nWe here demonstrate that the algorithm is equivalent to PI2. The symbols are replaced as follows:\nPt−1,i = exp (−ηtgt,i)\n∑N j=1 exp (−ηtgt,j) (A.1)\nMt−1,i = 1 (A.2)\nThat is, Eq. (17) becomes similar to Eq. (A.3):\nµ̂t = µ̂t−1 + N ∑\ni=1\n(Pt−1,iMt−1,iǫt−1,i) (A.3)\nThe PI2 algorithm is equivalent to Eq. (A.3), provided that we employ DMP [12] as the policy function and ignore Mt−1,i. The algorithm here can be written such that it resembles that in Table Table A.2. There are some differences between PI2 and the algorithms obtained here, however. For instance, with PI2, the decision variable is updated sequentially using the provisional cumulative reward at each point during one trial, whereas the G-MDS uses only cumulative rewards. Moreover, G-MDS is an algorithm with fewer procedures. Finally, PI2 assumes a Gaussian distribution, whereas G-MDS can be generalized as MDS, which calculate similar algorithms using arbitrary probability distributions."
    } ],
    "references" : [ {
      "title" : "Policy improvement methods: Between black-box optimization and episodic reinforcement learning",
      "author" : [ "F. Stulp", "O. Sigaud" ],
      "venue" : "34 pages ",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Y",
      "author" : [ "J. Peters", "K. Mulling" ],
      "venue" : "Altun, Relative entropy policy search., in: AAAI, Atlanta",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Hierarchical relative entropy policy search",
      "author" : [ "C. Daniel", "G. Neumann", "J.R. Peters" ],
      "venue" : "in: International Conference on Artificial Intelligence and Statistics",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "ROCK*- Efficient black-box optimization for policy learning",
      "author" : [ "J. Hwangbo", "C. Gehring", "H. Sommer", "R. Siegwart", "J. Buchli" ],
      "venue" : "in: 2014 IEEE-RAS International Conference on Humanoid Robots, IEEE",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "A generalized path integral control approach to reinforcement learning",
      "author" : [ "E. Theodorou", "J. Buchli", "S. Schaal" ],
      "venue" : "Journal of Machine Learning Research 11 (Nov) ",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Reinforcement learning of motor skills in high dimensions: A path integral approach",
      "author" : [ "E. Theodorou", "J. Buchli", "S. Schaal" ],
      "venue" : "in: Robotics and Automation (ICRA), 2010 IEEE International Conference on, IEEE",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Completely derandomized self-adaptation in evolution strategies",
      "author" : [ "N. Hansen", "A. Ostermeier" ],
      "venue" : "Evolutionary computation 9 (2) ",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Efficient bregman projections onto the simplex",
      "author" : [ "W. Krichene", "S. Krichene", "A. Bayen" ],
      "venue" : "in: Decision and Control (CDC), 2015 IEEE 54th Annual Conference on, IEEE",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Accelerated mirror descent in continuous and discrete time",
      "author" : [ "W. Krichene", "A. Bayen", "P.L. Bartlett" ],
      "venue" : "in: Advances in Neural Information Processing Systems",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Learning attractor landscapes for learning motor primitives",
      "author" : [ "A.J. Ijspeert", "J. Nakanishi", "S. Schaal" ],
      "venue" : "Tech. rep. ",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2002
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Introduction In recent years, as stated in [1], attention has focused on the relationship between black box optimization and reinforcement learning.",
      "startOffset" : 43,
      "endOffset" : 46
    }, {
      "referenceID" : 2,
      "context" : "Episode-based REPS [4] is a derivation method formalized by considering the upper-level policy.",
      "startOffset" : 19,
      "endOffset" : 22
    }, {
      "referenceID" : 0,
      "context" : "In [1, 5], the authors focus on the relationship between reinforcement learning and black box optimization.",
      "startOffset" : 3,
      "endOffset" : 9
    }, {
      "referenceID" : 3,
      "context" : "In [1, 5], the authors focus on the relationship between reinforcement learning and black box optimization.",
      "startOffset" : 3,
      "endOffset" : 9
    }, {
      "referenceID" : 0,
      "context" : "Specifically, [1] explains the history of black box optimization and reinforcement learning, and proposes PI.",
      "startOffset" : 14,
      "endOffset" : 17
    }, {
      "referenceID" : 4,
      "context" : "PI refers to Policy Improvement with Path Integrals (PI) [6, 7].",
      "startOffset" : 57,
      "endOffset" : 63
    }, {
      "referenceID" : 5,
      "context" : "PI refers to Policy Improvement with Path Integrals (PI) [6, 7].",
      "startOffset" : 57,
      "endOffset" : 63
    }, {
      "referenceID" : 6,
      "context" : "It is considered a black box optimization method, where PI is derived on the basis of the Covariance Matrix Adaptation Evolutionary Strategy (CMA-ES) [8], a black box optimization algorithm.",
      "startOffset" : 150,
      "endOffset" : 153
    }, {
      "referenceID" : 7,
      "context" : "We can select the Bregman divergence as the KL divergence φ (xt) = ∑N j=1 xt,j log (xt,j), but we can also use the Euclidean distances assumed on the simplex [10].",
      "startOffset" : 158,
      "endOffset" : 162
    }, {
      "referenceID" : 7,
      "context" : "Moreover, we can select a different Bregman divergence, as discussed in [10, 11].",
      "startOffset" : 72,
      "endOffset" : 80
    }, {
      "referenceID" : 8,
      "context" : "Moreover, we can select a different Bregman divergence, as discussed in [10, 11].",
      "startOffset" : 72,
      "endOffset" : 80
    }, {
      "referenceID" : 8,
      "context" : "AMDS Next, the accelerated mirror descent (AMD) method [11] is applied to the proposed method.",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 4,
      "context" : "Moreover, the existing method [6, 7] includes simulated annealing heuristically, yet AMD can include it naturally.",
      "startOffset" : 30,
      "endOffset" : 36
    }, {
      "referenceID" : 5,
      "context" : "Moreover, the existing method [6, 7] includes simulated annealing heuristically, yet AMD can include it naturally.",
      "startOffset" : 30,
      "endOffset" : 36
    }, {
      "referenceID" : 9,
      "context" : "2− y250ms) 2) (22) Here, DMP [12] is used for the parameterization of the policy, and the agent is seeking a policy for each x-axis and y-axis.",
      "startOffset" : 29,
      "endOffset" : 33
    } ],
    "year" : 2017,
    "abstractText" : "In recent years, attention has been focused on the relationship between black box optimization and reinforcement learning. Black box optimization is a framework for the problem of finding the input that optimizes the output represented by an unknown function. Reinforcement learning, by contrast, is a framework for finding a policy to optimize the expected cumulative reward from trial and error. In this research, we propose a reinforcement learning algorithm based on the mirror descent method, which is general optimization algorithm. The proposed method is called Mirror Descent Search. The contribution of this research is roughly twofold. First, an extension method for mirror descent can be applied to reinforcement learning and such a method is here considered. Second, the relationship between existing reinforcement learning algorithms is clarified. Based on these, we propose Mirror Descent Search and derivative methods. The experimental results show that learning with the proposed method progresses faster.",
    "creator" : "LaTeX with hyperref package"
  }
}