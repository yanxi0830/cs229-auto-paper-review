{
  "name" : "1602.07570.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Bayesian Exploration: Incentivizing Exploration in Bayesian Games",
    "authors" : [ "Yishay Mansour", "Aleksandrs Slivkins", "Vasilis Syrgkanis", "Zhiwei Steven Wu" ],
    "emails" : [ "mansour@microsoft.com", "slivkins@microsoft.com", "vasy@microsoft.com", "wuzhiwei@cis.upenn.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n60 2.\n07 57\n0v 1\n[ cs\n.G T\n] 2\n4 Fe\nWe consider a ubiquitous scenario in the Internet economywhen individual decision-makers (henceforth, agents) both produce and consume information as they make strategic choices in an uncertain environment. This creates a three-way tradeoff between exploration (trying out insufficiently explored alternatives to help others in the future), exploitation (making optimal decisions given the information discovered by other agents), and incentives of the agents (who are myopically interested in exploitation, while preferring the others to explore). We posit a principal who controls the flow of information from agents that came before to the ones that arrive later, and strives to coordinate the agents towards a socially optimal balance between exploration and exploitation, not using any monetary transfers. The goal is to design a recommendation policy for the principal which respects agents’ incentives and minimizes a suitable notion of regret.\nWe extend prior work in this direction to allow the agents to interact with one another in a shared environment: at each time step, multiple agents arrive to play a Bayesian game, receive recommendations, choose their actions, receive their payoffs, and then leave the game forever. The agents now face two sources of uncertainty: the actions of the other agents and the parameters of the uncertain game environment.\nOur main contribution is to show that the principal can achieve constant regret when the utilities are deterministic (where the constant depends on the prior distribution, but not on the time horizon), and logarithmic regret when the utilities are stochastic. As a key technical tool, we introduce the concept of explorable actions, the actions which some incentive-compatible policy can recommend with non-zero probability. We show how the principal can identify (and explore) all explorable actions, and use the revealed information to perform optimally. In particular, our results significantly improve over the prior work on the special case of a single agent per round, which relies on assumptions to guarantee that all actions are explorable. Interestingly, we do not require the principal’s utility to be aligned with the cumulative utility of the agents; instead, the principal can optimize an arbitrary notion of per-round reward.\n∗Microsoft Research and Tel Aviv University. mansour@microsoft.com †Microsoft Research, New York, NY, USA. slivkins@microsoft.com ‡Microsoft Research, New York, NY, USA. vasy@microsoft.com §University of Pennsylvania, Philadelphia, PA, USA. wuzhiwei@cis.upenn.edu"
    }, {
      "heading" : "1 Introduction",
      "text" : "A common phenomenon of the Internet economy is that individual decision-makers (henceforth, agents) both produce and consume information as they make strategic decisions in an uncertain environment. Agents produce information through their selection of actions and the resulting outcomes.1 Agents consume information from other agents who made similar choices in the past, when and if such information is available, in order to optimize their utilities. The collection and dissemination of information relevant to agents’ decisions can be instrumented on a very large scale. Numerous online services do it to provide recommendations concerning various products, services and experiences: movies to watch, products to buy, restaurants to dine in, and so forth.\nThe main issue of interest for this work is that the agents tend to be myopic, optimizing their own immediate reward, whereas the society would benefit if they also explore new or insufficiently explored alternatives. While the tension between acquisition and usage of information is extremely well-studied (under the name of exploration-exploitation tradeoff ), a crucial new dimension here is the incentives of the agents: since the agents are self-interested, one cannot expect agents to explore only because they are asked to do so. This creates a new intriguing three-way tradeoff between exploration, exploitation and incentives. To study this problem, we introduce a principal, who abstracts the society or a recommendation system, and whose goal is optimize some social objective function such as the social welfare. The principal can control the flow of information from past experiences to the agents, but is not allowed to use monetary transfers. Absent any new information, an agent will only perform the a priori better action, resulting in no exploration. Full transparency — revealing to an agent all information currently known by the principal — is not a good solution, either, because then an agent would only exploit. The goal is to understand how the principal can induce sufficient exploration for achieving near-optimal outcomes.\nThe prior work on this problem (Kremer et al., 2014; Mansour et al., 2015) has a crucial limitation: given the principal’s recommendation, the utility of a given agent is assumed to be unaffected by the choices of other agents. This is often not the case in practice. We lift this restriction, and allow the agents to affect one another. Informally, we posit that the agents operate in a shared environment, and an agent’s decision can affect this environment for a limited period of time after the decision is made.\nMotivating example. Let us consider a motivating example based on traffic routing. Consider a GPS-based navigation application such asWaze that gives each user a recommended driving route based on the current traffic conditions in the road network (consuming information received from other drivers), and uses his GPS signal to monitor his progress and the traffic conditions along the route (producing information for future recommendations). A natural goal for the navigation system (the “principal” in this example) is to minimize the average delay experienced by the users.\nIf the application is used by only a few drivers, we can view each user in isolation. However, once the application serves a substantial fraction of drivers in the region, there is a new aspect: recommendations may impact the traffic conditions experienced by other drivers. For example, if the application suggests to all the users in a given region to move to a certain lightly loaded route, then when/if they all follow the recommendation, the route may become highly congested. The uncertainty on how congestion affects delays on various routes can be reduced via exploration.\n1This information can be collected explicitly, e.g., as reviews on products, or implicitly, e.g., by observing the routes chosen by a driver and the associated driving times via a GPS-enabled device.\nA natural simplified model for this scenario is that at each round, multiple drivers arrive and interact through a routing game on fixed road network. In this game, the agents simultaneously choose their routes, and the delay at each link is determined by the load: the number of routes that use this link. Typically one assumes a parameterized Bayesian model, where the delay at a particular link is a known function of the load and a vector of parameters. The parameter vector is initially not known, but comes from a known Bayesian prior. Thus, we have a Bayesian game among the drivers. Furthermore, there is a principal that recommends routes to drivers: before each round, it recommends a route to each driver that arrives in this round, and observes the associated delays. Since the drivers are not obliged to follow the recommended routes, the recommendations must be “compatible” with drivers’ incentives, in the sense that they must form a correlated equilibrium in the routing game. At each time-step, the principal has a two-pronged goal: to minimize the driving times (exploit) and also to obtain information about the unknown parameters (explore) for the sake of giving better recommendations in the future. The challenge is to find an optimal balance between exploration and exploitation under the constraint that even the exploration must be compatible with drivers’ incentives.\nOur contributions. We put forward a model, called Bayesian Exploration, which has the following ingredients. Initially, some realized state of nature θ0 is selected from a known prior distribution (and never changes). There are T rounds. In each round, a new set of n agents arrive and play a game (the same game in all rounds): each agent selects an action, and receives utility determined by the joint action and the state θ0. There is a single principal which, in each round, recommends an action to each participating agent, and observes the chosen actions and the resulting utilities. The recommendations must be Bayesian incentive-compatible (BIC), so that the agents are interested in following them. The principal has a reward function, also determined by the joint action and the state θ0. The goal of the principal is to maximize her cumulative reward over all rounds, in expectation over the prior (henceforth, expected reward).\nWe design policies for the principal that are near-optimal compared to the best-in-hindsight policy: a BIC policy which maximizes the expected reward for a particular problem instance. More formally, we strive to minimize a version of Bayesian regret: the difference between the expected reward of the best-in-hindsight policy and the reward of the principal. Following the literature on regret minimization, we are mainly interested in the asymptotic dependence of regret on the time horizon T . We consider two versions, depending on whether the utilities are deterministic or stochastic. For both versions, we achieve optimal asymptotic dependence on T : constant for deterministic utilities, and logarithmic for stochastic utilities (with a slightly altered definition of the best-in-hindsight policy). In fact, this dependence is known to be optimal even for policies not bound by BIC constraints. The asymptotic constants depend on the Bayesian prior, which is inevitable because of the BIC constraints. Our policies are computationally efficient: their running time per round is polynomial in the input size.\nThe major new contribution of our work over (Kremer et al., 2014; Mansour et al., 2015) is the introduction of multiple agents at each round, and considering an interaction through an arbitrary Bayesian game. This additional interaction allows us to abstract a new host of scenarios, and introduces a new level of complexity to the framework. In particular, the principal now has a variety of joint actions it can recommend, and in many cases it has to randomize its recommendation in order maintain incentive-compatibility. (Technically, in each round the principal needs to select a Bayes correlated equilibrium (Bergemann and Morris, 2013), whereas with a single agent in each round it suffices to select a single action, which is a much simpler object.)\nAnother important feature of our model is that the utilities of the principal may be unaligned with the (cumulative) utility of all agents. For example, the principal may be interested in some form of fairness, such as maximizing the minimum utility in each round. Such objective is very different from the sum of all utilities. At the extreme, the principal might want to minimize the cumulative utility. Interestingly, while the BIC constraint generally limits the principal’s ability to harm the agents, the principal may still be able to significantly lower the Bayesian-expected social welfare, compared to the worst Bayes-Nash equilibrium that would exist without the principal. (In fact, this effect can be achieved even in a single round, see Bradonjic et al. (2014).) This is in stark contrast to the case of single agent per round, where the Bayesian-expected reward of any BIC policy must be at least that of the a-priori best action.\nOur policy for stochastic utilities does not need to know the distribution of “noise” added to the utilities. This is somewhat surprising because, for example, one would need to know the noise distribution in order to do a Bayesian update on the noisy inputs. Such detail-free properties are deemed desirable in the economics literature.\nIdeas and techniques. One important technical contribution concerns the concept of the explorable joint actions: those that can be explored by some BIC policy in some round. In general, not all joint actions are explorable, and the set of explorable joint actions may depend on the state θ0. Thus, we are facing two challenges. First, we identify which joint actions are explorable (and explore them). This has to be done inductively, whereby the utilities revealed by exploring one joint action may enable the policy to explore some others, and so forth until no further progress can be achieved. The second challenge is to define optimal (randomized) recommendation after all explorable joint actions have been explored, and prove that it outperforms any other BIC policy. While the latter seems intuitive (and it is), the proof is far from simple.\nTo address these two challenges, we develop the theory of the single-round game in our setting. Termed the recommendation game, it is a generalization of the well-knownBayesian Persuasion game (Kamenica and Gentzkow, 2011) where the signal observed by the principal is distinct from, but correlated with, the unknown state of nature. (Here the principal’s signal captures the history observed in the previous rounds.) This generalization allows us to argue about comparative statics: what happens if the principal’s signal is modified to contain more information relevant to the state θ0 (and possibly less information irrelevant to θ0). We show that a more informative signal can only improve the optimal reward achievable by the principal, and can only increase the set of “explorable” joint actions (under a suitable definition of “explorable”). These results, intuitive but non-trivial to prove, may be of independent interest.\nMore generally, while our algorithms and analyses involvemany notions, steps and statements that seem intuitive, making these intuitions precise required a multi-layered framework to reason about Bayesian Exploration in precise terms. Building up this framework has been a major part of the overall effort.\nWith stochastic utilities, we have an additional obstacle: the deviations between the expected utilities and our estimates thereof, albeit small and low-probability, may distort agents’ incentives. Moreover, the number of different possible observations from the same joint action becomes exponential in the number of rounds, which blows up the running time if we use the techniques from the deterministic case without modifications. To address these obstacles, we carefully go back and forth between the noisy observations and the corresponding problem instance with deterministic utilities.\nWe would like to stress that our “algorithmic characterization” of all explorable joint actions is an important contribution even for the case of single agent per round. While (Kremer et al., 2014; Mansour et al., 2015) provide necessary and sufficient conditions when there are only two actions, for multiple actions Mansour et al. (2015) only give a sufficient condition which is far from necessary. Our work rectifies the situation and provides an explicit BIC policy to identify all explorable joint actions.\nAdditional motivating examples. The essential features of Bayesian Exploration — the threeway tradeoff between exploration, exploitation and incentives, and presence of multiple agents in a shared environment— can be found in a variety of scenarios, in addition to the routing scenario described above.\nThe first scenario concerns coordinating participants in a market. Consider sellers which are selling tickets for a particular sports event on an online platform such as StubHub. Implicitly, these sellers are involved in a game where they set the prices and the buyers can select which tickets to buy. A principal (the same platform or a third party) can suggest to the sellers how to set the prices so as to optimize the sellers’ combined revenue or other notion of social optimality. The principal learns about the demand of the buyers (through exploration) in order to help the sellers to set the right prices (exploitation). The price recommendations need to be incentivecompatible because the sellers need to be convinced to follow them. Similarly, one can imagine a principal that coordinates the buyers in the same type of setting, recommending bidding strategies that optimize some global notion of “happiness”.\nThe second scenario concerns optimizing a shared access to a computing resource. For a simple example, consider a university data center shared by the faculty members, where each user can specify the machine to run his jobs on. The principal can collect the information from multiple users (to learn their typical resource requirements), and recommend which machines they should connect to. As a non-critical component, such recommendation system would be easier to maintain compared to a scheduler which enforces the same resource allocation, e.g., it can be ignored when/if it malfunctions.\nThe third scenario addresses a congestion game in which the agents are incentivized towards homophily. Consider a population of individuals choosing which experience to attend in the near future (e.g., go to a movie, watch a video on Youtube, read a news article, or attend a sports event). Often people are interested not only in the inherent quality of the experience, but also in sharing it with a particular group. This group may be quite large, e.g., it could include many people that share the same demographic, political views, general tastes, etc. Thus, one could imagine a recommendation service that would coordinate people towards sharing the experiences they would like with the people that they would want to share them with. Such recommendation service would need to explore to learn people’s tastes, and be compatible with people’s incentives.\nOrganization of the paper. Model and results are described in Section 2. In Section 3, we solve a simplified version of the problem, and along the way develop some essential tools. Section 4 presents the main result for the deterministic utilities. Stochastic utilities are treated in Section 5. A useful fact about the single-round game (a major tool in the preceding sections) is summarized and proved in Section 6. Conclusions and open questions are in Section 7.\nRelated work. A version of Bayesian Exploration without strategic interactions between agents (i.e., with a single agent in each round) was introduced by Kremer et al. (2014). They focused on the special case of two actions, and provided an optimal policy for deterministic utilities,\nand a preliminary result for stochastic utilities. Mansour et al. (2015) obtained optimal regret for stochastic utilities and a constant number of actions, as well as a reduction from an arbitrary non-BIC policy to a BIC one. Bahar et al. (2015) enrich the model to allow agents to observe their “friends” in a social network (but restrict to deterministic utilities and two actions). Frazier et al. (2014) and Che and Hörner (2013) study related, but technically different problems: in the former, the principal can pay the agents, and the latter consider continuous information flow and a continuum of agents (and restrict to two actions and binary rewards).\nBayesian Exploration is closely related to three prominent subareas of theoretical economics andmachine learning: multi-armed bandits, design of information structures, and strategic learning in games. We briefly survey these connections below.\nMulti-armed bandits (Bubeck and Cesa-Bianchi, 2012) is a well-studiedmodel for explorationexploitation tradeoff. Absent the BIC constraint (and assuming the agents always follow the recommendations) Bayesian Exploration reduces to the multi-armed bandit problem with action set A and rewards equal to the principal’s utility.\nThe recommendation game (a single round of Bayesian Exploration) is a version of the Bayesian Persuasion game (Kamenica and Gentzkow, 2011) with multiple agents, where the signal observed by the principal is distinct from, but correlated with, the unknown “state”. Our analysis of this game contributes to the line of work on Bayesian Persuasion and, more generally, on the design of information structures, see (Bergemann and Morris, 2016; Taneva, 2016) for background and references, and Dughmi and Xu (2016) for a more algorithmic perspective. Our notion of BIC constitutes (an appropriate version of) Bayes Correlated Equilibrium (Bergemann and Morris, 2013).\nA version of our setting with long-lived agents and no principal to coordinate them has been studied in (Bolton and Harris, 1999; Keller et al., 2005) under the name strategic experimentation. A vast literature on learning in games (Fudenberg and Levine, 1998) posits reasonable learning dynamics for the agents, as a proxy for their strategic behavior, and studies convergence to a given solution concept (e.g., an equilibrium)."
    }, {
      "heading" : "2 Bayesian Exploration: our model and results",
      "text" : "Our model, called Bayesian Exploration, is a game between a principal and multiple agents. It consists of T rounds, where the time horizon T is common knowledge.\nThere is a global parameter θ0, called the (realized) state. It is drawn from a Bayesian prior distribution ψ over a finite state space Θ; it is chosen before the first round, and stays the same throughout the game. The state space Θ and the prior ψ are common knowledge, but the state itself is not revealed neither to the principal, nor to the agents. (Instead, the principal can learn it over time.) Elements of Θ will be called (feasible) states, to distinguish them from the realized state θ0.\nIn each round, there is a fresh set of agents, denoted [n] := {1 , . . . ,n}, playing a simultaneous game. Each agent i chooses an action ai from some finite set Ai of possible actions. A tuple a = (ai)i∈[n] is called a joint action. The set of all possible joint actions, termed the action set, is A = A1 × . . . × An. The utility of each agent i is determined by the joint action a chosen by the agents and the realized state θ0. More formally, it is given by a function ui : A×Θ → [0,1], called the utility function of agent i. The utility of the principal, a.k.a. reward, is also determined by the\npair (a,θ0), and given by the reward function f : A×Θ → [0,1]. The action set A and the functions (f ;u1 , . . . ,un) are the same for all rounds, and are common knowledge.\nIn each round t, the principal recommends a joint action to the agents. Specifically, the round proceeds as follows: a fresh set of n agents arrives; the principal recommends a joint action a ∈ A; each agent i only observes his recommended action ai ; the agents choose their actions in the simultaneous game, and the utilities are realized. The principal observes the chosen actions and the realized utilities. Agents know the t, but do not observe the previous rounds.\nThe principal commits to an algorithm π that proceeds in rounds, so that in each round it outputs a joint action, and then inputs the chosen actions and the realized utilities. This algorithm is called the iterative recommendation policy.\nWe will now define the incentive-compatibility constraint. Let πt be the joint action recommended by π in round t (as a random variable taking values in A), and let Et−1 be the event that the agents have followed principal’s recommendations up to (but not including) round t. Here and henceforth, we will use a standard game-theoretic notation: we will represent a joint action a ∈ A as a pair (ai ,a−i ), where i is a agent, ai ∈ A is its action, and a−i = (a1, . . . ,ai−1,ai+1, . . . ,an) is the joint action of all other agents; we will sometimes write (ai ,a−i ;θ) to denote the pair (a,θ), where θ ∈ Θ is a state. We will write A−i for the set of all possible a−i ’s. In particular, π t −i ∈ A−i denotes the joint action of all other agents chosen by policy π in round t.\nDefinition 2.1. An iterative recommendation policy π is Bayesian Incentive Compatible (BIC) if for all rounds t and agents i ∈ [n] we have\nE [ ui(ai , π t −i ; θ0)− ui(a ′ i , π t −i ; θ0) | π t i = ai , Et−1 ] ≥ 0, (1)\nwhere ai ,a ′ i ∈ Ai are any two distinct actions such that Pr[π t i = ai | Et−1] > 0. (The probabilities are over the realized state θ0 ∼ ψ and the internal randomization in π.)\nIn words, suppose agent i is recommended to play action ai in a given round t. Assume that all agents followed the recommendations in the previous rounds, and that all agents but i follow the recommendations in round t. Then agent i cannot improve his conditional expected utility (given the information available to this agent) by choosing a different action a′i .\nThe goal of the policy is to optimize the expected reward, as defined by the reward function f , subject to the BIC constraint. Throughout, we assume that the agents follow recommendations of a BIC iterative recommendation policy, so that the expected reward is well-defined.\nStochastic utilities. We allow amore general version, termed Bayesian Exploration with stochastic utilities, where given the joint action a ∈ A and state θ0, the vector of all realized utilities is drawn independently from some distribution D(a,θ0) over such vectors, so that the expected utility of each agent i and the principal are, respectively, ui(a,θ0) and f (a,θ0). All realized utilities lie in [0,1].2 The distributionsD(a,θ), (a,θ) ∈ A×Θ are known to the agents; however, for our results we can assume they are not known to the principal. The special case when the realized utilities are always equal to their expectations is termed Bayesian Exploration with deterministic utilities.\nDiscussion. One can consider a more general version in which the principal can send arbitrary messages to agents. However, the restriction of messages to recommended actions is without loss of generality (see the discussion in Section 3.1).\n2Our results extend to a more general case of sub-Gaussian noise; we omit the easy details.\nOne natural reward function f is social welfare, i.e., the sum of utilities of all agents in this round. However, we allow for an arbitrary notion of principal’s utility.\nUtility structure and time horizon. We define the utility structure U to be a tuple that consists of the action set A, the state space Θ, the prior ψ, the utility functions (u1 , . . . ,un), and the reward function f . Note that U completely determines the simultaneous game. A problem instance of Bayesian Exploration consists of U and time time horizon T (and the distributions D(a,θ), (a,θ) ∈ A×Θ for stochastic utilities).\nFor ease of presentation, we will assume that every BIC iterative recommendation policy π is well-defined (and BIC) for the infinite time horizon, i.e., for all rounds t ∈ N. This assumption is without loss of generality; e.g., one can extend π by setting πt = πT for all rounds t > T . The extended policy is BIC for the infinite time horizon as long as the original policy π is BIC for the original time horizon T .\nComputational model. Our results include bounds on the running time. These bounds assume infinite-precision arithmetic (infinite-precision arithmetic operations can be done in unit time), and continuous random seed (a number can be drawn independently and uniformly at random from interval (0,1) in unit time). Such assumptions are commonly used in theoretical computer science to simplify exposition.\nFrom the computational point of view, an iterative recommendation policy inputs the utility structure. We assume the utilities are represented generically, as a (n + 1) × |A| × |Θ| table. In particular, the input size is (more than) |A| · |Θ|. We obtain per-round running times that are polynomial in the input size."
    }, {
      "heading" : "2.1 Statement of the main results",
      "text" : "Given an iterative recommendation policy π, let REWt(π) = E[f (π t ,θ0)] denote the expected reward of π in a given round t. Here the expectation is taken over the realized state θ0 and the internal randomness in π. Let REW(π) = ∑T t=1REWt(π\nt) be the (cumulative) expected reward over all rounds. Given the classΠ of BIC iterative recommendation policies, our benchmarkwill be the optimal\nexpected reward achieved in any one round by any policy in Π:\nOPT(Π) = sup t∈N,π∈Π REWt(π). (2)\nNote that our benchmark does not have a built-in time horizon, and instead takes a sup over all rounds t. It follows that OPT(Π) is at least as large as the optimal time-averaged expected reward supπ∈Π REW(π)/T , andmay bemuch larger for some policy classes and/or some problem instances. However, the restriction to BIC policies is a real limitation, and is essential in our setting.\nDeterministic utilities. We compete with the class of all BIC iterative recommendation policies with infinite time horizon, henceforth denotedΠBIC. Our main result is a BIC policy whose timeaveraged expected reward is close to OPT(ΠBIC).\nTheorem 2.2. Consider Bayesian Exploration with deterministic utilities. There exists a BIC iterative recommendation policy π that satisfies\nREW(π) ≥ (T −C) OPT(ΠBIC), (3)\nwhere C is a constant that depends only on the utility structure, but not on the time horizon T . The per-round running time of π is polynomial in |A| · |Θ|.\nThus, we construct an efficiently computable policy π whose expected reward is close to optimal. Moreover, this policy achieves constant regret as a function of T . Note that a priori our benchmark could have been much larger than the optimal time-averaged expected reward of any BIC policy, but we show otherwise. The per-round running time is polynomial in the input size, assuming a generic representation of the input.\nStochastic utilities. Our results for stochastic utilities compete against a slightly restricted class of BIC policies. For a given parameter δ > 0, a policy is called δ-BIC if it satisfies a stronger version of Definition 2.1 in which right-hand side of (1) is δ rather than 0. The class of all such policies is denotedΠδ. We construct a BIC policy whose time-averaged expected reward is close to OPT(Πδ).\nTheorem 2.3. Consider Bayesian Exploration with stochastic utilities. For any given δ > 0, there exists a BIC iterative recommendation policy π such that\nREW(π) ≥ (T −Cδ logT ) OPT(Πδ),\nwhere Cδ is a constant that depends only on the utility structure and the parameter δ, but not on the time horizon T . The per-round running time of π is polynomial in |A| · |Θ|. Policy π does not input the parameterized utility distributions D(a,θ), (a,θ) ∈ A×Θ.\nIn fact, we prove a stronger version of Theorem 2.3, in which OPT(Πδ) is replaced with a similar benchmark for deterministic utilities. Given a problem instance with stochastic utilities, we consider the deterministic instance: a version of the original problem instance where all utilities are deterministically equal to the corresponding expected utilities in the original instance. For a policy classΠ, we define OPTdet(Π) as the value of OPT(Π) for the deterministic instance. We focus on the class of all iterative recommendation policies that are δ-BIC for the deterministic instance, denoted Πdetδ . The new (and stronger) benchmark is defined as OPT det(Πdetδ ).\nBayesian regret and optimality. The two theorems provide bounds on Bayesian regret, defined here as R(T ;Π) = T OPT(Π) − REW(π), where Π is the benchmark class of policies. Specifically: R(T ;ΠBIC) ≤ C for deterministic utilities, and R(T ;Πδ) ≤ Cδ log(T ) for stochastic utilities and any δ > 0. These regret bounds are optimal in terms of the asymptotic dependence on T . This dependence is optimal even for the version without the BIC constraint, i.e., for multi-armed bandits (MAB). To see this, consider the special case of a single agent per round. First, it is implicit in (Mansour et al., 2015) that the benchmark OPT(Πδ) is the expected reward of the best fixed action (the standard benchmark in MAB) as long as δ is sufficiently small and all actions are explorable. Second, constant regret is obviously the best possible for the deterministic version, and O(logT ) regret is optimal for any instance of MAB with stochastic rewards (Lai and Robbins, 1985). Third, it is implicit in (Kremer et al., 2014; Mansour et al., 2015) that upper bounds on Bayesian regret must depend on the prior."
    }, {
      "heading" : "3 Warm-up and tools",
      "text" : "As a warm-up, let us consider the version with deterministic utilities, and focus on a relatively simple scenario when a BIC iterative recommendation policy explores all joint actions, and then exploits (in the sense that wemake precise later). We show that this policy can achieve optimal perround performance once all joint actions are explored. Recall that our benchmark is OPT(ΠBIC), where ΠBIC is the class of all BIC iterative recommendation policies, and OPT(·) is defined in (2).\nLemma 3.1. Consider Bayesian Exploration with deterministic utilities. Let π be a BIC iterative recommendation policy that explores all joint actions by a fixed time Tπ ≤ T . Then there exists a BIC policy π′ which coincides with π before round Tπ, and achieves expected reward at least OPT(ΠBIC) in all subsequent rounds. Therefore, f (π′) ≥ (T −Tπ) OPT(ΠBIC).\nWhile very intuitive, this result is surprisingly technical to prove from scratch. Essentially, one needs to specify what “exploitation” means in this context, and argue that this notion of exploitation is BIC and can only benefit from having full information about the utility structure. Thus, we develop a framework to reason about this, which will be an essential toolbox throughout the paper. More specifically, we define and analyze a game which captures a single round of Bayesian Exploration, and formulate a framework to combine BIC “subroutines” into a BIC iterative recommendation policy. A (very simple) proof of Lemma 3.1 using these tools is in the very end of this section.\nWhile Lemma 3.1 relies on the ability to explore all joint actions, this ability is not guaranteed. This can be seen even in the special case of a single agent per round and only two actions. For this special case, Kremer et al. (2014) and Mansour et al. (2015) present necessary and sufficient conditions under which all actions are explorable, as well as simple examples when these conditions fail. Mansour et al. (2015) also provides sufficient conditions for the version with a single agent per round and an arbitrary number of actions."
    }, {
      "heading" : "3.1 The recommendation game: a single round of Bayesian Exploration",
      "text" : "We view a single round of Bayesian Exploration as a stand-alone game between the principal and the agents, termed the recommendation game. Here the principal observes an auxiliary “signal”, which represents the information received in the previous rounds (and possibly also the internal random seed). Then the principal recommends a joint action, and the agents choose their actions.\nFormally, the recommendation game is a version of the Bayesian Persuasion game (Kamenica and Gentzkow, 2011) with multiple agents. Unlike the original Bayesian Persuasion game, in our version the signal observed by the principal is distinct from (but correlated with) the unknown “state”.\nGame specification. For a problem instance of Bayesian Explorationwith a given utility structure, the corresponding recommendation game proceeds as follows:\n• the state θ0 is drawn from a Bayesian prior distribution ψ over Θ; • the principal observes a signal S , then recommends action ai ∈ Ai for each agent i; • the agents choose their actions in the simultaneous game; • the principal and the agents receive utilities according to the utility structure.\nSignal S is an arbitrary random variable with finite support X (the elements of X are called feasible signals). The signal can be correlated with the state: formally, S and θ0 are random variables on the same probability space. The signal structure associated with S is the tuple (Θ,X ,ψ∗), where ψ∗ is the joint distribution of (S,θ0).\nThe utility structure and the signal structure are common knowledge. The realized state θ0 is not revealed to the principal (other than through the signal S). Each agent i only observes his own recommendation ai ; it does not observe the state θ0, the signal S , or the other recommendations\nThe principal commits to a recommendation policy: a randomized mapping π : X → A, which takes as input a feasible signal and outputs an action for each agent. The corresponding incentivecompatibility constraint is defined as follows:\nDefinition 3.2. Given signal S , a recommendation policy π is Bayesian incentive compatible (BIC) if for each agent i ∈ [n] and any two distinct actions ai ,a ′ i ∈ Ai such that Pr[πi(S) = ai ] > 0 we have\nE [ ui (ai , π−i(S); θ0)− ui(a ′ i , π−i (S) ;θ0) | πi (S) = ai ] ≥ 0. (4)\nIn words, whenever agent i is recommended to play some action ai , he could not improve his expected utility (given the information available to this agent, and assuming that all other agents follow the recommendations) by choosing a different action a′i . We assume that the agents follow the recommendations of a BIC policy, so that the expected reward is well-defined.\nFor the recommendation game, the distinction between stochastic and deterministic utilities is unimportant (for statements that only involve expected utilities and rewards). In particular, a given recommendation policy is BIC for stochastic utilities if and only if it is BIC for the corresponding problem instance with deterministic utilities.\nAn important special case is the empty signal, defined as a signal which always takes the same value. Such signal will be denoted as S = ⊥.\nFor the computational results, we assume that the joint distribution of (S,θ0) is given explicitly, as a |X | × |Θ| table of probabilities.\nRelation to Bayesian Exploration. Consider an iterative recommendation policy π. W.l.o.g., the internal random seed ω of policy π is chosen once, before the first round, and persists throughout. Let Ht be the history up to round t: the chosen joint actions and the realized utilities over all past rounds. Then policy π can be represented as a sequence (π(t) : t ∈ N), where for each round t, π(t) is a randomized mapping from St = (ω,Ht) to joint actions (called the restriction of π to round t). Each round t can be seen as a recommendation game with signal St, and π\n(t) is a recommendation policy in this game. It is easy to see that π is BIC if and only if π(t) is BIC for all t.\nDiscussion. The notion of Bayesian Incentive Compatibility in the recommendation game is closely connected to the notion of Bayes Correlated Equilibrium (Bergemann and Morris, 2013): modulo the differences in terminology, the former is a special case in which the agents do not receive private signals. In particular, it follows that a BIC recommendation policy always exists.\nOne can consider a more general version of the recommendation game in which the principal can send arbitrary messages to agents. Then the principal commits to a messaging policy: a randomized mapping which inputs the signal S and outputs a message mi for each agent i. However, such messaging policies can without loss of generality be restricted to recommendation policies. More precisely, suppose a messaging policy τ induces a Bayes Nash Equilibrium ρ (which, in our notation, is a randomized mapping that inputs the joint message (m1 , . . . ,mn) and outputs a joint action a ∈ A). Then the composition ρ ⊕ τ is a randomized mapping that inputs signal S and outputs a joint action, i.e., a recommendation policy. According to Bergemann and Morris (2013), this composition is BIC.3 Thus, the principal can use ρ⊕ τ instead of τ."
    }, {
      "heading" : "3.2 Properties of the recommendation game",
      "text" : "Multiple signals. Throughout, we consider a fixed utility structure U , but allow the signal S to vary from one game instance to another. We will suppress U from our notation, but explicify the\n3This follows from the ”only if” direction of Theorem 1 in Bergemann and Morris (2013). In their notation, we use the special case where the information structure S is empty, and S∗ is induced by the messaging policy τ. Then the “decision rule” π in the theorem corresponds to the recommendation policy ρ⊕ τ.\ndependence on S . To study how the properties of a game depend on a particular signal S , we consider multiple signals such that the signals and the realized state θ0 are random variables in the same probability space; such signals will be called coupled. While each of these signals corresponds to a separate game instance (with a shared θ0), we will refer to all these game instances jointly as recommendation game with coupled signals.\nOptimality. Given a recommendation policy π for signal S , its expected reward is\nREW(π) = E [ f (π(S),θ0) ] = E (s,θ)∼ψ∗ [ f (π(s),θ) ] .\nThe expectation is over the joint distribution of signal S and realized state θ0, and the internal randomness in policy π. The optimal reward given signal S is defined as\nREW∗[S] = sup BIC recommendation policies π for S REW(π).\nThus, it is the largest expected reward achievable in a recommendation game with signal S .4 A BIC policy π in this game will be called optimal for S if REW(π) = REW∗[S].\nLP representation. We will represent the problem of finding an optimal recommendation policy π as a linear program (henceforth, LP). We represent π as a set of numbers xa,s = Pr[π(s) = a], for each joint action a ∈ A and each feasible signal s ∈ X . These numbers, termed the LP-representation of π, will be the decision variables in the LP. The linear program is as follows:\nmaximize ∑\na∈A, s∈X , θ∈Θ\nψ(θ) ·ψ∗(s | θ) · xa,s · f (a,θ)\nfor all i ∈ [n],ai ,a ′ i ∈ Ai ,\n∑\na−i∈A−i , s∈X , θ∈Θ\nψ(θ) ·ψ∗(s | θ) · ( ui (ai ,a−i ;θ)− ui(a ′ i ,a−i ;θ) ) · xa,s ≥ 0\nfor all s ∈ X ,a ∈ A xa,s ≥ 0 and ∑ a∈A xa,s = 1\nIn the above LP, ψ(θ) stands for Pr[θ0 = θ], and ψ∗(s | θ) stands for Pr[S = s | θ0 = θ]. The objective is REW(π), written in terms of the LP-representation, and the first constraint states that π is BIC.5 The feasible region of this LP is polytope in R|X |×|A|, which we denote by BIC[S].\nClaim 3.3. A policy π is BIC if and only if its LP-representation lies in BIC[S].\nCorollary 3.4. An optimal recommendation policy exists. Given the utility structure and the signal structure, an optimal recommendation policy and the optimal reward REW∗[S] can be computed in time polynomial in |A| · |X | · |Θ|. Further, a convex combination of BIC policies is also BIC.\n4We write REW∗[S] rather than REW∗(S) to emphasize that the optimal reward is a function of the random variable S , rather than a function of a particular realization of this random variable. We will use a similar notation function[S] for some other functions of signal S as well, e.g., for the polytope BIC[S].\n5For a particular agent i and actions ai ,a ′ i ∈ Ai , the BIC constraint in Definition 3.2 states that Pr[E] > 0 implies E[W | E] ≥ 0, where W = ui(ai ,π−i (S),θ0) − ui (a ′ i ,π−i (S),θ0) and E = {πi (S) = ai }. This is equivalent to E[1E ·W ] ≥ 0. And E[1E ·W ] is precisely the left-hand side of the first constraint in the LP.\nMonotonicity in information. We prove that REW∗[S] can only increase if signal S becomes more informative. To make this statement formal, we consider a recommendation game with coupled signals S,S ′. We give a definition that compares the two signals in terms of their “state-relevant” information content, and state the “monotonicity-in-information” lemma (proved in Appendix 6).\nDefinition 3.5. Consider a recommendation game with coupled signals S,S ′ , with resp. supports X ,X ′ . We say that signal S is at least as informative as S ′ if\nPr[θ0 = θ | S = s, S ′ = s′] = Pr[θ0 = θ | S = s] ∀s ∈ X , s ′ ∈ X ,θ ∈Θ.\nAn important special case is when S determines S ′ : S ′ = g(S) for some g : X → X ′.\nLemma 3.6 (monotonicity-in-information). In a recommendation game with coupled signals S,S ′ , if signal S is at least as informative as S ′ then REW∗[S] ≥ REW∗[S ′].\nA recommendation policy π for signal S is also well-defined for signal S ′ that determines S . Formally, π induces a recommendation policy π′ that inputs S ′, maps it to the corresponding value of S , and returns π(S). Note that π and π′ choose the same joint actions, and π is BIC given S if and only if π′ is BIC given S ′ . Henceforth we identify all such “induced” policies π′ with π.\nOptimality for limited exploration. Let us study a recommendation game with a signal that corresponds to exploring a given (possibly randomized) subset B ⊂ A of joint actions. Formally, the subset B ⊂ A will be a 2A-valued signal: a signal whose values are subsets of A; e.g., its realization may depend on the realized state θ0.\nLet us consider the signal which corresponds to exploring all joint actions in B. This signal consists of all relevant utilities, and also includes B itself:\nAllInfo(B) := ( B; (f (a,θ0);u1(a,θ0) , . . . ,un(a,θ0)) : a ∈ B) . (5)\nNote that it is a random variable, because it depends on random variables B and θ0. Despite a rather complicated definition, S = AllInfo(B) is just a signal in a recommendation game. In particular, we can consider an optimal recommendation policy given this signal. By Corollary 3.4, such policy exists, and its LP-representation (and its expected reward) can be computed in time polynomial in |Θ| · |A| · |support(S)|.\nWe prove that if a BIC iterative recommendation policy is restricted to joint actions in B, then its expected per-round reward cannot exceed REW∗[AllInfo(B)].\nClaim 3.7. Consider Bayesian Exploration with stochastic utilities. Let B ⊂ A be a 2A-valued signal (i.e., a randomized set of joint actions). Let π be a BIC iterative recommendation policy such that before a given round t it can only choose joint actions in B. Then REWt(π) ≤ REW ∗[AllInfo(B)].\nProof. Consider the recommendation game that corresponds to the t-th round of Bayesian Exploration. Recall that the signal in this game is St = (ω,Ht), where ω is the internal random seed of policy π, and Ht is the history before round t. Then π\n(t), the restriction of policy π to round t, is a BIC recommendation policy in this game.\nThe expected reward of policy π in round t is the same as the expected reward of the restricted policy π(t), i.e.: REWt(π) = REW(π (t)). By optimality of REW∗[St] we have REW(π (t)) ≤ REW∗[St]. Since policy π is restricted to actions in B, signal AllInfo(B) is at least as informative as signal St. So by Lemma 3.6 we have REW∗[St] ≤ REW ∗[AllInfo(B) ], completing the proof."
    }, {
      "heading" : "3.3 Subroutines and the proof of Lemma 3.1",
      "text" : "We design iterative recommendation policies in a modular way, via “subroutines” that comprise multiple rounds and accomplish a particular task. In particular, we need a formal framework to argue that our “subroutines” are BIC if considered separately, and jointly form a BIC policy. We model a “subroutine” as an iterative recommendation policy that inputs the history of the previous rounds and chooses its own duration.\nFormally, we consider a common generalization of Bayesian Exploration and the recommendation game: the state θ0 is drawn and the signal S is observed exactly as in the recommendation game, and then the game proceeds over multiple rounds, exactly as in Bayesian Exploration. Note that the recommendation game is simply a special case with time horizon T = 1.\nWe focus on a versionwhere the time horizon is infinite (and irrelevant). Instead, each iterative recommendation policy π runs for Tπ rounds, where the number Tπ, termed duration, is chosen by the policy rather than given exogenously. The duration is chosen before the signal is observed, and thus can only depend on the utility structure and the signal structure. (This restriction is crucial for proving Claim 3.8.) The notion of BIC carries over word-by-word from Definition 2.1. Iterative recommendation policies in this model will also be called subroutines.\nFrom the computational point of view, the input to a subroutine consists of the utility structure, the signal structure, and the realization of the signal. The output of a subroutine is the tuple (S,H), where H is the history: the chosen joint actions and the rewards/utilities for all rounds in the execution, or any function of (S,H).\nA recommendation game and aGeneral Bayesian Exploration game with the same utility/signal structure are called associated. As in Section 3.2, we fix the utility structure, but allow the signal S to vary from one subroutine to another. A subroutine π initialized with a particular feasible signal s will be denoted π(s).\nNow we can define the composition of subroutines, so that the composition of BIC subroutines is BIC. Consider two subroutines π,π′ with respective signals S,S ′ . Then the composition of π followed by π′, denoted π ⊕ π′, is a subroutine with signal S and duration Tπ + Tπ′ . The first Tπ rounds of π⊕π′ are controlled by π, and the subsequent Tπ′ rounds are controlled by π\n′. In order for the composition to be well-defined, the signal for π′ should be expressed in terms of the output of π. Formally, we say that π′ is a valid sequel for π if the pair (S,H) determines S ′ , where H is the history of π.\nClaim 3.8. Fix the utility structure. Consider BIC subroutines π,π′ such that π′ is a valid sequel for π. Then the composition π⊕π′ is BIC.\nA sequence of subroutines (π,π′ ,π′′ , . . .) is called valid if the every next subroutine is a valid sequel for the previous one. It is easy to see that the composition is associative: π ⊕ (π′ ⊕ π′′) = (π ⊕ π′) ⊕ π′′, so it is uniquely defined by the sequence, and can be denoted π ⊕ π′ ⊕ π′′. Thus, a BIC iterative recommendation policy can be presented as a composition of a valid sequence of subroutines, where the first subroutine inputs an empty signal, and all durations sum up to T .\nProof of Lemma 3.1. Let π be the policy from the lemma statement, and let σ be the subroutine of duration Tπ which coincides with π on the first Tπ rounds. (In other words, σ is a version of policy π that is “truncated” after round Tπ.) Let π\n∗ be an optimal recommendation policy for signal S = AllInfo(A). Note that REW∗[S] ≥ OPT(ΠBIC) by Claim 3.7. Let π\n′ be an iterative recommendation policy defined as the composition of σ followed by T − Tσ copies of π ∗. Note\nthat π′ is BIC by Claim 3.8, and receives expected reward REW∗[A] in each round after Tπ by construction."
    }, {
      "heading" : "4 Bayesian Exploration with deterministic utilities",
      "text" : "In this section, we focus on deterministic utilities, and prove Theorem 2.2. In particular, we will construct a BIC iterative recommendation policy π whose expected reward REW(π) satisfies (3).\nRather than comparing π directly to OPT(ΠBIC), we will use an intermediate benchmark of the form REW∗[AllInfo(Adetθ0 )], for some subset A det θ0 ⊂ A of joint actions that depends on the realized state θ0. While we usedA det θ0\n=A to prove Lemma 3.1, this is not a “fair” intermediate benchmark, because there may be some joint actions that cannot be explored by any BIC policy. Instead, we will only consider joint actions that can be explored.\nDefinition 4.1 (explorability). A joint action a ∈ A is called eventually-explorable given a state θ ∈Θ if Pr[πt = a | θ0 = θ] > 0 for some BIC iterative recommendation policy π and some round t ∈ N. The set of all such joint actions is denoted Adetθ .\nThus,Adetθ0 is the set of all joint actions that can be explored by some BIC policy in some round, given the realized state θ0. Note that it is a random variable whose realization is determined by θ0. Using REW\n∗[Adetθ0 ] as an intermediate benchmark suffices because, by Claim 3.7 we have\nREW∗[Adetθ0 ] ≥ OPT(ΠBIC). (6)\nWe construct a BIC iterative recommendation policy which explores all of Adetθ0 . Note that the existence of such policy does not immediately follow from Definition 4.1, because the definition only guarantees a BIC policy separately for each (θ,a) pair, whereas we need one BIC policy which “works” for the specific (and unknown) realized state θ0, and all “matching” joint actions a.\nDefinition 4.2. A subroutine is called maximally-exploring if it is a BIC subroutine that inputs an empty signal and explores all joint actions in Adetθ0 by the time it stops.\nA maximally-exploring subroutine can be followed by exploitation, using the corresponding optimal recommendation policy. The resulting BIC iterative recommendation policy achieves the regret bound claimed in Theorem 2.2.\nLemma 4.3. Let σ be a maximally-exploring subroutine of duration Tσ . Let π ∗ be an optimal recommendation policy for signal AllInfo(Adetθ0 ). Let π be the composition of subroutine σ followed by T −Tσ copies of π∗. Then π is a BIC iterative recommendation policy that satisfies\nREW(π) ≥ (T −Tσ ) OPT(ΠBIC). (7)\nThe lemma easily follows from the machinery developed in Section 3. Namely, we use the notion and existence of Adetθ0 -optimal policy (see Corollary 3.4), the “monotonicity-in-information” analysis which guarantees (6), and the “composition of subroutines” analysis which guarantees that π is BIC (via Claim 3.8).\nThe rest of this section is organized as follows. In Section 4.1 we develop the theory of “explorability” in a recommendation game. Then in Section 4.2 we use this theory to define a natural BIC subroutine and prove that this subroutine is maximally-exploring (and has the desired perround computation time). Finally, in Section 4.3 we put it all together to prove Theorem 2.2."
    }, {
      "heading" : "4.1 Explorability in the recommendation game",
      "text" : "In this subsectionwe investigate which joint actions can be explored in the recommendation game. We adopt a very permissive definition of “explorability”, study some of its properties, and design a subroutine which explores all such joint actions. Throughout, we consider a recommendation game with signal S whose support is X .\nDefinition 4.4. Consider a recommendation game with signal S . A joint action a is called signalexplorable, for a given feasible signal s ∈ X , if there exists a BIC recommendation policy πa,s such that Pr[πa,s(s) = a] > 0. The set of all such joint actions is denoted EXs[S]. The signal-explorable set is defined as EX[S] = EXS [S].\nNote that EXs[S] is a fixed subset of joint actions (determined by the feasible signal s ∈ X ), whereas EX[S] is a random variable (in fact, a 2A-valued signal) whose realization is determined by the realization of signal S .\nWe show that the signal-explorable set EX[S] can only increase if signal S becomes more informative. (The proof is deferred to Appendix 6.)\nLemma 4.5 (monotonicity-in-information for explorability). Consider a recommendation game with coupled signals S,S ′ . If signal S is at least as informative as S ′ then EX[S] ⊃ EX[S ′].\nNote that EX[S] and EX[S ′] are set-valued random variables, and the claim asserts that one random set is always contained in the other. (In general, if X and Y are random variables, we will write X ∈ Y and X ⊂ Y to mean that the corresponding event holds for all realizations of randomness.)\nA recommendation policy withmaximal support. Observe that policies πa,s in Definition 4.4 can be replaced with a single BIC recommendation policy πmax such that EXs[S] = support(π\nmax(s)) for each feasible signal s ∈ X . We will call such πmax a max-support policy for signal S . For example, we can set\nπmax = 1 |X | ∑ s∈X 1 |EXs[S]| ∑ a∈EXs[S] πa,s. (8)\nThis policy is BIC as a convex combination of BIC policies, and max-support by design. We compute a max-support policy as follows. For each joint action a ∈ A and each feasible signal s ∈ X , we solve the following LP:\nmax x∈BIC(S) ηa,s subject to xa,s ≥ ηa,s. (9)\nThis LP always has a solution, because BIC policies exist and any BIC policy gives a solution with objective value ηa,s = 0. Further, ηa,s > 0 if and only if a ∈ EXs[S], in which case the solution x is the LP-representation of a policy πa,s such that Pr[πa,s(s) = a] > 0. Then the max-support policy πmax is computed via (8). The computational procedure is given in Algorithm 1. Note that ComputeMaxSupport computes subsets As = EXs[S], and the output x is the LP-representation of the policy given by (8).\nThe “quality” of a max-support policy πmax is, for our purposes, expressed by the minimal probability of choosing a joint action in its support:\npmin(π max) := min s∈X , a∈support(πmax(s)) Pr[πmax(s) = a]. (10)\nAlgorithm 1 ComputeMaxSupport: computes a max-support policy.\nInput: the utility structure and the signal structure. Output: an LP-representation of a max-support policy.\nFor each joint action a ∈ A and feasible signal s ∈ X : solve the linear program (9); let xa,s be a solution with objective value ηa,s. Let As = {a ∈ A : ηa,s > 0} for each s ∈ X . Output x = 1\n|X |\n∑ s∈X\n1 |As | ∑ a∈As xa,s.\nWe relate this quantity to the min-max probability in Definition 4.4:\npmin[S] := min s∈X , a∈EXs[S] max x∈BIC[S] xa,s. (11)\nIt is easy to see that ComputeMaxSupport constructs policy πmax with pmin(π max) ≥ 1 |A|·|X | pmin[S]. To summarize, we have proved the following:\nClaim 4.6. There is a max-support policy πmax with pmin(π max) ≥ 1\n|A|·|X | pmin[S]. It can be computed\nby ComputeMaxSupport in time polynomial in |A|, |X |, and |Θ|.\nMaximal exploration given a signal. Let us design a subroutine σ which explores all signalexplorable joint actions. More specifically, given a recommendation game with signal S , we are looking for a subroutine σ in the associated Generalized Bayesian Exploration game which explores all joint actions in EX[S]. Such subroutine will be called maximally-exploring for signal S .\nWe start with a max-support policy πmax returned by ComputeMaxSupport. Let xmax be its LP-representation, so that xmaxa,s := Pr[π\nmax(S) = a]. Given a particular signal realization s ∈ X , we proceed as follows. We compute the signal-explorable set EXs[S] as the support of π\nmax(s). For each joint action a ∈ EXs[S], we choose the dedicated round τs(a) when this action is chosen. The dedicated rounds are chosen uniformly at random, in the sense that an injective function τs : EXs[S]→ [Tσ ] is chosen uniformly at random among all such functions (where Tσ is the duration of the subroutine). To guarantee that the subroutine is BIC, we ensure that\nPr[σ t(s) = a] = xmaxa,s for each round t and joint action a ∈ A. (12)\nTo this end, for each non-dedicated round the joint action is chosen independently from a fixed distribution Ds that is constructed to imply (12). Specifically, we write Ns = |EXs[S]| and define:\nDs(a) = xmaxa,s − 1/Tσ Tσ −Ns ∀a ∈ EXs[S]. (13)\nIt is easy to see that Ds is a distribution, provided that the duration is large enough. Specifically, it suffices to set Tσ =max(1 +Ns , ⌈1/pmin(π max)⌉). Then for each round t and action a ∈ EXs[S],\nPr[σ t(s) = a] = 1/Tσ +Ds(a) · (Tσ −Ns) = x max a,s ,\nwhere the 1/Tσ is the probability that round t is dedicated for action a. This completes the description of the subroutine, and the proof that it is BIC. The computational procedure for this subroutine is summarized in Algorithm 2.\nOur discussion of MaxExplore can be summarized as the following claim:\nAlgorithm 2 Subroutine MaxExplore: maximal exploration given signal S .\nInput: utility structure U , signal structure S , and signal realization s ∈ X .\n// compute the parameters x← ComputeMaxSupport(U ,S ) // LP-representation of a max-support policy πmax B← {a :∈ A : xa,s > 0} // signal-explorable set EXs[S] pmin ←min(xa,s′ : a ∈ B, s ′ ∈ X ) // computes pmin(π max) T ←max(1+ |B|, ⌈1/pmin⌉) // the duration of the subroutine Pick injective function τ : B→ [T ] u.a.r. from all such functions. D(a)← xa,s−1/T T−|B| ∀a ∈ B. // distribution (13) for non-dedicated rounds\n// issue the recommendations for rounds t = 1 . . .T do\nif t = τ(a) for some a ∈ B then at ← a else choose at from distribution D Recommend action at , observe the corresponding utilities\nOutput: signal AllInfo({a1 , . . . ,aT }).\nClaim 4.7. MaxExplore is a maximally-exploring subroutine for signal S . Its duration is at most |A| · |X | / pmin[S] rounds. The running time is polynomial in |A| · |X | in pre-processing, and linear in |A| per each round."
    }, {
      "heading" : "4.2 A maximally-exploring subroutine",
      "text" : "We come back to Bayesian Exploration, and strive to explore as many joint actions as possible. We define a natural BIC subroutine for this goal, and prove that it is indeed maximally-exploring.\nOur BIC subroutine is based on the following intuition. Initially, one can explore some joint actions via “maximal exploration” for an empty signal S1 = ⊥. This gives some additional observations, so one can now run ”maximal exploration” for a new signal S2 which comprises these new observations. This in turn provides some new observations, and so forth. We stop after |A| iterations, which (as we prove) suffices to guarantee that no further progress can be made.\nFormally, the subroutine proceeds in phases ℓ = 1,2,3 , . . . , |A|. In each phase, we start with the “current” signal Sℓ, and perform “maximal exploration” given this signal by calling MaxExplore. This call computes the “next” signal Sℓ+1 = AllInfo(EX[Sℓ]). After the last phase, we outputs the latest signal S|A|+1 which (as we prove) encompasses all observations received throughout the subroutine. Each call to MaxExplore must be parameterized with the signal structure for the corresponding signal Sℓ. Since the signal is uniquely determined by the realized state θ0, the signal structure is completely specified by the tuple (Sℓ(θ) : θ ∈Θ), where Sℓ(θ) is the value of the signal that corresponds to realized state θ0 = θ. The pseudocode is summarized in Algorithm 3.\nRepeatMaxExplore is BIC as a composition of BIC subroutines. It is easy to see, by induction on phase ℓ, that the realization of signal Sℓ is determined by the state θ0. In particular, it follows that the support of each signal Sℓ is of size at most |Θ|. Therefore, the per-round running time is polynomial in |A| · |Θ|, because so is the the running time of ComputeMaxSupport and the per-round running time of MaxExplore. Thus:\nClaim 4.8. RepeatMaxExplore is BIC; its per-round running time is polynomial in |A| · |Θ|.\nAlgorithm 3 Subroutine RepeatMaxExplore: maximal exploration.\nInput: the utility structure U .\nInitialize: S1 = S1 =⊥. // “phase-1 signal” is empty For each phase ℓ = 1,2 , . . . , |A|\nSℓ+1 ←MaxExplore(U ,Sℓ ,Sℓ) // compute signal Sℓ+1 = AllInfo(EX[Sℓ])\n// compute the signal structure Sℓ+1 for signal Sℓ+1 x ← ComputeMaxSupport(U ,Sℓ) // LP-representation of a max-support policy Sℓ+1 ← (Sℓ+1(θ) : θ ∈Θ),\nwhere Sℓ+1(θ)← AllInfo({a ∈ A : xa,Sℓ(θ) > 0})\nOutput: signal S|A|+1.\nLet Bℓ+1 be the set of all joint actions explored during phase ℓ. For the sake of the argument, let us define B1 = ∅, and extend the definition of sets Bℓ and signals Sℓ to phases ℓ = 1,2,3, . . . (i.e., without an upper bound on the number of phases). By construction, Bℓ+1 = EX[Sℓ] is a random variable whose realization is determined by the realized state θ0, and Sℓ = AllInfo(Bℓ).\nWe show that Bℓ+1 is the set of all joint actions explored during the first ℓ phases, and that stopping after |A| phases is without loss of generality:\nClaim 4.9. Sets (Bℓ : ℓ ∈ N) are non-decreasing in ℓ, and identical for ℓ ≥ |A|+1.\nProof. To prove that sets (Bℓ : ℓ ∈ N) are non-decreasing in ℓ, we use induction on ℓ and Lemma 4.5(a) on “monotonicity-in-information”. Now, a strictly increasing sequence of subsets of A, starting from an empty set, cannot have more than |A| + 1 elements. It follows that Bℓ = Bℓ+1 for some phase ℓ ≤ |A|+1. By definition of Bℓ, the sets (Bℓ′ : ℓ ′ ≥ ℓ) are identical.\nDepending on the state θ0, some of the later phases may be redundant in terms of exploration, in the sense that Bℓ = Bℓ+1 = . . . = B|A|+1 starting from some phase ℓ. Nevertheless, we use a fixed number of phases so as to ensure that the duration is fixed in advance. (This is required by the definition of a subroutine so as to ensure composability, as per Claim 3.8.)\nLemma 4.10. Subroutine RepeatMaxExplore is maximally-exploring.\nProof. We consider an arbitrary BIC iterative recommendation policy π, and prove that all joint actions explored by π are also explored by RepeatMaxExplore.\nDenote the internal random seed in π by ω. Without loss of generality, the ω is chosen once, before the first round, and persists throughout. Let At be the set of joint actions explored by π in the first t − 1 rounds. It is a random variable whose realization is determined by the random seed ω and realized state θ0. Let us represent policy π as a sequence of (π\n(t) : t ∈ [T ]), where each π(t) is a BIC recommendation policy which inputs signal S∗t which consists of the random seed and the observations so far: S∗t = (ω,AllInfo(At)), and is deterministic given that signal. In each round t, the recommended joint action is chosen by policy π(t); it is denoted π(t)(S∗t ).\nWe will prove the following claim using induction on round t:\nfor each round t, there exists phase ℓ ∈ N such that π(t)(S∗t ) ∈ Bℓ. (14)\nFor the induction base, consider round t = 1. Then A1 = ∅, so the signal is simply S ∗ 1 = (ω,⊥).\nNote that the empty signal ⊥ is at least as informative as the signal S∗1. Therefore:\nπ(1)(S∗1) ∈ EX[S ∗ 1] (by definition of EX[S ∗ 1])\n⊂ EX[⊥] (by Lemma 4.5)\n= B1 (by definition of B1).\nFor the induction step, assume that (14) holds for all rounds t < t0, for some t0. Then At0 ⊂ Bℓ for some phase ℓ, so signal Sℓ = AllInfo(Bℓ) for phase ℓ of RepeatMaxExplore is at least as informative as signal S∗t0 = (ω,AllInfo(At0)) for round t0 of policy π. Therefore:\nπ(t0)(S∗t0) ∈ EX[S ∗ t0 ] (by definition of EX[S∗t0])\n⊂ EX[Sℓ] (by Lemma 4.5)\n= Bℓ (by definition of Bℓ).\nThis completes the proof of (14). Therefore policy π can only explore joint actions in ∪ℓ∈NBℓ, and by Claim 4.9 this is just B|A+1|, the set of joint actions explored by RepeatMaxExplore."
    }, {
      "heading" : "4.3 Putting this all together: proof of Theorem 2.2",
      "text" : "We use the maximally-exploring subroutine RepeatMaxExplore from Section 4.2 (let T0 denote its duration), and an optimal recommendation policy for signal S = AllInfo(Adetθ0 ), denote it π\n∗. Let π be the composition of subroutine RepeatMaxExplore followed by T − T0 copies of π\n∗. By Lemma 4.3, this policy has the claimed reward guarantee.\nLet us argue about the computational implementation of π. For brevity, let us say polytimecomputable to mean “computable in time polynomial in |A| · |Θ|”. We need to prove that each round of π is polytime-computable. Each round of RepeatMaxExplore is polytime-computable by Claim 4.8, and each round of π∗ is polytime-computable by definition, so it remains to prove that a suitable π∗ is polytime-computable.\nBy Corollary 3.4, policy π∗ is poly-time computable given the signal structure for signal S (because the signal is determined by the realized state θ0, and therefore has support of size at most |Θ|). Recall that S = S|A+1| is the signal computed in the last phase of RepeatMaxExplore. So the corresponding signal structure is poly-time computable using a version of RepeatMaxExplore without the calls to MaxExplore."
    }, {
      "heading" : "5 Bayesian Exploration with stochastic utilities",
      "text" : "We turn our attention to stochastic utilities. As we pointed out in Section 2.1, we compete against δ-BIC iterative recommendation policies, for a given δ > 0.6 We construct a BIC policy whose time-averaged expected reward is close to OPT(Πδ), where Πδ is the class of all δ-BIC policies.\nIn fact, we achieve a stronger result. We will try to compete with a benchmark in the deterministic instance: a version of the original problem instance with deterministic utilities. Our benchmark is OPTdet(Πdetδ ), where Π det δ is the class of all policies that are δ-BIC for the deterministic instance, and OPTdet(·) is the value of OPT(·) for the deterministic instance. This is indeed a stronger benchmark:\n6Recall that δ-BIC policies satisfy a stronger version of Definition 2.1 in which right-hand side of (1) is δ.\nClaim 5.1. OPTdet(Πdetδ ) ≥ OPT(Πδ) for all δ > 0.\nOur main result for stochastic utilities (which implies Theorem 2.3) is stated as follows:\nTheorem 5.2. Consider Bayesian Exploration with stochastic utilities. Then, for any given parameter δ > 0, there exists an iterative recommendation policy π that is BIC and satisfies\nREW(π) ≥ (T −C · logT ) OPTdet(Πdetδ ).\nwhere C is a constant that depends only on the utility structure and the parameter δ, but not on the time horizon T . The per-round running time of π is polynomial in |A| · |Θ|. Moreover, the policy π does not input the parameterized utility distributions Da,θ , (a,θ) ∈ A×Θ.\nOur policy (and regret bounds) depend on an important parameter of the utility structure: the minimal amount of separation between the utilities for two states.\nDefinition 5.3 (separation parameter). The separation parameter is the smallest number ζ > 0 such that for any states θ,θ′ ∈Θ, any agent i ∈ [n], and any joint action a ∈ A we have\nui(a,θ) , ui(a,θ ′) ⇒ |ui(a,θ)− ui(a,θ ′)| ≥ ζ.\nThe main steps in our solution are as follows:\n1. We extend some basic tools and concepts of BIC recommendation policies to δ-BIC recommendation policies.\n2. We give a useful subroutine that approximates the expected utilities using multiple samples of the stochastic utilities. We show that any δ-BIC recommendation policy for the deterministic instance remains BIC when it inputs approximate utilities instead of the expected utilities, as long as the approximation is sufficiently good.\n3. We present a maximally-exploring subroutine, analogous to the one in Section 4, and use it to construct a BIC recommendation policy that achieves logarithmic regret compared to the class of all δ-BIC policies."
    }, {
      "heading" : "5.1 Preliminaries: from BIC to δ-BIC",
      "text" : "We extend some of the machinery developed in the previous sections to δ-BIC policies. In the interest of space, we sometimes refer to the said machinery rather than spell out the full details.\nWe start with the notions of “eventually-explorable joint action” and “maximally-exploring subroutine”. (Note that the former only considers the deterministic instance.)\nDefinition 5.4 (δ-explorability). Consider the deterministic instance and fix δ ≥ 0. A joint action a ∈ A is called δ-eventually-explorable given a state θ ∈ Θ if Pr[πt = a | θ] > 0 for some δ-BIC iterative recommendation policy π and some round t ∈ N. The set of all such joint actions is denotedAδθ .\nDefinition 5.5. A subroutine is called δ-maximally-exploring, for a given δ > 0, if it is a BIC subroutine that inputs an empty signal and explores all joint actions in Aδθ0 by the time it stops.\nLet us turn to the recommendation game.\nDefinition 5.6. Consider a recommendation game with signal S , and fix δ ≥ 0. Policy π is called δ-BIC if it satisfies a version of Definition 3.2 in which the right-hand side of (4) is δ rather than 0. Further:\n• A joint action a is called δ-signal-explorable, for a particular feasible signal s ∈ X , if there exists a δ-BIC recommendation policy πa,s such that Pr[πa,s(s) = a] > 0. The set of all such joint actions is denoted EXδs [S]. The δ-signal-explorable set is defined as EX δ S [S]. • A δ-BIC policy π is called δ-max-support if support(π(s)) = EXδs [S] for each signal s ∈ X .\nThe LP-representation of a δ-BIC recommendation policy satisfies a version of the LP from Section 3.2 where the first constraint expresses δ-BIC condition rather than BIC. Specifically, the right-hand side in the first constraint should be changed from 0 to δ · Pr[πi(S) = ai ]. In terms of the LP variables, the right-hand side becomes δ ·\n∑ a−i∈A−i , s∈X , θ∈Θ\nψ(θ) ·ψ∗(s | θ) · xa,s. The feasible region of the modified LP will be denoted BICδ[S].\nClaim 5.7. A recommendation policy π is δ-BIC if and only if its LP-representation lies in BICδ[S].\nAssuming there exists a δ-BIC policy, a max-support policy can be computed by a version of Algorithm 1, where in the linear program (9) the feasible set is BICδ[S] rather than BIC[S]. Let us call this modified algorithm ComputeMaxSupportδ. Thus:\nClaim 5.8. If there exists a δ-BIC policy, then there exists a δ-max-support policyπmax with pmin(π max) ≥\n1 |A|·|X | pδmin[S]. It can be computed by ComputeMaxSupport δ in time polynomial in |A|, |X |, and |Θ|.\nHere pδmin[S] is defined as the natural extension of pmin[S] (see (11)), whereby EXs[S] is re-\nplaced with EXδs [S], and BIC[S] is replaced with BICδ[S]."
    }, {
      "heading" : "5.2 Approximate deterministic utilities using stochastic utilities",
      "text" : "Let us discuss how to to approximate the expected utilities using multiple samples of stochastic utilities, and how to use the resulting approximate signal. In particular, we introduce an important tool (Lemma 5.10): a fact about approximate signals which we use throughout this section.\nNotation. Formulating this discussion in precise terms requires some notation. Fix subset B ⊂ A of joint actions. Given state θ, the expected utilities of joint actions in B form a table\nU(B,θ) := ((f (a,θ);u1(a,θ) , . . . ,un(a,θ)) : a ∈ B) .\nRecall that in Section 3.2, full information pertaining to the joint actions in B is expressed as a signal AllInfo(B) = (B, U(B,θ0)), as defined in (5).\nNow, let θ = θ0 be the realized state. Let an IID utility vector for a joint action a ∈ B be an independent sample from distribution D(a,θ). Recall from Section 2 that such sample is a random vector in [0,1]n+1 whose expectation is U(a,θ). Further, the d-sample for B, d ∈ N, is a collection UB = (v(a, j) : a ∈ B,j ∈ [d]), where each v(a, j) is an IID utility vector for the corresponding joint action a. We would like to use the sample UB to approximate U(B,θ).\nApproximation procedure. Our approximation procedure is fairly natural. We input a d-sample UB for some subset B ⊂ A. Then we compute average utilities across the d samples:\nU = ( 1 d ∑d j=1 v(a, j) : a ∈ B ) .\nThen we map these averages to the state θ which provides the best fit for the averages:\nθ̂ = argmin θ∈Θ\n∥∥∥U −U(B,θ) ∥∥∥ ∞ ,\nwhere the ties are broken uniformly at random (any fixed tie-breaking rule would suffice).\nWe will represent the output of this procedure as a signal DeNoise(UB) = ( B, U(B,θ̂) ) . Note that such signal has the same structure as the “correct” signal AllInfo(B), in the sense that DeNoise(UB) ∈ support(AllInfo(B)), so we can directly compare the two signals.\nProperties of the approximation. First, we observe that the approximate signal DeNoise(UB) is exactly equal to the correct signal AllInfo(B) with high probability, as long as the number of samples is large compared to log(n |B|) and the inverse of the separation parameter ζ.\nLemma 5.9. Fix subset B ⊆ A of joint actions. Let UB be a d-sample for B such that d is large enough, namely d ≥ ζ−2 · ln(2n |B|/β) for some β > 0, where ζ is the separation parameter. Then\nPr[AllInfo(B) = DeNoise(UB) | θ0 = θ] ≥ 1− β, ∀θ ∈Θ. (15)\n(This is an easy consequence of Chernoff-Hoeffding Bound, see Appendix A). The crucial point here is that DeNoise(UB) can be used instead of AllInfo(B) in the recommendation game. More specifically, if π be a δ-BIC recommendation policy for signal AllInfo(B), then it is also a BIC recommendation policy for signal DeNoise(B), as long as the approximation parameter β is sufficiently small compared to the minimal probability pmin(π).\nLemma 5.10. Consider the setting of Lemma 5.9. Let π be a δ-BIC recommendation policy for signal AllInfo(B). Then π is also a BIC recommendation policy for signal DeNoise(B), as long as β ≤ δ · pmin(π)/2|Θ|.\nWe derive Lemma 5.10 as a corollary of a general fact about approximate signals in a recommendation game, which we develop in the next subsection."
    }, {
      "heading" : "5.3 Approximate signals in a recommendation game",
      "text" : "We abstract (15) as a property of two coupled signals, and state the corresponding generalization of Lemma 5.10.\nDefinition 5.11. Consider a recommendation game with coupled signals S,S ′ , where the two signals have same support X . Signal S ′ is called a β-approximation for S , β > 0, if\nPr[S = S ′ | θ0 = θ] ≥ 1− β, ∀θ ∈Θ,\nwhere the randomness is taken over the realization of (S,S ′ ,θ0).\nLemma 5.12. Consider the setting of Definition 5.11. Let π be a δ-BIC recommendation policy for signal S . Then π is also a BIC recommendation policy for signal S ′ , as long as β ≤ δ · pmin(π)/2|X |.\nAs an intermediate step, we will show the following technical lemma.\nLemma 5.13. Consider the setting of Definition 5.11. Let g be a random variable in the same probability space as (S,S ′ ,θ0), with bounded range [0,H]. Then\n∣∣∣Pr[S = s] ·E [g | S = s] − Pr[S ′ = s] ·E[g | S ′ = s] ∣∣∣ ≤ βH, ∀s ∈ X . (16)\nProof. Let E denote the event of (S = S ′), which occurs with probability at least 1−β by definition. We will also write ¬E to denote the event of (S , S ′), and we know Pr[¬E] ≤ β. Observe that the event (S ′ = s)∧E is equivalent to the event (S = s)∧E . Also note that g ∈ [0,H], so for each s ∈ X , we could write\nPr[(S ′ = s)]E[g | (S ′ = s)] =Pr[(S ′ = s),E]E[g | (S ′ = s),E] + Pr[(S ′ = s),¬E]E[g | (S ′ = s),¬E]\n≤Pr[(S ′ = s),E]E[g | (S ′ = s),E] + Pr[¬E]E[g | (S ′ = s),¬E]\n=Pr[(S = s),E]E[g | (S = s),E] + βH\n≤Pr[(S = s),E]E[g | (S = s),E] + Pr[(S = s),¬E]E[g | (S = s),¬E] + βH\n=Pr[(S = s)]E[g | S = s] + βH\nSimilarly,\nPr[(S ′ = s)]E[g | (S ′ = s)] =Pr[(S ′ = s),E]E[g | (S ′ = s),E] + Pr[(S ′ = s),¬E]E[g | (S ′ = s),¬E]\n≥Pr[(S ′ = s),E]E[g | (S ′ = s),E]\n=Pr[(S = s),E]E[g | (S = s),E]\n≥Pr[(S = s),E]E[g | (S = s),E] + Pr[(S = s),¬E]E[g | (S = s),¬E]− βH\n≥Pr[S = s]E[g | S = s]− βH,\nwhich completes the proof.\nProof of Lemma 5.12. For any agent i ∈ [n] and any joint action a ∈ A, the utility ui(a,θ) is a random variable with bounded range [0,1]. In particular, we obtain (16) for random variable g = ui (a,θ0).\nLet x be the LP-representation of policy π from the lemma statement. Pick some action ai ∈ Ai such that Pr[πi(S) = ai ] > 0, and some other action a ′ i ∈ Ai \\ {ai }. Denote\nW (a−i ) = ui(ai ,a−i ;θ0)− ui(a ′ i ,a−i , ;θ0),\nwhere a−i ∈ A−i is a joint action of all agents but i. Then\n∑\na−i∈A−i , s∈X\nPr[S = s] ·E [W (a−i ) | S = s] xa,s ≥ δ · ∑\na−i∈A−i , s∈X\nPr[S = s] xa,s ≥ δ · pmin(π)\nTo show that policyπ is a BIC policy for signal S ′, it suffices to show thatE [W (π−i (S) | πi(S) = ai ] is non-negative, and we could write\n∑\na−i∈A−i ,s∈X\nPr[S ′ = s]E [ W (a−i ) | S ′ = s ] xa,s\n≥ ∑\na−i∈A−i ,s∈X\n(Pr[S = s]E [W (a−i ) | S = s]− 2β) xa,s ((16) with g = ui (a,θ0))\n= ∑\na−i∈A−i ,s∈X\nPr[S = s]E [W (a−i ) | S = s] xa,s − 2β |X |\n≥δpmin(π)− 2β |X | ≥ 0 (by our assumption of Lemma 5.10)\nTherefore, we know that π is BIC w.r.t. the signal S ."
    }, {
      "heading" : "5.4 A δ-maximally-exploring subroutine under stochastic utitlities",
      "text" : "In this subsection, we will give our maximal exploration algorithm (with access to stochastic utilities) for exploring all the joint actions explorable by any δ-strictly BIC policy (with access to deterministic utitlities). Our BIC subroutine is largely similar to the one in Section 4.2 except that our recommendation policy will only have access to approximate signals based on stochastic utilities.\nWe will first introduce a BIC subroutine MaxExploreδ that given an approximation signal Ŝ to the signal S , explores all the joint actions in the set A′ = EXδ[S]. In the process, the subroutine will collect multiple utility samples of each explored joint action, which will allow us to construct a new signal approximation for the signal AllInfo(A′).\nAlgorithm 4 Subroutine MaxExploreδ(U ,S , Ŝ ,β,β ′): maximal exploration given an approximate signal Ŝ .\nInput: the utility structure U , the signal structure S with associated signal S , Ŝ as a β-signal approximation to S with β being the input signal confidence parameter, and the confidence parameter for output signal β ′\n// compute the parameters let s ∈ X be the realization of signal Ŝ x← ComputeMaxSupportδ(U ,S ) // LP-representation of a maximal-support policy πmax B← {a :∈ A : xa,s > 0} // δ-strictly signal-explorable set EX δ s [S] pmin ←min(xa,s′ : a ∈ B, s ′ ∈ X ) // computes pmin(π max) T ←max(1+ |B|, ⌈1/pmin⌉) and R← 1 ζ2 ln ( 2n|B| β′ ) // number of rounds and meta-rounds Pick injective function τ : B→ [T ] u.a.r. from all such functions. // “dedicated rounds” D(a)← xa,s−1/T T−|B| ∀a ∈ B. // distribution (13) for non-dedicated rounds\nInitiate a set UB for storing stochastic utilities samples. // issue the recommendations formeta-rounds r = 1 . . .R do for rounds t = 1 . . .T do\nif t = τ(a) for some a ∈ B then at ← a else choose at from distribution D Recommend action at , store the corresponding utilities in UB\nS ′ = (B,Û )← DeNoise(B,UB,β ′), which is a β ′-signal approximation to AllInfo(S )\nOutput: the signal S ′\nWe can first show that the subroutine MaxExploreδ is BIC under some condition of the signal approximation parameter.\nLemma 5.14. The subroutine MaxExploreδ is BIC as long as the input signal approximation parameter of Ŝ satisfies β ≤ δ/(C|X |), where C is some prior-dependent constant.\nProof. Let π be the policy computed by ComputeMaxSupportδ(U ,S ) within our instantiation of MaxExploreδ. Then there exists a constantC such that pmin(π) ≥ 1/C. Note thatComputeMaxSupport δ is simply a composition ofR copies ofπ, so the stated result follows fromClaim 3.8 and Lemma 5.10.\nNote that if our algorithm has access to deterministic utilities, in the end it will be able to construct a signal AllInfo(B), where B is the (random) set of actions explored by the algorithm. We now show that even though our algorithm only has access to stochastic utilities, the output signal will be a β-signal approximation to AllInfo(B).\nLemma 5.15. The output signal S ′ byMaxExploreδ is a (β+β ′)-approximation to the signal AllInfo(EXδ[S]), where β and β ′ are the input and output signal signal approximation parameters respectively.\nProof. Fix any state θ as our true state θ0. First, we know that with probability at least 1 − β over the randomness of S and Ŝ , we have S = Ŝ . We will condition on this event, which is the case except with probability β. This means the subroutine will explore the same subset of joint actions: B = EXδS [S]. Then, the stated result simply follows from the accuracy guarantee of DeNoise (Lemma 5.9).\nWe now formally introduce our δ-maximally exploring subroutine. Similar to RepeatMaxExplore,\nit will proceed in phases ℓ = 1,2,3, . . . , |A|. In each phase ℓ, we will use the approximate signal S β ℓ to perform maximal exploration by instantiating the function MaxExploreδ. This in turn will allow us to construct an approximate signal for the next phase. See Algorithm 5 for details of the parameters.\nAlgorithm 5 Subroutine RepeatMaxExploreδ(U ,β): δ-maximal exploration.\nInput: the utility structure U and confidence parameter β Initialize: β ′ = β/ |A| and Ŝ1 = S1 = ⊥. // “phase-1 signal” is empty for each phase ℓ = 1,2 , . . . , |A| do\n// compute phase-(ℓ +1) approximate signal to Sℓ+1 = AllInfo(EX δ[Sℓ])\nŜℓ+1 = (Bℓ+1, Ûℓ+1)←MaxExplore δ[U ,Sℓ, Ŝℓ , (ℓ − 1)β ′,β ′] // compute signal structure Sℓ+1 ← (Sℓ+1(θ) : θ ∈Θ), where Sℓ+1(θ)← AllInfo(EX\nδ[Sℓ]) Output: final signal Ŝ|A|+1\nLemma 5.16. RepeatMaxExploreδ(U ,β) is BIC as long as the parameter satisfies β ≤ δ/(C|Θ|), where C is some constant depending on the prior and δ; its per-round running time is polynomial in |A| · |Θ|.\nProof. Note that RepeatMaxExploreδ could be viewed as a composition of subroutinesMaxExploreδ such that for each ℓ ∈ [|A|] the instantiation in phase (ℓ + 1) is a valid sequel of the one in phase ℓ. Also observe that the number of realizations for the signal structure Sℓ in each phase is bounded by the number of states |Θ|. Then by Lemma 5.14, we know that the instantiation of MaxExploreδ in each phase is BIC as long as β ≤ O(δ/ |Θ|), so RepeatMaxExploreδ(U ,β) is also BIC given the condition on β.\nFinally, we will show that RepeatMaxExploreδ is δ-maximally exploring and outputs a β approximate signal for AllInfo(Aδθ).\nLemma 5.17. The subroutine RepeatMaxExploreδ is δ-maximally-exploring with probability at least 1 − β over the randomness of the stochastic utilities, and outputs a signal Ŝ|A|+1 that is a β-signal approximation to AllInfo(Aδθ0).\nProof. Fix any game state θ as our true state θ0. Let π be any δ-BIC iterative recommendation policy with access to deterministic utilities. Our goal is to show that all actions explored by π are also explored by RepeatMaxExploreδ with probability at least 1− β.\nNote that in each phase ℓ of the RepeatMaxExploreδ, we know by Lemma 5.15 that except with probability at most β ′ over the randomness of the stochastic utilities, the output signal satisfies\nŜℓ+1 = AllInfo(EX δ[Sℓ])\nFor the remainder of the proof, we will condition on this event over all phases of RepeatMaxExploreδ, which by union bound occurs with probability at least 1− |A|β ′ = 1− β.\nDenote the internal random seed in π by ω. We will think of ω being drawn ahead of first round of our Bayesian exploration game and fixed throughout the game. Let At be the set of actions explored by π in the first (t − 1) rounds, which is determined by the realization of ω and the true state θ0 (since π sees deterministic utitlities). We will represent π as a sequence of( π(t) : t ∈ [T ] ) , where each π(t) is a BIC recommendation policy which inputs the signal S∗t which consists of the random seed and the observations so far: S∗t = (ω,AllInfo(At)), and is determined given that signal. In each round t, we will denote the recommended joint action by π(t)(S∗t ). We will prove the following claim using induction on round t:\nfor each round t, there exists phase ℓ ∈ N such that π(t)(S∗t ) ∈ Bℓ , (17)\nwhere Bℓ is the random set of joint actions explored by the algorithm RepeatMaxExplore δ in phase ℓ. For the base case (t = 1), we know that A1 = ∅, so the signal S ∗ 1 = (ω,⊥). Since the random seed is independent of the game state, we know that the empty signal (⊥) is at least as informative as the signal S∗1. This means\nπ(1)(S∗1) ∈ EX δ[S∗1] (by Definition 5.6)\n⊆ EXδ[⊥] (by Lemma 4.5)\n= B1 (by definition of B1)\nFor the induction step, assume that (17) holds for all rounds t < t0, for some t0 ∈ N. This impliesAt0 ⊆ Bℓ for some phase ℓ of RepeatMaxExplore\nδ, and so the signal Ŝℓ is at least as informative as the signal S∗t = (ω,AllInfo(At0)). It follows that\nπ(t0)(S∗t0) ∈ EX δ[S∗t0] (by Definition 5.6)\n⊆ EXδ[Ŝℓ] (by Lemma 4.5)\n= Bℓ (by definition of Bℓ)\nThis gives a proof for the claim in 17. By the same reasoning of Claim 4.9, the (Bℓ : ℓ ∈ N) are nondecreasing in ℓ, and identical for all ℓ ≥ |A|+1, so we have shown that all joint actions explored by π must be contained in B|A|+1. It follows that RepeatMaxExplore δ is δ-maximally exploring given access to stochastic utitlities, and Ŝℓ+1 = AllInfo(A δ θ0 )."
    }, {
      "heading" : "5.5 Putting this all together: proof of Theorem 2.3",
      "text" : "Lastly, we will show how the δ-maximal exploration subroutine will lead to a near-optimal iterative recommendation policy. As an intermediate step, we show that if an exploration subroutine outputs an approximate signal for AllInfo(Aδθ0), we can then obtain close to optimal rewards.\nLemma 5.18. Let δ,β ∈ (0,1/2) and σ be a δ-maximally exploring subroutine with duration Tσ that outputs a β-signal approximation Ŝ to the signal S∗ = AllInfo(Aδθ). Let π\n∗ be an optimal δ-BIC recommendation policy for signal S∗, and π be the composition of subroutine σ followed by T −Tσ copies of π∗(Ŝ). Then π is BIC as long as β ≤ δ/(2C|Θ|), and has reward\nREW(π) ≥ (T −Tσ )(1− β)OPT det(Πdetδ )\nwhere C is some prior-dependent constant.\nProof. First, we know that there exists some constantC > 0 such that pmin(π ∗) ≥ 1/C. Furthermore, we know that the number of realizations for the signal S∗ is at most the number of states |Θ|. It follows from Lemma 5.10 that π∗ ∈ BIC(Ŝ), and since σ is BIC, we also have that π is BIC by Claim 3.8.\nIn the following, we will write s∗ and ŝ to denote the realization of signals S∗ and Ŝ respectively, and use ψ∗ to denote the joint distribution over the two signals S\n∗, Ŝ and the state θ. By the definition of β-signal approximation in Definition 5.11, we have for each θ ∈Θ,\nψ∗(Ŝ = S ∗ | θ0 = θ) ≥ 1− β.\nFor t > Tσ , we can write the reward at round t as\nE π,ψ∗\n[f (πt(ŝ),θ0)] = ∑\nθ∈Θ\nψ(θ) E π,ψ∗ [f (πt(ŝ),θ) | θ0 = θ]\n= ∑\nθ∈Θ\nψ(θ) ( ψ∗(Ŝ = S\n∗ | θ0 = θ) E π,ψ∗\n[ f (πt(ŝ),θ) | θ0 = θ, Ŝ = S ∗ ]\n+ψ∗(Ŝ , S ∗ | θ) E\nπ,ψ∗\n[ f (πt(ŝ),θ) | θ0 = θ, Ŝ , S\n∗ ])\n≥ ∑\nθ∈Θ\nψ(θ)(1− β) E π,ψ∗\n[ f (πt(ŝ),θ) | θ0 = θ, Ŝ = S ∗ ]\n= (1− β) ∑\nθ∈Θ\nψ(θ) E π,ψ∗\n[ f (πt(s ∗),θ) | θ0 = θ, Ŝ = S ∗ ]\nNote that the random variable Eπ,ψ∗ [f (πt(s ∗),θ) | θ0 = θ] is independent of 1[Ŝ = S ∗] since the reward is fully determined by the state and the randomness of π. In particular, for each θ ∈Θ, we have\nE π,ψ∗\n[ f (πt(s ∗),θ) | θ0 = θ, Ŝ = S ∗ ] = E\nπ [f (πt(S\n∗(θ),θ))]\nIn other words,\nE π,ψ∗\n[f (πt(ŝ),θ)] ≥ (1− β) ∑\nθ∈Θ\nψ(θ)E π [f (πt(S\n∗(θ),θ))] = (1− β)REW∗[S∗]\nNote that we also have REW∗[S∗] ≥ OPTdet(Πdetδ ) by Claim 3.7. Therefore, in the last (T −Tσ) rounds, the expected reward of π is at least (T −Tσ )(1− β)OPT det(Πdetδ ).\nTo establish the result in Theorem 2.3, we will instantiate RepeatMaxExploreδ(U ,β) as our δmaximally exploring subroutine and works out the duration Tσ ’s dependence on the confidence parameter β.\nRecall that RepeatMaxExploreδ consists of a total number of |A| phases, and in each phase it instantiates the subroutine MaxExploreδ with output signal approximation parameter β ′ = β/ |A|, which takes ln(1/β)poly(|A|, |Θ|) rounds, where poly denotes some polynomial on two variables. It follows that RepeatMaxExploreδ(U ,β) also has duration at most ln(1/β)poly(|A|, |Θ|) number of rounds.\nTherefore, if we instantiate σ = RepeatMaxExploreδ with confidence parameter β = 1/T and followed by the optimal policy π∗ for the signal S∗ =Aδθ for the remaining T −Tσ rounds, the total reward of π satisfies\nREW(π) ≥ (T − ln(T )poly(|A|, |Θ|)) (1− 1/T )OPTdet(Πdetδ ) ≥ (T −C ln(T ))OPT det(Πdetδ )\nwhere C is some constant depending on the prior and game specification. This recovers our stated bound of Theorem 2.3."
    }, {
      "heading" : "6 Monotonicity in information: proof of Lemma 3.6 and Lemma 4.5",
      "text" : "In this section we consider a recommendation game with coupled signals, and prove the following lemma which unifies Lemma 3.6 and Lemma 4.5.\nLemma 6.1 (monotonicity-in-information). Consider a recommendation game with coupled signals S,S ′ such that S is at least as informative than S ′ . Then REW∗[S] ≥ REW∗[S ′] and EX[S ′] ⊂ EX[S].\nThroughout this section, signals S,S ′ are as in Lemma 6.1, and X ,X ′ are their respective supports. All expectations are over the random choice of (S,S ′ ,θ0), and also the internal randomness of recommendation policies (if applicable).\nWe will use the notion of “at least as informative” via the following corollary:\nClaim 6.2. For any function h :Θ → R and any feasible signal s ∈ S,s′ ∈ S ′\nE[h(θ0) | S = s] = E[h(θ0) | S = s,S ′ = s′].\nGiven a policy π′ for signal S ′, one can define the induced policy π for signal S by setting\nPr[π(s) = a] = Pr[π′(S ′) = a | S = s] ∀a ∈ A, s ∈ X .\nWe use Claim 6.2 to derive a more elaborate technical property: essentially, that policies π and π′ are equivalent when applied to any given function of h :A×Θ → R.\nClaim 6.3. Let π′ be an arbitrary recommendation policy for signal S ′, and let π be the induced policy for signal S . Then for any function h :A×Θ → R,\nE[h(π(S),θ0)] = E[h(π ′(S ′),θ0)]. (18)\nProof. Fix a feasible signal s ∈ X . Assume for now that policy π′ is deterministic. Then for any joint action a ∈ X we have\nPr[π(s) = a | S = s]\n= Pr[π(s) = a] (because randomization in π is independent) = Pr[π′(S ′) = a | S = s] (by definition of induced policy) = ∑\ns′∈X ′\nPr[π′(s′) = a] ·Pr[S ′ = s′ | S = s]\n= ∑\ns′∈X ′ : π′(s′ )=a\nPr[S ′ = s′ | S = s] (since π′ is deterministic). (19)\nTherefore,\nE[h(π(S),θ0) | S = s]\n= ∑\na∈A\nE[h(a,θ0) | S = s] ·Pr[π(s) = a | S = s]\n= ∑\na∈A, s′∈X ′ : π′(s′ )=a\nE[h(a,θ0) | S = s] ·Pr[S ′ = s′ | S = s] (by (19))\n= ∑\ns′∈X ′\nE[h(π′(s′),θ0) | S = s] ·Pr[S ′ = s′ | S = s]\n= ∑\ns′∈X ′\nE[h(π′(s′),θ0) | S = s, S = s ′] ·Pr[S ′ = s′ | S = s] (by Claim 6.2)\n= E[h(π′(s′),θ0) | S = s].\nTaking expectations over the realizations of signal S , we obtain the lemma (namely, (18)) for the special case when policy π′ is deterministic. For a randomized policy π′, we obtain the lemma by taking expectation over all possible realizations of π′.\nWe use Claim 6.3 in two ways: to argue about the rewards and to argue about BIC.\nCorollary 6.4. Let π′ be a policy for signal S ′, and let π be the induced policy for signal S . (a) REW(π) ≥ REW(π′). (b) If π′ is BIC, then the induced policy π is BIC, too.\nProof. For part (a), simply use Claim 6.3 with function h(a,θ) = f (a,θ). For part (b), fix agent i and any two actions ai , ãi ∈ Ai . Let us use the following shorthand:\n∆u(b,θ) := ui ((ai ,b−i ),θ)− ui ((ãi ,b−i ),θ) , b ∈ A, θ ∈Θ.\nUse Claim 6.3 with function\nh(b,θ) := 1{bi=ai } ·∆u(b,θ), b ∈ A, θ ∈Θ.\nIt follows that\nE [ 1{πi(S)=ai } ·∆u(π(S);θ0) ] = E [ 1{π′i (S ′)=ai } ·∆u(π ′(S ′);θ0) ] .\nConsequently, since Pr[πi(S) = ai ] = Pr[π ′ i(S ′) = ai], we have\nE [∆u(π(S);θ0) | πi(S) = ai ] = E [ ∆u(π′(S ′);θ0) | π ′ i(S ′) = ai ] ,\nwhenever Pr[πi(S) = ai ] > 0. The right-hand side of this equation is non-negative because policy π′ is BIC for signal S ′. Since this holds for any agent i and any two actions ai , ãi ∈ Ai , the induced policy π is BIC, too.\nProof of Lemma 6.1. To prove that REW∗[S] ≥ REW∗[S ′], let π′ be an optimal policy for signal S ′ , and π be the corresponding induced policy for signal S . Then by Corollary 6.4 it follows that policy π is BIC and has the same expected reward; therefore, REW∗[S] ≥ REW(π) = REW(π′) = REW∗[S ′].\nTo prove that EX[S ′] ⊂ EX[S], let π′ be a maximal-support policy for signal S ′, and π be the corresponding induced policy for signal S . Policy π is BIC by Corollary 6.4(b). Moreover, for all feasible signals s ∈ X , s′ ∈ X ′ such that Pr[S ′ = s′ | S = s] > 0 we have:\nEXs[S] = {a ∈ A : Pr[π(s) = a] > 0}\n= {a ∈ A : Pr[π′(S ′) = a | S = s] > 0} ⊃ {a ∈ A : Pr[π′(s′) = a] > 0} = EXs′ [S ′].\nIt follows that EX[S] ⊃ EX[S ′], as claimed."
    }, {
      "heading" : "7 Conclusions and open questions",
      "text" : "We introduce a model which captures incentivizing exploration in Bayesian games, and resolve the first-order issues in this model: explorability and constant/logarithmic regret. Our policies are computationally efficient: the per-round running time is polynomial in the input size under a generic game representation.\nOur results pave the way for futurework in several directions. Themost immediate direction is computational: can we achieve polynomial per-round running time if the game has a succinct representation? A simultaneous paper (Dughmi and Xu, 2016) studies a similar question for Bayesian Persuasion. In terms of statistical guarantees, one may want to improve the regret bounds, e.g., reduce the dependence of the asymptotic constants on the number of joint actions and parameters of the prior. In the economics direction, it is appealing to address agent heterogeneity by incorporating idiosyncratic signals that can be observed and/or elicited by the principal.\nAcknowledgements. The authors wish to thankDirk Bergemann, Yeon-Koo Che, ShaddinDughmi, Johannes Horner, Bobby Kleinberg, and Stephen Morris for stimulating discussions on Bayesian Exploration and related topics."
    }, {
      "heading" : "A Approximating the expected utilities: proof of Lemma 5.9",
      "text" : "We will use the Chernoff-Hoeffding Bound, a standard result on concentration of measure.\nLemma A.1 (Chernoff-Hoeffding Bound). Let X1, . . . ,Xn be i.i.d. random variables with EX[Xi] = µ and a ≤ Xi ≤ b for all i. Then for every δ > 0,\nPr [∣∣∣∣∣ ∑ iXi n −µ ∣∣∣∣∣ ≥ δ ] ≤ 2exp ( −2δ2n (b − a)2 ) .\nTo show that S ′ is a β-approximation for S , we will first show that for any fixed state θ, with probability at least 1−β over the realization of the stochastic utilities, the realizations of S ′ matches with S . In particular, we will show that the average utilities U = (ui(a))i∈[n],a∈A are close to the expected utilities ui(a,θ). Note that each realized utilities has bounded range u j i (a) ∈ [0,1], by Chernoff-Hoeffding bound and an application of union bound, we know with probability at least 1− β, the average utilities satisfy\nfor all i ∈ [n],a ∈ B, |ui (a)− ui(a)| ≤\n√ 1\n2d ln\n( 2|B|n\nβ\n)\nNote that for d ≥ 1 ζ2 ln ( 2|B|n β ) , then we will have |ui(a) − ui(a)| < ζ/2 for all i ∈ [n],a ∈ B with\nprobability at least 1 − β. We will condition on this event. This means ∥∥∥U −U(θ) ∥∥∥ ∞\n< ζ, so Û = U(θ) and therefore, S ′ = S ."
    } ],
    "references" : [ {
      "title" : "Incentivizing exploration",
      "author" : [ "Peter Frazier", "David Kempe", "Jon M. Kleinberg", "Robert Kleinberg" ],
      "venue" : null,
      "citeRegEx" : "Frazier et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Frazier et al\\.",
      "year" : 2016
    }, {
      "title" : "Strategic Experimentation with Exponential Ban",
      "author" : [ "Keller", "Sven Rady", "Martin Cripps" ],
      "venue" : null,
      "citeRegEx" : "Keller et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Keller et al\\.",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Frazier et al. (2014) and Che and Hörner (2013) study related, but technically different problems: in the former, the principal can pay the agents, and the latter consider continuous information flow and a continuum of agents (and restrict to two actions and binary rewards).",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 0,
      "context" : "Frazier et al. (2014) and Che and Hörner (2013) study related, but technically different problems: in the former, the principal can pay the agents, and the latter consider continuous information flow and a continuum of agents (and restrict to two actions and binary rewards).",
      "startOffset" : 0,
      "endOffset" : 48
    }, {
      "referenceID" : 0,
      "context" : "Frazier et al. (2014) and Che and Hörner (2013) study related, but technically different problems: in the former, the principal can pay the agents, and the latter consider continuous information flow and a continuum of agents (and restrict to two actions and binary rewards). Bayesian Exploration is closely related to three prominent subareas of theoretical economics andmachine learning: multi-armed bandits, design of information structures, and strategic learning in games. We briefly survey these connections below. Multi-armed bandits (Bubeck and Cesa-Bianchi, 2012) is a well-studiedmodel for explorationexploitation tradeoff. Absent the BIC constraint (and assuming the agents always follow the recommendations) Bayesian Exploration reduces to the multi-armed bandit problem with action set A and rewards equal to the principal’s utility. The recommendation game (a single round of Bayesian Exploration) is a version of the Bayesian Persuasion game (Kamenica and Gentzkow, 2011) with multiple agents, where the signal observed by the principal is distinct from, but correlated with, the unknown “state”. Our analysis of this game contributes to the line of work on Bayesian Persuasion and, more generally, on the design of information structures, see (Bergemann and Morris, 2016; Taneva, 2016) for background and references, and Dughmi and Xu (2016) for a more algorithmic perspective.",
      "startOffset" : 0,
      "endOffset" : 1358
    } ],
    "year" : 2017,
    "abstractText" : "We consider a ubiquitous scenario in the Internet economywhen individual decision-makers (henceforth, agents) both produce and consume information as they make strategic choices in an uncertain environment. This creates a three-way tradeoff between exploration (trying out insufficiently explored alternatives to help others in the future), exploitation (making optimal decisions given the information discovered by other agents), and incentives of the agents (who are myopically interested in exploitation, while preferring the others to explore). We posit a principal who controls the flow of information from agents that came before to the ones that arrive later, and strives to coordinate the agents towards a socially optimal balance between exploration and exploitation, not using any monetary transfers. The goal is to design a recommendation policy for the principal which respects agents’ incentives and minimizes a suitable notion of regret. We extend prior work in this direction to allow the agents to interact with one another in a shared environment: at each time step, multiple agents arrive to play a Bayesian game, receive recommendations, choose their actions, receive their payoffs, and then leave the game forever. The agents now face two sources of uncertainty: the actions of the other agents and the parameters of the uncertain game environment. Our main contribution is to show that the principal can achieve constant regret when the utilities are deterministic (where the constant depends on the prior distribution, but not on the time horizon), and logarithmic regret when the utilities are stochastic. As a key technical tool, we introduce the concept of explorable actions, the actions which some incentive-compatible policy can recommend with non-zero probability. We show how the principal can identify (and explore) all explorable actions, and use the revealed information to perform optimally. In particular, our results significantly improve over the prior work on the special case of a single agent per round, which relies on assumptions to guarantee that all actions are explorable. Interestingly, we do not require the principal’s utility to be aligned with the cumulative utility of the agents; instead, the principal can optimize an arbitrary notion of per-round reward. ∗Microsoft Research and Tel Aviv University. mansour@microsoft.com †Microsoft Research, New York, NY, USA. slivkins@microsoft.com ‡Microsoft Research, New York, NY, USA. vasy@microsoft.com §University of Pennsylvania, Philadelphia, PA, USA. wuzhiwei@cis.upenn.edu",
    "creator" : "LaTeX with hyperref package"
  }
}