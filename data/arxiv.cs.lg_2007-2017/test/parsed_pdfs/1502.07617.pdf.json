{
  "name" : "1502.07617.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Online Learning with Feedback Graphs: Beyond Bandits",
    "authors" : [ "Noga Alon", "Nicolò Cesa-Bianchi", "Ofer Dekel", "Tomer Koren" ],
    "emails" : [ "nogaa@post.tau.ac.il.", "bianchi@unimi.it.", "oferd@microsoft.com.", "tomerk@technion.ac.il." ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n50 2.\n07 61\n7v 1\n[ cs\n.L G\n] 2\n6 Fe\n∗Tel Aviv University, Tel Aviv, Israel, and Microsoft Research, Herzliya, Israel, nogaa@post.tau.ac.il. †Dipartimento di Informatica, Università degli Studi di Milano, Milan, Italy, nicolo.cesabianchi@unimi.it. Parts of this work were done while the author was at Microsoft Research, Redmond. ‡Microsoft Research, Redmond, Washington; oferd@microsoft.com. §Technion—Israel Institute of Technology, Haifa, Israel, and Microsoft Research, Herzliya, Israel, tomerk@technion.ac.il. Parts of this work were done while the author was at Microsoft Research, Redmond."
    }, {
      "heading" : "1 Introduction",
      "text" : "Online learning can be formulated as a repeated game between a randomized player and an arbitrary, possibly adversarial, environment (see, e.g., Cesa-Bianchi and Lugosi, 2006; Shalev-Shwartz, 2011). We focus on the version of the game where, on each round, the player chooses one ofK actions and incurs a corresponding loss. The loss associated with each action on each round is a number between 0 and 1, assigned in advance by the environment. The player’s performance is measured using the game-theoretic notion of regret, which is the difference between his cumulative loss and the cumulative loss of the best fixed action in hindsight. We say that the player is learning if his regret after T rounds is o(T ).\nAfter choosing an action, the player observes some feedback, which enables him to learn and improve his choices on subsequent rounds. A variety of different feedback models are discussed in online learning. The most common is full feedback, where the player gets to see the loss of all the actions at the end of each round. This feedback model is often called prediction with expert advice (Cesa-Bianchi et al., 1997; Littlestone and Warmuth, 1994; Vovk, 1990). For example, imagine a single-minded stock market investor who invests all of his wealth in one of K stocks on each day. At the end of the day, the investor incurs the loss associated with the stock he chose, but he also observes the loss of all the other stocks.\nAnother common feedback model is bandit feedback (Auer et al., 2002), where the player only observes the loss of the action that he chose. In this model, the player’s choices influence the feedback that he receives, so he has to balance an exploration-exploitation trade-off. On one hand, the player wants to exploit what he has learned from the previous rounds by choosing an action that is expected to have a small loss; on the other hand, he wants to explore by choosing an action that will give him the most informative feedback. The canonical example of online learning with bandit feedback is online advertising. Say that we operate an Internet website and we present one of K ads to each user that views the site. Our goal is to maximize the number of clicked ads and therefore we incur a unit loss whenever a user doesn’t click on an ad. We know whether or not the user clicked on the ad we presented, but we don’t know whether he would have clicked on any of the other ads.\nFull feedback and bandit feedback are special cases of a general framework introduced by Mannor and Shamir (2011), where the feedback model is specified by a feedback graph. A feedback graph is a directed graph whose nodes correspond to the player’s K actions. A directed edge from action i to action j (when i = j this edge is called a self-loop) indicates that whenever the player chooses action i he gets to observe the loss associated with action j. The full feedback model is obtained by setting the feedback graph to be the directed clique (including all self-loops, see Fig. 1a). The bandit feedback model is obtained by the graph that only includes the self-loops (see Fig. 1b). Feedback graphs can describe many other interesting online learning scenarios, as discussed below.\nOur main goal is to understand how the structure of the feedback graph controls the inherent difficulty of the induced online learning problem. While regret measures the performance of a specific player or algorithm, the inherent difficulty of the game itself is measured by the minimax regret, which is the regret incurred by an optimal player that plays against\nthe worst-case environment. Freund and Schapire (1997) proves that the minimax regret of the full feedback game is Θ( √ T lnK) while Auer et al. (2002) proves that the minimax re-\ngret of the bandit feedback game is Θ̃( √ KT ). Both of these settings correspond to feedback graphs where all of the vertices have self-loops —we say that the player in these settings is self-aware: he observes his own loss value on each round. The minimax regret rates induced by self-aware feedback graphs were extensively studied in Alon et al. (2014). In this paper, we focus on the intriguing situation that occurs when the feedback graph is missing some self-loops, namely, when the player does not always observe his own loss. He is still accountable for the loss on each round, but he does not always know how much loss he incurred. As revealed by our analysis, the absence of self-loops can have a significant impact on the minimax regret of the induced game.\nAn example of a concrete setting where the player is not always self-aware is the apple tasting problem (Helmbold et al., 2000). In this problem, the player examines a sequence of apples, some of which may be rotten. For each apple, he has two possible actions: he can either discard the apple (action 1) or he can ship the apple to the market (action 2). The player incurs a unit loss whenever he discards a good apple and whenever he sends a rotten apple to the market. However, the feedback is asymmetric: whenever the player chooses to discard an apple, he first tastes the apple and obtains full feedback; on the other hand, whenever he chooses to send the apple to the market, he doesn’t taste it and receives no feedback at all. The feedback graph that describes the apple tasting problem is shown in Fig. 1d. Another problem that is closely related to apple tasting is the revealing action or label efficient problem (Cesa-Bianchi and Lugosi, 2006, Example 6.4). In this problem, one action is a special action, called the revealing action, which incurs a constant unit loss. Whenever the player chooses the revealing action, he receives full feedback. Whenever the player chooses any other action, he observes no feedback at all (see Fig. 1e).\nYet another interesting example where the player is not self-aware is obtained by setting the feedback graph to be the loopless clique (the directed clique minus the self-loops, see Fig. 1c). This problem is the complement to the bandit problem: when the player chooses an action, he observes the loss of all the other actions, but he does not observe his own loss. To motivate this, imagine a police officer who wants to prevent crime. On each day, the officer chooses to stand in one of K possible locations. Criminals then show up at some of these locations: if a criminal sees the officer, he runs away before being noticed and the crime is prevented; otherwise, he goes ahead with the crime. The officer gets a unit reward for each crime he prevents,1 and at the end of each day he receives a report of all the crimes that occurred that day. By construction, the officer does not know if his presence prevented a planned crime, or if no crime was planned for that location. In other words, the officer observes everything but his own reward.\nOur main result is a full characterization of the minimax regret of online learning problems defined by feedback graphs. Specifically, we categorize the set of all feedback graphs into three distinct sets. The first is the set of strongly observable feedback graphs, which induce\n1It is easier to describe this example in terms of maximizing rewards, rather than minimizing losses. In our formulation of the problem, a reward of r is mathematically equivalent to a loss of 1− r.\nonline learning problems whose minimax regret is Θ̃(α1/2T 1/2), where α is the independence number of the feedback graph. This slow-growing minimax regret rate implies that the problems in this category are easy to learn. The set of strongly observable feedback graphs includes the set of self-aware graphs, so this result extends the characterization given in Alon et al. (2014). The second category is the set of weakly observable feedback graphs, which induce learning problems whose minimax regret is Θ̃(δ1/3T 2/3), where δ is a new graph-dependent quantity called the weak domination number of the feedback graph. The minimax regret of these problems grows at a faster rate of T 2/3 with the number of rounds, which implies that the induced problems are hard to learn. The third category is the set of unobservable graphs, which induce unlearnable Θ(T ) online problems.\nOur characterization bears some surprising implications. For example, the minimax regret for the loopless clique is the same, up to constant factors, as the Θ( √ T lnK) minimax regret for the full feedback graph. However, if we start with the full feedback graph (the directed clique with self-loops) and remove a self-loop and an incoming edge from any node (see Fig. 1f), we are left with a weakly observable feedback graph, and the minimax regret jumps to order T 2/3. Another interesting property of our characterization is how the two learnable categories of feedback graphs depend on completely different graph-theoretic quantities: the independence number α and the weak domination number δ.\nThe setting of online learning with feedback graphs is closely related to the more general setting of partial monitoring (see, e.g., Cesa-Bianchi and Lugosi, 2006, Section 6.4), where the player’s feedback is specified by a feedback matrix, rather than a feedback graph. Partial monitoring games have also been categorized into three classes: easy problems with T 1/2 regret, hard problems with T 2/3 regret, and unlearnable problems with linear regret (Bartók et al., 2014, Theorem 2). If the loss values are chosen from a finite set (say {0, 1}), then bandit feedback, apple tasting feedback, and the revealing action feedback models are all known to be special cases of partial monitoring. In fact, in Appendix D we show that any problem in our setting (with binary losses) can be reduced to the partial monitoring setting. Nevertheless, the characterization presented in this paper has several clear advantages over the more general characterization of partial monitoring games. First, our regret bounds are minimax optimal not only with respect to T , but also with respect to the other relevant problem parameters. Second, we obtain our upper bounds with a simple and efficient algorithm. Third, our characterization is stated in terms of simple and intuitive combinatorial properties of the problem.\nThe paper is organized as follows. In Section 2 we define the problem setting and state our main results. In Section 3 we describe our player algorithm and prove upper bounds on the minimax regret. In Section 4 we prove matching lower bounds on the minimax regret. Finally, in Section 5 we extend our analysis to the case where the feedback graph is neither fixed nor known in advance."
    }, {
      "heading" : "2 Problem Setting and Main Results",
      "text" : "Let G = (V,E) be a directed feedback graph over the set of actions V = {1, . . . , K}. For each i ∈ V , let N in(i) = {j ∈ V : (j, i) ∈ E} be the in-neighborhood of i in G, and let Nout(i) = {j ∈ V : (i, j) ∈ E} be the out-neighborhood of i in G. If i has a self-loop, that is (i, i) ∈ E, then i ∈ N in(i) and i ∈ Nout(i).\nBefore the game begins, the environment privately selects a sequence of loss functions ℓ1, ℓ2. . . . , where ℓt : V 7→ [0, 1] for each t ≥ 1. On each round t = 1, 2, . . . , the player randomly chooses an action It ∈ V and incurs the loss ℓt(It). At the end of round t, the player receives the feedback { ( j, ℓt(j) ) : j ∈ Nout(It)}. In words, the player observes the loss associated with each vertex in the out-neighborhood of the chosen action It. In particular, if It has no self-loop, then the player’s loss ℓt(It) remains unknown, and if the out-neighborhood of It is empty, then the player does not observe any feedback on that round. The player’s expected regret against a specific loss sequence ℓ1, . . . , ℓT is defined as E [∑T t=1 ℓt(It) ] − mini∈V ∑T t=1 ℓt(i). The inherent difficulty of the T -round online learning problem induced by the feedback graph G is measured by the minimax regret, denoted by R(G, T ) and defined as the minimum over all randomized player strategies, of the maximum over all loss sequences, of the player’s expected regret."
    }, {
      "heading" : "2.1 Main Results",
      "text" : "The main result of this paper is a complete characterization of the minimax regret when the feedback graph G is fixed and known to the player. Our characterization relies on various properties of G, which we define below.\nDefinition (Observability). In a directed graph G = (V,E) a vertex i ∈ V is observable if N in(i) 6= ∅. A vertex is strongly observable if either {i} ⊆ N in(i), or V \\ {i} ⊆ N in(i), or both. A vertex is weakly observable if it is observable but not strongly. A graph G is observable if all its vertices are observable and it is strongly observable if all its vertices are strongly observable. A graph is weakly observable if it is observable but not strongly.\nIn words, a vertex is observable if it has at least one incoming edge (possibly a selfloop), and it is strongly observable if it has either a self-loop or incoming edges from all other vertices. Note that a graph with all of the self-loops is necessarily strongly observable. However, a graph that is missing some of its self-loops may or may not be observable or strongly observable.\nDefinition (Weak Domination). In a directed graph G = (V,E) with a set of weakly observable vertices W ⊆ V , a weakly dominating set D ⊆ V is a set of vertices that dominates W . Namely, for any w ∈ W there exists d ∈ D such that w ∈ Nout(d). The weak domination number of G, denoted by δ(G), is the size of the smallest weakly dominating set.\nOur characterization also relies on a more standard graph-theoretic quantity. An independent set S ⊆ V is a set of vertices that are not connected by any edges. Namely, for any u, v ∈ S, u 6= v it holds that (u, v) 6∈ E. The independence number α(G) of G is the size of its largest independent set. Our characterization of the minimax regret rates is given by the following theorem.\nTheorem 1. Let G = (V,E) be a feedback graph with |V | ≥ 2, fixed and known in advance. Let α = α(G) denote its independence number and let δ = δ(G) denote its weak domination number. Then the minimax regret of the T -round online learning problem induced by G, where T ≥ |V |3, is (i) R(G, T ) = Θ̃(α1/2 T 1/2) if G is strongly observable; (ii) R(G, T ) = Θ̃(δ1/3 T 2/3) if G is weakly observable; (iii) R(G, T ) = Θ(T ) if G is not observable.\nAs mentioned above, this characterization has some interesting consequences. Any strongly observable graph can be turned into a weakly observable graph by removing at most two edges. Doing so will cause the minimax regret rate to jump from order √ T to order T 2/3. Even more remarkably, removing these edges will cause the minimax regret to switch from depending on the independence number to depending on the weak domination number. A striking example of this abrupt change is the loopy star graph, which is the union of the directed star (Fig. 1e) and all of the self-loops (Fig. 1b). In other words, this example is a multi-armed bandit problem with a revealing action. The independence number of this\nAlgorithm 1: Exp3.G: online learning with a feedback graph\nParameters: Feedback graph G = (V,E), learning rate η > 0, exploration set U ⊆ V , exploration rate γ ∈ [0, 1]\nLet u be the uniform distribution over U ; Initialize q1 to the uniform distribution over V ; For round t = 1, 2, . . .\nCompute pt = (1− γ)qt + γu; Draw It ∼ pt, play It and incur loss ℓt(It); Observe {(i, ℓt(i)) : i ∈ Nout(It)}; Update\n∀ i ∈ V ℓ̂t(i) = ℓt(i) Pt(i) I { i ∈ Nout(It) } , with Pt(i) = ∑\nj∈N in(i)\npt(j) ; (1)\n∀ i ∈ V qt+1(i) = qt(i) exp(−ηℓ̂t(i))∑\nj∈V qt(j) exp(−ηℓ̂t(j)) ; (2)\ngraph is K − 1, while its weak domination number is 1. Since the loopy star is strongly observable, it induces a game with minimax regret Θ̃( √ TK). However, removing a single loop from the feedback graph turns it into a weakly observable graph, and its minimax regret rate changes to Θ̃(T 2/3) (with no polynomial dependence on K)."
    }, {
      "heading" : "3 The Exp3.G Algorithm",
      "text" : "The upper bounds for weakly and strongly observable graphs in Theorem 1 are both achieved by an algorithm we introduce, called Exp3.G (see Algorithm 1), which is a variant of the Exp3-SET algorithm for undirected feedback graphs (Alon et al., 2013).\nSimilarly to Exp3 and Exp3.SET, our algorithm uses importance sampling to construct unbiased loss estimates with controlled variance. Indeed, notice that Pt(i) = P(i ∈ Nout(It)) is simply the probability of observing the loss ℓt(i) upon playing It ∼ pt. Hence, ℓ̂t(i) is an unbiased estimate of the true loss ℓt(i), and for all t and i ∈ V we have\nEt[ℓ̂t(i)] = ℓt(i) and Et[ℓ̂t(i) 2] =\nℓt(i) 2 Pt(i) . (3)\nThe purpose of the exploration distribution u is to control the variance of the loss estimates by providing a lower bound on Pt(i) for those i ∈ V in the support of u; this ingredient will turn out to be essential to our analysis.\nWe now state the upper bounds on the regret achieved by Algorithm 1.\nTheorem 2. Let G = (V,E) be a feedback graph with K = |V |, independence number α = α(G) and weakly dominating number δ = δ(G). Let D be a weakly dominating set such\nthat |D| = δ. The expected regret of Algorithm 1 on the online learning problem induced by G satisfies the following:\n(i) if G is strongly observable, then for U = V , γ = min {(\n1 αT )1/2 , 1 2 } and η = 2γ, the\nexpected regret against any loss sequence is O(α1/2T 1/2 ln(KT )); (ii) if G is weakly observable and T ≥ K3 ln(K)/δ2, then for U = D, γ = min {( δ lnK T )1/3 , 1 2 }\nand η = γ 2 δ , the expected regret against any loss sequence is O\n( (δ lnK)1/3T 2/3 ) .\nIn the previously studied self-aware case (i.e., strongly observable with self-loops), our result matches the bounds of Alon et al. (2014); Kocák et al. (2014). The tightness of our bounds in all cases is discussed in Section 4 below."
    }, {
      "heading" : "3.1 A Tight Bound for the Loopless Clique",
      "text" : "One of the simplest examples of a feedback graph that is not self-aware is the loopless clique (Fig. 1c). This graph is strongly observable with an independence number of 1, so Theorem 2 guarantees that the regret of Algorithm 1 in the induced game is O( √ T ln(KT )). However, in this case we can do better than Theorem 2 and prove (see Appendix C) that the regret of the same algorithm is actually O( √ T lnK), which is the same as the regret rate of the full feedback game (Fig. 1a). In other words, if we start with full feedback and then hide the player’s own loss, the regret rate remains the same (up to constants).\nTheorem 3. For any sequence of loss functions ℓ1, . . . , ℓT , where ℓt : V 7→ [0, 1], the regret of Algorithm 1, with the loopless clique feedback graph and with parameters η = √ (lnK)/(2T ) and γ = 2η, is upper-bounded by 5 √ T lnK."
    }, {
      "heading" : "3.2 Refined Second-order Bound for Hedge",
      "text" : "Our analysis of Exp3.G builds on a new second-order regret bound for the classic Hedge algorithm.2 Recall that Hedge (Freund and Schapire, 1997) operates in the full feedback setting (see Fig. 1a), where at time t the player has access to losses ℓs(i) for all s < t and i ∈ V . Hedge draws action It from the distribution pt defined by\n∀ i ∈ V , qt(i) = exp\n( − η∑t−1s=1 ℓs(i) ) ∑\nj∈V exp ( − η∑t−1s=1 ℓs(j) ) , (4)\nwhere η is a positive learning rate. The following novel regret bound is key to proving that our algorithm achieves tight bounds over the regret (to within logarithmic factors).\nLemma 4. Let q1, . . . , qT be the probability vectors defined by Eq. (4) for a sequence of loss functions ℓ1, . . . , ℓT such that ℓt(i) ≥ 0 for all t = 1, . . . , T and i ∈ V . For each t, let St be\n2A second-order regret bound controls the regret with an expression that depends on a quantity akin to the second moment of the losses.\na subset of V such that ℓt(i) ≤ 1/η for all i ∈ St. Then, for any i⋆ ∈ V it holds that T∑\nt=1\n∑\ni∈V\nqt(i)ℓt(i)− T∑\nt=1\nℓt(i ⋆) ≤ lnK\nη + η\nT∑\nt=1\n( ∑\ni∈St\nqt(i) ( 1− qt(i) ) ℓt(i) 2 + ∑\ni/∈St\nqt(i)ℓt(i) 2 ) .\nSee Appendix A for a proof of this result. The standard second-order regret bound of Hedge (see, e.g., Cesa-Bianchi et al., 2007) is obtained by setting St = ∅ for all t. Therefore, our bound features a slightly improved dependence (i.e., the 1 − qt(i) factors) on actions whose losses do not exceed 1/η. Indeed, in the analysis of Exp3.G, we apply the above lemma to the loss estimates ℓ̂t(i), and include in the sets St all strongly observable vertices i that do not have a self-loop. This allows us to gain a finer control on the variances ℓt(i) 2 / Pt(i) of such vertices."
    }, {
      "heading" : "3.3 Proof of Theorem 2",
      "text" : "We now turn to prove Theorem 2. For the proof, we need the following graph-theoretic result, which is a variant of Alon et al. (2014, Lemma 16); for completeness, we include a proof in Appendix A.\nLemma 5. Let G = (V,E) be a directed graph with |V | = K, in which each node i ∈ V is assigned a positive weight wi. Assume that ∑ i∈V wi ≤ 1, and that wi ≥ ǫ for all i ∈ V for some constant 0 < ǫ < 1 2 . Then\n∑\ni∈V\nwi wi + ∑ j∈N in(i) wj ≤ 4α ln 4K αǫ\nwhere α = α(G) is the independence number of G.\nProof of Theorem 2. Without loss of generality, we may assume that K ≥ 2. The proof proceeds by applying Lemma 4 and upper bounding the second-order terms it introduces. Indeed, since the distributions q1, q2, . . . generated by Algorithm 1 via Eq. (2) are of the form given by Eq. (4), with the losses ℓt replaced by the nonnegative loss estimates ℓ̂t, we may apply Lemma 4 to these distributions and loss estimates. The way we apply the lemma differs between the strongly observable and weakly observable cases, and we treat each separately.\nFirst, assume that G is strongly observable, implying that the exploration distribution u is uniform on V . Notice that for any i ∈ V without a self-loop, namely with i /∈ N in(i), we have j ∈ N in(i) for all j 6= i, and so Pt(i) = 1 − pt(i). On the other hand, by the definition of pt and since η = 2γ and K ≥ 2, we have pt(i) = (1− γ)qt(i) + γK ≤ 1− γ + γ 2 = 1− η, so that Pt(i) ≥ η. Thus, we can apply Lemma 4 with St = S = {i : i /∈ N in(i)} to the vectors ℓ̂1, . . . , ℓ̂T and take expectations, and obtain that\nE\n[ T∑\nt=1\n∑\ni∈V\nqt(i)Et[ℓ̂t(i)]− T∑\nt=1\nEt[ℓ̂t(i ⋆)] ] ≤ lnK\nη\n+ η\nT∑\nt=1\nE\n[ ∑\ni∈S\nqt(i)(1− qt(i))Et[ℓ̂t(i)2] + ∑\ni/∈S\nqt(i)Et[ℓ̂t(i) 2]\n]\nfor any fixed i⋆ ∈ V . Recalling Eq. (3) and Pt(i) = 1− pt(i) for all i ∈ S, we get\nE\n[ T∑\nt=1\n∑\ni∈V\nqt(i)ℓt(i)\n] − T∑\nt=1\nℓt(i ⋆) ≤ lnK\nη + η\nT∑\nt=1\nE\n[ ∑\ni∈S\nqt(i) 1− qt(i) 1− pt(i) + ∑\ni/∈S\nqt(i) Pt(i)\n] .\nThe sum over i ∈ S on the right-hand side is bounded as follows: T∑\nt=1\n∑\ni∈S\nqt(i) 1− qt(i) 1− pt(i) ≤ 2 T∑\nt=1\n∑\ni∈S\nqt(i) ≤ 2T .\nFor the second sum, recall that any i /∈ S has a self-loop in the feedback graph, and also that pt(i) ≥ γK as a result of mixing in the uniform distribution over V . Hence, we can use pt(i) ≥ (1− γ)qt(i) ≥ 12qt(i) and apply Lemma 5 with ǫ = γ K that yields\n∑\ni/∈S\nqt(i) Pt(i) ≤ 2\n∑\ni/∈S\npt(i) Pt(i) ≤ 8α ln K\n2\n4γ .\nPutting everything together, and using the fact that pt(i) ≤ qt(i) + γu(i) to obtain ∑\ni∈V\npt(i)ℓt(i) ≤ ∑\ni∈V\nqt(i)ℓt(i) + γ , (5)\nresults with the regret bound\nE\n[ T∑\nt=1\n∑\ni∈V\npt(i)ℓt(i)\n] − T∑\nt=1\nℓt(i ⋆) ≤ γT + lnK\nη + 2ηT\n( 1 + 4α ln K2\n4γ\n) .\nSubstituting the chosen values of η and γ gives the first claim of the theorem. Next, assume that G is only weakly observable. Let D ⊆ V be a weakly dominating set supporting the exploration distribution u, with |D| = δ. Similarly to the strongly observable case, we apply Lemma 4 to the vectors ℓ̂1, . . . , ℓ̂T , but in this case we set St = ∅ for all t. Using Eqs. (3) and (5) and proceeding exactly as the strongly observable case, we obtain\nE\n[ T∑\nt=1\n∑\ni∈V\npt(i)ℓt(i)\n] − T∑\nt=1\nℓt(i ⋆) ≤ γT + lnK\nη + η\nT∑\nt=1\nE\n[ ∑\ni∈V\nqt(i) Pt(i)\n]\nfor any fixed i⋆ ∈ V . In order to bound the expectation in the right-hand side, consider again the set S = {i : i /∈ N in(i)} of vertices without a self-loop, and observe that Pt(i) =∑\nj∈N in(i)pt(j) ≥ γδ for all i ∈ S. Indeed, if i is weakly observable then there exists some k ∈ D such that k ∈ N in(i) and pt(k) ≥ γδ because the exploration distribution u is uniform over D; if i is strongly observable then the same holds since i does not have a self-loop and thus must be dominated by all other vertices in the graph. Hence,\n∑\ni∈V\nqt(i) Pt(i) =\n∑\ni∈S\nqt(i) Pt(i) + ∑\ni/∈S\nqt(i) Pt(i) ≤ δ γ + 2K ,\nwhere we used Pt(i) ≥ pt(i) ≥ (1−γ)qt(i) ≥ 12qt(i) to bound the sum over the vertices having a self-loop. Therefore, we may write\nE\n[ T∑\nt=1\n∑\ni∈V\npt(i)ℓt(i)\n] − T∑\nt=1\nℓt(i ⋆) ≤ γT + lnK\nη +\nηδ\nγ T + 2ηKT .\nSubstituting our choices of η and γ, we obtain the second claim of the theorem."
    }, {
      "heading" : "4 Lower Bounds",
      "text" : "In this section we prove lower bounds on the minimax regret for non-observable and weakly observable graphs. Together with Theorem 2 and the known lower bound of Ω (√ α(G)T ) for strongly observable graphs (Alon et al., 2014, Theorem 5),3 these results complete the proof of Theorem 1. We remark that their lower bound applies when T ≥ α(G)3, which includes our regime of interest. We begin with a simple lower bound for non-observable feedback graphs.\nTheorem 6. If G = (V,E) is not observable and |V | ≥ 2, then for any player algorithm there exists a sequence of loss functions ℓ1, ℓ2, . . . : V 7→ [0, 1] such that the player’s expected regret is at least 1\n4 T .\nThe proof is straightforward: if G is not observable, then it is possible to find a vertex of G with no incoming edges; the environment can then set the loss of this vertex to be either 0 or 1 on all rounds of the game, and the player has no way of knowing which is the case. For the formal proof, refer to Appendix B. Next, we prove a lower bound for weakly observable feedback graphs.\nTheorem 7. If G = (V,E) is weakly observable with K = |V | ≥ 2 and weak domination number δ = δ(G), then for any randomized player algorithm and for any time horizon T there exists a sequence of loss functions ℓ1, . . . , ℓT : V 7→ [0, 1] such that the player’s expected regret is at least 1\n150\n( δ/ ln2K )1/3 T 2/3.\nThe proof relies on the following graph-theoretic result, relating the notions of domination and independence in directed graphs.\nLemma 8. Let G = (V,E) be a directed graph over |V | = n vertices, and let W ⊆ V be a set of vertices whose minimal dominating set is of size k. Then, W contains an independent set U of size at least 1\n50 k/ lnn, with the property that any vertex of G dominates at most lnn\nvertices of U .\n3While Alon et al. (2014) only consider the special case of graphs that have self-loops at all vertices, their lower bound applies to any strongly observable graph: we can simply add any missing self-loops to the graph, without changing its independence number α. The resulting learning problem, whose minimax regret is Ω (√ αT ) , is only easier for the player who may ignore the additional feedback.\nProof. If k < 50 lnn the statement is vacuous; hence, in what follows we assume k ≥ 50 lnn. Let β = (2 lnn)/k < 1. Our first step is to prove thatW contains a non-empty setR such that each vertex of G dominates at most β fraction of R, namely such that |Nout(v) ∩R| ≤ β|R| for all v ∈ V . To prove this, consider the following iterative process: initialize R = W , and as long as there exists a vertex v ∈ V such that |Nout(v)∩R| > β|R|, remove all the vertices v dominates from R. Notice that the process cannot continue for k (or more) iterations, since each step the size of R decreases at least by a factor of 1− β, so after k − 1 steps we have |R| ≤ n(1 − β)k−1 < ne−βk/2 = 1. On the other hand, the process cannot end with R = ∅, as in that case the vertices v found along the way form a dominating set of W whose size is less than k, which is a contradiction to our assumption. Hence, the set R at the end of process must be non-empty and satisfy |Nout(v) ∩ R| ≤ β|R| for all v ∈ V , as claimed.\nNext, consider a random set S ⊆ R formed by picking a multiset S̃ of m = ⌊ 1 10β ⌋ elements from R independently and uniformly at random (with replacement), and discarding any repeating elements. Notice that m ≤ 1\n10 |R|, as |R| ≥ 1 β |Nout(v) ∩ R| for any v ∈ V , and for\nsome v the right-hand side is non-zero. The proof proceeds via the probabilistic method: we will show that with positive probability, S contains an independence set as required, which would give the theorem.\nWe first observe the following properties of the set S.\nClaim. With probability at least 3 4 , it holds that |S| ≥ 1 10 m.\nTo see this, note that each element from R is not included in S̃ with probability (1− 1 r )m ≤ e−m/r with r = |R|. Since m ≤ 1 10 r, the expected size of S is at least r(1 − e−m/r) = re−m/r(em/r − 1) ≥ me−m/r ≥ 9 10 m, where both inequality use ex ≥ x + 1. Since always |S| ≤ m, Markov’s inequality shows that |S| ≥ 1 10 m with probability at least 3 4 ; otherwise,\nwe would have E [ |S| ] ≤ 1\n10 m+mP\n( |S| ≥ 1 10 m ) < 9 10 m.\nClaim. With probability at least 3 4 , we have |Nout(v) ∩ S| ≤ lnn for all v ∈ V .\nIndeed, fix some v ∈ V and recall that v dominates at most a β fraction of the vertices in R, so each element of S̃ (that was chosen uniformly at random from R) is dominated by v with probability at most β. Hence, the random variable X̃v = |Nout(v) ∩ S̃| has a binomial distribution Bin(m, p) with p ≤ β. By a standard binomial tail bound,\nP(X̃v ≥ lnn) ≤ ( m\nlnn\n) β lnn ≤ (mβ)lnn ≤ e−2 lnn = 1\nn2 .\nThe same bound holds also for the random variable Xv = |Nout(v) ∩ S|, that can only be smaller than X̃v. Our claim now follows from a union bound over all v ∈ V . Claim. With probability at least 3\n4 , we have 1 |S| ∑ v∈S |Nout(v) ∩ S| ≤ 12 .\nTo obtain this, we note that for each v ∈ V the random variable Xv = |Nout(v) ∩ S| defined above has E[Xv] ≤ E[X̃v] ≤ mβ ≤ 110 , and therefore E [ 1 |S| ∑ v∈S Xv ] ≤ 1 10 . By Markov’s inequality we then have 1 |S| ∑ v∈S Xv > 1 2 with probability less than 1 5 , which gives the claim.\nThe three claims together imply that there exists a set S ⊆ W of size at least 1 10 m, such that any v ∈ V dominates at most lnn vertices of S, and the average degree of the induced undirected graph over S is at most 1. Hence, by Turán’s Theorem,4 S contains an independent set U of size 1\n20 m ≥ 1 50 k/ lnn. This concludes the proof, as each v ∈ V\ndominates at most lnn vertices of U .\nGiven Lemma 8, the idea of the proof is quite intuitive; here we only give a sketch of the proof, and defer the formal details to Appendix B.\nProof of Theorem 7 (sketch). First, we use the lemma to find an independent set U of weakly observable vertices of size Ω̃(δ), with the crucial property that each vertex in the entire graph dominates at most Õ(1) vertices of U . Then, we embed in the set U a hard instance of the stochastic multiarmed bandit problem, in which the optimal action has expected loss smaller by ǫ than the expected loss of the other actions in U . To all other vertices of the graph, we assign the maximal loss of 1. Hence, unless the player is able to detect the optimal action, his regret cannot be better than Ω(ǫT ).\nThe main observation is that, due to the properties of the set U , in order to obtain accurate estimates of the losses of all actions in U the player has to use Ω̃(δ) different actions outside of U and pick each for Ω(1/ǫ2) times. Since each such action entails a constant instantaneous regret, the player has to pay an Ω(δ/ǫ2) penalty in his cumulative regret for exploration. The overall regret is thus of order Ω ( min{ǫT, δ/ǫ2} ) , which is maximized at ǫ = (δ/T )1/3 and gives the stated lower bound."
    }, {
      "heading" : "5 Time-Varying Feedback Graphs",
      "text" : "The setting discussed above can be generalized by allowing the feedback graphs to change arbitrarily from round to round (see Mannor and Shamir (2011); Alon et al. (2013); Kocák et al. (2014)). Namely, the environment chooses a sequence of feedback graphs G1, . . . , GT along with the sequence of loss functions. We consider two different variants of this setting: in the informed model, the player observes Gt at the beginning of round t, before drawing the action It. In the harder uninformed model, the player observes Gt at the end of round t, after drawing It. In this section, we discuss how our algorithm can be modified to handle time-varying feedback graphs, and whether this generalization increases the minimax regret of the induced online learning problem.\nStrongly Observable. If G1, . . . , GT are all strongly observable, Algorithm 1 and its analysis can be adapted to the time-varying setting (both informed and uninformed) with only a few cosmetic modifications. Specifically, we replace G with Gt, to define time-dependent neighborhoods, Noutt and N in t , in Eq. (1) of the algorithm. This modification holds in both the informed and uninformed models because the structure of the feedback graph is only used\n4Turán’s Theorem (e.g., Alon and Spencer, 2008) states that in any undirected graph whose average degree is d, there is an independent set of size n/(d+ 1).\nto update qt+1, which takes place after the action It is chosen. Moreover, the upper-bound in Theorem 2 can be adapted to the time-varying model by replacing α with 1\nT ∑T t=1 αt, where\neach αt is the independence number of the corresponding Gt (e.g., using a doubling trick, or an adaptive learning rate as in Kocák et al. (2014)).\nWeakly Observable, Informed. If G1, . . . , GT are all weakly observable, Algorithm 1 can again be adapted to the informed time-varying model, but the required modification is more substantial than before, and in particular, relies on the fact that Gt is known before the prediction on round t is made. The exploration set U must change from round to round, according to the feedback graph. Specifically, we choose the exploration set on round t to be Dt, the smallest weakly dominating set in Gt. We then define ut to be the uniform distribution over this set, and pt = (1 − γ)qt + γut. Again, via standard techniques, the upper-bound in Theorem 2 can be adapted to this setting by replacing δ with 1\nT ∑T t=1 δt,\nwhere δt = |Dt|.\nWeakly Observable, Uninformed. So far, we discussed cases where the minimax regret rates of our problem do not increase when we allow the feedback graphs to change from round to round. However, if G1, . . . , GT are all weakly observable and they are revealed according to the uninformed model, then the minimax regret can strictly increase. Recall that Theorem 1 states that the minimax regret for a constant weakly observable graph is Θ̃(δ1/3 T 2/3), where δ is the size of the smallest weakly dominating set. We now show that the minimax regret in the analogous uninformed setting is Θ̃(K1/3 T 2/3), where K is the number of actions. The Õ(K1/3 T 2/3) upper bound is obtained by running Algorithm 1 with uniform exploration over the entire set of actions (namely, U = V ). To show that this bound is tight, we state the following matching lower bound.\nTheorem 9. For any randomized player strategy in the uninformed feedback model, there exists a sequence of weakly observable graphs G1, . . . , GT over a set V of K ≥ 4 actions with δ(Gt) = α(Gt) = 1 for all t, and a sequence of loss functions ℓ1, . . . , ℓT : V 7→ [0, 1], such that the player’s expected regret is at least 1\n16 K1/3T 2/3.\nWe sketch the proof below, and present it in full detail in Appendix B.\nProof (sketch). For each t = 1, . . . , T , construct the graph Gt as follows: start with the complete graph over K vertices (that includes all self-loops), and then remove the self-loop and all edges incoming to i = 1 except of a single edge incoming from some vertex jt 6= 1 chosen arbitrarily. Notice that the resulting graph is weakly observable (each vertex is observable, but i = 1 is only weakly observable), has δ(Gt) = 1 since jt dominates the entire graph, and α(Gt) = 1 as each two vertices are connected by at least one edge. However, for observing the loss of i = 1 the player has to “guess” the revealing action jt, that might change arbitrarily from round to round. This random guessing of one out of Ω(K) actions introduces the K1/3 factor in the resulting bound."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We thank Sébastien Bubeck for helpful discussions during various stages of this work, and Gábor Bartók for clarifying the connections to observability in partial monitoring."
    }, {
      "heading" : "A Additional Proofs",
      "text" : "A.1 Proof of Lemma 4\nIn order to prove our new regret bound for Hedge, we first state and prove the standard second-order regret bound for this algorithm.\nLemma 10. For any η > 0 and for any sequence ℓ1, . . . , ℓT of loss functions such that ℓt(i) ≥ −1/η for all t and i, the probability vectors q1, . . . , qT of Eq. (4) satisfy\nT∑\nt=1\n∑\ni∈V\nqt(i)ℓt(i)−min k∈V\nT∑\nt=1\nℓt(k) ≤ lnK\nη + η\nT∑\nt=1\n∑\ni∈V\nqt(i)ℓt(i) 2 .\nProof. The proof follows the standard analysis of exponential weighting schemes: let wt(i) = exp ( −η∑t−1s=1 ℓs(i) ) and let Wt = ∑ i∈V wt(i). Then qt(i) = wt(i)/Wt and we can write\nWt+1 Wt\n= ∑\ni∈V\nwt+1(i)\nWt\n= ∑\ni∈V\nwt(i) exp ( −η ℓt(i) )\nWt\n= ∑\ni∈V\nqt(i) exp ( −η ℓt(i) )\n≤ ∑\ni∈V\nqt(i) ( 1− ηℓt(i) + η2ℓt(i)2 ) (using ex ≤ 1 + x+ x2 for all x ≤ 1)\n≤ 1− η ∑\ni∈V\nqt(i)ℓt(i) + η 2 ∑\ni∈V\nqt(i)ℓt(i) 2 .\nTaking logs, using ln(1− x) ≤ −x for all x ≥ 0, and summing over t = 1, . . . , T yields\nln WT+1 W1\n≤ T∑\nt=1\n∑\ni∈V\n( −η qt(i)ℓt(i) + η2 qt(i)ℓt(i)2 ) .\nMoreover, for any fixed action k, we also have\nln WT+1 W1 ≥ ln wT+1(k) W1\n= − η T∑\nt=1\nℓt(k)− lnK .\nPutting together and rearranging gives the result.\nWe can now prove Lemma 4, restated here for the convenience of the reader.\nLemma 4 (restated). Let q1, . . . , qT be the probability vectors defined by Eq. (4) for a sequence of loss functions ℓ1, . . . , ℓT such that ℓt(i) ≥ 0 for all t = 1, . . . , T and i ∈ V . For each t, let St be a subset of V such that ℓt(i) ≤ 1/η for all i ∈ St. Then, it holds that\nT∑\nt=1\n∑\ni∈V\nqt(i)ℓt(i)−min k∈V\nT∑\nt=1\nℓt(k) ≤ lnK\nη + η\nT∑\nt=1\n( ∑\ni∈St\nqt(i) ( 1− qt(i) ) ℓt(i) 2 + ∑\ni/∈St\nqt(i)ℓt(i) 2 ) .\nProof. For all t, let ℓ̄t = ∑ i∈St pt(i)ℓt(i) for which ℓ̄t ≤ 1/η by construction. Notice that executing Hedge on the loss vectors ℓ1, . . . , ℓT is equivalent to executing in on vectors ℓ′1, . . . , ℓ ′ T with ℓ ′ t(i) = ℓt(i)− ℓ̄t for all i. Applying Lemma 10 for the latter case (notice that ℓ′t(i) ≥ −1/η for all t and i), we obtain T∑\nt=1\n∑\ni∈V\npt(i)ℓt(i)−min k∈V\nT∑\nt=1\nℓt(k) = T∑\nt=1\n∑\ni∈V\npt(i)ℓ ′ t(i)−min\nk∈V\nT∑\nt=1\nℓ′t(k)\n≤ lnK η\n+ η T∑\nt=1\n∑\ni∈V\npt(i)ℓ ′ t(i) 2\n= lnK\nη + η\nT∑\nt=1\n∑\ni∈V\npt(i)(ℓt(i)− ℓ̄t)2 .\nOn the other hand, for all t,\n∑\ni∈St\npt(i)(ℓt(i)− ℓ̄t)2 = ∑\ni∈St\npt(i)ℓt(i) 2 −\n(∑\ni∈St\npt(i)ℓt(i) )2\n≤ ∑\ni∈St\npt(i)ℓt(i) 2 −\n∑\ni∈St\npt(i) 2ℓt(i) 2\n= ∑\ni∈St\npt(i)(1− pt(i))ℓt(i)2\nwhere the inequality follows from the non-negativity of the losses ℓt(i). Also, since ℓt(i) > 1/η ≥ ℓ̄t for all i /∈ St, we also have\n∑\ni/∈St\npt(i)(ℓt(i)− ℓ̄t)2 ≤ ∑\ni/∈St\npt(i)ℓt(i) 2 .\nCombining the inequalities gives the lemma.\nA.2 Proof of Lemma 5\nLemma 5 (restated). Let G = (V,E) be a directed graph with |V | = K, in which each node i ∈ V is assigned a positive weight wi. Assume that ∑ i∈V wi ≤ 1, and that wi ≥ ǫ for all i ∈ V for some constant 0 < ǫ < 1 2 . Then\n∑\ni∈V\nwi wi + ∑ j∈N in(i) wj ≤ 4α ln 4K αǫ ,\nwhere α = α(G) is the independence number of G.\nProof. Following the proof idea of Alon et al. (2013), let M = ⌈2K/ǫ⌉ and introduce a discretization of the values w1, . . . , wT such that (mi − 1)/M ≤ wi ≤ mi/M for positive integers m1, . . . , mT . Since each wi ≥ ǫ, we have mi ≥ Mwi ≥ 2Kǫ · ǫ = 2K. Hence, we obtain\n∑\ni∈V\nwi wi + ∑ j∈N in(i) wj\n= ∑\ni∈V\nmi mi + ∑ j∈N in(i) mj −K\n≤ 2 ∑\ni∈V\nmi mi + ∑ j∈N in(i)mj , (6)\nwhere the final inequality is true since K ≤ 1 2 mi ≤ 12 ( mi + ∑ j∈N in(i) mj ) .\nNow, consider a graph G′ = (V ′, E ′) created from G by replacing each node i ∈ V with a clique Ci over mi vertices, and connecting each vertex of Ci to each vertex of Cj if and only if the edge (i, j) is present in G. Then, the right-hand side of Eq. (6) equals ∑ i∈V ′ 1 1+di\n, where di is the in-degree of the vertex i ∈ V ′ in the graph G′. Applying Lemma 13 of Alon et al. (2013) to the graph G′, we can show that\n∑\ni∈V\nmi mi + ∑ j∈N in(i)mj\n≤ 2α ln ( 1 + ∑ i∈V mi\nα\n) ≤ 2α ln ( 1 + M +K\nα\n) ≤ 2α ln 4K\nαǫ ,\nand the lemma follows."
    }, {
      "heading" : "B Proofs of Lower Bounds",
      "text" : "B.1 Non-observable Feedback Graphs\nWe first prove Theorem 6.\nTheorem 6 (restated). If G = (V,E) is not observable and |V | ≥ 2, then for any player algorithm there exists a sequence of loss functions ℓ1, ℓ2, . . . : V 7→ [0, 1] such that the player’s expected regret is at least 1\n4 T .\nProof. Since G is not observable, there exists a node with no incoming edges, say node i = 1. Consider the following randomized construction of loss functions L1, L2, . . . : V 7→ [0, 1]: draw χ ∈ {0, 1} uniformly at random and set\nLt(i) = { χ if i = 1, 1 2 if i 6= 1 t = 1, 2, . . .\nNow fix some strategy of the player (which, without loss of generality, we may assume to be deterministic) and denote by M the random number of times it chooses action i = 1. Notice that the player’s actions, and consequently M , are independent of the random variable χ since the player never observes the loss value assigned to action i = 1. Letting RT denote the player’s regret after T rounds, it holds that E[RT ] (where expectation is taken with respect to the randomization of the loss functions) satisfies\nE[RT ] = 1 2 E [ 1 2 M | χ = 1 ] + 1 2 E [ 1 2 (T −M) | χ = 0 ]\n= 1 2 E [ 1 2 M + 1 2 (T −M) ] = 1 4 T .\nThis implies that there exists a realization ℓ1, . . . , ℓT of the random functions for which the regret is at least 1\n8 T , as claimed.\nB.2 Weakly observable Feedback Graphs\nWe now turn to prove our main lower bound for weakly observable graphs, stated in Theorem 7.\nTheorem 7 (restated). If G = (V,E) is weakly observable with K = |V | ≥ 2 and weak domination number δ = δ(G), then for any randomized player algorithm and for any time horizon T there exists a sequence of loss functions ℓ1, . . . , ℓT : V 7→ [0, 1] such that the player’s expected regret is at least 1\n150 (δ/ ln2K)1/3T 2/3.\nBefore proving the theorem, we recall the key combinatorial lemma it relies upon.\nLemma 8 (restated). Let G = (V,E) be a directed graph over |V | = n vertices, and let W ⊆ V be a set of vertices whose minimal dominating set is of size k. Then, W contains an independent set U of size at least 1\n50 (k/ lnn), with the property that any vertex of G\ndominates at most lnn vertices of U .\nProof of Theorem 7. As the minimal dominating set of the weakly observable part of G is of size δ, Lemma 8 says that G must contain an independent set U of m ≥ δ/(50 lnK) weakly observable vertices, such that any v ∈ V dominates at most lnK vertices of U . For simplicity, we shall assume that δ ≥ 100 lnK which ensures that the set U consists of at least m ≥ 2 vertices; a proof of the theorem for the (less interesting) case where δ < 100 lnK is given after the current proof.\nConsider the following randomized construction of loss functions L1, . . . , LT : V 7→ [0, 1]: fix ǫ = m1/3(32T lnK)−1/3, choose χ ∈ U uniformly at random and for all t and i, and let the loss Lt(i) ∼ Ber(µi) be a Bernoulli random variable with parameter\n∀ i ∈ V , µi =    1 2 − ǫ if i = χ, 1 2\nif i ∈ U, i 6= χ, 1 i /∈ U .\nWe refer to actions in U as “good” actions (whose expected instantaneous regret is at most ǫ), and to actions in V \\U as “bad” actions (with expected instantaneous regret larger than 1 2 ). Notice that N in(i) ⊆ V \\ U for all good actions i ∈ U , since U is an independent set of weakly observable vertices (that do not have self-loops). In other words, in order to observe the loss of a good action in a given round, the player has to pick a bad action on that round.\nFix some strategy of the player which we assume to be deterministic (again, this is without loss of generality). Up to a constant factor in the resulting regret lower bound, we may also assume that the strategy chooses bad actions at most ǫT times with probability one (i.e., over any realization of the stochastic loss functions). Indeed, we can ensure this is the case by simply halting the player’s algorithm once it chooses bad actions for more than ǫT times, and picking an arbitrary good action in the remaining rounds; since the instantaneous regret of a good action is at most ǫ, the regret of the modified algorithm is at most 3 times larger than the regret of the original algorithm (the latter regret is at least 1\n2 ǫT , while the\nmodification results in an increase of at most ǫT in the regret). Denote by I1, . . . , IT the sequence of actions played by the player’s strategy throughout the game, in response to the loss functions L1, . . . , LT . For all t, let Yt be the vector of loss values observed by the player on round t; we think about Yt as being a full K-vector, with the unobserved values replaced by −1. For all i ∈ U , let Mi be the number of times the player picks the good action i, and Ni be the number of times the player picks a bad action from N in(i). Also, let M be the total number of times the player picks a good action, and N be the number of times he picks a bad action. Notice that ∑ i∈U Ni ≤ N lnK as each vertex in V \\ U dominates at most lnK vertices of U by construction. This, together with our assumption that N ≤ ǫT with probability one (i.e., that the player picks bad actions for at most ǫT times), implies that\n∑\ni∈U\nNi ≤ ǫT lnK . (7)\nIn order to analyze the amount of information on the value of χ the player obtains by observing the Yt’s, we let F be the σ-algebra generated by the observed variables Y1, . . . , YT , and define the conditional probability functions Qi(·) = P( · | χ = i) over F , for all i ∈ U . Notice that under Qi, action i is the optimal action. For technical purposes, we also let Q0(·) denote the fictitious probability function induced by picking χ = 0; under this distribution, all good actions in U have an expected loss equal to 1\n2 . For two probability functions Q,Q′\nover F , we denote by\nDTV(Q,Q′) = sup A∈F |Q(A)−Q′(A)|\nthe total variation distance between Q and Q′ with respect to F . Then, we can bound the total variation distance between Q0 and each of the Qi’s in terms of the random variables Ni, as follows. Lemma. For each i ∈ U , we have DTV(Q0,Qi) ≤ ǫ √ 2EQ0 [Ni].\nProof. As an intermediate step, we first upper bound the KL-divergence between Qi and Q0 in terms of the random variable Ni. Let Qjt = Qj( · | Y1, . . . , Yt−1) for all j. Notice that Qit and Q0t are identical unless the player picked an action from N in(i) on round t. In this latter case, DKL(Q0t ,Qit) equals the KL-divergence between two Bernoulli random variables with biases 1\n2 and 1 2 − ǫ, which is upper bounded by 4ǫ2 for ǫ ≤ 1 4 .5 Thus, using the chain rule for\nrelative entropy we may write\nDKL(Q0,Qi) = T∑\nt=1\nDKL(Q0t ,Qit)\n=\nT∑\nt=1\nQ0 ( It ∈ N in(i) ) · DKL(Ber(12),Ber(12 − ǫ))\n≤ 4ǫ2 T∑\nt=1\nQ0 ( It ∈ N in(i) ) = 4ǫ2 EQ0 [Ni] .\nBy Pinsker’s inequality we have DTV(Q0,Qi) ≤ √ 1 2 DKL(Q0,Qi), which gives the lemma.\nAveraging the lemma’s inequality over i ∈ U , using the concavity of the square-root and recalling Eq. (7), we obtain\n1\nm\n∑\ni∈U\nDTV(Q0,Qi) ≤ √√√√2ǫ2 m EQ0 [ ∑\ni∈U\nNi\n] ≤ √ 2ǫ3\nm T lnK =\n1 4 , (8)\nwhere the final equality follows from our choice of ǫ. We now turn to lower bound the player’s expected regret. Since the player incurs (at least) ǫ regret each time he picks an action different from χ, his overall regret is lower bounded by ǫ(T −Mχ), whence\nE[RT ] ≥ 1\nm\n∑\ni∈U\nE [ ǫ(T −Mχ) | χ = i ] = ǫT − ǫ\nm\n∑\ni∈U\nEQi[Mi] . (9)\nIn order to bound the sum on the right-hand side, note that\nEQi [Mi]− EQ0 [Mi] = T∑\nt=1\n( Qi(It = i)−Q0(It = i) ) ≤ T · DTV(Q0,Qi) ,\nand average over i ∈ U to obtain\n1\nm\n∑\ni∈U\nEQi [Mi] ≤ T\nm\n∑\ni∈U\nDTV(Q0,Qi) + 1\nm EQ0\n[ ∑\ni∈U\nMi\n] ≤ 1\n4 T +\n1 m T ≤ 3 4 T ,\n5This KL-divergence equals 1 2 ln 1/2 1/2−ǫ + 1 2 ln 1/2 1/2+ǫ = 1 2 ln ( 1 + 4ǫ 2 1−4ǫ2 ) ≤ 1 2 · 4ǫ2 1−4ǫ2 ≤ 4ǫ2, where the last\nstep is valid for ǫ ≤ 1 4 .\nwhere the last inequality is due to m ≥ 2. Combining this with Eq. (9) yields E[RT ] ≥ 14ǫT , and plugging in our choice of ǫ gives\nE[RT ] ≥ 1\n4 ( m 32 lnK ) 1/3T 2/3 ≥ δ 1/3T 2/3 50 ln2/3K ,\nwhich concludes the proof (recall the additional 1 3 -factor stemming from our simplifying assumption made earlier).\nThe claim of the theorem for the case δ < 100 lnK, that remained unaddressed in the proof above, follows from a simpler lower bound that applies to weakly observable graphs of any size.\nTheorem 11. If G = (V,E) is weakly observable and |V | ≥ 2, then for any player algorithm and for any time horizon T there exists a sequence of loss functions ℓ1, . . . , ℓT : V 7→ [0, 1] such that the player’s expected regret is at least 1\n8 T 2/3.\nProof. First, we observe that any graph over less than 3 vertices is either non-observable or strongly observable; in other words, any weakly observable graph has at least 3 vertices, so |V | ≥ 3. Now, if G is weakly observable, then there is a node of G, say i = 1, without a self-loop and without an incoming edge from (at least) one of the other nodes of the graph, say from j = 2. Since |V | ≥ 3 and the graph is observable, i = 1 has at least one incoming edge from a third node of the graph.\nConsider the following randomized construction of loss functions L1, . . . , LT : V 7→ [0, 1]: fix ǫ = 1\n2 T−1/3, choose χ ∈ {−1,+1} uniformly at random and for all t and i, let the loss\nLt(i) ∼ Ber(µi) be a Bernoulli random variable with parameter\nµi =    1 2 − ǫ χ if i = 1, 1 2 if i = 2,\n1 otherwise.\nHere, the “good” actions (whose expected instantaneous regret is at most ǫ) are i = 1 and i = 2, and all other actions are “bad” actions (with expected instantaneous regret larger than 1\n2 ).\nNow, fix a deterministic strategy of the player and let the random variable N1 be the number of times the player chooses a bad action from N in(1). Define the conditional probability functions Q1(·) = P( · | χ = +1) and Q2(·) = P( · | χ = −1) where under Qi action i is the optimal action. Also, define Q0 to be the fictitious distribution induced by setting χ = 0, under which the actions i = 1 and i = 2 both have an expected loss of 1\n2 . Then,\nexactly as in the proof of Theorem 7, we can show that\nDTV(Q0,Qi) ≤ ǫ √ 2EQi[N1] , i = 1, 2 .\nAveraging the two inequalities and using the concavity of the square root, we obtain\n1 2 DTV(Q0,Q1) + 12DTV(Q0,Q2) ≤ ǫ √ EQ1 [N1] + EQ2 [N1] = ǫ √ 2E[N1] , (10)\nwhere we have used the fact that P(·) = 1 2 Q1(·) + 1 2 Q2(·).\nWe can now analyze the player’s expected regret, again denoted by E[RT ]. Notice that if E[N1] > 1 32 ǫ−2, we have E[RT ] ≥ E[12N1] > 164ǫ−2 = 18T 2/3 (since each action that reveals the loss of i = 1 is a bad action whose instantaneous regret is at least 1 2 ), which gives the required lower bound. Hence, we may assume that E[N1] ≤ 132ǫ−2, in which case the right-hand side of Eq. (10) is bounded by 1\n4 . This yields an analogue of Eq. (8), from which we can proceed\nexactly as in the proof of Theorem 7 to obtain that E[RT ] ≥ 14ǫT . Using our choice of ǫ gives the theorem.\nB.3 Separation Between the Informed and Uninformed Models\nFinally, we prove our separation result for weakly observable time-varying graphs, which shows that the uninformed model is harder than the informed model (in terms of the dependence on the feedback structure) for weakly observable feedback graphs.\nTheorem 9 (restated). For any randomized player strategy in the uninformed feedback model, there exists a sequence of weakly observable graphs G1, . . . , GT over a set V of K ≥ 4 actions with δ(Gt) = α(Gt) = 1 for all t, and a sequence of loss functions ℓ1, . . . , ℓT : V 7→ [0, 1], such that the player’s expected regret is at least 1\n16 K1/3T 2/3.\nProof. As before, it is enough to demonstrate a randomized construction of weakly observable graphs G1, . . . , GT and loss functions L1, . . . , LT such that the expected regret of any deterministic algorithm is Ω(K1/3T 2/3).\nThe random loss functions L1, . . . , LT are constructed almost identically to those used in the proof of Theorem 11; the only change is in the value of ǫ, which is now fixed to ǫ = 1\n4 (K/T )1/3. In order to construct the random sequence of weakly observable graphs\nG1, . . . , GT , first pick nodes J1, . . . , JT independently and uniformly at random from V ′ = {3, . . . , K}. Then, for each t, form the graph Gt by taking the complete graph over V (that includes all directed edges and self-loops) and removing all edges incoming to node i = 1 (including its self-loop), except for the edge incoming from Jt. In other words, the only way to observe the loss Lt(1) of node 1 on round t is by picking the action Jt on that round. Notice that Gt is weakly observable, as each of its nodes has at least one incoming edge, but there is a node (node 1) which is not strongly observable. Also, we have δ(Gt) = 1 since Jt dominates the entire graph, and α(Gt) = 1 as any pair of vertices is connected by at least one directed edge.\nWe now turn to analyze the expected regret of any player on our construction; the analysis is very similar to that of Theorem 11, and we only describe the required modifications. Fix any deterministic algorithm, and define the random variables I1, . . . , IT and N1 exactly as in the proof of Theorem 11. In addition, define the distributions Q0, Q1, and Q2 as in that proof, for which we proved (recall Eq. (10)) that\n1 2 DTV(Q0,Q1) + 12DTV(Q0,Q2) ≤ ǫ √ 2E[N1] . (11)\nNow, define another random variable N to be the number of times the player picked an action from V ′ throughout the game. Notice that in case E[N ] > 1\n4 K1/3T 2/3, we have\nE[RT ] ≥ E[12N ] > 18K1/3T 2/3 which implies the stated lower-bound on the expected regret. Hence, in what follows we assume that E[N ] ≤ 1\n4 K1/3T 2/3. Notice that for the graphs we\nconstructed, Q(It = Jt) ≤ 2KQ(It ∈ V ′) since Jt is picked uniformly at random from V ′ (and independently from It because in the uninformed model Gt is not known when It is drawn) and since K ≥ 4. Summing this over t = 1, . . . , T , we obtain that E[N1] ≤ 2KE[N ] ≤ 1 2 (T/K)2/3, and with our choice of ǫ this shows that the right-hand size of Eq. (11) is upper bounded by 1 4 . Again, continuing exactly as in the proof of Theorem 7, we finally get that E[RT ] ≥ 14ǫT , and with our choice of ǫ this concludes the proof."
    }, {
      "heading" : "C Tight Bounds for the Loopless Clique",
      "text" : "We restate and prove Theorem 3.\nTheorem 3 (restated). For any sequence of loss functions ℓ1, . . . , ℓT , where ℓt : V 7→ [0, 1], the expected regret of Algorithm 1, with the loopless clique feedback graph and with parameters η = √ (lnK)/(2T ) and γ = 2η, is upper-bounded by 5 √ T lnK.\nProof. Since G is strongly observable, the exploration distribution u is uniform on V . Fix any i⋆ ∈ V . Notice that for any i ∈ V we have j ∈ N in(i) for all j 6= i, and so Pt(i) = 1 − pt(i). On the other hand, by the definition of pt and since η = 2γ and K ≥ 2, we have pt(i) = (1 − γ)qt(i) + γK ≤ 1 − γ + γ 2\n= 1 − η, so that Pt(i) ≥ η. Thus, we can apply Lemma 4 with St = V to the vectors ℓ̂1, . . . , ℓ̂T and take expectations,\nE\n[ T∑\nt=1\n( K∑\ni=1\nqt(i)Et[ℓ̂t(i)]− Et[ℓ̂t(i⋆)] )]\n≤ lnK η + η\nT∑\nt=1\nE\n[ ∑\ni∈V\nqt(i)(1− qt(i))Et[ℓ̂t(i)2] ] .\nRecalling Eq. (3) and Pt(i) = 1− pt(i), we get\nE\n[ T∑\nt=1\nk∑\ni=1\nqt(i)ℓt(i)\n] − T∑\nt=1\nℓt(i ⋆) ≤ lnK\nη + η\nT∑\nt=1\nE\n[ ∑\ni∈S\nqt(i) 1− qt(i) 1− pt(i)\n] .\nFinally, for the distributions pt and qt generated by the algorithm we note that\n1− pt(i) ≥ ( 1− γ\nK\n)( 1− qt(i) ) ≥ 1\n2\n( 1− qt(i) )\nwhere the last inequality holds since K ≥ 2. Hence, T∑\nt=1\n∑\ni∈V\nqt(i) 1− qt(i) 1− pt(i) ≤ 2 T∑\nt=1\n∑\ni∈V\nqt(i) ≤ 2T .\nCombining this with Eq. (5) gives\nE\n[ T∑\nt=1\n∑\ni∈V\npt(i)ℓt(i)− T∑\nt=1\nℓt(i ⋆) ] ≤ γT + lnK\nη + 2ηT =\nlnK\nη + 4ηT ,\nwhere we substituted our choice γ = 2η. Picking η = √ (lnK)/2T proves the theorem."
    }, {
      "heading" : "D Connections to Partial Monitoring",
      "text" : "In online learning with partial monitoring the player is given a loss matrix L over [0, 1] and a feedback matrix H over a finite alphabet Σ. The matrices L and H are both of size K ×M , where K is the number of player’s actions and M is the number of environment’s actions. The environment preliminarily fixes a sequence y1, y2, . . . of actions (i.e., matrix column indices) hidden from the player.6 At each round t = 1, 2, . . . , the loss ℓt(It) of the player choosing action It (i.e., a matrix row index) is given by the matrix entry L(It, yt) ∈ [0, 1]. The only feedback that the player observes is the symbol H(It, yt) ∈ Σ; in particular, the column index yt and the loss value L(It, yt) remain both unknown. The player’s goal is to control a notion of regret analogous to ours, where the minimization over V is replaced by a minimization over the set of row indices, corresponding to the player’s K actions.\nWe now introduce a reduction from our online setting to partial monitoring for the special case of {0, 1}-valued loss functions (note that our lower bounds still hold under this restriction, and so does our characterization of Theorem 1). Given a feedback graph G, we create a partial monitoring game in which the environment has a distinct action for each binary assignment of losses to vertices in V . Hence, L and H have K rows and M = 2K columns, where the union of columns in L is the set {0, 1}K. The entries of H encode G using any alphabet Σ such that, for any row i ∈ V and for any two columns y 6= y′,\nH(i, y) = H(i, y′) ⇔ {( k, L(k, y) ) : k ∈ Nout(i) } ≡ {( k, L(k, y′) ) : k ∈ Nout(i) } . (12)\nNote that this is a bona fide reduction: given a partial monitoring algorithm A, we can define an algorithm A′ for solving any online learning problem with known feedback graph G = (V,E) and {0, 1}-valued loss functions. The algorithm A′ pre-computes a mapping from{(\nk, L(k, y) ) : k ∈ Nout(i) } for each i ∈ V and for each y = 1, . . . ,M to the alphabet Σ such that Eq. (12) is satisfied. Then, at each round t, A′ asks A to draw a row (i.e., a vertex of V ) It and obtains the feedback {( k, L(k, yt) ) : k ∈ Nout(It) } from the environment. Finally, A′ uses the pre-computed mapping to obtain the symbol σt ∈ Σ which is fed to A. The minimax regret of partial monitoring games is determined by a set of observability conditions on the pair (L,H). These conditions are expressed in terms of a canonical representation of H as the set of matrices Si for i ∈ V . Si has a row for each distinct symbol σ ∈ Σ in the i-th row of H , and Si(σ, y) = I{H(i, y) = σ} for y = 1, . . . ,M . When cast to the class of pairs (L,H) obtained from feedback graphs G through the above encoding, the partial monitoring observability conditions of Bartók et al. (2014, Definitions 5 and 6) can be expressed as follows. Let L(i, ·) be the column vector denoting the i-th row of L. Let also rowsp be the rowspace of a matrix and ⊕ be the cartesian product between linear spaces. Then\n6 The standard definition of partial monitoring (see, e.g., Cesa-Bianchi and Lugosi, 2006, Section 6.4) assumes a harder adaptive environment, where each action yt is allowed to depend on all of past player’s actions I1, . . . , It−1. However, the partial monitoring lower bounds of Antos et al. (2013, Theorem 13) and Bartók et al. (2014, Theorem 3) hold for our weaker notion of environment as well.\n• (L,H) is globally observable if for all pairs i, j ∈ V of actions,\nL(i, ·)− L(j, ·) ∈ ⊕\nk=1,...,K\nrowsp(Sk)\n• (L,H) is locally observable if for all pairs i, j ∈ V of actions,\nL(i, ·)− L(j, ·) ∈ rowsp(Si ⊕ rowsp(Sj) .\nThe characterization result for partial monitoring of Bartók et al. (2014, Theorem 2) states that the minimax regret is of order √ T for locally observable games and of order T 2/3 for globally observable games. We now prove that the above encoding of feedback graphs G as instances (L,H) of partial monitoring games preserves the observability conditions. Namely, our encoding maps weakly (resp., strongly) observable graphs G to globally (resp., locally) observable instances of partial monitoring. Combining this with our characterization result (Theorem 1) and the partial monitoring characterization result (Bartók et al., 2014, Theorem 2), we conclude that the minimax rates are preserved by our reduction.\nClaim 12. If j ∈ Nout(i) then there exists a subset Σ0 of rows of Si such that\nL(j, ·) = ∑\nσ∈Σ0\nSi(σ, ·) .\nProof. Let Σ0 to be the union of rows Si(σy, ·) such that H(i, y) = σy and L(j, y) = 1 for some y. Each such row has a 1 in position y because Si(σy, y) = 1 holds by definition. Moreover, no such row has a 1 in a position y′ where L(j, y′) = 0. Indeed, combining j ∈ Nout(i) with Eq. (12), we get that L(j, y′) = 0 implies H(i, y′) 6= σy, which in turn implies Si(σy, y ′) = 0.\nTheorem 13. Any feedback graph G can be encoded as a partial monitoring problem (L,H) such that the observability conditions are preserved.\nProof. If G is weakly observable, then for every j ∈ V there is some i ∈ V such that j ∈ Nout(i). By Claim 12, L(j, ·) ∈ rowsp(Si) and the global observability condition follows. If G is strongly observable, then for any distinct i, j ∈ V the subgraph G′ of G restricted to the pair of vertices i, j is weakly observable. By the previous argument, this implies that L(i, ·)− L(j, ·) ∈ rowsp(Si)⊕ rowsp(Sj) and the proof is concluded."
    } ],
    "references" : [ {
      "title" : "The Probabilistic Method",
      "author" : [ "N. Alon", "J.H. Spencer" ],
      "venue" : null,
      "citeRegEx" : "Alon and Spencer.,? \\Q2008\\E",
      "shortCiteRegEx" : "Alon and Spencer.",
      "year" : 2008
    }, {
      "title" : "From bandits to experts: A tale of domination and independence",
      "author" : [ "N. Alon", "N. Cesa-Bianchi", "C. Gentile", "Y. Mansour" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Alon et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Alon et al\\.",
      "year" : 2013
    }, {
      "title" : "Nonstochastic multi-armed bandits with graph-structured feedback",
      "author" : [ "N. Alon", "N. Cesa-Bianchi", "C. Gentile", "S. Mannor", "Y. Mansour", "O. Shamir" ],
      "venue" : "CoRR, abs/1409.8428,",
      "citeRegEx" : "Alon et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Alon et al\\.",
      "year" : 2014
    }, {
      "title" : "Toward a classification of finite partialmonitoring games",
      "author" : [ "A. Antos", "G. Bartók", "D. Pál", "C. Szepesvári" ],
      "venue" : "Theoretical Computer Science,",
      "citeRegEx" : "Antos et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Antos et al\\.",
      "year" : 2013
    }, {
      "title" : "The nonstochastic multiarmed bandit problem",
      "author" : [ "P. Auer", "N. Cesa-Bianchi", "Y. Freund", "R.E. Schapire" ],
      "venue" : "SIAM Journal on Computing,",
      "citeRegEx" : "Auer et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Auer et al\\.",
      "year" : 2002
    }, {
      "title" : "Partial monitoring— classification, regret bounds, and algorithms",
      "author" : [ "G. Bartók", "D.P. Foster", "D. Pál", "A. Rakhlin", "C. Szepesvári" ],
      "venue" : "Mathematics of Operations Research,",
      "citeRegEx" : "Bartók et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bartók et al\\.",
      "year" : 2014
    }, {
      "title" : "Prediction, learning, and games",
      "author" : [ "N. Cesa-Bianchi", "G. Lugosi" ],
      "venue" : null,
      "citeRegEx" : "Cesa.Bianchi and Lugosi.,? \\Q2006\\E",
      "shortCiteRegEx" : "Cesa.Bianchi and Lugosi.",
      "year" : 2006
    }, {
      "title" : "How to use expert advice",
      "author" : [ "N. Cesa-Bianchi", "Y. Freund", "D. Haussler", "D. Helmbold", "R. Schapire", "M. Warmuth" ],
      "venue" : "Journal of the ACM,",
      "citeRegEx" : "Cesa.Bianchi et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Cesa.Bianchi et al\\.",
      "year" : 1997
    }, {
      "title" : "Improved second-order bounds for prediction with expert advice",
      "author" : [ "N. Cesa-Bianchi", "Y. Mansour", "G. Stoltz" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Cesa.Bianchi et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Cesa.Bianchi et al\\.",
      "year" : 2007
    }, {
      "title" : "A decision-theoretic generalization of on-line learning and an application to boosting",
      "author" : [ "Y. Freund", "R. Schapire" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "Freund and Schapire.,? \\Q1997\\E",
      "shortCiteRegEx" : "Freund and Schapire.",
      "year" : 1997
    }, {
      "title" : "Efficient learning by implicit exploration in bandit problems with side observations",
      "author" : [ "T. Kocák", "G. Neu", "M. Valko", "R. Munos" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Kocák et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kocák et al\\.",
      "year" : 2014
    }, {
      "title" : "The weighted majority algorithm",
      "author" : [ "N. Littlestone", "M.K. Warmuth" ],
      "venue" : "Information and Computation,",
      "citeRegEx" : "Littlestone and Warmuth.,? \\Q1994\\E",
      "shortCiteRegEx" : "Littlestone and Warmuth.",
      "year" : 1994
    }, {
      "title" : "From bandits to experts: On the value of side-observations",
      "author" : [ "S. Mannor", "O. Shamir" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Mannor and Shamir.,? \\Q2011\\E",
      "shortCiteRegEx" : "Mannor and Shamir.",
      "year" : 2011
    }, {
      "title" : "Online learning and online convex optimization",
      "author" : [ "S. Shalev-Shwartz" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "Shalev.Shwartz.,? \\Q2011\\E",
      "shortCiteRegEx" : "Shalev.Shwartz.",
      "year" : 2011
    }, {
      "title" : "Aggregating strategies",
      "author" : [ "V. Vovk" ],
      "venue" : "In Proceedings of the 3rd Annual Workshop on Computational Learning Theory,",
      "citeRegEx" : "Vovk.,? \\Q1990\\E",
      "shortCiteRegEx" : "Vovk.",
      "year" : 1990
    }, {
      "title" : "Following the proof idea of Alon et al. (2013), let M = ⌈2K/ǫ⌉ and introduce a discretization of the values",
      "author" : [ "G. Proof" ],
      "venue" : null,
      "citeRegEx" : "Proof.,? \\Q2013\\E",
      "shortCiteRegEx" : "Proof.",
      "year" : 2013
    }, {
      "title" : "Definitions 5 and 6) can be expressed as follows. Let L(i, ·) be the column vector denoting the i-th row of L. Let also rowsp be the rowspace of a matrix and ⊕ be the cartesian product between linear spaces",
      "author" : [ "Bartók" ],
      "venue" : null,
      "citeRegEx" : "Bartók,? \\Q2014\\E",
      "shortCiteRegEx" : "Bartók",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 13,
      "context" : "Online learning can be formulated as a repeated game between a randomized player and an arbitrary, possibly adversarial, environment (see, e.g., Cesa-Bianchi and Lugosi, 2006; Shalev-Shwartz, 2011).",
      "startOffset" : 133,
      "endOffset" : 197
    }, {
      "referenceID" : 7,
      "context" : "This feedback model is often called prediction with expert advice (Cesa-Bianchi et al., 1997; Littlestone and Warmuth, 1994; Vovk, 1990).",
      "startOffset" : 66,
      "endOffset" : 136
    }, {
      "referenceID" : 11,
      "context" : "This feedback model is often called prediction with expert advice (Cesa-Bianchi et al., 1997; Littlestone and Warmuth, 1994; Vovk, 1990).",
      "startOffset" : 66,
      "endOffset" : 136
    }, {
      "referenceID" : 14,
      "context" : "This feedback model is often called prediction with expert advice (Cesa-Bianchi et al., 1997; Littlestone and Warmuth, 1994; Vovk, 1990).",
      "startOffset" : 66,
      "endOffset" : 136
    }, {
      "referenceID" : 4,
      "context" : "Another common feedback model is bandit feedback (Auer et al., 2002), where the player only observes the loss of the action that he chose.",
      "startOffset" : 49,
      "endOffset" : 68
    }, {
      "referenceID" : 4,
      "context" : "Another common feedback model is bandit feedback (Auer et al., 2002), where the player only observes the loss of the action that he chose. In this model, the player’s choices influence the feedback that he receives, so he has to balance an exploration-exploitation trade-off. On one hand, the player wants to exploit what he has learned from the previous rounds by choosing an action that is expected to have a small loss; on the other hand, he wants to explore by choosing an action that will give him the most informative feedback. The canonical example of online learning with bandit feedback is online advertising. Say that we operate an Internet website and we present one of K ads to each user that views the site. Our goal is to maximize the number of clicked ads and therefore we incur a unit loss whenever a user doesn’t click on an ad. We know whether or not the user clicked on the ad we presented, but we don’t know whether he would have clicked on any of the other ads. Full feedback and bandit feedback are special cases of a general framework introduced by Mannor and Shamir (2011), where the feedback model is specified by a feedback graph.",
      "startOffset" : 50,
      "endOffset" : 1097
    }, {
      "referenceID" : 5,
      "context" : "Freund and Schapire (1997) proves that the minimax regret of the full feedback game is Θ( √ T lnK) while Auer et al.",
      "startOffset" : 0,
      "endOffset" : 27
    }, {
      "referenceID" : 2,
      "context" : "Freund and Schapire (1997) proves that the minimax regret of the full feedback game is Θ( √ T lnK) while Auer et al. (2002) proves that the minimax regret of the bandit feedback game is Θ̃( √ KT ).",
      "startOffset" : 105,
      "endOffset" : 124
    }, {
      "referenceID" : 1,
      "context" : "The minimax regret rates induced by self-aware feedback graphs were extensively studied in Alon et al. (2014). In this paper, we focus on the intriguing situation that occurs when the feedback graph is missing some self-loops, namely, when the player does not always observe his own loss.",
      "startOffset" : 91,
      "endOffset" : 110
    }, {
      "referenceID" : 1,
      "context" : "The set of strongly observable feedback graphs includes the set of self-aware graphs, so this result extends the characterization given in Alon et al. (2014). The second category is the set of weakly observable feedback graphs, which induce learning problems whose minimax regret is Θ̃(δT ), where δ is a new graph-dependent quantity called the weak domination number of the feedback graph.",
      "startOffset" : 139,
      "endOffset" : 158
    }, {
      "referenceID" : 1,
      "context" : "G (see Algorithm 1), which is a variant of the Exp3-SET algorithm for undirected feedback graphs (Alon et al., 2013).",
      "startOffset" : 97,
      "endOffset" : 116
    }, {
      "referenceID" : 1,
      "context" : ", strongly observable with self-loops), our result matches the bounds of Alon et al. (2014); Kocák et al.",
      "startOffset" : 73,
      "endOffset" : 92
    }, {
      "referenceID" : 1,
      "context" : ", strongly observable with self-loops), our result matches the bounds of Alon et al. (2014); Kocák et al. (2014). The tightness of our bounds in all cases is discussed in Section 4 below.",
      "startOffset" : 73,
      "endOffset" : 113
    }, {
      "referenceID" : 9,
      "context" : "Recall that Hedge (Freund and Schapire, 1997) operates in the full feedback setting (see Fig.",
      "startOffset" : 18,
      "endOffset" : 45
    }, {
      "referenceID" : 1,
      "context" : "While Alon et al. (2014) only consider the special case of graphs that have self-loops at all vertices, their lower bound applies to any strongly observable graph: we can simply add any missing self-loops to the graph, without changing its independence number α.",
      "startOffset" : 6,
      "endOffset" : 25
    }, {
      "referenceID" : 9,
      "context" : "The setting discussed above can be generalized by allowing the feedback graphs to change arbitrarily from round to round (see Mannor and Shamir (2011); Alon et al.",
      "startOffset" : 126,
      "endOffset" : 151
    }, {
      "referenceID" : 1,
      "context" : "The setting discussed above can be generalized by allowing the feedback graphs to change arbitrarily from round to round (see Mannor and Shamir (2011); Alon et al. (2013); Kocák et al.",
      "startOffset" : 152,
      "endOffset" : 171
    }, {
      "referenceID" : 1,
      "context" : "The setting discussed above can be generalized by allowing the feedback graphs to change arbitrarily from round to round (see Mannor and Shamir (2011); Alon et al. (2013); Kocák et al. (2014)).",
      "startOffset" : 152,
      "endOffset" : 192
    }, {
      "referenceID" : 10,
      "context" : ", using a doubling trick, or an adaptive learning rate as in Kocák et al. (2014)).",
      "startOffset" : 61,
      "endOffset" : 81
    } ],
    "year" : 2015,
    "abstractText" : "We study a general class of online learning problems where the feedback is specified by a graph. This class includes online prediction with expert advice and the multiarmed bandit problem, but also several learning problems where the online player does not necessarily observe his own loss. We analyze how the structure of the feedback graph controls the inherent difficulty of the induced T -round learning problem. Specifically, we show that any feedback graph belongs to one of three classes: strongly observable graphs, weakly observable graphs, and unobservable graphs. We prove that the first class induces learning problems with Θ̃(α1/2T 1/2) minimax regret, where α is the independence number of the underlying graph; the second class induces problems with Θ̃(δ1/3T 2/3) minimax regret, where δ is the domination number of a certain portion of the graph; and the third class induces problems with linear minimax regret. Our results subsume much of the previous work on learning with feedback graphs and reveal new connections to partial monitoring games. We also show how the regret is affected if the graphs are allowed to vary with time. Tel Aviv University, Tel Aviv, Israel, and Microsoft Research, Herzliya, Israel, nogaa@post.tau.ac.il. Dipartimento di Informatica, Università degli Studi di Milano, Milan, Italy, nicolo.cesabianchi@unimi.it. Parts of this work were done while the author was at Microsoft Research, Redmond. Microsoft Research, Redmond, Washington; oferd@microsoft.com. Technion—Israel Institute of Technology, Haifa, Israel, and Microsoft Research, Herzliya, Israel, tomerk@technion.ac.il. Parts of this work were done while the author was at Microsoft Research, Redmond.",
    "creator" : "LaTeX with hyperref package"
  }
}