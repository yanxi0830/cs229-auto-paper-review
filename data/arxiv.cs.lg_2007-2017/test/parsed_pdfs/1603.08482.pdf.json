{
  "name" : "1603.08482.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Estimating Mixture Models via Mixtures of Polynomials",
    "authors" : [ "Sida I. Wang", "Arun Tejasvi Chaganty", "Percy Liang" ],
    "emails" : [ "sidaw@cs.stanford.edu", "chaganty@cs.stanford.edu", "pliang@cs.stanford.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Mixture models play a central role in machine learning and statistics, with diverse applications including bioinformatics, speech, natural language, and computer vision. The idea of mixture modeling is to explain data through a weighted combination of simple parametrized distributions [52, 38]. In practice, maximum likelihood estimation via Expectation Maximization (EM) has been the workhorse for these models, as the parameter updates are often easily derivable. However, EM is well-known to suffer from local optima. The method of moments, dating back to Pearson [47] in 1894, is enjoying a recent revival [3, 2, 5, 27, 26, 11, 28, 24, 22, 8] due to its strong global theoretical guarantees. However, current methods depend strongly on the specific distributions and are not easily extensible to new ones.\nIn this paper, we present a method of moments approach, which we call Polymom, for estimating a wider class of mixture models in which the moment equations are polynomial equations (Section 2). Solving general polynomial equations is NP-hard, but our key insight is that for mixture models, the moments equations are mixtures of polynomials equations and we can hope to solve them if the moment equations for each mixture component are simple polynomials equations that we can solve. Polymom proceeds as follows: First, we recover mixtures of monomials of the parameters from the data moments by solving an instance of the Generalized Moment Problem (GMP) [34, 33] (Section 3). We show that for many mixture models, the GMP can be solved with basic linear algebra and in the general case, can be approximated by an SDP in which the moment\nar X\niv :1\n60 3.\n08 48\n2v 1\n[ st\nat .M\nL ]\n2 8\nM ar\n2 01\nequations are linear constraints. Second, we extend multiplication matrix ideas from the computer algebra literature [48, 39, 50, 25] to extract the parameters by solving a generalized eigenvalue problem (Section 4).\nPolymom improves on previous method of moments approaches in both generality and flexibility. First, while tensor factorization has been the main driver for many of the method of moments approaches for many types of mixture models, [5, 4, 11, 26, 6, 22], each model required specific adaptations which are non-trivial even for experts. In contrast, Polymom provides a unified principle for tackling new models that is as turnkey as computing gradients or EM updates. To use Polymom (Figure 1), one only needs to provide a list of observation functions (φn) and derive their expected values expressed symbolically as polynomials in the parameters of the specified model (fn). Polymom then estimates expectations of φn and outputs parameter estimates of the specified model. Since Polymom works in an optimization framework, we can easily incorporate constraints such as non-negativity and parameter tying which is difficult to do in the tensor factorization paradigm. In simulations, we compared Polymom with EM and tensor factorization and found that Polymom performs similarly or better on some models (Section 5)."
    }, {
      "heading" : "2 Problem formulation",
      "text" : ""
    }, {
      "heading" : "2.1 The method of moments estimator",
      "text" : "In a mixture model, each data point x ∈ RD is associated with a latent component z ∈ [K]:\nz ∼ Multinomial(π), x | z ∼ p(x;θ∗z), (1)\nwhere π = (π1, . . . , πK) are the mixing coefficients, θ ∗ k ∈ RP are the true model parameters for the kth mixture component, and x ∈ RD is the random variable representing data. We restrict\nour attention to mixtures where each component distribution comes from the same parameterized family. For example, for a mixture of Gaussians, θ∗k = (ξ ∗ k ∈ RD,Σ∗k ∈ RD×D) consists of the mean and covariance of component k. We define N observation functions φn : RD → R for n ∈ [N ] and define fn(θ) to be the expectation of φn over a single component with parameters θ, which we assume is a simple polynomial:\nfn(θ) := Ex∼p(x;θ)[φn(x)] = ∑\nα\nanαθ α, (2)\nwhere θα = ∏P p=1 θ αp p . The expectation of each observation function E[φn(x)] can then be expressed as a mixture of polynomials of the true parameters\nE[φn(x)] = K∑\nk=1\nπkE[φn(x)|z = k] = K∑\nk=1\nπkfn(θ ∗ k). (3)\nThe method of moments for mixtures seeks parameters [θk] K k=1 that satisfy the moment condi-\ntions expressed as the following polynomial equations:\nE[φn(x)] = K∑\nk=1\nπkfn(θk). (4)\nwhere E[φn(x)] can be estimated from the data: 1T ∑T t=1 φn(xt)\np→ E[φn(x)]. Clearly, the true parameters [θ∗k] K k=1 satisfy these conditions as in (3). The goal of this work is to find parameters satisfying moment conditions that can be written in the mixture of polynomial form (4). We assume that the N given observations functions φ1, . . . , φN uniquely identify the model parameters (up to permutation of the components).\nExample 2.1 (1-dimensional Gaussian mixture). Consider a K-mixture of 1D Gaussians with parameters θk = [ξk, σ 2 k] corresponding to the mean and variance, respectively, of the k-th component (Figure 1: steps 1 and 2). We choose the observation functions, φ(x) = [x1, . . . , x6], which have corresponding moment polynomials,\nf(θ) = [ξ, ξ2 + σ2, ξ3 + 3ξσ2, . . . ].\nFor example, instantiating (4), E[x2] = ∑K k=1 πk(ξ 2 k + σ 2 k). Given φ(x) and f(θ\n∗), and data, the Polymom framework can recover the parameters. Note that the 6 moments we use have been shown by Pearson [47] to be sufficient for a mixture of two Gaussians.\nExample 2.2 (Mixture of linear regressions). Consider a mixture of linear regressions [54, 11], where each data point x = [x, y] is drawn from component k by sampling x from an unknown distribution independent of k and setting y = wkx + , where ∼ N (0, σ2k). The parameters θk = (wk, σ 2 k) are the slope and noise variance for each component k. Let us take our observation functions to be φ(x) = [x, xy, xy2, x2, . . . , x3y2],\nfor which the moment polynomials are\nf(θ) = [E[x],E[x2]w,E[x3]w2 + E[x]σ2,E[x2], . . .].\nIn Example 2.1, the coefficients anα in the polynomial fn(θ) are just constants determined by integration. For the conditional model in Example 2.2, the coefficients depends on the data. While Example 2.2 works, Polymom cannot handle arbitrary data dependence, see Appendix B for sufficient conditions and counterexamples."
    }, {
      "heading" : "2.2 Solving the moment conditions",
      "text" : "Our goal is to recover model parameters θ∗1, . . . ,θ ∗ K ∈ RP for each of the K components of the mixture model that generated the data as well as their respective mixing proportions π1, . . . , πK ∈ R. To start, let’s ignore sampling noise and identifiability issues and suppose that we are given exact moment conditions as defined in (4). Each condition fn ∈ R[θ] is a polynomial of the parameters θ, for n = 1, . . . , N .\nEquation 4 is a polynomial system of N equations in the K + K × P variables [π1, . . . , πK ] and [θ1, . . . ,θK ] ∈ RP×K . It is natural to ask if standard polynomial solving methods can solve (4) in the case where each fn(θ) is simple. Unfortunately, the complexity of general polynomial equation solving is lower bounded by the number of solutions, and each of the K! permutations of the mixture components corresponds to a distinct solution of (4) under this polynomial system representation. While several methods can take advantage of symmetries in polynomial systems [51, 14], they still cannot be adapted to tractably solve (4) to the best of our knowledge.\nThe key idea of Polymom is to exploit the mixture representation of the moment equations (4). One idea is to seek a equivalent representation of the moment conditions expressed as polynomial equations (4) that is invariant to permutations of the K components. Specifically, let µ∗ be a particular “mixture” over the component parameters θ∗1, . . . ,θ ∗ k (i.e. µ\n∗ is a probability measure). Then we can express the moment conditions (4) in terms of µ∗:\nE[φn(x)] = ∫ fn(θ) µ ∗(dθ), where µ∗(θ) = K∑\nk=1\nπkδ(θ − θ∗k). (5)\nConceptually, we no longer have any permutation invariance because the variable is µ. While permuted solutions of (4) are not equal to each other in the parameter space, µ remains the same measure regardless of the “order in summing delta functions”. As a result, solving the original moment conditions (4) is equivalent to solving the following feasibility problem over µ, but where we deliberately “forget” the permutation of the components by using µ to represent the problem:\nfind µ ∈M+(RP ), the set of probability measures over RP s.t. ∫ fn(θ) µ(dθ) = E[φn(x)], n = 1, . . . , N\nµ is K-atomic (i.e. sum of K deltas). (6)\nIf the true model parameters [θ∗k] K k=1 can be identified by the N observed moments up to permu-\ntation, then the measure µ∗(θ) = ∑K k=1 πkδ(θ − θ∗k) solving Problem 6 is also unique.\nPolymom solves Problem 6 in two steps:\n1. Moment completion (Section 3): We show that Problem 6 over the measure µ can be relaxed to an SDP over a certain (parameter) moment matrix Mr(y) whose optimal solution is\nMr(y ∗) = ∑K k=1 πkvr(θ ∗ k)vr(θ ∗ k) >, where vr(θ ∗ k) is the vector of all monomials of degree at most r.\n2. Solution extraction (Section 4): We then take Mr(y) and construct a series of generalized eigendecomposition problems, whose eigenvalues yield [θ∗k] K k=1.\nRemark. From this point on, distributions and moments refer to µ∗ which is over parameters, not over the data. All the structure about the data is captured in the moment conditions (4)."
    }, {
      "heading" : "3 Moment completion",
      "text" : "The first step is to reformulate Problem 6 as an instance of the Generalized Moment Problem (GMP) introduced by [33]. A reference on the GMP, algorithms for solving GMPs, and its various extensions is [34]. We start by observing that Problem 6 only depends on the integrals of monomials under the measure µ: for example, if fn(θ) = 2θ 3 1 − θ21θ2, then we only need to know the integrals\nover the constituent monomials (y3,0 := ∫ θ31µ(dθ) and y2,1 := ∫ θ21θ2µ(dθ)) in order to evaluate the integral over fn. This suggests that we can optimize over the (parameter) moment sequence y = (yα)α∈NP , rather than the measure µ itself. We say that the moment sequence y has a representing measure µ if yα = ∫ θα µ(dθ) for all α, but we do not assume that such a µ exists. The Riesz linear functional Ly : R[θ]→ R is defined to be the linear map such that Ly(θα) := yα and Ly(1) = 1. For example, Ly(2θ31−θ21θ2+3) = 2y3,0−y2,1+3. If y has a representing measure µ, then Ly simply maps polynomials f to integrals of f against µ.\nThe key idea of the GMP approach is to convexify the problem by treating y as free variables and then introduce constraints to guarantee that y has a representing measure. First, let vr(θ) := [θα : |α| ≤ r] ∈ R[θ]s(r) be the vector of all s(r) monomials of degree no greater than r. Then, define the truncated moment matrix as\nMr(y) := Ly(vr(θ)vr(θ) T),\nwhere the linear functional Ly is applied elementwise (see Example 3.1 below). If y has a representing measure µ, then Mr(y) is simply a (positive) integral over rank 1 matrices vr(θ)vr(θ) T with respect to µ, so necessarily Mr(y) 0 holds. Furthermore, by Theorem 1 [16], for y to have a K-atomic representing measure, it is sufficient that rank(Mr(y)) = rank(Mr−1(y)) = K. So Problem 6 is equivalent to\nfind y ∈ RN (or equivalently, find M(y)) s.t. ∑ α anαyα = E[φn(x)], n = 1, . . . , N\nMr(y) 0, y0 = 1 rank(Mr(y)) = K and rank(Mr−1(y)) = K.\n(7)\nUnfortunately, the rank constraints in Problem 7 are not tractable. We use the following relaxation to obtain our final (convex) optimization problem\nminimize y tr(CMr(y))\ns.t. ∑\nα anαyα = E[φn(x)], n = 1, . . . , N Mr(y) 0, y0 = 1\n(8)\nwhere C 0 is a chosen scaling matrix. A common choice is C = Is(r) corresponding to minimizing the nuclear norm of the moment matrix, the usual convex relaxation for rank. Appendix C discusses some other choices of C and more theory on Problem 8. However, for special cases like three-view mixture models, mixture of linear regressions, etc. Problem 7 can also be solved with basic linear algebra, and there is no need to solve Problem 8 (see Section 5).\nExample 3.1 (moment matrix for a 1-dimensional Gaussian mixture). Recall that the parameters θ = [ξ, σ2] are the mean and variance of a one dimensional Gaussian. Let us choose the monomials v2(θ) = [1, ξ, ξ\n2, σ2]. Step 4 for Figure 1 shows the moment matrix when using r = 2. The moment matrix for r = 2 is then:\nMr=2(y) =\n \n1 ξ ξ2 σ2 ξ3 ξc\n1 y0,0 y1,0 y2,0 y0,1 y3,0 y1,1 ξ y1,0 y2,0 y3,0 y1,1 y4,0 y2,1 ξ2 y2,0 y3,0 y4,0 y2,1 y5,0 y3,1 c y0,1 y1,1 y2,1 y0,2 y3,1 y1,2 ξ3 y3,0 y4,0 y5,0 y3,1 y6,0 y4,1 ξc y1,1 y2,1 y3,1 y1,2 y4,1 y2,2  \n(9)\nEach row and column of the moment matrix is labeled with a monomial and entry (i, j) is subscripted by the product of the monomials in row i and column j. For φ2(x) = x\n2, we have f2(θ) = ξ\n2 + c, which leads to the linear constraint y2,0 + y0,1 − E[x2] = 0. For φ3(x) = x3, f3(θ) = ξ 3 + 3ξc, leading to the constraint y3,0 + 3y1,1 − E[x3] = 0.\nRelated work. Readers familiar with the sum of squares and polynomial optimization literature [32, 36, 46, 45] will note that Problem 8 is similar to the SDP relaxation of a polynomial optimization problem. However, in typical polynomial optimization, we are only interested in solutions θ∗ that actually satisfy the given constraints, whereas here we are interested in K solutions [θ∗k] K k=1, whose mixture satisfies constraints corresponding to the moment conditions (4). Within machine learning, generalized PCA has been formulated as a moment problem [44] and the Hankel matrix (basically the moment matrix) has been used to learn weighted automata [8]. While similar tools are used, the conceptual approach and the problems considered are different. For example, the moment matrix of this paper consists of unknown moments of the model parameters, whereas exisiting works considered moments of the data that are always directly observable.\nConstraints. Constraints such as non-negativity (for parameters which represent probabilities or variances) and parameter tying [29] are quite common in graphical models and are not easily addressed with existing method of moments approaches. The GMP framework allows us to incorporate some constraints using localizing matrices [15]. Consider the case of a 2D mixture of Gaussians where the mean parameters ξ1, ξ2 lies on the parabola ξ1 − ξ22 = 0 for all components. In this case, we just need to add constraints to Problem 8: y(1,0)+β − y(0,2)+β = 0 for all β ∈ N2 up to degree |β| ≤ 2r − 2. Thus, we can handle constraints during the estimation procedure rather than projecting back onto the constraint set as a post-processing step. This is necessary for models that only become identifiable by the observed moments after constraints are taken into account. By incorporating these constraints into parameter estimation, we can possibly identify the model parameters with fewer moments. We describe this method and its learning implications in Appendix D.1.\nGuarantees and statistical efficiency. In some circumstances, e.g. in three-view mixture models or the mixture of linear regressions, the constraints fully determine the moment matrix; we consider these cases in Section 5 and Appendix A. While there are no general guarantee on\nProblem 8, the flat extension theorem tells us when the moment matrix corresponds to a unique solution (more discussions in Appendix C):\nTheorem 1 (Flat extension theorem [16]). Let y be the solution to Problem 8 for a particular r. If Mr(y) 0 and rank(Mr−1(y)) = rank(Mr(y)) then y is the optimal solution to Problem 7 for K = rank(Mr(y)) and there exists a unique K-atomic supporting measure µ of Mr(y).\nRecovering Mr(y) is linearly dependent on small perturbations of the input [21], suggesting that the method has polynomial sample complexity for most models where the moments concentrate at a polynomially rate. In Appendix D, we discuss a few other important considerations like noise robustness, making Problem 8 more statistically efficient, and some open problems."
    }, {
      "heading" : "4 Solution extraction",
      "text" : "Having completed the (parameter) moment matrix Mr(y) (Section 3), we now turn to the problem of extracting the model parameters [θ∗k] K k=1. The solution extraction method we present is based on ideas from solving multivariate polynomial systems where the solutions are eigenvalues of certain multiplication matrices [48, 39, 13, 49].1 The main advantage of the solution extraction view is that higher-order moments and structure in parameters are handled in the framework without model-specific effort.\nRecall that the true moment matrix is\nMr(y ∗) =\nK∑\nk=1\nπkv(θ ∗ k)v(θ ∗ k) T ,\nwhere v(θ) := [θα1 , . . . ,θαs(r) ] ∈ R[θ]s(r) contains all the monomials up to degree r. We use θ = [θ1, . . . , θP ] for variables and [θ ∗ k] K k=1 for the true solutions to these variables (note the boldface). For example, θ∗k,p := (θ ∗ k)p denotes the p\nth value of the kth component, which corresponds to a solution for the variable θp. Typically, s(r) K,P and the elements of v(θ) are arranged in a degree ordering so that ||αi||1 ≤ ||αj ||1 for i ≤ j. We can also write Mr(y∗) = VPV>, where V := [v(θ∗1), . . . ,v(θ ∗ K)] ∈ Rs(r)×K is the canonical basis and P := diag(π1, . . . , πK) contains the mixing proportions. At the high level, we want to factorize Mr(y ∗) to get V, however we cannot simply eigen-decompose Mr(y ∗) since V is not orthogonal. To overcome this challenge, we will exploit the internal structure of V to construct several other matrices that share the same factors and perform simultaneous diagonalization.\nSpecifically, let V[β1; . . . ;βK ] ∈ RK×K be a sub-matrix of V with only the rows corresponding to monomials with exponents β1, . . . ,βK ∈ NP . Typically, β1, . . . ,βK are just the first K monomials in v. Now consider the exponent γp ∈ NP which is 1 in position p and 0 elsewhere, corresponding to the monomial θγp = θp. The key property of the canonical basis is that multiplying each column k by a monomial θ∗k,p just performs a “shift” to another set of rows:\nV[β1; . . . ;βK ] Dp = V [ β1 + γp; . . . ;βK + γp ] , where Dp := diag(θ ∗ 1,p, . . . , θ ∗ K,p). (10)\nNote that Dp contains the p th parameter for all K mixture components.\nExample 4.1 (Shifting the canonical basis). Let θ = [θ1, θ2] and the true solutions be θ ∗ 1 = [2, 3] and θ∗2 = [−2, 5]. To extract the solution for θ1 (which are (θ∗1,1, θ∗2,1)), let β1 = (1, 0),β2 = (1, 1), 1 Dreesen et al. [20] is a short overview and Stetter [49] is a comprehensive treatment including numerical issues.\nand γ1 = (1, 0).\nV =\n  v(θ1) v(θ2) 1 1 1 θ1 2 −2 θ2 3 5 θ21 4 4 θ1θ2 6 −10 θ22 9 25 θ21θ2 12 20  \n[ v1 v2 θ1 2 −2 θ1θ2 6 −10 ] ︸ ︷︷ ︸ V[β1;β2] [ 2 0 0 −2 ] ︸ ︷︷ ︸ diag(θ1,1,θ2,1) = [ v1 v2 θ21 4 4 θ21θ2 12 20 ] ︸ ︷︷ ︸ V[β1+γ1;β2+γ1]\n(11)\nWhile (10) reveals the structure of V, we don’t know V. However, we recover its column space U ∈ Rs(r)×K from the moment matrix Mr(y∗), for example with an SVD. Thus, we can relate U and V by a linear transformation: V = UQ, where Q ∈ RK×K is some unknown invertible matrix. (10) can now be rewritten as:\nU[β1; . . . ;βK ]Q Dp = U [ β1 + γp; . . . ;βK + γp ] Q, p = 1, . . . , P, (12)\nwhich is a generalized eigenvalue problem where Dp are the eigenvalues and Q are the eigenvectors. Crucially, the eigenvalues, Dp = diag(θ ∗ 1,p, . . . , θ ∗ K,p) give us solutions to our parameters. Note that for any choice of β1, . . . ,βK and p ∈ [P ], we have generalized eigenvalue problems that share eigenvectors Q, though their eigenvectors Dp may differ. Corresponding eigenvalues (and hence solutions) can be obtained by solving a simultaneous generalized eigenvalue problem, e.g., by using random projections like Algorithm B of [3] or more robust [30] simutaneous diagonalization algorithms [10, 9, 1].\nAlgorithm 1 Basic solution extraction Input: column space basis U ∈ Rs(r)×K , β1, . . . ,βK ∈ NP so that rank (U[β1; . . . ;βK ]) = K Output: Estimated solutions θ̂1, . . . , θ̂K ∈ RP\nfor parameter dimensions p = 1, . . . , P do Bp ← U [ γp + [β1, . . . ,βK ] ]\nγq ← [1p=q]p=1,...,P end for\nFind Q: solve the simultaneous eigenvalue problems: BpQ = U[β1; . . . ;βK ]QDp for p = 1, . . . , P Find θ̂k: Let [q1, . . . ,qK ] := Q for qk ∈ RK×1 θ̂k,p ← ρ\nTBpqk ρTU[β1;...;βK ]qk for p = 1, . . . , P , k = 1, . . . ,K, and arbitrary ρ\nWe describe one approach to solve (12) (Algorithm 1), which is similar to Algorithm B of [3]. The idea is to take P random weighted combinations of the equations (12) and solve the resulting (generalized) eigendecomposition problems. Let R ∈ RP×P be a random matrix whose entries are drawn from N (0, 1). A simple approach to find Q is solving\nU[β1; . . . ;βK ] −1\n( P∑\np=1\nRq,pU [ β1 + γp; . . . ;βK + γp\n] )\nQ = QDq\nfor each q = 1, . . . , P . The resulting eigenvalues can be collected in Λ ∈ RP×K , where Λq,k = Dq,k,k. Note that by definition Λq,k = ∑P p=1Rq,pθ ∗ k,p, so we can simply invert to obtain [θ ∗ 1, . . . ,θ ∗ K ] =\nR−1Λ. Although this simple approach does not have great numerical properties, these eigenvalue problems are solvable if the eigenvalues [λq,1, . . . , λq,K ] are distinct for all q, which happens with probability 1 as long as the parameters θ∗k are different from each other. In Appendix A.1, we show how the tensor decomposition algorithm from [3] can be seen as solving (12) for a particular instantiation of β1, . . .βK ."
    }, {
      "heading" : "5 Applications",
      "text" : "Let us now look at some applications of Polymom. Table 2 presents several models with corresponding observation functions and moment polynomials. It is fairly straightforward to write down observation functions for a given model. The moment polynomials can then be derived by computing expectations under the model, a computation comparable to deriving updates for EM.\n2 hα(ξ, c) = ∑bα/2c i=0 aα,α−2iξ α−2ici and aα,i be the absolute value of the coefficient of the degree i term of the αth (univariate) Hermite polynomial. For example, the first few are h1(ξ, c) = ξ, h2(ξ, c) = ξ2+c, h3(ξ, c) = ξ3+3ξc, h4(ξ, c) = ξ4 + 6ξ2c+ 3c2.\nWe implemented Polymom for several mixture models in Python and the code can be found at https://github.com/sidaw/polymom. A simpler and cleaner demostration of solving a mixture of Gaussian in the noiseless case can be found at https://github.com/sidaw/mompy in the form of an IPython Notebook (extra examples.ipynb). We used CVXOPT to handle the SDP and the random projections algorithm to extract solutions. In Table 3, we show the relative error maxk ||θk − θ∗k||2/||θ∗k||2 averaged over 10 random models of each class.\nGuarantees. In the rest of this section, we will discuss guarantees on parameter recovery for each of these models. In summary, we match many of the existing results in the literature for the mixture of linear regressions and multiview mixtures when K ≤ D. In these case the moment matrix is fully determined by the linear constraints and Problem 8 is just a linear solve. More discussions can be found in Appendix A.2.\nIn addition, we can obtain per-instance guarantees in the following sense. Recall that Polymom involves solving an SDP relaxation and performing solution extraction. If the SDP solution has a flat extension (Theorem 1) at the true number of components K (a checkable assumption), then we have solved the moment completion problem exactly, and since solution extraction always works, we are guaranteed to obtain the true parameters. On the other hand, if the SDP solution has a higher rank K ′ > K, then as a consolation prize, we have found a K ′-mixture model that matches the moments (that we observed) of the true K-mixture model."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We presented an unifying framework for learning many types of mixture models via the method of moments. For example, for the mixture of Gaussians, we can apply the same algorithm to both mixtures in 1D needing higher-order moments [47, 24] and mixtures in high dimensions where lower-order moments suffice [5]. The Generalized Moment Problem [33, 34] and its semidefinite relaxation hierarchies is what gives us the generality, although we rely heavily on the ability of nuclear norm minimization to recover the underlying rank. As a result, while we always obtain parameters satisfying the moment conditions, we do not have formal guarantees on consistent estimation in general, although we do have guarantees for several model families. The second main tool is solution extraction, which characterizes a more general structure of mixture models compared the tensor structure observed by [5, 3]. This view draws connections to the literature on\nsolving polynomial systems, where many techniques might be useful [49, 50, 25]. Finally, through the connections we’ve drawn, it is our hope that Polymom can make the method of moments as turnkey as EM on more latent-variable models, and provide a way to improve the statistical efficiency of method of moments procedures.\nAcknowledgments. This work was supported by a Microsoft Faculty Research Fellowship to the third author and a NSERC PGS-D fellowship for the first author."
    }, {
      "heading" : "A Examples",
      "text" : "In this section, we first describe how undercomplete tensor factorization can be seen as a special case of the solution extraction framework, and elaborate on the mixture of Gaussians, the mixture of linear regressions and the multiview mixture model.\nA.1 Tensor factorization as solution extraction\nExample A.1 (Tensor decomposition as solution extraction). Many latent variable models have been tackled via tensor decomposition [5], and symmetric, undercomplete tensor decomposition can be framed as a solution extraction problem. Suppose we observe the tensor T := ∑K k=1 θ ∗⊗3 k ∈ RP×P×P . We would like to recover the components θ∗k. For us, the inputs are constraints θrθsθt− Trst = 0 for all r, s, t = 1, . . . , P . Choose v(θ) = [1, θ1, . . . , θP , θ 2 1, θ1θ2, . . . , θ 2 P ] = [1,θ, vecs(θ⊗θ)], where vecs : RP×P → RP 2 just flattens the matrix. In the simplest case, suppose P = K and rank(U) = K. Then the fully observed U is\nU =\n  size P\n1 U1 P U2 P 2 U3\n  =\n  terms θ 1 Ly(θ) θ Ly(θ ⊗ θ)\nvecs(θ⊗θ) Ly(vecs(θ ⊗ θ)⊗ θ)\n  (13)\nwhere the linear functional Ly applies elementwise. One choice of basis is just all the variables U[β1; . . . ;βK ] = U2 and the eigenvalue problem we are required to solve is the generalized Hermi-\ntian eigenvalue problem U2QD = (∑P p=1 ηpLy(θpθ ⊗ θ) ) Q. [3] proposed an algorithm that is procedurally identical, where, in their notation Pairs := U2 and Triples(η) := (∑P p=1 ηpLy(θpθ ⊗ θ) ) , and the algorithm proposed needed to solve the eigenvalue problem B(η) = Pairs−1 Triples(η).\nTypically, β1, . . . ,βK are just the first K monomials in v (i.e. the K monomials of the smallest degree).\nUnder this formulation, generalization to the fully-observed overcomplete tensor decomposition case K ≥ D = P is clear if we observe enough moments to have enough basis vectors such that rank(U[β1; . . . ;βK ]) = K:\nProposition A.2. If K ≤ 1+P+P 2+· · ·+P r = P r+1−1P−1 , then solution extraction succeeds if we observe moments up to order 2r+1 and monomials vectors of the true parameters vr(θ1), . . . ,vr(θK) are linearly independent.\nProof. To get the theoretical result, it suffices to consider higher-order moments:\nU =\n[terms vecs(θ ⊗r)\nvecs(θ⊗r) Ly(vecs(θ ⊗r)⊗ vecs(θ⊗r))\nvecs(θ⊗r+1) Ly(vecs(θ ⊗r+1)⊗ vecs(θ⊗r))\n] (14)\nwhere we can take the U[β1; . . . ;βK ] from the top block, and U [ β1 + γq; . . . ;βK + γq ] belongs to the bottom block for all q. So 2r + 1 order moments is needed if K ≤ P r and this result is comparable to [7]. In practice, we would take all moments vecs(θ⊗1), . . . , vecs(θ⊗r+1). We may use lower order moments as well:\nU =\n \nterms vecs(θ⊗1) vecs(θ⊗2) ··· vecs(θ⊗r)\nvecs(θ⊗1) ... vecs(θ⊗2)\n... · · · Ly(vecs(θ⊗l)⊗ vecs(θ⊗m)) · · · vecs(θ⊗r+1) ...\n \n(15)\nwhere the entry of this matrix at block l,m is Ly(vecs(θ ⊗l)⊗ vecs(θ⊗m)) as expected. While this still requires observing 2r + 1th order moments, lower order moments are more accurate and can result in better parameter estimates.\nA.2 Moment completion for specific models\nFor several mixture models, we work out the polynomial constraints, and then discuss the moment completion problem."
    }, {
      "heading" : "A.2.1 Mixture of Linear Regressions",
      "text" : "In Example 2.2, we described the mixture of linear regressions model in 1-dimension with parameters θ∗k = (wk, σ 2 k). Let us now consider the D-dimensional extension: we observe x = [x, υ]\n3 where x := [x1, . . . , xD] is drawn from an unspecified distribution and υ = w ·x+ with ∼ N (0, σ2) for a known σ. The parameters are θ∗k = (wk) for 1 ≤ k ≤ K. Next, we choose observation functions φα,b(x) = x\nαυb for α : 0 ≤ |α| ≤ 3 and 0 ≤ b ≤ 3, with corresponding moment polynomials: fα,b(θ,x) = x αE ∼N (0,σ2) [ (w · x+ )b ] . These polynomials can be expressed in closed form using Hermite polynomials (see Section A.2.2). For example, f0,2(θ,x) = ( (w · x)2 + σ2 ) .\nGiven these observation functions and moment polynomials, and data, the Polymom framework solves the moment completion problem (Problem 7) followed by solution extraction (Section 4) to recover the parameters. Further, we can guarantee that Polymom can recover parameters for this model when K ≤ D by showing that Problem 7 can be solved exactly. Note that while no entry of the moment matrix is directly observed, each observation gives us a linear constraint on the entries of the moment matrix. Let γp ∈ NP be the vector with value 1 at position p and 0 elsewhere, then Ly(fα,1(θ)) = ∑P p=1 E[x α+γp ]yγp , and Ly(fα,2(θ)) = ( E[xα]σ2 + ∑P p,q=1 E[x α+γp+γq ]yγp+γq ) , etc. When K ≤ D, there are enough equations that this system admits an unique solution for y. Note that [11] recover parameters for this model by solving a series of low-rank tensor recovery problems, which ultimately requires the computation of the same moments described above. In contrast, the Polymom framework makes the dependence on moments upfront and takes care of the heavy-lifting in a problem-agnostic manner. Furthermore, we can even obtain parameters outside the regime of [11]: with the above observation functions and moment polynomials, we can recover parameters (with a certificate) ."
    }, {
      "heading" : "A.2.2 Mixture of Gaussians",
      "text" : "We now look at D-dimensional extensions to Example 2.1. Let the data be drawn from Gaussians with diagonal covariance, x|z ∼ N (ξz,diag(cz)). The parameters of this model are θ∗k = (ξk, ck) ∈\n3 We use υ here since y is reserved for the parameter moments.\nR2D. The observable functions are φα(x) := xα, and the moment polynomials are fα(θ) = E[xα] = ∏D d=1 hα[d](ξ[d], c[d]), where hα(ξ, c) = ∑bα/2c i=0 aα,α−2iξ\nα−2ici and aα,i be the absolute value of the coefficient of the degree i term of the αth (univariate) Hermite polynomial. The first few are h1(ξ, c) = ξ, h2(ξ, c) = ξ 2 + c, h3(ξ, c) = ξ 3 + 3ξc, h4(ξ, c) = ξ\n4 + 6ξ2c+ 3c2. Using this set of φα and fα, Polymom will attempt to solve the SDP in Problem 8 and recover the parameters. In this case however, the moment conditions are non-trivial and we cannot guarantee recovery of the true parameters. However, Polymom is guaranteed to recover parameters that match the moments and that minimizes nuclear norm.\nThis full covariance case poses no conceptual trouble for Polymom. In the case of full covariance, Isserlis theorem (or Wicks theorem) allows us to derive these polynomials and [53] provides an algorithm for computing these polynomials. Toeplitz covariance or other structured covariances with parameter sharing or constraints are also conceptually handled under Polymom.\nWe can modify this model by introducing constraints: consider the case of 2D mixture where the mean parameters for all components lies on a parabola ξ1 − ξ22 = 0. In this case, we just need to add constraints to Problem 8: y(1,0)+β − y(0,2)+β = 0 for all β ∈ N2 up to degree |β| ≤ 2r− 2.\nBy incorporating these contraints at estimation time, we can possibly identify the model parameters with less moments. See Section D for more details."
    }, {
      "heading" : "A.2.3 Mixture of Binomials",
      "text" : "We include a quick example on the mixture of binomials in 1 dimension to illustrate how Polymom can be applied to a discrete model. In this model, x ∈ N and 0 ≤ x ≤ m and each component is a binomial distribution for m trials each with probabiliy p of success. The probability mass function for the entire mixture model is p(x) = ∑K k=1 πk ( m x ) pxk(1 − pk)m−x. There are only K scalar parameters p1, . . . , pK and the observation function is just the empirical probabilities φi(x) = 1x=i for x, i ∈ N, 0 ≤ x, i ≤ n, with corresponding polynomials fi(p) = ( m i ) pi(1− p)m−i, which can be expanded to become linear constraints in Problem 8."
    }, {
      "heading" : "A.2.4 Multiview Mixtures",
      "text" : "Here we consider the three-view mixture model which has been well studied in [5, section 3.3]. We will show that we can solve the model without explicit whitening, a transformation that has been shown to introduce noise[31]. The model is a mixture of three conditionally independent arbitrary distributions parameterized by their conditional means: we have z ∼ Multinomialπ,xl|z ∼ pl(ξ(l)z ) where pl(ξ (l) z ) is such that Exl|z[xl] = ξ. The parameters are θk = [ξ (1), ξ(2), ξ(3)]. Using the observation functions φ = [x(1), x(2), x(3), x(1) ⊗ x(2), . . . , x(1) ⊗ x(2) ⊗ x(3)], we have the following moment polynomials, f = [ξ(1), ξ(2), ξ(3), ξ(1) ⊗ ξ(2), . . . , ξ(1) ⊗ ξ(2) ⊗ ξ3].\nThe multiview mixture model is another model for which we can guarantee parameter recovery when K ≤ D. To prove this is the case, we will again show that Problem 8 can be solved exactly. It suffices to consider just the first P columns of the moment matrix M2, which are almost directly observable. As before, vecs(·) just flattens a matrix into a vector.\nMT2 =\n  ξ1 ξ2 ξ3 vecs(ξ1⊗ξ2) vecs(ξ1⊗ξ3) vecs(ξ2⊗ξ3) ξ1 Z2,0,0 Y1,1,0 Y1,0,1 Z2,1,0 Z2,0,1 Y1,1,1 ξ2 Y1,1,0 Z0,2,0 Y0,1,1 Z1,2,0 Y1,1,1 Z0,2,1 ξ3 Y1,0,1 Y0,1,1 Z0,0,2 Y1,1,1 Z1,0,2 Z0,1,2   (16)\nwhere Yα1,α3,α3 and Zα1,α3,α3 are both equal to Ly(ξ ⊗α1 1 ⊗ξ⊗α22 ⊗ξ⊗α33 ), but are used to respectively denote observed and unknown variables. However, this equation is only partially true as both sides contain the same set of values but the precise arrangements depends on where the minor matrix\nappears in the moment matrix. We ignore this problem as it should be clear from the row and column labels. In the undercomplete case, it is assumed that rank(U) = K ≤ min(P1, P2, P3), thus we can easily complete this matrix using simple linear algebra in the exact case by repeatedly applying Lemma A.3 below. Generally, we may try to complete the moment matrix by solving Problem 8 from these partial observations, provided that optimizing with the nuclear norm recovers the true rank.\nLemma A.3 (low rank completion of missing corner). For any matrix Γ = [ A B C X ] with a missing block X, where rank(Γ) = rank(A) = rank(B) = K and A ∈ RK×K , X = CA−1B uniquely completes Γ. Proof. Because A contains the entire K elements basis, there exists unique Y,Z ∈ RK×K so that B = AY and C = ZA. Similarly, X = ZB = CA −1 B."
    }, {
      "heading" : "B Separability",
      "text" : "For conditional models, the coefficients of the moment polynomials can depend on the data but such dependence can sometimes break the process of converting from component moment constraints to mixture moment constraints. In this section, we define separability, which is a sufficient condition on what dependence is allowed under Polymom and then we give some counterexamples.\nConsider a mixture of linear regressions [54, 11], where the parameters θk = (wk, σ 2 k) are the slope and noise variance for each component k. Then each data point x = [x, y] is drawn from component k by sampling x from an unknown distribution independent of k and setting y = wkx+ , where ∼ N (0, σ2k). If we take observation function φb,c(x) = xbyc, then the corresponding fb,c(θ) depends on the unknown distribution of x: for example, f1,2(θ) = E[x3]w + E[x]σ2. In contrast, for the mixture of Gaussians, we had f2(θ) = µ\n2 + σ2, which only depends on the parameters. However, not all is lost, since the key thing is that f1,2(θ) depends only on the distribution of x, which is independent of the component k and furthermore can be estimated from data. More generally, we allow fn to depend on x but in a restricted way. We say that fn(θ,x) is separable if E[fn(θ,x)] does not depend on the parameters [θk]Kk=1 of the mixture generating x. In other words,\nE[φn(x)] = E[fn(θ,x)] where for all k : E[fn(θ,x) | z = k] = E[fn(θ,x)] ∈ R[θ]. (17)\nIn this case, we can define fn(θ) := E[fn(θ,x)], and (4) is still valid. For the mixture of linear regressions, we would define fb,c(θ,x) = x\nb E ∼N (0,σ2)[(wx+ )c]. In this more general setup, the approximate moment equations on T data points is 1T ∑T t=1[fn(θ,xt)] = 1 T ∑T t=1 φn(xt).\nAn example of non-separability is a mixture of linear regressions where the variance is not a parameter and is different across mixture components: θ = (w) and x = (x, y). Recall that E[xy2] = ∑K k=1 πk(E[x3]w2k + E[x]σ2k), but E[x3]w2k + E[x]σ2k cannot be written as E[fn(wk,x)] for any fn, since it depends on σ 2 k. Thus, this example falls outside our framework. In the simplest case, we can make fn(w,x) separable by introducing σk as a parameter, but this is not always possible if the noise distribution is unknown or if σk(x) depends on x. For example, if we have heteroskedastic noise, E[xa(y−w ·x)] = 0 are valid moment constraints for individual components, but it is not clear how to convert this to the mixture case."
    }, {
      "heading" : "C Theory of the moment completion problem",
      "text" : "For solution extraction, we assumed that moments of all monomials are observed but for many models only polynomials of parameters can be estimated from the data. For example, in a Gaussian\nmixture the 2nd moment observable function φ(x) = ξ2 +c is a polynomial, but solution extraction requires moments of monomials like ξ2 and c. Furthermore, we assumed in Section 4 that there exists underlying true parameters [θ∗k] K k=1 while an arbitrary moment sequence of the parameters y and its corresponding moment matrix M(y) may not correspond to any parameters (i.e. no representing measure). In Section 5, we showed how moment completion can be done with just linear algebra for multiview models, and we now focus on the harder case of having to solve the SDP Problem 8.\nWhile we do not have a complete answer since the rank constrained Problem 7 cannot be solved, we point to the relevant literature and give some sufficient conditions for solution extraction and sufficient conditions for parameter recovery."
    }, {
      "heading" : "C.1 Conditions for solution extraction",
      "text" : "In Section 4, we showed that simple conditions based only on the column space basis is sufficient for solution extraction to be successful. However, to further investigate consistency and noise, we need to address a few more important issues. First consider the noiseless setting, we may not have enough moment contraints to guarantee a unique solution (identifiability). Even if we assume that we have enough constraints for identifying a K mixture, we still do not know if solving the relaxed Problem 8 that relaxed the rank = K constraint can recover the true parameters. Second, under noise, there may not exist a rank K basis of the moment matrix and even when a rank K basis exists, it may not correspond to any true parameters.\nIn the case when some moment matching parameters can be extracted, the moment matrix satisfies the flat extension condition, which is the same as conditions in Section 4 where “Bp := U [ γp + [β1, . . . ,βK ] ] is observed” and U[β1; . . . ;βK ] is a column space basis of Mr(y). Let the highest degree monomial of U[β1; . . . ;βK ] be of degree r− 1 = deg(θβK ) = |βK |, and the highest degree monomial of Bp := U [ γp + [β1, . . . ,βK ] ] be of degree r = ∣∣γp + βK ∣∣ = |deg(βK)| + 1. Since U[β1; . . . ;βK ] is a basis of col(Mr(y))\nrank (Mr−1(y)) = rank (U[β1; . . . ;βK ]) = K (18)\n= rank (Mr(y)) ≥ rank ( U [ γp + [β1, . . . ,βK ] ]) . (19)\nIf we got this basis from the moment matrix, then we say that the moment matrix Mr−1(y) corresponding to U[β1; . . . ;βK ] has a flat extension, because Mr−1(y) can be extended to a moment matrix Mr(y) with higher degree monomials without an increase in rank. The concept of flat extension and its consequences are of central importance for the truncated moment problem, which is quite relevant to our problem and studied by [16, 17, 15, 18]. Next, we reproduce the simplest flat extension theorem:\nTheorem C.1 ([16]: flat extension theorem). Suppose Mr−1(y) 0 and there exists Mr(y) so that rank(Mr(y)) = rank(Mr−1(y)) (i.e. a flat extension), then there exists an unique rank(Mr(y))atomic representing measure µ of Mr(y).\nHere the first column of Mr(y) contains every monomial of degree up to r so that deg(vr(θ)) = r. However, several generalizations of the flat extension theorem are also useful for estimation of mixture models where sparse monomials are handled [35, 37] or where constraints are handled [18].\nThe conceptual importance is that Theorem C.1 allows us to work with just the moment matrix satisfying constraints from possibly noisy observations, without assuming the moment matrix is generated by some true parameters. Of course, it also provides a checkable criterion for when solutions can be extracted [40]. We still do not know if solving Problem 8 provides a flat extension in a finite number of steps. [42, 43, 41] investigated this issue very recently and showed that linear optimization over the cone of moments have finite convergence under generic conditions (theorem 4.2 of [41]).\nStill, our issue is not fully resolved as representing measures under linear constraints may not be unique, and as a result even a flat moment matrix may not correspond to the true parameters. For parameter fitting, we’d like to find the solution with minimal rank or otherwise optimal in some way. We explore this issue next but unfortunately we can only give some partial answers.\nProposition C.2 (existence of C). In the noiseless setting, there exist C so that minimizing C •Mr(y)) = c · y will give the right solution. Proof. Let Mr(y) = UΣU\nT be the SVD with U ∈ Rs(r)×K and Σ ∈ RK×K . Let U⊥ ∈ Rs(r)×(s(r)−K) be the orthogonal compliment of U, then any C = U⊥DUT⊥ suffices and D ∈ R(s(r)−K)×(s(r)−K) is an arbitrary diagonal matrix with positive diagonal elements.\nThe convex iteration algorithm [19] is one way to reduce rank that sometimes works for us empirically, where if the convex iteration algorithm converges to 0, then the moment matrix has rank K."
    }, {
      "heading" : "D Extensions",
      "text" : ""
    }, {
      "heading" : "D.1 Constraints on parameters",
      "text" : "Constraints on parameters is a common and important consideration in applications. While constraints can often be addressed in maximum likelihood or maximum a prioterior learning using EM [29, see shared parameters], it is less clear how to address constraints under the tensor decomposition approach because of its reliance on special tensor structure and it is well-known that MME generally can give us parameters outside of the parameter space even in the well-specified case.\nExample D.1. Examples of constraints on parameters Some parameters are known: Gaussian with sparse covariance matrix where we already know that some dimensions are uncorrelated; to solve a substitution cipher using an HMM, the transitions matrix is a language model that is given.\nParameters are tied: transitions in an HMM might only depend on the relatively difference between states if the states are ordered i.e. the transition matrix is Toeplitz.\nPolytope constraints: some of the parameters might be probabilities (e.g. multinomial distribution):\nθ = [π1, . . . , πP , ξ1, . . .], πp ≥ 0, P∑\np=1\nπp = 1\nSemialgebraic constraints: For some polynomial g ∈ R[θ], gi(θ∗k) ≥ 0, i = 1, . . . , I. This includes discrete sets θi ∈ {0, 1} and ellipsoids.\nThe obvious attempt is to project to the feasible set after computing an unconstrained estimation with MME. But this approach has several serious issues. First, some constrained models are only identifiable after the constraints are taken into account, which happens when the model has a lot of parameters and we cannot observe correspondingly more moments. In this case, unconstrained estimation is useful only if we can characterize the entire subset of the parameters space satisfying moment conditions, which is generally not possible in the tensor decomposition approach. Second, we need to determine what projection to use. In the case of two equal parameters, if one estimate is much more noisy than the other, it can be better to just ignore the more noisy estimate than to project under the wrong metric (see Example D.3). Third and strangely, even in the case when the first two issues are handled, it was observed by [12] for probablities parameters, that clipping to 0 is empirically inferior compared to heuristics like taking the absolute value, which is not a projection.\nUnder the Polymom formulation, we can take constraints into account during estimation. The technique of localizing matrix [15] in moment theory allows us to deal with semialgebraic constraints. Of course, the computational complexity increases if the constraints are themselves complicated and high degree. Next, we define the localization matrix, give an example, and then give a constrained version of the flat extension theorem.\nExample D.2 (localizing matrix for an inequality constraint). Let θ = [c, ξ], so that θα = cα1ξα2 and Ly(θ α) = yα, and chose the monomials v2(θ) = [1, c, ξ, c\n2, cξ, ξ2]. Suppose that c is the variance and we want to have constraint that c− 1 ≥ 0, then\nM1((c− 1)y) =\n \n1 c ξ\n1 y1,0 − 1 y2,0 − y1,0 y1,1 − y1,0 c y2,0 − y1,0 y3,0 − y2,0 y2,1 − y2,0 ξ y1,1 − y0,1 y2,1 − y1,1 y1,2 − y0,2\n  (20)\nit is clear that a necessary condition for extracted solutions to satisfy the constraint c − 1 ≥ 0 is that M1((c− 1)y) 0 since fTM1((c− 1)y)f = Ly(f(θ)2(c− 1)) ≥ 0."
    }, {
      "heading" : "D.2 Noise and statistical efficiency",
      "text" : "In the presense of noise Problem 8 may not be feasible and even if it was, it may not be ideal to exactly match noisy moments. Furthermore, it is argued that higher order moments are too noisy to be useful, but there are also more of them and they do contain more information about the model parameters as long as we can model how noisy they are. We consider the problem with slack and a weighting matrix W 0 ∈ RN×N modelling how much noise is present in each constraint function. This effect is fairly well-known, and here is a very simple example which shows that even much more noisy measurements can improve efficiency.\nExample D.3 (efficient estimation). Suppose X ∼ N ([ξ, ξ],diag[σ2, cσ2]) and we would like to estimate the mean parameter ξ by matching moments. Any estimators of the form ξ̂ = 1T ∑T t=1(γxt,1+ (1− γ)xt,2) are consistent and has risk\nR = E [ (ξ̂ − ξ)2 ] = E   ( γ T∑\nt=1\nxt,2 − γξ + (1− γ) T∑\nt=1\nxt,2 − (1− γ)ξ )2  (21)\n= E  γ2 ( ξ − 1\nT\nT∑\nt=1\nxt,1\n)2 + (1− γ)2 ( ξ − 1\nT\nT∑\nt=1\nxt,2\n)2  (22)\n= 1\nT (γ2σ2 + (1− γ)2cσ2) (23)\nunder the squared loss, and the efficient estimator would have γ = c−1c and a risk of σ2 T c2−c+1 c2 . For c = 10, the risk for efficient estimation is 0.91σ 2\nT whereas for γ = 0.5, the risk is 2.75 σ2 T .\nThis example suggests that a weighting matrix W has the potential to make use of higher order moments and also give better estimates. Consider\nminimize g,y C •M(y) s.t. gn = ∑ α anαyα − E[φn(x)\ngTWg ≤ M(y) 0.\n(24)\nIn the simplest case when W = IN , and = 0, Problem 24 is the same as Problem 8.\nminimize g,y C •M(y) s.t. gn = ∑ α anαyα − E[φn(x)]\nW • F ≤ M(y) 0[ 1 gT\ng F\n] 0\n(25)\nA good weighting matrix W should put more weights on moment conditions that can be estimated more precisely. The asymptotically efficient weighting matrix suggested by the Generalized Method of Moments [23] is\nW −1 = E [ g([θk] K k=1,x)g([θk] K k=1,x) T ] ≈ 1 T T∑\nt=1\ng([θk] K k=1,x)g([θk] K k=1,x) T (26)\nTheorem 2 (Gen.MM is asymptotically efficient [23]). Let gn(θ,X) := ∑ k fn(θk) − hn(X) so that E[hn(X)] = E[φn(x)]. Let W −1 = E[g(θ,X)g(θ,X)T] ≈ 1T ∑T t=1 g(θ,Xt)g(θ,Xt)\nT Iterative Gen.MM is efficient with this weighting matrix W."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "Mixture modeling is a general technique for making any simple model more expressive<lb>through weighted combination. This generality and simplicity in part explains the<lb>success of the Expectation Maximization (EM) algorithm, in which updates are easy<lb>to derive for a wide class of mixture models. However, the likelihood of a mixture<lb>model is non-convex, so EM has no known global convergence guarantees. Recently,<lb>method of moments approaches offer global guarantees for some mixture models, but<lb>they do not extend easily to the range of mixture models that exist. In this work, we<lb>present Polymom, an unifying framework based on method of moments in which es-<lb>timation procedures are easily derivable, just as in EM. Polymom is applicable when<lb>the moments of a single mixture component are polynomials of the parameters. Our<lb>key observation is that the moments of the mixture model are a mixture of these<lb>polynomials, which allows us to cast estimation as a Generalized Moment Problem.<lb>We solve its relaxations using semidefinite optimization, and then extract parame-<lb>ters using ideas from computer algebra. This framework allows us to draw insights<lb>and apply tools from convex optimization, computer algebra and the theory of mo-<lb>ments to study problems in statistical estimation. Simulations show good empirical<lb>performance on several models.",
    "creator" : "LaTeX with hyperref package"
  }
}