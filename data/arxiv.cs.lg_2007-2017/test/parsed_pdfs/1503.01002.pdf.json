{
  "name" : "1503.01002.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Weiran Wang" ],
    "emails" : [ "weiranwang@ttic.edu", "canyilu@gmail.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n50 3.\n01 00\n2v 1\n[ cs\n.L G\n] 3\nM ar\nWe provide a simple and efficient algorithm for computing the Euclidean projection of a point onto\nthe capped simplex, formally defined as\nmin x∈RD\n1 2 ‖x− y‖2 s.t. x⊤1 = s, 0 ≤ x ≤ 1,\ntogether with an elementary proof. Both the MATLAB and C++ implementations of the proposed algorithm can be downloaded at https://eng.ucmerced.edu/people/wwang5."
    }, {
      "heading" : "1 The Problem",
      "text" : "In this report, we consider the following optimization problem\nmin x∈RD\n1 2 ‖x− y‖2 (1a)\ns.t. x⊤1 = s (1b)\n0 ≤ x ≤ 1, (1c)\nwhere s ∈ [0, D] is a parameter of the problem, 0 and 1 are vectors of 0’s and 1’s respectively, and ≤ means elementwise comparison. The feasible set of this problem is the intersection of the unit cube and a hyperplane with normal 1. Alternatively, the feasible set is the simplex {x : x ≥ 0, x⊤1 = s} with an additional capping constraints x ≤ 1, so we call it the capped simplex. Problem 1 is a quadratic program and the objective function is strictly convex, so there is a unique solution which we denote by x = [x1, . . . , xD] ⊤ with a slight abuse of notation.\nRemark 1.1. This problem is a slight generalization of the projection onto the probability simplex (see Duchi et al., 2008; Wang and Carreira-Perpiñán, 2013 and the references therein), which is a special case of (1) by setting s = 1 and can be solved exactly with O(D logD) time complexity. An elementary proof of the corresponding algorithm can be found in Wang and Carreira-Perpiñán (2013) and the cost mainly comes from sorting the dimensions of y. Our solution to (1) in this report is derived using a similar idea.\nRemark 1.2. The constraint 0 ≤ x ≤ 1 in (1) can be generalized to 0 ≤ x ≤ t1 where t is any positive number. We only need to solve the following instance of (1):\nmin x̂\n1 2 ‖x̂− y/t‖ 2 , s.t. x̂⊤1 = s/t, 0 ≤ x̂ ≤ 1,\nand then scale its solution x̂ by t to obtain the solution of the original problem."
    }, {
      "heading" : "2 The algorithm",
      "text" : "We provide an O(D2) algorithm for solving (1) in Algorithm 1.\nAlgorithm 1 Euclidean projection of a vector onto the section of cube.\nInput: y ∈ RD is sorted in ascending order: y1 ≤ y2 ≤ · · · ≤ yD. 1: Set y0 = −∞ and yD+1 = ∞, compute partial sums T0 = 0, and Tk = ∑k j=1 yk, k = 1, . . . , D.\n2: for a = 0, 1, . . . , D do 3: if (s == D − a) && (ya+1 − ya ≥ 1) then 4: Set b=a. 5: break 6: end if 7: for b = a+ 1, . . . , D do 8: Compute γ = s+b−n+Ta−Tb\nb−a .\n9: if (ya + γ ≤ 0) && (ya+1 + γ > 0) && (yb + γ < 1) && (yb+1 ≥ 1) then 10: break 11: end if 12: end for 13: end for Output: x = [0, . . . , 0, ya+1 + γ, . . . , yb + γ, 1, . . . , 1]."
    }, {
      "heading" : "3 The proof",
      "text" : "As mentioned earlier, (1) has a unique solution which is characterized by its KKT system (Nocedal and Wright, 2006). The Lagrangian function of the problem is\nL(x,α,β, γ) = 1\n2 ‖x− y‖\n2 −α⊤x− β⊤(1− x)− γ(1⊤x− s)\nwhere α = [α1, . . . , αD] ⊤ and β = [β1, . . . , βD] ⊤ are the Lagrange multipliers for the inequality constraints x ≥ 0 and 1 − x ≥ 0 respectively, and γ is the Lagrange multiplier for the equality constraint. At the optimal solution x the following KKT conditions hold:\nxi − yi − αi + βi − γ = 0, i = 1, . . . , D (2a)\nxi ≥ 0, i = 1, . . . , D (2b)\nxi ≤ 1, i = 1, . . . , D (2c)\nαi ≥ 0, i = 1, . . . , D (2d)\nβi ≥ 0, i = 1, . . . , D (2e) ∑D\ni=1 xi = s, (2f)\nαixi = 0, i = 1, . . . , D (2g)\nβi(1− xi) = 0, i = 1, . . . , D, (2h)\nwhere (2g) and (2g) are complementary slackness (CS) conditions. Without loss of generality, we assume the components of the optimal solution x are in ascending order:\n0 = x1 = · · · = xa < xa+1 ≤ · · · ≤ xb < xb+1 = . . . xD = 1, (3)\nwhere a is the number of 0’s in the solution while D − b is the number of 1’s in the solution. The valid ranges for (a, b) are 0 ≤ a ≤ D and a ≤ b ≤ D. The KKT conditions can be simplified for different set of dimensions of the solution:\n(i) For i = 1, . . . , a, the CS condition (2h) indicates βi = 0, and thus\n0 = xi = yi + αi + γ ≥ yi + γ, (4)\nwhere the last inequality uses the fact that αi ≥ 0.\n(ii) For j = b+ 1, . . . , D, the CS condition (2g) indicates αj = 0, and thus\n1 = xj = yj − βj + γ ≤ yj + γ, (5)\nwhere the last inequality uses the fact that βj ≥ 0.\n(iii) For k = a+ 1, . . . , b, the CS conditions indicate αk = βk = 0, and thus\n0 < xk = yk + γ < 1. (6)\nIt is then clear that for any 1 ≤ i ≤ a, b+ 1 ≤ j ≤ D and a+ 1 ≤ k ≤ b, we have\nyi ≤ −γ < yk < 1− γ ≤ yj . (7)\nIn other words, if the dimensions of y are sorted in ascending order, the corresponding dimensions of the solution x is also in ascending order. Therefore, the first step of our algorithm is to sort dimensions of y into ascending order. And all that is left is to find (a, b), the partition of x into the three segments. The only KKT condition we have not used so far is the sum constraint (2f), which now reduces to\nD∑\ni=1\nxi = a · 0 + b∑\nk=a+1\n(yk + γ) + (n− b) · 1 = s. (8)\nThis means that if we know (a, b) for the solution x, we must have\nγ = s+ b− n−\n∑b k=a+1 yk\nb− a . (9)\nSince there are only (D+1)(D+2)2 possible combinations for the indices (a, b), we could test each combination and compute the hypothesized γ value using (9). With the hypothesized γ, the tests we need for (a, b, γ) to produce the optimal x are the following:\nya + γ ≤ 0, ya+1 + γ > 0, yb + γ < 1, yb+1 + γ ≥ 1. (10)\nIt is easy to verify that the (a, b, γ) combination that passes the above test leads to (x,α,β, γ) that satisfy all the KKT conditions, where {αi} a i=1 and {βj} D j=b+1 can be retrieved using (4) and (5) respectively.\nRemark 3.1. A special case is when a = b, for which (9) is ill-defined. In this case, the optimal solution consists of a 0’s and n − a 1’s. For this to happen, we must have s = D − a and then (10) reduces to ya+1 + γ ≥ 1 and ya + γ ≤ 0, and so ya+1 − ya ≥ 1. Remark 3.2. The reason why the computational complexity of our algorithm is O(D2) for (1) as opposed to O(D logD) for projection onto probability simplex is due to the constraint x ≤ 1. This constraint is automatically satisfied for the projection onto probability simplex problem where s = 1, in which case we only need to figure out a—the number of zero dimensions in the solution.\nRemark 3.3. In view of the previous remark, another way of solving (1) is to alternatively project the estimate onto the (scaled) probability simplex {x : x ≥ 0, x⊤1 = s} and the set {x : x ≤ 1} for which the projection is trivial to compute (we simply threshold the dimensions that are greater than 1 to 1). And yet another approach is to apply the Alternating Direction Method of Multipliers (Boyd et al., 2011), which introduces another copy of the variables x and alternately optimize each copy with simple steps while encouraging the two copies to agree. We note that these approaches are iterative and the number of iterations depends on the desired accuracy. On the contrary, our method finds the exact solution within a fixed number of steps."
    }, {
      "heading" : "4 Experiment",
      "text" : "In this section, we demonstrate the effectiveness of our proposed method in Algorithm 1 for solving (1). We compare our method with another two solvers. The first one is the CVX package (Grant and Boyd,\n2012), a general convex program solver which transforms the problem into a semi-definite program and then applies interior point method. And the second one is the MATLAB command lsqlin for solving constrained linear least squares. We implement Algorithm 1 in both MATLAB and C++ and conduct all experiments in MATLAB (the C++ code is compiled within MATLAB and the mex-file is used).\nAll experiments are run on a PC with an Intel Core 2 Quad CPU Q9550 of frequency 2.83GH and 8GB main memory, under Windows 7 and MATLAB version 8.0. We generate y and s using the MATLAB commands\ny=rand(D,1)-0.5;\ns=round(rand*D);\nand record the running time of each method using tic and toc. We choose the dimension D of y from {50, 100, 500, 1000, 2000, 5000, 10000, 20000, 100000}. For each choice of D, the experiments are repeated 20 times and the average running times are reported for comparison.\nThe results are shown in Table 1. It can be seen that our proposed method is always faster than CVX and lsqlin for different dimensions D of y. And as D increases, the improvement of our method over the others becomes more significant. If D is relatively large, the compared solvers may run out of memory (denoted as ‘−’ in Table 1). These results confirm that it is beneficiary to explore the special structures of our problem rather than using general convex program solvers. Furthermore, the C++ version of our method is by two orders of magnitude faster than the MATLAB version; this improvement is important for projections in very high dimensions."
    } ],
    "references" : [ {
      "title" : "Distributed optimization and statistical learning via the alternating direction method of multipliers",
      "author" : [ "S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "Boyd et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Boyd et al\\.",
      "year" : 2011
    }, {
      "title" : "Efficient projections onto the l1-ball for learning in high dimensions",
      "author" : [ "J. Duchi", "S. Shalev-Shwartz", "Y. Singer", "T. Chandra" ],
      "venue" : "Proc. of the 25th Int. Conf. Machine Learning",
      "citeRegEx" : "Duchi et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Duchi et al\\.",
      "year" : 2008
    }, {
      "title" : "CVX: Matlab software for disciplined convex programming, version 2.0 beta",
      "author" : [ "M. Grant", "S. Boyd" ],
      "venue" : null,
      "citeRegEx" : "Grant and Boyd.,? \\Q2012\\E",
      "shortCiteRegEx" : "Grant and Boyd.",
      "year" : 2012
    }, {
      "title" : "Numerical Optimization. Springer Series in Operations Research and Financial Engineering",
      "author" : [ "J. Nocedal", "S.J. Wright" ],
      "venue" : null,
      "citeRegEx" : "Nocedal and Wright.,? \\Q2006\\E",
      "shortCiteRegEx" : "Nocedal and Wright.",
      "year" : 2006
    }, {
      "title" : "Projection onto the probability simplex: An efficient algorithm with a simple proof, and an application",
      "author" : [ "W. Wang", "M.Á. Carreira-Perpiñán" ],
      "venue" : null,
      "citeRegEx" : "Wang and Carreira.Perpiñán.,? \\Q2013\\E",
      "shortCiteRegEx" : "Wang and Carreira.Perpiñán.",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "This problem is a slight generalization of the projection onto the probability simplex (see Duchi et al., 2008; Wang and Carreira-Perpiñán, 2013 and the references therein), which is a special case of (1) by setting s = 1 and can be solved exactly with O(D logD) time complexity. An elementary proof of the corresponding algorithm can be found in Wang and Carreira-Perpiñán (2013) and the cost mainly comes from sorting the dimensions of y.",
      "startOffset" : 92,
      "endOffset" : 381
    }, {
      "referenceID" : 3,
      "context" : "3 The proof As mentioned earlier, (1) has a unique solution which is characterized by its KKT system (Nocedal and Wright, 2006).",
      "startOffset" : 101,
      "endOffset" : 127
    }, {
      "referenceID" : 0,
      "context" : "And yet another approach is to apply the Alternating Direction Method of Multipliers (Boyd et al., 2011), which introduces another copy of the variables x and alternately optimize each copy with simple steps while encouraging the two copies to agree.",
      "startOffset" : 85,
      "endOffset" : 104
    } ],
    "year" : 2015,
    "abstractText" : "We provide a simple and efficient algorithm for computing the Euclidean projection of a point onto the capped simplex, formally defined as min x∈RD 1 2 ‖x− y‖2 s.t. x1 = s, 0 ≤ x ≤ 1, together with an elementary proof. Both the MATLAB and C++ implementations of the proposed algorithm can be downloaded at https://eng.ucmerced.edu/people/wwang5.",
    "creator" : "LaTeX with hyperref package"
  }
}