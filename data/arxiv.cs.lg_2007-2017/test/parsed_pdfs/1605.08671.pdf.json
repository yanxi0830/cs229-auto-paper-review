{
  "name" : "1605.08671.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "An optimal algorithm for the Thresholding Bandit Problem",
    "authors" : [ "Andrea Locatelli", "Maurilio Gutzeit", "Alexandra Carpentier" ],
    "emails" : [ "ANDREA.LOCATELLI@UNI-POTSDAM.DE", "MGUTZEIT@UNI-POTSDAM.DE", "CARPENTIER@UNI-POTSDAM.DE" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "In this paper we study a specific combinatorial, pure exploration, stochastic bandit setting. More precisely, consider a stochastic bandit setting where each arm has mean µk. The learner can sample sequentially T > 0 samples from the arms and aims at finding as efficiently as possible the set of arms whose means are larger than a threshold τ ∈ R. In this paper, we refer to this setting as the Thresholding Bandit Problem (TBP), which is a specific instance of the combinatorial pure exploration bandit setting introduced in (Chen et al., 2014). A simpler ”one armed” version of this problem is known as the SIGN-ξ problem, see (Chen & Li, 2015).\nThis problem is related to the popular combinatorial pure exploration bandit problem known as the TopM problem where the aim of the learner is to return the set of M arms with highest mean (Bubeck et al., 2013b; Gabillon et al., 2012; Kaufmann et al., 2015; Zhou et al., 2014; Cao et al., 2015) - which is a combinatorial version of the best arm identification problem (Even-Dar et al., 2002; Mannor & Tsitsiklis, 2004; Bubeck et al., 2009; Audibert & Bubeck,\nProceedings of the 33 rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W&CP volume 48. Copyright 2016 by the author(s).\n2010; Gabillon et al., 2012; Jamieson et al., 2014; Karnin et al., 2013; Kaufmann et al., 2015; Chen & Li, 2015). To formulate this link with a simple metaphor, the TopM problem is a ”contest” and the TBP problem is an ”exam”: in the former, the learner wants to select the M arms with highest mean, in the latter the learner wants to select the arms whose means are higher than a certain threshold. We believe that this distinction is important and that in many applications the TBP problem is more relevant than the TopM, as in many domains one has a natural ”efficiency”, or ”correctness” threshold above which one wants to use an option. For instance in industrial applications, one wants to keep a machine if its production’s value is above its functioning costs, in crowd-sourcing one wants to hire a worker as long as its productivity is higher than its wage, etc. In addition to these applications derived from the TopM problem, the TBP problem has applications in dueling bandits and is a natural way to cast the problem of active and discrete level set detection, which is in turn related to the important applications of active classification, and active anomaly detection - we detail this point more in Subsection 3.1.\nAs mentioned previously, the TBP problem is a specific instance of the combinatorial pure exploration bandit framework introduced in (Chen et al., 2014). Without going into the details of the combinatorial pure exploration setting for which the paper (Chen et al., 2014) derives interesting general results, we will summarize what these results imply for the particular TBP and TopM problems, which are specific cases of the combinatorial pure exploration setting. As it is often the case for pure exploration problems, the paper (Chen et al., 2014) distinguishes between two settings:\n• The fixed budget setting where the learner aims, given a fixed budget T , at returning the set of arms that are above the threshold (in the case of TBP) or the set of M best arms (in the case of TopM), with highest possible probability. In this setting, upper and lower bounds are on the probability of making an error when returning the set of arms.\nar X\niv :1\n60 5.\n08 67\n1v 1\n[ st\nat .M\nL ]\n2 7\nM ay\n2 01\n• The fixed confidence setting where the learner aims, given a probability δ of acceptable error, at returning the set of arms that are above the threshold (in the case of TBP) or the set of M best arms (in the case of TopM) with as few pulls of the arms as possible. In this setting, upper and lower bounds are on the number of pulls T that are necessary to return the correct set of arm with probability at least 1− δ.\nThe similarities and dissemblance of these two settings have been discussed in the literature in the case of the TopM problem (in particular in the case M = 1), see (Gabillon et al., 2012; Karnin et al., 2013; Chen et al., 2014). While as explained in (Audibert & Bubeck, 2010; Gabillon et al., 2012), the two settings share similarities in the specific case when additional information about the problem is available to the learner (such as the complexityH defined in Table 1), they are very different in general and results do not transfer from one setting to the other, see (Bubeck et al., 2009; Audibert & Bubeck, 2010; Karnin et al., 2013; Kaufmann et al., 2015). In particular we highlight the following fact: while the fixed confidence setting is relatively well understood in the sense that there are constructions for optimal strategies (Kalyanakrishnan et al., 2012; Jamieson et al., 2014; Karnin et al., 2013; Kaufmann et al., 2015; Chen & Li, 2015), there is an important knowledge gap in the fixed budget setting. In this case, without the knowledge of additional information on the problem such as e.g. the complexity H defined in Table 1, there is a gap between the known upper and lower bounds, see (Audibert & Bubeck, 2010; Gabillon et al., 2012; Karnin et al., 2013; Kaufmann et al., 2015). This knowledge gap is more acute for the general combinatorial exploration bandit problem defined in the paper (Chen et al., 2014) (see their Theorem 3) - and therefore for the TBP problem (where in fact no fixed budget lower bound exists to the best of our knowledge). We summarize in Table 1 the state of the art results for the TBP problem and for the TopM problem with M = 1.\nThe summary of Table 1 highlights that in the fixed budget setting, both for the TopM and the TBP problem, the correct complexity H∗ that should appear in the bound, i.e. what is the problem dependent quantity H∗ such that the upper and lower bounds on the probability of error is of order exp(−n/H∗), is still an open question. In the TopM problem, Table 1 implies that H ≤ H∗ ≤ log(2K)H2. In the TBP problem, Table 1 implies 0 ≤ H∗ ≤ log(2K)H2, since to the best of our knowledge a lower bound for this problem exists only in the case of the fixed confidence setting. Note that although this gap may appear small in particular in the case of the TopM problem as it involves ”only” a log(K) multiplicative factor, it is far from being negligible since the log(K) gap factor acts on a term of order exponential minus T exponentially.\nIn this paper we close, up to constants, the gap in the fixed budget setting for the TBP problem - we prove that H∗ = H . In addition, we also prove that our strategy minimizes at the same time the cumulative regret, and identifies optimally the best arm, provided that the highest mean of the arms is known to the learner. Our findings are summarized in Table 1. In order to do that, we introduce a new algorithm for the TBP problem which is entirely parameter free, and based on an original heuristic. In Section 2, we describe formally the TBP problem, the algorithm, and the results. In Section 3, we describe how our algorithm can be applied to the active detection of discrete level sets, and therefore to the problem of active classification and active anomaly detection. We also describe what are the implications of our results for the TopM problem. Finally Section 4 presents some simulations for evaluating our algorithm with respect to the state of the art competitors. The proofs of all theorems are in Appendix A, as well as additional simulation results."
    }, {
      "heading" : "2. The Thresholding Bandit Problem",
      "text" : ""
    }, {
      "heading" : "2.1. Problem formulation",
      "text" : "Learning setting Let K be the number of arms that the learner can choose from. Each of these arms is characterized by a distribution νk that we assume to be R-subGaussian. Definition (R-sub-Gaussian distribution). Let R > 0. A distribution ν is R-sub-Gaussian if for all t ∈ R we have\nEX∼ν [exp(tX − tE[X])] ≤ exp(R2t2/2). This encompasses various distributions such as bounded distributions or Gaussian distributions of variance R2 for R ∈ R. Such distributions have a finite mean, let µk = EX∼νk [X] be the mean of arm k.\nWe consider the following dynamic game setting which is common in the bandit literature. For any time t ≥ 1, the learner chooses an arm It from A = {1, ...,K}. It receives a noisy reward drawn from the distribution νIt associated to the chosen arm. An adaptive learner bases its decision at time t on the samples observed in the past.\nSet notations Let u ∈ R and A be the finite set of arms. We define Su as the set of arms whose means are over u, that is Su := {k ∈ A, µk ≥ u}. We also define SCu as the complimentary set of Su in A, i.e. SCu = {k ∈ A, µk < u}.\nObjective Let T > 0 (not necessarily known to the learner beforehand) be the horizon of the game, let τ ∈ R be the threshold and ≥ 0 be the precision. We define the (τ, ) thresholding problem as such : after T rounds of the game described above, the goal of the learner is to correctly identify the arms whose means are over or under the threshold τ up to a certain precision , i.e. to correctly discriminate arms that belong to Sτ+ from those in SCτ− . In the rest of the paper, the sentence ”the arm is over the threshold τ” is to be understood as ”the arm’s mean is over the threshold”.\nAfter T rounds of the previously defined game, the learner has to output a set Ŝτ := Ŝτ (T ) ⊂ A of arms and it suffers the following loss:\nL(T ) = I(Sτ+ ∩ ŜCτ 6= ∅ ∨ SCτ− ∩ Ŝτ 6= ∅).\nA good learner minimizes this loss by correctly discriminating arms that are outside of a 2 band around the threshold: arms whose means are smaller than (τ − ) should not belong to the output set Ŝτ , and symmetrically those whose means are bigger than (τ + ) should not belong to ŜCτ . If it manages to do so, the algorithm suffers no loss and otherwise it incurs a loss of 1. For arms that lie inside this 2 strip, mistakes on the other hand bear no cost. If we set to 0 we recover the exact TBP thresholding problem described in the introduction, and the algorithm suffers no loss if it discriminates exactly arms that are over the threshold from those under.\nLet E be the expectation according to the samples collected by an algorithm, its expected loss is:\nE[L(T )] = P(Sτ+ ∩ ŜCτ 6= ∅ ∨ SCτ− ∩ Ŝτ 6= ∅),\ni.e. it is the probability of making a mistake, that is rejecting an arm over (τ + ) or accepting an arm under (τ − ). The lower this probability of error, the better the algorithm, as an oracle strategy would simply rightly classify each arm and suffer an expected loss of 0.\nOur problem is a pure exploration bandit problem, and is in fact, shifting the means by −τ , a specific case of the pure exploration bandit problem considered in (Chen et al., 2014) - namely the specific case where the set of sets of arms that they callM and which is their decision class is the set of all possible set of arms. We will comment more on this later in Subsection 2.4.\nProblem complexity We define ∆τ, i the gap of arm i with respect to τ and as:\n∆i := ∆ τ, i = |µi − τ |+ . (1)\nWe also define the complexity H of the problem as\nH := Hτ, = K∑ i=1 (∆τ, i ) −2. (2)\nWe callH complexity as it is a characterization of the hardness of the problem. A similar quantity was introduce for general combinatorial bandit problems (Chen et al., 2014) and is similar in essence to the complexity introduced for the best arm identification problem, see (Audibert & Bubeck, 2010)."
    }, {
      "heading" : "2.2. A lower bound",
      "text" : "In this section, we exhibit a lower bound for the thresholding problem. More precisely, for any sequence of gaps (dk)k, we define a finite set of problems where the distributions of the arms of these problems correspond to these gaps and are Gaussian of variance 1. We lower bound the largest probability of error among these problems, for the best possible algorithm. Theorem 1. Let K,T ≥ 0. Let for any i ≤ K, di ≥ 0. Let τ ∈ R, > 0.\nFor 0 ≤ i ≤ K, we write Bi for the problem where the distribution of arm j ∈ {1, . . . ,K} is N (τ + di + , 1) if i 6= j and N (τ − di − , 1) otherwise. For all these problems, H := Hτ, = ∑ i(di + 2 )\n−2 is the same by definition."
    }, {
      "heading" : "It holds that for any bandit algorithm",
      "text" : "max\ni∈{0,...,K} EBi(L(T )) ≥ exp\n( − 3T/H−\n4 log(12(log(T ) + 1)K) ) ,\nwhere EBi is the expectation according to the samples of problem Bi.\nThis lower bound implies that even if the learner is given the distance of the mean of each arm to the threshold and the shape of the distribution of each arm (here Gaussian of variance 1), any algorithm still makes an error of at least exp(−3T/H − 4 log(12(log(T ) + 1)K)) on one of the problems. This is a lower bound in a very strong sense because we really restrict the set of possibilities to a setting where we know all gaps and prove that nevertheless this lower bounds holds. Also it is non-asymptotic and holds for any T , and implies therefore a non-asymptotic minimax lower bound. The closer the means of the distributions to the threshold, the larger the complexity H , and the larger the lower bound. The proof is to be found in Appendix A.\nThis theorem’s lower bound contains two terms in the exponential, a term that is linear in T and a term that is of order log((log(T ) + 1)K) ≈ log(log(T )) + log(K). For large enough values of T , one has the following simpler corollary. Corollary. Let H̄ > 0 and R > 0, τ ∈ R and ≥ 0. Consider BH̄,R the set of K-armed bandit problems where the distributions of the arms are R-sub-Gaussian and which have all a complexity smaller than H̄ .\nAssume that T ≥ 4H̄R2 log(12(log(T ) + 1)K). It holds that for any bandit algorithm\nsup B∈BH̄,R\nEB(L(T )) ≥ exp ( − 4T/(R2H̄) ) ,\nwhere EB is the expectation according to the samples of problem B ∈ BH̄,R."
    }, {
      "heading" : "2.3. Algorithm APT and associated upper bound",
      "text" : "In this section we introduce APT (Anytime Parameter-free Thresholding algorithm), an anytime parameter-free learning algorithm. Its heuristic is based on a simple observation, namely that a near optimal static strategy that allocates Tk samples to arm k is such that Tk∆2k is constant across k (and increasing with T ) - see Theorem 1, and in particular the second half of Step 3 of its proof in Appendix A - and that therefore a natural idea is to simply pull at time t the arm that minimizes an estimator of this quantity. Note that in this paper, we consider for the sake of simplicity that each arm is tested against the same threshold, however this can be relaxed to (τk)k at no additional cost.\nAlgorithm The algorithm receives as input the definition of the problem (τ, ). First, it pulls each arm of the game once. At time t > K, APT updates Ti(t), the number of pulls up to time t of arm i, and the empirical mean µ̂i(t) of arm k after Ti(t) pulls. Formally, for each k ∈ A it computes Ti(t) = ∑t s=1 I(Is = i) and the updated means\nµ̂i(t) = 1\nTi(t) Ti(t)∑ s=1 Xi,s, (3)\nAlgorithm 1 APT algorithm Input: τ , Pull each arm once for t = K + 1 to T do\nPull arm It = arg min k≤K Bk(t) from Equation (5)\nObserve reward X ∼ νIt end for Output: Ŝτ = {k : µ̂k(T ) ≥ τ}\nwhere Xi,s denotes the sample received when pulling i for the s-th time. The algorithm then computes:\n∆̂i(s) := ∆̂ τ, i (s) = |µ̂i(t)− τ |+ , (4)\nthe current empirical estimate of the gap associated with arm i. The algorithm then computes:\nBi(t+ 1) = √ Ti(t)∆̂i(t). (5)\nand pulls the arm It+1 = arg min i≤K Bi(t+1) that minimizes this quantity. At the end of the horizon T , the algorithm outputs the set of arms Ŝτ = {k : µ̂k(T ) ≥ τ}.\nThe expected loss of this algorithm can be bounded as follows.\nTheorem 2. Let K ≥ 0, T ≥ 2K, and consider a problem B. Assume that all arms νk of the problem are R-subGaussian with means µk. Let τ ∈ R, ≥ 0\nAlgorithm APT’s expected loss is upper bounded on this problem as\nE(L(T )) ≤ exp ( − 1\n64R2 T H + 2 log((log(T ) + 1)K)\n) ,\nwhere we remind thatH = ∑ i(|µi−τ |+ )−2 and where E is the expectation according to the samples of the problem.\nThe bound of Theorem 2 holds for any R-sub-Gaussian bandit problem. Note that one does not need to know R in order to implement the algorithm, e.g. if the distributions are bounded, one does not need to know the bound. This is a desirable feature for an algorithm, yet e.g. all algorithms based on upper confidence bounds need a bound on R. This bound is non-asymptotic (one just needs T ≥ 2K so that one can initialize the algorithm) and therefore Theorem 2 provides a minimax upper bound result over the class of problems that have sub-Gaussian constant R and complexity H .\nThe term in the exponential of the lower bound of Theorem 2 matches the lower bound of Theorem 1 up to a multiplicative factor and the log((log(T )+1)K) term. Now as in the case of the lower bound, for large enough values of T , one has the following simpler corollary.\nCorollary. Let H̄ > 0 and R > 0, τ ∈ R and ≥ 0. Consider BH̄,R the set of K-armed bandit problems where the distributions of the arms are R-sub-Gaussian and whose complexity is smaller than H̄ .\nAssume that T ≥ 256H̄R2 log((log(T ) + 1)K). For Algorithm APT it holds that\nsup B∈BH̄,R\nEB(L(T )) ≤ exp ( − T/(128R2H) ) ,\nwhere EB is the expectation according to the samples of problem B ∈ BH̄,R\nThis corollary and Corollary 2.2 imply that for T large enough - i.e. of larger order than HR2 log((log(T ) + 1)K) - Algorithm APT is order optimal over the class of problems whose complexity is bounded by H̄ and whose arms are R-sub-Gaussian."
    }, {
      "heading" : "2.4. Discussion",
      "text" : "A parameter free algorithm An important point that we want to highlight for our strategy APT is that it does not need any parameter, such as the complexity H , the horizon T or the sub-Gaussian constant R. This contrasts with any upper confidence based approach as in e.g. (Audibert & Bubeck, 2010; Gabillon et al., 2012) (e.g. the UCB-E algorithm in (Audibert & Bubeck, 2010)), which need as parameter an upper bound on R and the exact knowledge of H , while the bound of Theorem 2 will hold for any R and any H , and our algorithm adapts to these quantities. Also we would like to highlight that for the related problem of best arm identification, existing fixed budget strategies need to know the budget T in advance (Audibert & Bubeck, 2010; Karnin et al., 2013; Chen et al., 2014) - while our algorithm can be stopped at any time and the bound of Theorem 2 will hold.\nExtensions to distributions that are not sub-Gaussian as opposed to adaptation to sub- models It is easy to see in the light of (Bubeck et al., 2013a) that one could extend our algorithm to non sub-Gaussian distributions by using an estimator other than the empirical means, as e.g. the estimators in (Catoni et al., 2012) or in (Alon et al., 1996). These estimators have sub-Gaussian concentration asymptotically under the only assumption that the distributions have a finite (1 + v) moment with v > 0 (and the sub-Gaussian concentration will depend on v). Using our algorithm with a such estimator will therefore provide a result that is similar to the one of Theorem 2 - and that without requiring the knowledge of v, which means that our algorithm APT modified for using these robust estimators instead of the empirical mean will work for any bandit problem where the arm distributions have a finite (1 + v) moment with v > 0. On the other hand, if we consider more specific, e.g. exponential, models, it is possible to obtain a refined lower\nbound in terms of Kullback- Leibler divergences rather than gaps following (Kaufmann et al., 2015). However, an upper bound of the same order clearly comes at the cost of a more complicated strategy and holds in less generality than our bound.\nOptimality of our strategy As explained previously, the upper bound on the expected risk of algorithm APT is comparable to the lower bound on the expected risk up to a log ( (log(T ) + 1)K ) term (see Theorems 2 and Theorems 1) - and this term vanishes when the horizon T is large enough, namely when T ≥ O(HR2 log ( (log(T )+1)K ) ), which is the case for most problems. So for T large enough, our strategy is order optimal over the class of problems that have complexity smaller thanH and sub-Gaussian constant smaller than R.\nComparison with existing results Our setting is a specific combinatorial pure exploration setting with fixed budget where the objective is to find the set of arms that are above a given threshold. Settings related to ours have been analyzed in the literature and the state of the art result on our problem can be found (to the best of our knowledge) in the paper (Chen et al., 2014). In this paper, the authors consider a general pure exploration combinatorial problem. Given a set M of subsets of {1, . . . ,K}, they aim at finding a subset of arms M∗ ∈ M such that M∗ = arg maxM∈M ∑ k∈M∗ µk. In the specific case whereM is the set of all subsets of {1, . . . ,K}, their problem in the fixed budget setting is exactly the same as ours when = 0 and the means are shifted by −τ . Their algorithm CSAR’s upper bound on the loss is (see their Theorem 3):\nE(L(T )) ≤ K2 exp ( − T −K\n72R2 log(K)HCSAR,2\n) ,\nwhere HCSAR,2 = maxi i∆−2(i) . As HCSAR,2 log(K) ≥ H by definition, there is a gap for their strategy in the fixed budget setting with respect to the lower bound of Theorem 1, which is smaller and of order exp(−T/(HR2)). Our strategy on the contrary does not have this gap, and improves over the CSAR strategy. We believe that this lack of optimality for CSAR is not an artefact of the proof of the paper (Chen et al., 2014), and that CSAR is sub-optimal, as it is a successive reject algorithm with fixed and nonadaptive reject phase length. A similar gap between upper and lower bounds for successive reject based algorithms in the fixed budget setting was also observed for the best arm identification problem when no additional information such as the complexity are known to the learner, see (Audibert & Bubeck, 2010; Karnin et al., 2013; Kaufmann et al., 2015; Chen et al., 2014). It is therefore an interesting fact that there is a parameter free optimal algorithm for our fixed budget problem.\nThe paper (Chen et al., 2014) also provides results in the fixed confidence setting, where the objective is to provide an optimal set using the smallest possible sample size. In these results such a gap in optimality does not appear and the algorithm CLUCB they propose is almost optimal, see also (Kalyanakrishnan et al., 2012; Jamieson et al., 2014; Karnin et al., 2013; Kaufmann et al., 2015; Chen & Li, 2015) for related results in the fixed confidence setting. This highlights that the fixed budget setting and the fixed confidence setting are fundamentally different (at least in the absence of additional information such as the complexity H), and that providing optimal strategies in the fixed budget setting is a more difficult problem than providing an adaptive strategy in the fixed confidence problem - adaptive algorithms that are nearly optimal in the absence of additional information have only been exhibited in the latter case. To the best of our knowledge, all strategies except ours have such an optimality gap for fixed budget pure exploration combinatorial bandit problems, while there exists fixed confidence strategies for general pure exploration combinatorial bandits that are very close to optimal, see (Chen et al., 2014).\nNow in the case where the learner has additional information on the problem, as e.g. the complexity H , it has been proved in the TopM problem that a UCB-type strategy has probability of error upper bounded as exp(−T/H), see (Audibert & Bubeck, 2010; Gabillon et al., 2012). A similar UCB type of algorithm would also work in the TBP problem, implying the same upper bound results as APT. But we would like to highlight that the exact knowledge of H is needed by these algorithms for reaching this bound - which is unlikely in applications. Our strategy on the other hand reaches, up to constants, the optimal expected loss for the TBP problem, without needing any parameter."
    }, {
      "heading" : "3. Extensions of our results to related settings",
      "text" : "In this section we detail some implications of the results of the previous section to some specific problems."
    }, {
      "heading" : "3.1. Active level set detection : Active classification and active anomaly detection",
      "text" : "Here we explain how a simple modification of our setting transforms it into the setting of active level set detection, and therefore why it can be applied to active classification and active anomaly detection. We define the problem of discrete, active level set detection as the problem of deciding as efficiently as possible, in our bandit setting, whether for any k the probabilities that the samples of arms νk are above or below a given level L are higher or smaller than a threshold τ up to a precision , i.e. it is the problem of deciding for all k whether µ̃k(L) := PX∼νk(X > L) ≥ τ , or not up to a precision .\nThis problem can be immediately solved by our approach with a simple change of variable. Namely, for the sample Xt ∼ νIt collected by the algorithm at time t, consider the transformation X̃t = 1Xt>L. Then X̃t is a Bernoulli random variable of parameter µ̃It(L) (which is a 1/2-subGaussian distribution) - and applying our algorithm to the transformed samples X̃t solves the active level set detection problem. This has two interesting applications, namely in active binary classification and in active anomaly detection.\nActive binary classification In active binary classification, the learner aims at deciding, for k points (the arms of the bandit), whether each point belongs to the class 1 or the class 0.\nAt each round t, the learner can request help from a homogeneous mass of experts (which can be a set of previously trained classifiers, where one wants to minimize the computational cost, or crowd-sourcing, when one wants to minimize the costs of the task), and obtain a noisy label for the chosen data point It. We assume that for any point k, the expert’s responses are independent and stochastic random variables in {0, 1} of mean µk (i.e. the arm distributions are Bernoulli random variables of parameter µk). We assume that the experts are right on average and that the label lk of k is equal to lk := 1{µ̃k > 1/2}. The active classification task therefore amounts to deciding whether µk > τ := 1/2 or not, possibly up to a given precision . Our strategy therefore directly applies to this problem by choosing τ = 1/2.\nActive anomaly detection In the case anomaly detection, a common way to characterize anomalies is to describe them as naturally not concentrated (Steinwart et al., 2005). A natural way to characterize anomalies is thus to define a cutoff level L, and classify the samples e.g. above this level L as anomalous. Such an approach has already received attention for anomaly detection e.g in (Streeter & Smith), albeit in a cumulative regret setting.\nHere we consider an active anomaly detection setting where we face K sources of data (the arms), and we aim at sampling them actively to detect which sources emit anomalous samples with a probability higher than a given threshold τ - this threshold is chosen e.g. as the maximal tolerable amount of anomalous behavior of a source. This illustrates the fact that as described in (Steinwart et al., 2005), the problem of anomaly detection is indeed a problem of level set detection - and so the problem of active anomaly detection is a problem of active level set detection on which we can use our approach as explained above."
    }, {
      "heading" : "3.2. Best arm identification and cumulative reward maximization with known highest mean value",
      "text" : "Two classical bandit problems are the best-arm identification problem and the cumulative reward maximization problem. In the former, the goal of the learner is to identify the arm with the highest mean (Bubeck et al., 2009). In the latter, the goal is to maximize the sum of the samples collected by the algorithm up to time T (Auer et al., 1995). Intuitively, both problems should call for different strategies - in the best arm identification problem one wants to explore all arms heavily while in the cumulative reward maximization problem one wants to sample as much as possible the arm with the highest mean. Such intuition is backed up by Theorem 1 of (Bubeck et al., 2009), which states that in the absence of additional information and with a fixed budget, the lower the regret suffered in the cumulative setting, expressed in terms of rewards, the higher the regret suffered in the identification problem, expressed in terms of probability of error. We prove in this section the somewhat non intuitive fact that if one knows the value of best arm’s mean, its possible to perform both tasks simultaneously by running our algorithm where we choose = 0 and τ = µ∗ := maxk µk. Our algorithm then reduces to the GCL∗ algorithm that can be found in (Salomon & Audibert, 2011).\nBest arm identification In the best arm identification problem, the game setting is the same as the one we considered but the goal of the learner is different: it aims at returning an arm JT that with the highest possible mean. The following proposition holds for our strategy APT that runs for T times, and then returns the arm JT that was the most pulled.\nTheorem 3. Let K > 0, R > 0 and T ≥ 2K and consider a problem where the distribution of the arms νk is R-sub-Gaussian and has mean µk. Let µ∗ := maxk µk and Hµ∗ = ∑ i:µi 6=µ∗(µ ∗ − µi)−2. Then APT run with parameters τ = µ∗ and = 0, recommending the arm JT = arg max\nk∈A Tk(T ), is such that\nP(µJT 6= µ∗) ≤ exp ( − T\n36R2Hµ∗ +2 log(log(T )+1)K\n) .\nIf the complexity H is also known to the learner, algorithm UCB-E from (Audibert & Bubeck, 2010) would attain a similar performance.\nRemark This implies that if µ∗ is known to the learner, there exists an algorithm such that its probability of error is of order exp(−cT/H). The recent paper (Carpentier & Locatelli, 2016) actually implies that the knowledge of µ∗ is actually key here, since without this information, the simple regret is at least of order exp(−cT/(log(K)H)) in a minimax sense.\nCumulative reward maximization In the cumulative reward maximization problem, the game setting is the same as the the one we considered but the aim of the learner is different : if we write Xt for the sample collected at time t by the algorithm, it aims at maximizing ∑ t≤T Xt. The following proposition holds for our strategy APT that runs for T times.\nTheorem 4. Let K > 0, R > 0 and T ≥ 2K and consider a problem where the distribution of the arms νk is R-subGaussian.\nThen APT run with parameters τ = µ∗ and = 0 is such that\nTµ∗ − E ∑ t≤T Xt ≤ inf δ≥1 [ ∑ k 6=k∗ 4R2 log(T )δ µ∗ − µi\n+ (µ∗ − µi)(1 + K T 2δ−2 ) ] .\nThis bound implies both the problem dependent upper bound of order ∑ i ∆ −1 i log(T ) and the problem indepen-\ndent upper bound of order √ TK log(T ), and this matches the performance of algorithms like UCB for any tuning parameter. A similar result can also be found in (Salomon & Audibert, 2011).\nDiscussion Propositions 3 and 4, whose proofs are provided in Appendix A, imply that our algorithm APT is a good strategy for solving at the same time both problems when µ∗ is known. As mentioned previously, this is counter intuitive since one would expect a good strategy for the best arm identification problem to explore significantly more than a good strategy for the cumulative reward maximization problem. To convince oneself, it is sufficient to look at the two-armed case, for which in the fixed budget it is optimal to sample both arms equally, while this strategy has linear regret in the cumulative setting. This intuition is formalized in (Bubeck et al., 2009) where the authors prove that no algorithm can achieve this without additional information. Our results therefore imply that the knowledge of µ∗ by the learner is a sufficient information so that Theorem 1 of (Bubeck et al., 2009) does not hold anymore and there exists algorithms that solve both problems at the same time, as APT does.\nTopM problem An extension of the best arm identification problem is known as TopM arms identification problem, where one is concerned with identifying the set of the M arms with the highest means (Bubeck et al., 2013b; Gabillon et al., 2012; Kaufmann et al., 2015; Zhou et al., 2014; Chen et al., 2014; Cao et al., 2015). If the learner has some additional information, such as the mean values of the arms with M th and (M + 1)th highest means, then it is straightforward that one can apply our algorithm APT, setting τ in the middle between the M th and (M + 1)th highest means. The set Ŝτ would then be returned as the es-\ntimated set ofM optimal arms. The upper bound and proof for this problem is a direct consequence of Theorem 2, and granted one has such extra-information, outperforms existing results for the fixed budget setting, see (Bubeck et al., 2013b; Kaufmann et al., 2015; Chen et al., 2014; Cao et al., 2015). If the complexity H were also known to the learner, the strategy in (Gabillon et al., 2012) would attain a similar performance."
    }, {
      "heading" : "4. Experiments",
      "text" : "We illustrate the performance of algorithm APT in a number of experiments. For comparison, we use the following methods which include the state of the art CSAR algorithm of (Chen et al., 2014) and two minor adaptations of known methods that are also suitable for our problem. Uniform Allocation (UA): For each t ∈ {1, 2, . . . , T}, we choose It ∼ UA. This method is known to be optimal if all arms are equally difficult to classify, that is in our setting, if the quantities ∆τ, i , i ∈ A, are very close. UCB-type algorithm: The algorithm UCBE given and analyzed in (Audibert & Bubeck, 2010) is designed for finding the best arm - and its heuristic is to pull the arm that maximizes a UCB bound - see also (Gabillon et al., 2012) for an adaptation of this algorithm to the general TopM problem. The natural adaptation of the method for our problem corresponds to pulling the arm that minimizes ∆̂k(t) − √ a\nTk(t) . From the theoretical analysis in the\npaper (Audibert & Bubeck, 2010; Gabillon et al., 2012), it is not hard to see that setting a ≈ (T −K)/H minimizes their upper bound, and that this algorithm attains the same expected loss as ours - but it requires the knowledge of H . In the experiments we choose values ai = 4i T−KH , i ∈ {−1, 0, 4}, and denote the respective results as UCBE(4i). The value a0 can be seen as the optimal choice, while the two other choices give rise to strategies that are sub-optimal because they respectively explore too little or too much. CSAR: As mentioned before, this method is given in (Chen et al., 2014). In our specific setting, via the shift µ̃i = µi − τ , the lines 7-17 of the algorithm reduce\nto classifying the arm i that maximizes |µ̃i| based on its current mean. The set At corresponds to Ŝτ at time t. In fact in our specific setting the CSAR algorithm is a successive reject-type strategy (see (Audibert & Bubeck, 2010) where the arm whose empirical mean is furthest from the threshold is rejected at the end of each phase.\nFigure 1 displays the estimated probability of success on a logarithmic scale with respect to the horizon of the six algorithms based onN = 5000 simulated games with τ = 12 , = 0.1, K = 10, and T = 500. Experiment 1 (3 groups setting): K Bernoulli arms with means µ1:3 ≡ 0.1, µ4:7 = (0.35, 0.45, 0.55, 0.65) and µ8:10 ≡ 0.9, which amounts to 2 difficult relevant arms (that is, outside the 2 - band), 2 difficult irrelevant arms and six easy relevant arms. Experiment 2 (arithmetic progression): K Bernoulli arms with means µ1:4 = 0.2 + (0 : 3) · 0.05, µ5 = 0.45, µ6 = 0.55 and µ7:10 = 0.65+(0 : 3)·0.05, which amounts to 2 difficult irrelevant arms and eight arms arithmetically progressing away from τ . Experiment 3 (geometric progression): K Bernoulli arms with means µ1:4 = 0.4 − 0.21:4, µ5 = 0.45, µ6 = 0.55 and µ7:10 = 0.6 + d5−(1:4), which amounts to 2 difficult irrelevant arms and eight arms geometrically progressing away from τ .\nThe experimental results confirm that our algorithm may only be outperformed by methods that have an advantage in the sense that they have access to the underlying problem complexity and, in the case of UCBE(1), an additional optimal parameter choice. In particular, other choices for that parameter lead to significantly less accurate results comparable to the naive strategy UA. These effects are also visible in the further results given in Appendix B.\nConclusion In this paper we proposed a parameter free algorithm based on a new heuristic for the TBP problem in the fixed confidence setting - and we prove that it is optimal which is a kind of result which is highly non trivial for combinatorial pure exploration problems with fixed budget.\nAcknowledgement This work is supported by the DFG’s Emmy Noether grant MuSyAD (CA 1488/1-1)."
    }, {
      "heading" : "A. Proofs",
      "text" : "A.1. Proof of Theorem 1\nProof. In this proof, we will prove that on at least one instance of the problem, any algorithm makes a mistake of order at least exp(−cT/H).\nStep 0: Setting and notations. Let us consider K real numbers ∆i ≥ 0, and let us set τ = 0, = 0. Let us write νi := N (∆i, 1) for the Gaussian distribution of mean ∆i and variance 1, and ν′i := N (−∆i, 1) for the Gaussian distribution of mean −∆i and variance 1. Note that this construction is easily generalised to cases where τ 6= 0 or 6= 0 by translation or careful choice of the ∆i.\nWe define the product distributions Bi where i ∈ {0, ...,K} as νi1 ⊗ ... ⊗ νiK where for k ≤ K, νik := νi1k 6=i + ν′i1k=i is νi if k 6= i and ν′i otherwise. We also extend this notation to B0, where none of the arms is flipped with respect to the threshold (∀k, ν0k := νi). It is straightforward that the gap ∆i of arm i with respect to the threshold τ = 0 does not depend on Bi and is equal to ∆i. It follows that all these problems have the same complexity H as defined previously (with = 0 and τ = 0).\nWe write for i ≤ K, PiB for the probability distribution according to all the samples that a strategy could possibly collect up to horizon T , i.e. according to the samples (Xk,s)k≤K,s≤T ∼ (Bi)⊗T . Let (Tk)k≤K denote the numbers of samples collected by the algorithm on arm k.\nLet k ∈ {0, ...,K}. Note that\nKLk := KL(ν′k, νk) = 2∆ 2 k,\nwhere KL is the Kullback Leibler divergence. Let T ≥ t ≥ 0. We define the quantity:\nK̂Lk,t = 1\nt t∑ s=1 log( dν′k dνk (Xk,s)) = − 1 t t∑ s=1 2Xk,s∆k.\nStep 1: Concentration of the empirical KL. Let us define the event:\nξ = { ∀k ≤ K,∀t ≤ T, |K̂Lk,t − KLk| ≤ 4∆k √ log(4(log(T ) + 1)K)\nt\n} .\nSince K̂Lk,t = − 1t ∑t s=1 2Xk,s∆k and KLk = 2∆ 2 k, by Gaussian concentration (a peeling and the maximal martingale inequality), it holds that for any i that PBi(ξ) ≥ 3/4.\nStep 2: A change of measure. We will now use the change of measure introduced previously for a well chosen event A. Namely, we considerAi = {i ∈ Ŝτ}, the event where the algorithm classified arm i as being above the threshold. We have by doing a change of measure between Bi and B0 (since they only differ in arm i and only the Ti first samples of arm i by the algorithm):\nPBi(Ai) = EB0 [ 1Ai exp ( − TiK̂Li,Ti )] ≥ EB0 [ 1Ai∩ξ exp ( − TiK̂Li,Ti\n)] ≥ EB0 [ 1Ai∩ξ exp ( − 2∆2iTi − 4∆i √ Ti √ log((4 log(T ) + 1)K) )] ,\nby definition of ξ and KLi. Step 3: A union of events. We now consider the event A = K⋂ i=1 Ai, i.e. the event where all arms are classified as\nbeing above the threshold τ = 0. We have:\nmax i∈{1,...,K}\nPBi(Ai) ≥ 1\nK K∑ i=1 PBi(Ai) (6)\n≥ 1 K K∑ i=1 PBi(Ai ∩ ξ)\n≥ 1 K K∑ i=1 EB0 [ 1Ai∩ξ exp ( − 2Ti∆2i − 4∆i √ Ti √ log(4(log(T ) + 1)K) )]\n≥ EB0 [ 1A∩ξ 1\nK K∑ i=1 exp ( − 3Ti∆2i − 4 log(4(log(T ) + 1)K) )] ≥ exp ( − 4 log(4(log(T ) + 1)K) ) EB0 [ 1A∩ξS ] , (7)\nwhere the fourth line comes from using 2ab ≤ a2 + b2 with a = ∆i √ Ti and where:\nS = 1\nK K∑ i=1 exp ( − 3Ti∆2i ) .\nSince ∑ i Ti = T and all Ti are positive, there exists an arm i such that Ti ≤\nT H∆2i . This yields:\nS ≥ 1 K exp ( − 3T H ) = exp ( − 3T H − log(K) ) .\nThis implies by definition of the risk:\nmax i∈{0,...,K}\nEBi(L(T )) ≥ max (\nmax i∈{1,...,K}\nPBi(Ai), 1− PB0(A) )\n≥ 1 2 exp ( − 3T H − 4 log(4(log(T ) + 1)K) ) − log(K)EB0 [ 1A∩ξ ] + 1 2 (1− PB0(A)) = 1\n2 exp ( − 3T H − 4 log(4(log(T ) + 1)K − log(K)) ) PB0 [ A ∩ ξ ] + 1 2 (1− PB0(A))\n≥ 1 8 exp ( − 3T H − 4 log(4(log(T ) + 1)K)− log(K) ) ≥ exp\n( − 3T H − 4 log(12(log(T ) + 1)K) ) ,\nThe fourth line comes from P(ξ) ≥ 3/4, and we consider two cases PB0(A) ≥ 1/2 and PB0(A) ≤ 1/2. The first leads directly to the condition as the intersection is at least of probability 1/4; in the latter case, we have the same bound via\nmax i∈{0,...,K}\nEBi(L(T )) ≥ EB0(L(T )) = PB0(AC) ≥ 1/2.\nThis concludes the proof.\nA.2. Proof of Theorem 2\nProof. In this proof, we will show that on a well chosen event ξ, we classify correctly the arms which are over τ + , and reject the arms that are under τ − .\nStep 1: A favorable event. Let δ = (4 √ 2)−1. Towards this goal, we define the event ξ as follows:\nξ = { ∀i ∈ A,∀s ∈ {1, ..., T} : |1\ns s∑ t=1 Xi,t − µi| ≤ √ Tδ2 Hs } .\nWe know from Sub-Gaussian martingale inequality that for each i ∈ A and each u ∈ {0, ..., blog(T )c}:\nP ( ∃v ∈ [2u, 2u+1], {|1\nv v∑ t=1 Xi,t − µi| ≥ √ Tδ2 Hv } ) ≤ exp(− Tδ 2 2R2H ).\nξ is the union of these events for all i ≤ K and s ≤ blog(T )c. As there are less than (log(T ) + 1)K such combinations, we can lower-bound its probability of occurrence with a union bound by:\nP(ξ) ≥ 1− 2(log(T ) + 1)K exp(− Tδ 2\n2R2H ).\nStep 2: Characterization of some helpful arm. At time T , we consider an arm k that has been pulled after the initialization phase and such that Tk(T )− 1 ≥ (T−K)H∆2k . We know that such an arm exists otherwise we get:\nT −K = K∑ i=1 (Ti(T )− 1) < K∑ i=1 T −K H∆2i = T −K,\nwhich is a contradiction. Note that since T ≥ 2K, we have that Tk(T )− 1 ≥ T2H∆2k We now consider t ≤ T the last time that this arm k was pulled. Using Tk(t) ≥ 2 (by the initialisation of the algorithm), we know that:\nTk(t) ≥ Tk(T )− 1 ≥ T\n2H∆2k . (8)\nStep 3: Lower bound on the number of pulls of the other arms. On ξ, at time t as we defined previously, we have for every arm i:\n|µ̂i(t)− µi| ≤\n√ Tδ2\nHTi(t) . (9)\nFrom the reverse triangle inequality and Equation (4), we have:\n|µ̂i(t)− µi| = |(µ̂i(t)− τ)− (µi − τ)| ≥ ||µ̂i(t)− τ | − |µi − τ || ≥ |(|µ̂i(t)− τ |+ )− (|µi − τ |+ )|\n≥ |∆̂i(t)−∆i|.\nCombining this with (9) yields the following:\n∆k −\n√ Tδ2\nHTk(t) ≤ ∆̂k(t) ≤ ∆k +\n√ Tδ2\nHTk(t) . (10)\nBy construction, we know that at time t we pulled arm k, which yields for every i ∈ A:\nBk(t) ≤ Bi(t). (11)\nWe can lower bound the left-hand side of (11) using (8):( ∆k − √ Tδ2\nHTk(t)\n)√ Tk(t) ≤ Bk(t)\n( ∆k − √ 2δ∆k )√ T 2H∆2k ≤ Bk(t)\n( 1√ 2 − δ )√ T H ≤ Bk(t), (12)\nand upper bound the right hand side using (10) by: Bi(t) = ∆̂i √ Ti(t)\n≤ (\n∆i +\n√ Tδ2\nHTi(t)\n)√ Ti(t)\n≤ ∆i √ Ti(t) + δ\n√ T\nH . (13)\nAs both ∆̂i and ∆i are positive by definition, combining (12) and (13) yields the following lower bound on Ti(T ) ≥ Ti(t):( 1− 2 √ 2δ )2 T\n2H∆2i ≤ Ti(T ). (14)\nStep 4: Conclusion. On ξ, as ∆i is a positive quantity, combining (9) and (14) yields:\nµi −∆i √ 2δ\n1− 2 √ 2δ ≤ µ̂i(T ) ≤ µi + ∆i\n√ 2δ 1− 2 √ 2δ , (15)\nwhere √\n2δ 1−2 √ 2δ simplifies to 1/2 for δ = (4\n√ 2)−1.\nFor arms such that µi ≥ τ + , then ∆i = µi − τ + and we can rewrite (15):\nµi − τ − 1\n2 ∆i ≤ µ̂i(T )− τ\n(µi − τ)(1− 1\n2 )− 2 ≤ µ̂i(T )− τ\n0 ≤ µ̂i(T )− τ,\nwhere the last line uses µi ≥ τ + . One can easily check through similar derivations that µ̂i(T ) − τ < 0 holds for µi < τ − . On ξ, arms over τ + are all accepted, and arms under τ − are all rejected, which means the loss suffered by the algorithm is 0. As 1− P(ξ) ≤ 2(log(T ) + 1)K exp(− 164R2 T H ), this concludes the proof.\nA.3. Proof of Theorem 3\nProof. We will prove that on a well defined event ξ, sub-optimal arms are pulled at most T 2∆2kH − 1 times, which translates to the best arm being chosen at the end of the horizon as it was pulled more than half of the time.\nStep 1: A favorable event. Let δ = 1/18. We define the following events ∀i ∈ A:\nξi = {∀s ≤ T : |µ∗ − µ̂i(s)| ≤\n√ Tδ\nHTi(s) },\nWe now define ξ as the intersection of these events: ξ = ⋂ k∈A ξk.\nUsing the same Sub-Gaussian martingale inequality as in the proof of Theorem 2, we can lower bound its probability of occurrence with a union bound by:\nP (ξ) ≥ 1− 2(log(T ) + 1)K exp(− T 36R2H )\nStep 2: The wrong arm at the wrong time. Let us now suppose that a sub-optimal arm k was pulled at least T−K 2∆2kH times after the initialization which translates to Tk(T ) − 1 ≥ T−K2∆2kH . Let us now consider the last time t ≤ T that this arm was pulled. As it was pulled at time t, the following inequality holds:\nBk(t) ≤ Bk∗(t). (16)\nOn ξ, we can now lower bound the left hand side by:\n(∆k −\n√ Tδ\nHTk(t) ) √ Tk(t) ≤ Bk(t)\n∆k √ Tk(t)−\n√ Tδ\nH ≤ Bk(t), (17)\nWe also upper bound the right hand side of (16) by: Bk∗(t) ≤ √ Tδ\nH . (18)\nCombining both bounds (17) and (18) with (16), as well as rearranging the terms yields:\n∆k √ Tk(t) ≤ 2\n√ Tδ\nH\nTk(t)∆ 2 k ≤\n4Tδ\nH . (19)\nUsing Tk(t) ≥ Tk(T )− 1 ≥ T−K2∆2kH as well as T ≥ 2K, we have\nTk(t) ≥ T\n4∆2kH . (20)\nPlugging this in (19) brings the following condition:\nT\n4∆2kH ∆2k ≤\n4Tδ\nH . (21)\nwhich directly reduces to δ ≥ 1/16, which is a contradiction as we have set δ = 1/18.\nAs we have proved that for any sub-optimal arm i 6= k∗ it satisfies Ti(T ) < T2∆2iH , summing for all arms yields:\nT − Tk∗(T ) = ∑ i 6=k∗ Ti(T )\n< T\n2H ∑ i6=k∗ 1 ∆2i = T 2 . (22)\nWe conclude by observing that Tk∗(T ) > T/2, and as such will be chosen by the algorithm at the end as being the best arm.\nA.4. Proof of Theorem 4\nProof. In this proof we will show that with high probability the sub-optimal arms have been pulled at most at a logarithmic rate, and will then bound the expectation of the number of pulls of these arms.\nStep 1: A favorable event. We define the following events ∀s ≤ T :\nξk∗,s = {µ∗ − µ̂k∗(s) ≤ R\n√ log(T )δ\nTk∗(s) },\nas well as for all arms i 6= k∗:\nξi,s = {µ̂k(s)− µk ≤ R\n√ log(T )δ\nTki(s) }.\nBy Hoeffding’s inequality, the complimentary ξ̄k of each of these events has probability at most T−2δ . We now consider ξ the intersection of these events for all k ∈ A. By a union bound, as there are T such events for each arm, we have:\nP(ξ) ≥ 1− K T 2δ−1 . (23)\nWe also have: P(ξ̄) ≤ K\nT 2δ−1 . (24)\nWe will now prove a bound on the number of pulls on ξ.\nStep 2: Bound on pulls of sub-optimal arms. We now consider the last time t that arm k 6= k∗ was pulled, under the assumption that it was pulled at least once after the initialization. The decision rule of the algorithm yields:\nBk(t) ≤ Bk∗(t). (25)\nOn ξ, we can now lower-bound the left-side and upper-bound the right hand side, which yields:\n(∆k −R\n√ log(T )δ\nTk(t) ) √ Tk(t) ≤ R\n√ log(T )δ\nTk∗(t)\n√ Tk∗(t), (26)\nwhich can be rearranged as such: ∆k √ Tk(t) ≤ 2R √ log(T )δ, (27)\nand the following bound on Tk(T ):\nTk(T ) ≤ 4R2 log(T )δ\n∆2k + 1. (28)\nNote that we here make the assumption that the arm was pulled at least once by the algorithm after the initialization. If it has only been pulled during the initialization, the bound still trivially holds as we have at least one pull.\nStep 3: Conclusion. We can thus upper-bound the expectation of Tk(t), as when ξ does not hold we get at most T pulls:\nE[Tk(T )] ≤ 4R2 log(T )δ\n∆2k + 1 +\nK\nT 2δ−2 , (29)\nand we get the following bound on the pseudo-regret when ξ holds:\nR̄T ≤ ∑ k 6=k∗ 4R2 log(T )δ ∆k + ∆k(1 + K T 2δ−2 ). (30)\nPlugging δ = 1 yields:\nR̄T ≤ ∑ k 6=k∗ 4R2 log(T ) ∆k + ∆k(1 +K), (31)\nand we recover the classical bound of the UCB1 algorithm."
    }, {
      "heading" : "B. Further Experimental Results",
      "text" : "We now also provide simulation results for our three settings in the case of Gaussian arms with means µi and variances σ2i = 0.25. Again, only the correctly tuned UCBE- algorithm outperforms APT."
    } ],
    "references" : [ {
      "title" : "The space complexity of approximating the frequency moments",
      "author" : [ "Alon", "Noga", "Matias", "Yossi", "Szegedy", "Mario" ],
      "venue" : "In Proceedings of the twenty-eighth annual ACM symposium on Theory of computing,",
      "citeRegEx" : "Alon et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "Alon et al\\.",
      "year" : 1996
    }, {
      "title" : "Best arm identification in multi-armed bandits",
      "author" : [ "Audibert", "Jean-Yves", "Bubeck", "Sébastien" ],
      "venue" : "In Proceedings of the 23rd Conference on Learning Theory,",
      "citeRegEx" : "Audibert et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Audibert et al\\.",
      "year" : 2010
    }, {
      "title" : "Gambling in a Rigged Casino: The Adversarial Multi-Armed Bandit problem",
      "author" : [ "Auer", "Peter", "Cesa-Bianchi", "Nicolò", "Freund", "Yoav", "Schapire", "Robert" ],
      "venue" : "In Proceedings of the 36th Annual Symposium on Foundations of Computer Science,",
      "citeRegEx" : "Auer et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "Auer et al\\.",
      "year" : 1995
    }, {
      "title" : "Bandits with heavy tail",
      "author" : [ "Bubeck", "Sebastian", "Cesa-Bianchi", "Nicolo", "Lugosi", "Gábor" ],
      "venue" : "Information Theory, IEEE Transactions on,",
      "citeRegEx" : "Bubeck et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bubeck et al\\.",
      "year" : 2013
    }, {
      "title" : "Pure exploration in multi-armed bandits problems",
      "author" : [ "Bubeck", "Sébastien", "Munos", "Rémi", "Stoltz", "Gilles" ],
      "venue" : "In Algorithmic Learning Theory,",
      "citeRegEx" : "Bubeck et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Bubeck et al\\.",
      "year" : 2009
    }, {
      "title" : "Multiple identifications in multi-armed bandits",
      "author" : [ "Bubeck", "Séebastian", "Wang", "Tengyao", "Viswanathan", "Nitin" ],
      "venue" : "In Proceedings of The 30th International Conference on Machine Learning",
      "citeRegEx" : "Bubeck et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bubeck et al\\.",
      "year" : 2013
    }, {
      "title" : "On topk selection in multi-armed bandits and hidden bipartite graphs",
      "author" : [ "Cao", "Wei", "Li", "Jian", "Tao", "Yufei", "Zhize" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Cao et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Cao et al\\.",
      "year" : 2015
    }, {
      "title" : "Tight (lower) bounds for the fixed budget best arm identification bandit problem",
      "author" : [ "Carpentier", "Alexandra", "Locatelli", "Andrea" ],
      "venue" : "In Proceedings of the 29th Conference on Learning",
      "citeRegEx" : "Carpentier et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Carpentier et al\\.",
      "year" : 2016
    }, {
      "title" : "Challenging the empirical mean and empirical variance: a deviation study",
      "author" : [ "Catoni", "Olivier" ],
      "venue" : "In Annales de l’Institut Henri Poincaré, Probabilités et Statistiques,",
      "citeRegEx" : "Catoni and Olivier,? \\Q2012\\E",
      "shortCiteRegEx" : "Catoni and Olivier",
      "year" : 2012
    }, {
      "title" : "On the optimal sample complexity for best arm identification",
      "author" : [ "Chen", "Lijie", "Li", "Jian" ],
      "venue" : "arXiv preprint arXiv:1511.03774,",
      "citeRegEx" : "Chen et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2015
    }, {
      "title" : "Combinatorial pure exploration of multiarmed bandits",
      "author" : [ "Chen", "Shouyuan", "Lin", "Tian", "King", "Irwin", "Lyu", "Michael R", "Wei" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Chen et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2014
    }, {
      "title" : "Pac bounds for multi-armed bandit and markov decision processes",
      "author" : [ "Even-Dar", "Eyal", "Mannor", "Shie", "Mansour", "Yishay" ],
      "venue" : "In Computational Learning Theory,",
      "citeRegEx" : "Even.Dar et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Even.Dar et al\\.",
      "year" : 2002
    }, {
      "title" : "Best arm identification: A unified approach to fixed budget and fixed confidence",
      "author" : [ "Gabillon", "Victor", "Ghavamzadeh", "Mohammad", "Lazaric", "Alessandro" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Gabillon et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Gabillon et al\\.",
      "year" : 2012
    }, {
      "title" : "lil’ucb: An optimal exploration algorithm for multi-armed bandits",
      "author" : [ "Jamieson", "Kevin", "Malloy", "Matthew", "Nowak", "Robert", "Bubeck", "Sébastien" ],
      "venue" : "In Proceedings of the 27th Conference on Learning Theory,",
      "citeRegEx" : "Jamieson et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Jamieson et al\\.",
      "year" : 2014
    }, {
      "title" : "Pac subset selection in stochastic multiarmed bandits",
      "author" : [ "Kalyanakrishnan", "Shivaram", "Tewari", "Ambuj", "Auer", "Peter", "Stone" ],
      "venue" : "In Proceedings of the 29th International Conference on Machine Learning",
      "citeRegEx" : "Kalyanakrishnan et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Kalyanakrishnan et al\\.",
      "year" : 2012
    }, {
      "title" : "Almost optimal exploration in multi-armed bandits",
      "author" : [ "Karnin", "Zohar", "Koren", "Tomer", "Somekh", "Oren" ],
      "venue" : "In Proceedings of the 30th International Conference on Machine Learning",
      "citeRegEx" : "Karnin et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Karnin et al\\.",
      "year" : 2013
    }, {
      "title" : "On the complexity of best arm identification in multiarmed bandit models",
      "author" : [ "Kaufmann", "Emilie", "Cappé", "Olivier", "Garivier", "Aurélien" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Kaufmann et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kaufmann et al\\.",
      "year" : 2015
    }, {
      "title" : "The Sample Complexity of Exploration in the Multi-Armed Bandit Problem",
      "author" : [ "S Mannor", "Tsitsiklis", "J N" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Mannor et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Mannor et al\\.",
      "year" : 2004
    }, {
      "title" : "Deviations of stochastic bandit regret",
      "author" : [ "Salomon", "Antoine", "Audibert", "Jean-Yves" ],
      "venue" : "In Algorithmic Learning Theory,",
      "citeRegEx" : "Salomon et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Salomon et al\\.",
      "year" : 2011
    }, {
      "title" : "A classification framework for anomaly detection",
      "author" : [ "Steinwart", "Ingo", "Hush", "Don R", "Scovel", "Clint" ],
      "venue" : "In Journal of Machine Learning Research,",
      "citeRegEx" : "Steinwart et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Steinwart et al\\.",
      "year" : 2005
    }, {
      "title" : "Selecting among heuristics by solving thresholded k-armed bandit problems",
      "author" : [ "Streeter", "Matthew J", "Smith", "Stephen F" ],
      "venue" : "ICAPS",
      "citeRegEx" : "Streeter et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Streeter et al\\.",
      "year" : 2006
    }, {
      "title" : "Optimal pac multiple arm identification with applications to crowdsourcing",
      "author" : [ "Zhou", "Yuan", "Chen", "Xi", "Li", "Jian" ],
      "venue" : "In Proceedings of the 31st International Conference on Machine Learning",
      "citeRegEx" : "Zhou et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "In this paper, we refer to this setting as the Thresholding Bandit Problem (TBP), which is a specific instance of the combinatorial pure exploration bandit setting introduced in (Chen et al., 2014).",
      "startOffset" : 178,
      "endOffset" : 197
    }, {
      "referenceID" : 12,
      "context" : "This problem is related to the popular combinatorial pure exploration bandit problem known as the TopM problem where the aim of the learner is to return the set of M arms with highest mean (Bubeck et al., 2013b; Gabillon et al., 2012; Kaufmann et al., 2015; Zhou et al., 2014; Cao et al., 2015) - which is a combinatorial version of the best arm identification problem (Even-Dar et al.",
      "startOffset" : 189,
      "endOffset" : 294
    }, {
      "referenceID" : 16,
      "context" : "This problem is related to the popular combinatorial pure exploration bandit problem known as the TopM problem where the aim of the learner is to return the set of M arms with highest mean (Bubeck et al., 2013b; Gabillon et al., 2012; Kaufmann et al., 2015; Zhou et al., 2014; Cao et al., 2015) - which is a combinatorial version of the best arm identification problem (Even-Dar et al.",
      "startOffset" : 189,
      "endOffset" : 294
    }, {
      "referenceID" : 21,
      "context" : "This problem is related to the popular combinatorial pure exploration bandit problem known as the TopM problem where the aim of the learner is to return the set of M arms with highest mean (Bubeck et al., 2013b; Gabillon et al., 2012; Kaufmann et al., 2015; Zhou et al., 2014; Cao et al., 2015) - which is a combinatorial version of the best arm identification problem (Even-Dar et al.",
      "startOffset" : 189,
      "endOffset" : 294
    }, {
      "referenceID" : 6,
      "context" : "This problem is related to the popular combinatorial pure exploration bandit problem known as the TopM problem where the aim of the learner is to return the set of M arms with highest mean (Bubeck et al., 2013b; Gabillon et al., 2012; Kaufmann et al., 2015; Zhou et al., 2014; Cao et al., 2015) - which is a combinatorial version of the best arm identification problem (Even-Dar et al.",
      "startOffset" : 189,
      "endOffset" : 294
    }, {
      "referenceID" : 10,
      "context" : "As mentioned previously, the TBP problem is a specific instance of the combinatorial pure exploration bandit framework introduced in (Chen et al., 2014).",
      "startOffset" : 133,
      "endOffset" : 152
    }, {
      "referenceID" : 10,
      "context" : "Without going into the details of the combinatorial pure exploration setting for which the paper (Chen et al., 2014) derives interesting general results, we will summarize what these results imply for the particular TBP and TopM problems, which are specific cases of the combinatorial pure exploration setting.",
      "startOffset" : 97,
      "endOffset" : 116
    }, {
      "referenceID" : 10,
      "context" : "As it is often the case for pure exploration problems, the paper (Chen et al., 2014) distinguishes between two settings:",
      "startOffset" : 65,
      "endOffset" : 84
    }, {
      "referenceID" : 12,
      "context" : "The similarities and dissemblance of these two settings have been discussed in the literature in the case of the TopM problem (in particular in the case M = 1), see (Gabillon et al., 2012; Karnin et al., 2013; Chen et al., 2014).",
      "startOffset" : 165,
      "endOffset" : 228
    }, {
      "referenceID" : 15,
      "context" : "The similarities and dissemblance of these two settings have been discussed in the literature in the case of the TopM problem (in particular in the case M = 1), see (Gabillon et al., 2012; Karnin et al., 2013; Chen et al., 2014).",
      "startOffset" : 165,
      "endOffset" : 228
    }, {
      "referenceID" : 10,
      "context" : "The similarities and dissemblance of these two settings have been discussed in the literature in the case of the TopM problem (in particular in the case M = 1), see (Gabillon et al., 2012; Karnin et al., 2013; Chen et al., 2014).",
      "startOffset" : 165,
      "endOffset" : 228
    }, {
      "referenceID" : 12,
      "context" : "While as explained in (Audibert & Bubeck, 2010; Gabillon et al., 2012), the two settings share similarities in the specific case when additional information about the problem is available to the learner (such as the complexityH defined in Table 1), they are very different in general and results do not transfer from one setting to the other, see (Bubeck et al.",
      "startOffset" : 22,
      "endOffset" : 70
    }, {
      "referenceID" : 4,
      "context" : ", 2012), the two settings share similarities in the specific case when additional information about the problem is available to the learner (such as the complexityH defined in Table 1), they are very different in general and results do not transfer from one setting to the other, see (Bubeck et al., 2009; Audibert & Bubeck, 2010; Karnin et al., 2013; Kaufmann et al., 2015).",
      "startOffset" : 284,
      "endOffset" : 374
    }, {
      "referenceID" : 15,
      "context" : ", 2012), the two settings share similarities in the specific case when additional information about the problem is available to the learner (such as the complexityH defined in Table 1), they are very different in general and results do not transfer from one setting to the other, see (Bubeck et al., 2009; Audibert & Bubeck, 2010; Karnin et al., 2013; Kaufmann et al., 2015).",
      "startOffset" : 284,
      "endOffset" : 374
    }, {
      "referenceID" : 16,
      "context" : ", 2012), the two settings share similarities in the specific case when additional information about the problem is available to the learner (such as the complexityH defined in Table 1), they are very different in general and results do not transfer from one setting to the other, see (Bubeck et al., 2009; Audibert & Bubeck, 2010; Karnin et al., 2013; Kaufmann et al., 2015).",
      "startOffset" : 284,
      "endOffset" : 374
    }, {
      "referenceID" : 14,
      "context" : "In particular we highlight the following fact: while the fixed confidence setting is relatively well understood in the sense that there are constructions for optimal strategies (Kalyanakrishnan et al., 2012; Jamieson et al., 2014; Karnin et al., 2013; Kaufmann et al., 2015; Chen & Li, 2015), there is an important knowledge gap in the fixed budget setting.",
      "startOffset" : 177,
      "endOffset" : 291
    }, {
      "referenceID" : 13,
      "context" : "In particular we highlight the following fact: while the fixed confidence setting is relatively well understood in the sense that there are constructions for optimal strategies (Kalyanakrishnan et al., 2012; Jamieson et al., 2014; Karnin et al., 2013; Kaufmann et al., 2015; Chen & Li, 2015), there is an important knowledge gap in the fixed budget setting.",
      "startOffset" : 177,
      "endOffset" : 291
    }, {
      "referenceID" : 15,
      "context" : "In particular we highlight the following fact: while the fixed confidence setting is relatively well understood in the sense that there are constructions for optimal strategies (Kalyanakrishnan et al., 2012; Jamieson et al., 2014; Karnin et al., 2013; Kaufmann et al., 2015; Chen & Li, 2015), there is an important knowledge gap in the fixed budget setting.",
      "startOffset" : 177,
      "endOffset" : 291
    }, {
      "referenceID" : 16,
      "context" : "In particular we highlight the following fact: while the fixed confidence setting is relatively well understood in the sense that there are constructions for optimal strategies (Kalyanakrishnan et al., 2012; Jamieson et al., 2014; Karnin et al., 2013; Kaufmann et al., 2015; Chen & Li, 2015), there is an important knowledge gap in the fixed budget setting.",
      "startOffset" : 177,
      "endOffset" : 291
    }, {
      "referenceID" : 12,
      "context" : "the complexity H defined in Table 1, there is a gap between the known upper and lower bounds, see (Audibert & Bubeck, 2010; Gabillon et al., 2012; Karnin et al., 2013; Kaufmann et al., 2015).",
      "startOffset" : 98,
      "endOffset" : 190
    }, {
      "referenceID" : 15,
      "context" : "the complexity H defined in Table 1, there is a gap between the known upper and lower bounds, see (Audibert & Bubeck, 2010; Gabillon et al., 2012; Karnin et al., 2013; Kaufmann et al., 2015).",
      "startOffset" : 98,
      "endOffset" : 190
    }, {
      "referenceID" : 16,
      "context" : "the complexity H defined in Table 1, there is a gap between the known upper and lower bounds, see (Audibert & Bubeck, 2010; Gabillon et al., 2012; Karnin et al., 2013; Kaufmann et al., 2015).",
      "startOffset" : 98,
      "endOffset" : 190
    }, {
      "referenceID" : 10,
      "context" : "This knowledge gap is more acute for the general combinatorial exploration bandit problem defined in the paper (Chen et al., 2014) (see their Theorem 3) - and therefore for the TBP problem (where in fact no fixed budget lower bound exists to the best of our knowledge).",
      "startOffset" : 111,
      "endOffset" : 130
    }, {
      "referenceID" : 10,
      "context" : "The quantitiesH,H2 depend on the means μk of the arm distributions and are defined in (Chen et al., 2014) (and are not the same for TopM and TBP).",
      "startOffset" : 86,
      "endOffset" : 105
    }, {
      "referenceID" : 10,
      "context" : "Our problem is a pure exploration bandit problem, and is in fact, shifting the means by −τ , a specific case of the pure exploration bandit problem considered in (Chen et al., 2014) - namely the specific case where the set of sets of arms that they callM and which is their decision class is the set of all possible set of arms.",
      "startOffset" : 162,
      "endOffset" : 181
    }, {
      "referenceID" : 10,
      "context" : "A similar quantity was introduce for general combinatorial bandit problems (Chen et al., 2014) and is similar in essence to the complexity introduced for the best arm identification problem, see (Audibert & Bubeck, 2010).",
      "startOffset" : 75,
      "endOffset" : 94
    }, {
      "referenceID" : 12,
      "context" : "(Audibert & Bubeck, 2010; Gabillon et al., 2012) (e.",
      "startOffset" : 0,
      "endOffset" : 48
    }, {
      "referenceID" : 15,
      "context" : "Also we would like to highlight that for the related problem of best arm identification, existing fixed budget strategies need to know the budget T in advance (Audibert & Bubeck, 2010; Karnin et al., 2013; Chen et al., 2014) - while our algorithm can be stopped at any time and the bound of Theorem 2 will hold.",
      "startOffset" : 159,
      "endOffset" : 224
    }, {
      "referenceID" : 10,
      "context" : "Also we would like to highlight that for the related problem of best arm identification, existing fixed budget strategies need to know the budget T in advance (Audibert & Bubeck, 2010; Karnin et al., 2013; Chen et al., 2014) - while our algorithm can be stopped at any time and the bound of Theorem 2 will hold.",
      "startOffset" : 159,
      "endOffset" : 224
    }, {
      "referenceID" : 0,
      "context" : ", 2012) or in (Alon et al., 1996).",
      "startOffset" : 14,
      "endOffset" : 33
    }, {
      "referenceID" : 16,
      "context" : "exponential, models, it is possible to obtain a refined lower bound in terms of Kullback- Leibler divergences rather than gaps following (Kaufmann et al., 2015).",
      "startOffset" : 137,
      "endOffset" : 160
    }, {
      "referenceID" : 10,
      "context" : "Settings related to ours have been analyzed in the literature and the state of the art result on our problem can be found (to the best of our knowledge) in the paper (Chen et al., 2014).",
      "startOffset" : 166,
      "endOffset" : 185
    }, {
      "referenceID" : 10,
      "context" : "We believe that this lack of optimality for CSAR is not an artefact of the proof of the paper (Chen et al., 2014), and that CSAR is sub-optimal, as it is a successive reject algorithm with fixed and nonadaptive reject phase length.",
      "startOffset" : 94,
      "endOffset" : 113
    }, {
      "referenceID" : 15,
      "context" : "A similar gap between upper and lower bounds for successive reject based algorithms in the fixed budget setting was also observed for the best arm identification problem when no additional information such as the complexity are known to the learner, see (Audibert & Bubeck, 2010; Karnin et al., 2013; Kaufmann et al., 2015; Chen et al., 2014).",
      "startOffset" : 254,
      "endOffset" : 342
    }, {
      "referenceID" : 16,
      "context" : "A similar gap between upper and lower bounds for successive reject based algorithms in the fixed budget setting was also observed for the best arm identification problem when no additional information such as the complexity are known to the learner, see (Audibert & Bubeck, 2010; Karnin et al., 2013; Kaufmann et al., 2015; Chen et al., 2014).",
      "startOffset" : 254,
      "endOffset" : 342
    }, {
      "referenceID" : 10,
      "context" : "A similar gap between upper and lower bounds for successive reject based algorithms in the fixed budget setting was also observed for the best arm identification problem when no additional information such as the complexity are known to the learner, see (Audibert & Bubeck, 2010; Karnin et al., 2013; Kaufmann et al., 2015; Chen et al., 2014).",
      "startOffset" : 254,
      "endOffset" : 342
    }, {
      "referenceID" : 10,
      "context" : "The paper (Chen et al., 2014) also provides results in the fixed confidence setting, where the objective is to provide an optimal set using the smallest possible sample size.",
      "startOffset" : 10,
      "endOffset" : 29
    }, {
      "referenceID" : 14,
      "context" : "In these results such a gap in optimality does not appear and the algorithm CLUCB they propose is almost optimal, see also (Kalyanakrishnan et al., 2012; Jamieson et al., 2014; Karnin et al., 2013; Kaufmann et al., 2015; Chen & Li, 2015) for related results in the fixed confidence setting.",
      "startOffset" : 123,
      "endOffset" : 237
    }, {
      "referenceID" : 13,
      "context" : "In these results such a gap in optimality does not appear and the algorithm CLUCB they propose is almost optimal, see also (Kalyanakrishnan et al., 2012; Jamieson et al., 2014; Karnin et al., 2013; Kaufmann et al., 2015; Chen & Li, 2015) for related results in the fixed confidence setting.",
      "startOffset" : 123,
      "endOffset" : 237
    }, {
      "referenceID" : 15,
      "context" : "In these results such a gap in optimality does not appear and the algorithm CLUCB they propose is almost optimal, see also (Kalyanakrishnan et al., 2012; Jamieson et al., 2014; Karnin et al., 2013; Kaufmann et al., 2015; Chen & Li, 2015) for related results in the fixed confidence setting.",
      "startOffset" : 123,
      "endOffset" : 237
    }, {
      "referenceID" : 16,
      "context" : "In these results such a gap in optimality does not appear and the algorithm CLUCB they propose is almost optimal, see also (Kalyanakrishnan et al., 2012; Jamieson et al., 2014; Karnin et al., 2013; Kaufmann et al., 2015; Chen & Li, 2015) for related results in the fixed confidence setting.",
      "startOffset" : 123,
      "endOffset" : 237
    }, {
      "referenceID" : 10,
      "context" : "To the best of our knowledge, all strategies except ours have such an optimality gap for fixed budget pure exploration combinatorial bandit problems, while there exists fixed confidence strategies for general pure exploration combinatorial bandits that are very close to optimal, see (Chen et al., 2014).",
      "startOffset" : 284,
      "endOffset" : 303
    }, {
      "referenceID" : 12,
      "context" : "the complexity H , it has been proved in the TopM problem that a UCB-type strategy has probability of error upper bounded as exp(−T/H), see (Audibert & Bubeck, 2010; Gabillon et al., 2012).",
      "startOffset" : 140,
      "endOffset" : 188
    }, {
      "referenceID" : 19,
      "context" : "Active anomaly detection In the case anomaly detection, a common way to characterize anomalies is to describe them as naturally not concentrated (Steinwart et al., 2005).",
      "startOffset" : 145,
      "endOffset" : 169
    }, {
      "referenceID" : 19,
      "context" : "This illustrates the fact that as described in (Steinwart et al., 2005), the problem of anomaly detection is indeed a problem of level set detection - and so the problem of active anomaly detection is a problem of active level set detection on which we can use our approach as explained above.",
      "startOffset" : 47,
      "endOffset" : 71
    }, {
      "referenceID" : 4,
      "context" : "In the former, the goal of the learner is to identify the arm with the highest mean (Bubeck et al., 2009).",
      "startOffset" : 84,
      "endOffset" : 105
    }, {
      "referenceID" : 2,
      "context" : "In the latter, the goal is to maximize the sum of the samples collected by the algorithm up to time T (Auer et al., 1995).",
      "startOffset" : 102,
      "endOffset" : 121
    }, {
      "referenceID" : 4,
      "context" : "Such intuition is backed up by Theorem 1 of (Bubeck et al., 2009), which states that in the absence of additional information and with a fixed budget, the lower the regret suffered in the cumulative setting, expressed in terms of rewards, the higher the regret suffered in the identification problem, expressed in terms of probability of error.",
      "startOffset" : 44,
      "endOffset" : 65
    }, {
      "referenceID" : 4,
      "context" : "This intuition is formalized in (Bubeck et al., 2009) where the authors prove that no algorithm can achieve this without additional information.",
      "startOffset" : 32,
      "endOffset" : 53
    }, {
      "referenceID" : 4,
      "context" : "Our results therefore imply that the knowledge of μ∗ by the learner is a sufficient information so that Theorem 1 of (Bubeck et al., 2009) does not hold anymore and there exists algorithms that solve both problems at the same time, as APT does.",
      "startOffset" : 117,
      "endOffset" : 138
    }, {
      "referenceID" : 12,
      "context" : "TopM problem An extension of the best arm identification problem is known as TopM arms identification problem, where one is concerned with identifying the set of the M arms with the highest means (Bubeck et al., 2013b; Gabillon et al., 2012; Kaufmann et al., 2015; Zhou et al., 2014; Chen et al., 2014; Cao et al., 2015).",
      "startOffset" : 196,
      "endOffset" : 320
    }, {
      "referenceID" : 16,
      "context" : "TopM problem An extension of the best arm identification problem is known as TopM arms identification problem, where one is concerned with identifying the set of the M arms with the highest means (Bubeck et al., 2013b; Gabillon et al., 2012; Kaufmann et al., 2015; Zhou et al., 2014; Chen et al., 2014; Cao et al., 2015).",
      "startOffset" : 196,
      "endOffset" : 320
    }, {
      "referenceID" : 21,
      "context" : "TopM problem An extension of the best arm identification problem is known as TopM arms identification problem, where one is concerned with identifying the set of the M arms with the highest means (Bubeck et al., 2013b; Gabillon et al., 2012; Kaufmann et al., 2015; Zhou et al., 2014; Chen et al., 2014; Cao et al., 2015).",
      "startOffset" : 196,
      "endOffset" : 320
    }, {
      "referenceID" : 10,
      "context" : "TopM problem An extension of the best arm identification problem is known as TopM arms identification problem, where one is concerned with identifying the set of the M arms with the highest means (Bubeck et al., 2013b; Gabillon et al., 2012; Kaufmann et al., 2015; Zhou et al., 2014; Chen et al., 2014; Cao et al., 2015).",
      "startOffset" : 196,
      "endOffset" : 320
    }, {
      "referenceID" : 6,
      "context" : "TopM problem An extension of the best arm identification problem is known as TopM arms identification problem, where one is concerned with identifying the set of the M arms with the highest means (Bubeck et al., 2013b; Gabillon et al., 2012; Kaufmann et al., 2015; Zhou et al., 2014; Chen et al., 2014; Cao et al., 2015).",
      "startOffset" : 196,
      "endOffset" : 320
    }, {
      "referenceID" : 16,
      "context" : "The upper bound and proof for this problem is a direct consequence of Theorem 2, and granted one has such extra-information, outperforms existing results for the fixed budget setting, see (Bubeck et al., 2013b; Kaufmann et al., 2015; Chen et al., 2014; Cao et al., 2015).",
      "startOffset" : 188,
      "endOffset" : 270
    }, {
      "referenceID" : 10,
      "context" : "The upper bound and proof for this problem is a direct consequence of Theorem 2, and granted one has such extra-information, outperforms existing results for the fixed budget setting, see (Bubeck et al., 2013b; Kaufmann et al., 2015; Chen et al., 2014; Cao et al., 2015).",
      "startOffset" : 188,
      "endOffset" : 270
    }, {
      "referenceID" : 6,
      "context" : "The upper bound and proof for this problem is a direct consequence of Theorem 2, and granted one has such extra-information, outperforms existing results for the fixed budget setting, see (Bubeck et al., 2013b; Kaufmann et al., 2015; Chen et al., 2014; Cao et al., 2015).",
      "startOffset" : 188,
      "endOffset" : 270
    }, {
      "referenceID" : 12,
      "context" : "If the complexity H were also known to the learner, the strategy in (Gabillon et al., 2012) would attain a similar performance.",
      "startOffset" : 68,
      "endOffset" : 91
    }, {
      "referenceID" : 10,
      "context" : "For comparison, we use the following methods which include the state of the art CSAR algorithm of (Chen et al., 2014) and two minor adaptations of known methods that are also suitable for our problem.",
      "startOffset" : 98,
      "endOffset" : 117
    }, {
      "referenceID" : 12,
      "context" : "UCB-type algorithm: The algorithm UCBE given and analyzed in (Audibert & Bubeck, 2010) is designed for finding the best arm - and its heuristic is to pull the arm that maximizes a UCB bound - see also (Gabillon et al., 2012) for an adaptation of this algorithm to the general TopM problem.",
      "startOffset" : 201,
      "endOffset" : 224
    }, {
      "referenceID" : 12,
      "context" : "From the theoretical analysis in the paper (Audibert & Bubeck, 2010; Gabillon et al., 2012), it is not hard to see that setting a ≈ (T −K)/H minimizes their upper bound, and that this algorithm attains the same expected loss as ours - but it requires the knowledge of H .",
      "startOffset" : 43,
      "endOffset" : 91
    }, {
      "referenceID" : 10,
      "context" : "CSAR: As mentioned before, this method is given in (Chen et al., 2014).",
      "startOffset" : 51,
      "endOffset" : 70
    } ],
    "year" : 2016,
    "abstractText" : "We study a specific combinatorial pure exploration stochastic bandit problem where the learner aims at finding the set of arms whose means are above a given threshold, up to a given precision, and for a fixed time horizon. We propose a parameter-free algorithm based on an original heuristic, and prove that it is optimal for this problem by deriving matching upper and lower bounds. To the best of our knowledge, this is the first non-trivial pure exploration setting with fixed budget for which optimal strategies are constructed.",
    "creator" : "LaTeX with hyperref package"
  }
}