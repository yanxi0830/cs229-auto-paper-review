{
  "name" : "1506.02344.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Stay on path: PCA along graph paths",
    "authors" : [ "Megasthenis Asteris", "Anastasios Kyrillidis", "Alexandros G. Dimakis", "Han-Gyol Yi", "Bharath Chandrasekaran" ],
    "emails" : [ "MEGAS@UTEXAS.EDU", "ANASTASIOS@UTEXAS.EDU", "DIMAKIS@AUSTIN.UTEXAS.EDU", "GYOL@UTEXAS.EDU", "BCHANDRA@AUSTIN.UTEXAS.EDU" ],
    "sections" : [ {
      "heading" : null,
      "text" : "From a statistical perspective, information on the underlying network may potentially reduce the number of observations required to recover the population principal component. We consider the canonical estimator which optimally exploits the prior knowledge by solving a non-convex quadratic maximization on the empirical covariance. We introduce a simple network and analyze the estimator under the spiked covariance model. We show that side information potentially improves the statistical complexity.\nWe propose two algorithms to approximate the solution of the constrained quadratic maximization, and recover a component with the desired properties. We empirically evaluate our schemes on synthetic and real datasets."
    }, {
      "heading" : "1. Introduction",
      "text" : "Principal Component Analysis (PCA) is an invaluable tool in data analysis and machine learning. Given a set of n centered p-dimensional datapoints Y ∈ Rp×n, the first principal component is\narg max ‖x‖2=1\nx>Σ̂x, (1)\nProceedings of the 32nd International Conference on Machine Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).\nwhere Σ̂ = 1/n ·YY> is the empirical covariance matrix. The principal component spans the direction of maximum data variability. This direction usually involves all p variables of the ambient space, in other words the PC vectors are typically non-sparse. However, it is often desirable to obtain a principal component with specific structure, for example limiting the support of non-zero entries. From a statistical viewpoint, in the high dimensional regime n = O(p), the recovery of the true (population) principal component is only possible if additional structure information, like sparsity, is available for the former (Amini & Wainwright, 2009; Vu & Lei, 2012).\nThere are several approaches for extracting a sparse principal component. Many rely on approximating the solution to\nmax x∈Rp\nx>Σ̂x, subject to ‖x‖2 = 1, ‖x‖0 ≤ k. (2)\nThe non-convex quadratic optimization is NP hard (by a reduction from maximum clique problem), but optimally exploits the side information on the sparsity.\nGraph Path PCA. In this paper we enforce additional structure on the support of principal components. Consider a directed acyclic graph (DAG) G = (V,E) on p vertices. Let S and T be two additional special vertices and consider all simple paths from S to T on the graph G. Ignoring the order of vertices along a path, let P(G) denote the collection of all S-T paths in G. We seek the principal component supported on a path of G, i.e., the solution to\nmax x∈X (G)\nx>Σ̂x, (3)\nwhere X (G) , { x∈Rp : ‖x‖2 = 1, supp(x) ∈ P(G) } . (4)\nWe will argue that this formulation can be used to impose several types of structure on the support of principal components. Note that the covariance matrix Σ̂ and the graph\nar X\niv :1\n50 6.\n02 34\n4v 2\n[ st\nat .M\nL ]\n1 9\nJu n\ncan be arbitrary: the matrix is capturing data correlations while the graph is a mathematical tool to efficiently describe the possible supports of interest. We illustrate this through a few applications.\nFinancial model selection: Consider the problem of identifying which companies out of the S&P500 index capture most data variability. Running Sparse PCA with a sparsity parameter k will select k companies that maximize explained variance. However, it may be useful to enforce more structure: If we must select one company from each business sector (e.g., Energy, Health Care,etc.) how could we identify these representative variables?\nIn Section 4, we show that this additional requirement can be encoded using our graph path framework. We compare our variable selection with Sparse PCA and show that it leads to interpretable results.\nBiological and fMRI networks: Several problems involve variables that are naturally connected in a network. In these cases our Graph Path PCA can enforce interpretable sparsity structure, especially when the starting and ending points are manually selected by domain experts. In section 4, we apply our algorithm on fMRI data using a graph on regions of interest (ROIs) based on the Harvard-Oxford brain structural atlas (Desikan et al., 2006).\nWe emphasize that our applications to brain data is preliminary: the directional graphs we extract are simply based on distance and should not be interpreted as causality, simply as a way of encoding desired supports.\nWhat can we say about the tractability of (3)? We note that despite the additional constraints on the sparsity patterns, the number of admissible support sets (i.e. S-T paths) can be exponential in p, the number of variables. For example, consider a graph G as follows: S connected to two nodes who are then both connected to two nodes, etc. for k levels and finally connected to T . Clearly there are 2k S-T paths and therefore a direct search is not tractable.\nOur Contributions:\n1. From a statistical viewpoint, we show that side information on the underlying graph G can reduce the number of observations required to recover the population principal component x? ∈ X (G) via (3). For our analysis, we introduce a simple, sparsity-inducing network model on p vertices partitioned into k layers, with edges from one layer to the next, and maximum out-degree d (Fig. 1). We show that n = O(log p/k + k log d) observations yi ∼ N(0,Σ), suffice to obtain an arbitrarily good estimate via (3). Our proof follows the steps of (Vu & Lei, 2012).\n2. We complement this with an information-theoretic lower bound on the minimax estimation error, un-\nder the spiked covariance model with latent signal x? ∈ X (G), which matches the upper bound.\n3. We propose two algorithms for approximating the solution of (3), based on those of (Yuan & Zhang, 2013) and (Papailiopoulos et al., 2013; Asteris et al., 2014) for the sparse PCA problem. We empirically evaluate our algorithms on synthetic and real datasets.\nRelated Work There is a large volume of work on algorithms and the statistical analysis of sparse PCA (Johnstone & Lu, 2004; Zou et al., 2006; d’Aspremont et al., 2008; 2007; Johnstone & Lu, 2004; Vu & Lei, 2012; Amini & Wainwright, 2009). On the contrary, there is limited work that considers additional structure on the sparsity patterns. Motivated by a face recognition application, (Jenatton et al., 2010) introduce structured sparse PCA using a regularization that encodes higher-order information about the data. The authors design sparsity inducing norms that further promote a pre-specified set of sparsity patterns.\nFinally, we note that the idea of pursuing additional structure on top of sparsity is not limited to PCA: Modelbased compressive sensing seeks sparse solutions under a restricted family of sparsity patterns (Baldassarre et al., 2013; Baraniuk et al., 2010; Kyrillidis & Cevher, 2012), while structure induced by an underlying network is found in (Mairal & Yu, 2011) for sparse linear regression."
    }, {
      "heading" : "2. A Data Model – Sample Complexity",
      "text" : "The layer graph. Consider a directed acyclic graph G = (V,E) on p vertices, with the following properties:\n• V = {S, T} ∪ V̂ , where S is a source vertex, T is a terminal one, and V̂ is the set of remaining p− 2 vertices.\n• V̂ can be partitioned into k disjoint subsets (layers) L1, . . . ,Lk, i.e., ⋃ i Li = V̂ , and Li ∩ Lj = ∅, ∀i, j ∈\n[k], i 6= j, such that: – Γout(v) ⊂ Li+1, ∀v ∈ Li, for i = 1, . . . , k − 1,\nwhere Γout(v) denotes the out-neighborhood of v.\n– Γout(S) = L1, and Γout(v) = {T}, ∀v ∈ Lk.\nIn the sequel, for simplicity, we will further assume that p− 2 is a multiple of k and |Li|= (p− 2)/k, ∀i ∈ [k]. Further, |Γout(v)| = d, ∀v ∈ Li, i = 1, . . . , k − 1, and |Γin(v)| = d, ∀v ∈ Li, i = 2, . . . , k, where Γin(v) denotes the in-neighborhood of v. In words, the edges from one layer are maximally spread accross the vertices of the next. We refer to G as a (p, k, d)-layer graph.\nFig. 1 illustrates a (p, k, d)-layer graph G. The highlighted vertices form an S-T path π: a set of vertices forming a trail from S to T . Let P(G) denote the collection of S-T paths in a graphG for a given pair of source and terminal vertices. For the (p, k, d)-layer graph, |π| = k, ∀π ∈ P(G), and\n|P(G)| = |L1| · dk−1 = p−2k · dk−1 ≤ ( p−2 k ) ,\nsince d ∈ {1, . . . , (p−2)/k}.\nSpike along a path. We consider the spiked covariance model, as in the sparse PCA literature (Johnstone & Lu, 2004; Amini & Wainwright, 2008). Besides sparsity, we impose additional structure on the latent signal; structure induced by a (known) underlying graph G.\nConsider a p-dimensional signal x? and a bijective mapping between the p variables in x? and the vertices of G. For simplicity, assume that the vertices of G are labeled so that xi is associated with vertex i ∈ V . We restrict x? in\nX (G) , { x∈Rp : ‖x‖2 = 1, supp(x) ∈ P(G) } ,\nthat is, x? is a unit-norm vector whose active (nonzero) entries correspond to vertices along a path in P(G). We observe n points (samples) {yi}ni=1 ∈ Rp, generated randomly and independently as follows:\nyi = √ β · ui · x? + zi, (5)\nwhere the scaling coefficient ui ∼ N (0, 1) and the additive noise zi ∼ N (0, Ip) are independent. Equivalently, yis are i.i.d. samples, distributed according to N (0,Σ), where\nΣ = Ip + β · x?x>? . (6)"
    }, {
      "heading" : "2.1. Lower bound",
      "text" : "Theorem 1 (Lower Bound). Consider a (p, k, d)-layer graph G on p vertices, with k ≥ 4, and log d ≥ 4H(3/4). (Note that p− 2 ≥ k · d), and a signal x? ∈ X (G). Let {yi}ni=1 be a sequence of n random observations, independently drawn according to probability density function\nDp(x?) = N ( 0, Ip + β · x?x>? ) ,\nfor some β > 0. Let D(n)p (x?) denote the product measure over the n independent draws. Consider the problem of estimating x? from the n observations, given G. There exists x? ∈ X (G) such that for every estimator x̂, ED(n)p (x?) [ ‖x̂x̂> − x?x>? ‖F ] ≥\n1 2 √ 2 · √ min { 1, C ′·(1+β) β2 · 1n ( log p−2k + k 4 log d )} . (7)\nTheorem 1 effectively states that for some latent signal x? ∈ X (G), and observations generated according to the spiked covariance model, the minimax error is bounded away from zero, unless n = Ω (log p/k + k log d). In the sequel, we provide a sketch proof of Theorem 1, following the steps of (Vu & Lei, 2012).\nThe key idea is to discretize the space X (G) in order to utilize the Generalized Fano Inequality (Yu, 1997). The next lemma summarizes Fano’s Inequality for the special case in which the n observations are distibuted according to the n-fold product measure D(n)p (x?): Lemma 2.1 (Generalized Fano (Yu, 1997)). Let X ⊂ X (G) be a finite set of points x1, . . . ,x|X | ∈ X (G), each yielding a probability measure D(n)p (xi) on the n observations. If d(xi,xj) ≥ α, for some pseudo-metric1 d(·, ·) and the Kullback-Leibler divergences satisfy\nKL ( D(n)p (xi) ‖ D(n)p (xj) ) ≤ γ,\nfor all i 6= j, then for any estimator x̂\nmax xi∈X ED(n)p (xi)[d(x̂,xi)] ≥ α 2 · ( 1− γ + log 2 log |X | ) . (8)\nInequality (8), using the pseudo-metric\nd (x̂,x) , ‖x̂x̂> − xx>‖F, will yield the desired lower bound of Theorem 1 on the minimax estimation error (Eq. (7)). To that end, we need to show the existence of a sufficiently large set X ⊆ X (G) such that (i) the points inX are well separated under d(·, ·), while (ii) the KL divergence of the induced probability measures is upper appropriately bounded. Lemma 2.2. (Local Packing) Consider a (p, k, d)-layer graph G on p vertices with k ≥ 4 and log d ≥ 4 ·H(3/4). For any ∈ (0, 1], there exists a set X ⊂ X (G) such that\n/ √ 2 < ‖xi − xj‖2 ≤ √\n2 · , for all xi,xj ∈ X , xi 6= xj , and\nlog |X | ≥ log p− 2 k + 1/4 · k · log d.\n1 A pseudometric on a set X is a function d : Q2 → R that satisfies all properties of a distance (non-negativity, symmetry, triangle inequality) except the identity of indiscernibles: d(q,q) = 0, ∀q ∈ Q but possibly d(q1,q2) = 0 for some q1 6= q2 ∈ Q.\nProof. (See Appendix 7).\nFor a set X with the properties of Lemma 2.2, taking into account the fact that ‖xix>i − xjx>j ‖F ≥ ‖xi − xj‖2 (Lemma A.1.2 of (Vu & Lei, 2012)), we have\nd2(xi,xj) = ‖xix>i − xjx>j ‖2F > 2\n2 , α2. (9)\n∀xi,xj ∈ X , xi 6= xj . Moreover,\nKL(Dp(xi) ‖ Dp(xj)) = β2(1+β) · [ (1 + β)×\nTr (( I− xjx>j ) xix > i ) −Tr ( xjx > j (I− xix>i ) )] = β 2\n4(1+β) · ‖xix>i − xjx>j ‖2F ≤ β\n2\n(1+β) · ‖xi − xj‖22.\nIn turn, for the n-fold product distribution, and taking into account that ‖xi − xj‖2 ≤ √ 2 · ,\nKL ( D(n)p (xi) ‖ D(n)p (xj) ) ≤ 2nβ 2 2\n(1 + β) , γ. (10)\nEq. (9) and (10) establish the parameters α and γ required by Lemma 2.1. Substituting those into (8), along with the lower bound of Lemma 2.2 on |X |, we obtain\nmax xi∈X ED(n)p (xi)[d(x̂,xi)] ≥ 2 √ 2 1− n 2 2β2(1+β) + log 2 log |X | . (11) The final step towards establishing the desired lower bound in (7) is to appropriately choose . One can verify that if\n2 = min {\n1, C ′·(1+β) β2 · 1n ( log p−2k + k 4 · log d )} , (12)\nwhere C ′ > 0 is a constant to be determined, then\nn · 2 2β2\n(1 + β)\n1 log |X | ≤ 1 4 and log |X | ≥ 4 log 2, (13)\n(see Appendix 8 for details). Under the conditions in (13), the inequality in (11) implies that\nmax xi∈X\nED(n)p (xi)[d(x̂,xi)] ≥ 12√2 · . (14)\nSubstituting according to (12), yields the desired result in (7), completing the proof of Theorem 1."
    }, {
      "heading" : "2.2. Upper bound",
      "text" : "Our upper bound is based on the estimator obtained via the constrained quadratic maximizaton in (3). We note that the analysis is not restricted to the spiked covariance model; it applies to a broader class of distributions (see Assum. 1).\nTheorem 2 (Upper bound). Consider a (p, k, d)-layer graph G and x? ∈ X (G). Let {yi}ni=1 be a sequence of n i.i.d. N (0,Σ) samples, where Σ 0 with eigenvalues\nλ1 > λ2 ≥ . . ., and principal eigenvector x?. Let Σ̂ be the empirical covariance of the n samples, x̂ the estimate of x? obtained via (3), and , ‖x̂x̂> − x?x>? ‖F. Then,\nE[ ] ≤ C · λ1 λ1 − λ2 · 1 n ·max\n{√ nA, A } ,\nwhere A = O ( log p−2k + k log d ) .\nIn the sequel, we provide a sketch proof of Theorem 2. The proof closely follows the steps of (Vu & Lei, 2012) in developing their upper bound for the the sparse PCA problem.\nLemma 2.3 (Lemma 3.2.1 (Vu & Lei, 2012)). Consider Σ ∈ Sp×p+ , with principal eigenvector x? and λgap,λ1 − λ2(Σ). For any x̃ ∈ Rp with ‖x̃‖2 = 1,\nλ1−λ2 2 · ‖x̃x̃> − x?x>? ‖2F ≤ 〈 Σ, x?x > ? − x̃x̃> 〉 .\nLet x̂ be an estimate of x? via (3), and ,‖x̂x̂>−x?x>? ‖F. From Lemma 2.3, it follows (see (Vu & Lei, 2012)) that\nλ1−λ2 2 · 2 ≤ 〈 Σ̂−Σ, x̂x̂> − x?x>? 〉 . (15)\nBoth x? and x̂ belong to X (G): unit-norm vectors, with support of cardinality k+2 coinciding with a path inP(G). Their difference is supported in P2(G): the collection of sets formed by the union of two sets in P(G). Let X 2(G) denote the set of unit norm vectors supported in P2(G). Via an appropriate upper bounding of the right-hand side of (15), (Vu & Lei, 2012) show that\nE[ ] ≤ Ĉλ1−λ2 · E [ sup θ∈X 2 ∣∣θ>(Σ̂−Σ)θ∣∣] ,\nfor an appropriate constant Ĉ > 0. Further, under the assumptions on the data distribution, and utilizing a result due to (Mendelson, 2010),\nE [ sup θ∈X 2 ∣∣θ>(Σ̂−Σ)θ∣∣] ≤ C ′K2λ1 n max {√ nA, A2 } ,\nfor C ′ and K constants depending on the distribution, and\nA , EY∼N(0,Ip) [ sup θ∈X 2〈Y,θ〉 ] . (16)\nThis reduces the problem of bounding E[ ] to bounding the supremum of a Gaussian process. Let Nδ ⊂ X 2(G) be a minimal δ-covering of X 2(G) in the Euclidean metric with the property that ∀x ∈ X 2(G), ∃y ∈ Nδ such that ‖x− y‖2 ≤ δ and supp(x− y) ∈ P2(G). Then,\nsup θ∈X 2〈Y,θ〉 ≤ (1− δ)−1 · maxθ∈Nδ〈Y,θ〉. (17)\nTaking expectation w.r.t. Y and applying a union bound on the right hand side, we conclude\nA ≤ C̃ · (1− δ)−1 · √ log |Nδ|. (18)\nIt remains to construct a δ-covering Nδ with the desired properties. To this end, we associate isometric copies of S2k+12 with each support set in P2(G). It is known that there exists a minimal δ-covering for S2k+12 with cardinality at most (1 + 2/δ)2k+2. The union of the local δ-nets forms a set Nδ with the desired properties. Then,\nlog |Nδ| ≤ log |P2(G)|+ 2(k + 1) log(1 + 2/δ) = O ( log p−2k + k log d ) ,\nfor any constant δ. Substituting in (18), implies the desired bound on E[ ], completing the proof of Theorem 2."
    }, {
      "heading" : "3. Algorithmic approaches",
      "text" : "We propose two algorithms for approximating the solution of the constrained quadratic maximization in (3):\n1. The first is an adaptation of the truncated power iteration method of (Yuan & Zhang, 2013) for the problem of computing sparse eigenvectors. 2. The second relies on approximately solving (3) on a low rank approximation of Σ̂, similar to (Papailiopoulos et al., 2013; Asteris et al., 2014).\nBoth algorithms rely on a projection operation from Rp onto the feasible set X (G), for a given graph G = (V,E). Besides the projection step, the algorithms are oblivious to the specifics of the constraint set,2 and can adapt to different constraints by modifying the projection operation."
    }, {
      "heading" : "3.1. Graph-Truncated Power Method",
      "text" : "Algorithm 1 Graph-Truncated Power Method input Σ̂ ∈ Rp×p, G = (V,E), x0 ∈ Rp\n1: i← 0 2: repeat 3: wi ← Σ̂xi 4: xi+1 ← ProjX (G)(wi) 5: i← i+ 1 6: until Convergence/Stop Criterion\noutput xi\nWe consider a simple iterative procedure, similar to the truncated power method of (Yuan & Zhang, 2013) for the problem of computing sparse eigenvectors. Our algorithm produces sequence of vectors xi ∈ X (G), i ≥ 0, that serve as intermediate estimates of the desired solution of (3).\nThe procedure is summarized in Algorithm 1. In the ith iteration, the current estimate xi is multiplied by the empirical covariance Σ̂, The product wi ∈ Rp is projected back\n2For Alg. 2, the observation holds under mild assumptions: X (G) must be such that ‖x‖2 = Θ(1), while±x ∈ X (G) should both achieve the same objective value.\nto the feasible set X (G), yielding the next estimate xi+1. The core of Algorithm 1 lies in the projection operation,\nProjX (G)(w) , arg min x∈X (G)\n1 2 ‖x−w‖22, (19)\nwhich is analyzed separately in Section 3.3. The initial estimate x0 can be selected randomly or based on simple heuristics, e.g., the projection on X (G) of the column of Σ̂ corresponding to the largest diagonal entry. The algorithm terminates when some convergence criterion is satisfied.\nThe computational complexity (per iteration) of Algorithm 1 is dominated by the cost of matrix-vector multiplication and the projection step. The former is O(k · p), where k is cardinality of the largest support in X (G). The projection operation for the particular set X (G), boils down to solving the longest path problem on a weighted variant of the DAG G (see Section 3.3), which can be solved in time O(|V |+ |E|), i.e., linear in the size of G."
    }, {
      "heading" : "3.2. Low-Dimensional Sample and Project",
      "text" : "The second algorithm outputs an estimate of the desired solution of (3) by (approximately) solving the constrained quadratic maximization not on the original matrix Σ̂, but on a low rank approximation Σ̂r of Σ̂, instead:\nΣ̂r = r∑ i=1 λiqiq > i = r∑ i=1 viv > i = VV >, (20)\nwhere λi is the ith largest eigenvalue of Σ̂, qi is the corresponding eigenvector, vi, √ λi · qi, and V is the p × r matrix whose ith column is equal to vi. The approximation rank r is an accuracy parameter; typically, r p. Our algorithm operates3 on Σ̂r and seeks\nxr , arg max x∈X (G)\nx> Σ̂r x. (21)\nThe motivation is that an (approximate) solution for the low-rank problem in (21) can be efficiently computed. Intuitively, if Σ̂r is a sufficiently good approximation of the original matrix Σ̂, then xr would perform similarly to the solution x? of the original problem (3).\nThe Algorithm. Our algorithm samples points from the low-dimensional principal subspace of Σ̂, and projects them on the feasible set X (G), producing a set of candidate estimates for xr. It outputs the candidate that maximizes the objective in (21). The exact steps are formally presented in Algorithm 2. The following paragraphs delve into the details of Algorithm 2.\n3 Under the spiked covariance model, this approach may be asymptotically unsuitable; as the ambient dimension increases, it with fail to recover the latent signal. Empirically, however, if the spectral decay of Σ̂ is sharp, it yields very competitive results.\nAlgorithm 2 Low-Dimensional Sample and Project input Σ̂ ∈ Rp×p, G = (V,E), r ∈ [p], > 0\n1: [Q,Λ]← svd(Σ̂, r) 2: V← QΛ1/2 {Σ̂r,VV>} 3: C ← ∅ {Candidate solutions} 4: for i = 1 : O( −r · log p) do 5: ci ← uniformly sampled from Sr−1 6: wi ← Vci 7: xi ← ProjX (G)(wi) 8: C = C ∪ {xi} 9: end for\noutput x̂r ← argmaxx∈C ‖V>x‖22"
    }, {
      "heading" : "3.2.1. THE LOW RANK PROBLEM",
      "text" : "The rank-r maximization in (21) can be written as\nmax x∈X (G) x>Σ̂rx = max x∈X (G) ‖V>x‖22, (22)\nand in turn (see (Asteris et al., 2014) for details), as a double maximization over the variables c ∈ Sr−1 and x ∈ Rp:\nmax x∈X (G) ‖V>x‖22 = max c∈Sr−1 max x∈X (G)\n( (Vc) > x )2 . (23)\nThe rank-1 case. Let w,Vc; w is only a vector in Rp. For given c and w, the x that maximizes the objective in (23) (as a function of c) is\nx(c) ∈ arg max x∈X (G)\n( w>x )2 . (24)\nThe maximization in (24) is nothing but a rank-1 instance of the maximization in (22). Observe that if x ∈ X (G), then −x ∈ X (G), and the two vectors attain the same objective value. Hence, (24) can be simplified:\nx(c) ∈ arg max x∈X (G) w>x. (25)\nFurther, since ‖x‖2 = 1, ∀x ∈ X (G), the maximization in (25) is equivalent to minimizing 12‖w − x‖22. In other words, x(c) is just the projection of w ∈ Rp onto X (G):\nx(c) ∈ ProjX (G)(w). (26)\nThe projection operator is described in Section 3.3. Multiple rank-1 instances. Let ( cr,xr ) denote a pair that attains the maximum value in (23). If cr was known, then xr would coincide with the projection x(cr) of w = Vcr on the feasible set, according to (26).\nOf course, the optimal value cr of the auxiliary variable is not known. Recall, however, that cr lies on the low dimensional manifold Sr−1. Consider an -net N covering the r-dimensional unit sphere Sr−1; Algorithm 2 constructs\nsuch a net by random sampling. By definition,N contains at least one point, call it ĉr, in the vicinity of cr. It can be shown that the corresponding solution x(ĉr) in (24) will perform approximately as well as the optimal solution xr, in terms of the quadratic objective in (23), for a large, but tractable, number of points in the -net of Sr−1."
    }, {
      "heading" : "3.3. The Projection Operator",
      "text" : "Algorithms 1, and 2 rely on a projection operation from Rp onto the feasible set X (G) (Eq. (4)). We show that the projection effectively reduces to solving the longest path problem on (a weighted variant of) G.\nThe projection operation, defined in Eq. (19), can be equivalently4 written as\nProjX (G)(w) , arg max x∈X (G)\nw>x.\nFor any x ∈ X (G), supp(x) ∈ P(G). For a given set π, by the Cauchy-Schwarz inequality,\nw>x = ∑ i∈π wixi ≤ ∑ i∈π w 2 i = ŵ >1π, (27)\nwhere ŵ ∈ Rp is the vector obtained by squaring the entries of w, i.e., ŵi = w2i , ∀i ∈ [n], and 1π ∈ {0, 1}p denotes the characteristic of π. Letting x[π] denote the subvector of x supported on π, equality in (27) can be achieved by x such that x[π] = w[π]/‖w[π]‖2, and x[πc] = 0. Hence, the problem in (27) reduces to determining\nπ(w) ∈ arg max π∈P(G) ŵ>1π. (28)\nConsider a weighted graphGw, obtained fromG = (V,E) by assigning weight ŵv = w2v on vertex v ∈ V . The objective function in (28) equals the weight of the path π in Gw, i.e., the sum of weights of the vertices along π. Determining the optimal support π(w) for a given w, is equivalent to solving the longest (weighted) path problem5 on Gw.\nThe longest (weighted) path problem is NP-hard on arbitrary graphs. In the case of DAGs, however, it can be solved using standard algorithms relying on topological sorting in time O(|V |+ |E|) (Cormen et al., 2001), i.e., linear in the size of the graph. Hence, the projection x can be determined in time O(p+ |E|).\n4 It follows from expanding the quadratic 1 2 ‖x−w‖22 and the fact that ‖x‖2 = 1, ∀x ∈ X (G). 5 The longest path problem is commonly defined on graphs with weighted edges instead of vertices. The latter is trivially transformed to the former: set w(u, v)← w(v), ∀(u, v) ∈ E, where w(u, v) denotes the weight of edge (u, v), and w(v) that of vertex v. Auxiliary edges can be introduced for source vertices."
    }, {
      "heading" : "4. Experiments",
      "text" : ""
    }, {
      "heading" : "4.1. Synthetic Data.",
      "text" : "We evaluate Alg. 1 and 2 on synthetic data, generated according to the model of Sec. 2. We consider two metrics: the loss function ‖x̂x̂> − x?x?‖F and the Support Jaccard distance between the true signal x? and the estimate x̂.\nFor dimension p, we generate a (p, k, d)-layer graph G, with k = log p and out-degree d = p/k, i.e., each vertex is connected to all vertices in the following layer. We augment the graph with auxiliary source and terminal vertices S and T with edges to the original vertices as in Fig. 1.\nPer random realization, we first construct a signal x? ∈ X (G) as follows: we randomly select an S-T path π in G, and assign random zero-mean Gaussian values to the entries of x? indexed by π. The signal is scaled to unit length. Given x?, we generate n independent samples according to the spiked covariance model in (5).\nFig. 2 depicts the aforementioned distance metrics as a function of the number n of observations. Results are the average of 100 independent realizations. We repeat the procedure for multiple values of the ambient dimension p.\nComparison with Sparse PCA. We compare the performance of Alg. 1 and Alg. 2 with their sparse PCA counterparts: the Truncated Power Method of (Yuan & Zhang, 2013) and the Spannogram Alg. of (Papailiopoulos et al., 2013), respectively.\nFig. 3 depicts the metrics of interest as a function of the number of samples, for all four algorithms. Here, samples are drawn i.i.d from N(0,Σ), where Σ has principal eigenvector equal to x?, and power law spectral decay: λi = i −1/4. Results are an average of 100 realizations.\nThe side information on the structure of x? assists the recovery: both algorithms achieve improved performance compared to their sparse PCA counterparts. Here, the power method based algorithms exhibit inferior perfor-\nmance, which may be attributed to poor initialization. We note, though, that at least for the size of these experiments, the power method algorithms are significantly faster."
    }, {
      "heading" : "4.2. Finance Data.",
      "text" : "This dataset contains daily closing prices for 425 stocks of the S&P 500 Index, over a period of 1259 days (5- years): 02.01.2010 – 01.28.2015, collected from Yahoo! Finance. Stocks are classified, according to the Global Industry Classification Standard (GICS), into 10 business sectors e.g., Energy, Health Care, Information Technology, etc (see Fig. 4 for the complete list).\nWe seek a set of stocks comprising a single representative from each GICS sector, which captures most of the variance in the dataset. Equivalently, we want to compute a structured principal component constrained to have exactly 10 nonzero entries; one for each GICS sector.\nConsider a layer graph G = (V,E) (similar to the one depicted in Fig. 1) on p = 425 vertices corresponding to the 425 stocks, partitioned into k = 10 groups (layers) L1, . . . ,L10 ⊆ V , corresponding to the GICS sectors. Each vertex in layer Li has outgoing edges towards all (and only the) vertices in layer Li+1. Note that (unlike Fig. 1) layers do not have equal sizes, and the vertex out-degree varies across layers. Finally, we introduce auxiliary vertices S and T connected with the original graph as in Fig. 1.\nObserve that any set of sector-representatives corresponds to an S-T path in G, and vice versa. Hence, the desired set of stocks can be obtained by finding a structured principal component constrained to be supported along an S-T path in G. Note that the order of layers in G is irrelevant.\nFig. 4 depicts the subset of stocks selected by the proposed structure PCA algorithms (Alg. 1, 2). A single representative is selected from each sector. For comparison, we also run two corresponding algorithms for sparse PCA, with sparsity parameter k = 10, equal to the number of sectors. As expected, the latter yield components achieving higher values of explained variance, but the selected stocks origi-\nnate from only 5 out of the 10 sectors."
    }, {
      "heading" : "4.3. Neuroscience Data.",
      "text" : "We use a single-session/single-participant resting state functional magnetic resonance imaging (resting state fMRI) dataset. The participant was not instructed to perform any explicit cognitive task throughout the scan (Van Essen et al., 2013). Data was provided by the Human Connectome Project, WU-Minn Consortium.6\nMean timeseries of n = 1200 points for p = 111 regionsof-interest (ROIs) are extracted based on the HarvardOxford Atlas (Desikan et al., 2006). The timescale of analysis is restricted to 0.01–0.1Hz. Based on recent results on resting state fMRI neural networks, we set the posterior cingulate cortex as a source node S, and the prefrontal cortex as a target node T (Greicius et al., 2009). Starting from S, we construct a layered graph with k = 4, based on the physical (Euclidean) distances between the center of mass of the ROIs: i.e., given layer Li, we construct Li+1 from non-selected nodes that are close in the Euclidean sense. Here, |L1| = 34 and |Li| = 25 for i = 2, 3, 4. Each layer is fully connected with its previous one. No further assumptions are derived from neurobiology.\nThe extracted component suggests a directed pathway from the posterior cingulate cortex (S) to the prefrontal cortex (T ), through the hippocampus (1), nucleus accumbens (2), parahippocampal gyrus (3), and frontal operculum (4) (Fig. 5). Hippocampus and the parahippocampal gyrus are critical in memory encoding, and have been found to be structurally connected to the posterior cingu-\n6(Principal Investigators: David Van Essen and Kamil Ugurbil; 1U54MH091657) funded by the 16 NIH Institutes and Centers that support the NIH Blueprint for Neuroscience Research; and by the McDonnell Center for Systems Neuroscience at Washington University.\nlate cortex and the prefrontal cortex (Greicius et al., 2009). The nucleus accumbens receives input from the hippocampus, and plays an important role in memory consolidation (Wittmann et al., 2005). It is noteworthy that our approach has pinpointed the core neural components of the memory network, given minimal information."
    }, {
      "heading" : "5. Conclusions",
      "text" : "We introduced a new problem: sparse PCA where the set of feasible support sets is determined by a graph on the variables. We focused on the special case where feasible sparsity patterns coincide with paths on the underlying graph. We provided an upper bound on the statistical complexity of the constrained quadratic maximization estimator (3), under a simple graph model, complemented with a lower bound on the minimax error. Finally, we proposed two algorithms to extract a component accommodating the graph constraints and applied them on real data from finance and neuroscience.\nA potential future direction is to expand the set of graphinduced sparsity patterns (beyond paths) that can lead to interpretable solutions and are computationally tractable. We hope this work triggers future efforts to introduce and exploit such underlying structure in diverse research fields."
    }, {
      "heading" : "6. Acknowledgments",
      "text" : "The authors would like to acknowledge support from grants: NSF CCF 1422549, 1344364, 1344179 and an ARO YIP award."
    }, {
      "heading" : "7. Proof of Lemma 2.2 – Local Packing Set",
      "text" : "Towards the proof of Lemma 2.2, we develop a modified version of the Varshamov-Gilbert Lemma adapted to our specific model: the set of characteristic vectors of the S-T paths of a (p, k, d)-layer graph G.\nLet δH(x,y) denote the Hamming distance between two points x,y ∈ {0, 1}p:\nδH(x,y) , |{i : xi 6= yi}| .\nLemma 7.4. Consider a (p, k, d)-layer graph G on p vertices and the collection P(G) of S-T paths in G. Let\nΩ , { x ∈ {0, 1}p : supp(x) ∈ P(G) } ,\ni.e., the set of characteristic vectors of all S-T paths in G. For every ξ ∈ (0, 1), there exists a set, Ωξ ⊂ Ω such that\nδH(x,y) > 2(1− ξ) · k, ∀x,y ∈ Ωξ,x 6= y, (29)\nand\nlog |Ωξ| ≥ log p−2k + (ξ · k − 1) · log d− k ·H(ξ), (30)\nwhere H(·) is the binary entropy function.\nProof. Consider a labeling 1, . . . , p of the p vertices in G, such that variable ωi is associated with vertex i. Each point ω ∈ Ω is the characteristic vector of a set inP(G); nonzero entries of ω correspond to vertices along an S-T path in G. With a slight abuse of notation, we refer to ω as a path in G. Due to the structure of the (p, k, d)-layer graph G, all points in Ω have exactly k + 2 nonzero entries, i.e.,\nδH(ω,0) = k + 2, ∀ω ∈ Ω.\nEach vertex in ω lies in a distinct layer of G. In turn, for any pair of points ω,ω′ ∈ Ω,\nδH(ω,ω ′) = 2 · ( k − |{i : ωi = ω′i = 1}| − 2 ) . (31)\nNote that the Hamming distance between the two points is a linear function of the number of their common nonzero entries, while it can take only even values with a maximum value of 2k.\nWithout loss of generality, let S and T corresponding to vertices 1 and p, respectively. Then, the above imply that\nω1 = ωp = 1, ∀ω ∈ Ω.\nConsider a fixed point ω̂ ∈ Ω, and let B(ω̂, r) denote the Hamming ball of radius r centered at ω̂, i.e.,\nB(ω̂, r) , { ω ∈ {0, 1}p : δH(ω̂,ω) ≤ r } .\nThe intersection B(ω̂, r)∩Ω corresponds to S-T paths inG that have at least k − r/2 additional vertices in common with ω̂ besides vertices 1 and p that are common to all paths in Ω:\nB(ω̂, r) ∩ Ω = {ω ∈ Ω : δH(ω̂,ω) ≤ r} = { ω ∈ Ω : |{i : ω̂i = ωi = 1}| ≥ k − r2 + 2 } ,\nwhere the last equality is due to (31). In fact, due to the structure of G, the set B(ω̂, r) ∩ Ω corresponds to the S-T paths that meet ω̂ in at least k − r/2 intermediate layers. Taking into account that |Γin(v)| = |Γout(v)| = d, for all vertices v in V (G) (except those in the first and last layer),\n|B(ω̂, r) ∩ Ω| ≤ ( k\nk − r2\n) · dk−(k− r 2 ) = ( k\nk − r2\n) · d r 2 .\nNow, consider a maximal set Ωξ ⊂ Ω satisfying (29), i.e., a set that cannot be augmented by any other point in Ω. The union of balls B(ω, 2(1− ξ) · (k − 1)) over all ω ∈ Ωξ covers Ω. To verify that, note that if there existsω′ ∈ Ω\\Ωξ such that δH(ω,ω′) > 2(1− ξ) · (k − 1), ∀ω ∈ Ωξ, then Ωξ∪{ω′} satisfies (29) contradicting the maximality of Ωξ. Based on the above,\n|Ω| ≤ ∑ ω∈Ωξ |B(ω, 2(1− ξ) · k) ∩ Ω|\n≤ ∑ x∈Ωξ ( k k−(1−ξ)k ) · d(1−ξ)·k\n≤ ∑ x∈Ωξ ( k ξk ) · d(1−ξ)·k ≤ |Ωξ| · 2k·H(ξ) · d(1−ξ)·k.\nTaking into account that\n|Ω| = |P(G)| = p− 2 k · dk−1,\nwe conclude that\np− 2 k · dk−1 ≤ |Ωξ| · 2k·H(ξ) · d(1−ξ)·k,\nfrom which the desired result follows.\nLemma 2.2. (Local Packing) Consider a (p, k, d)-layer graph G on p vertices with k ≥ 4 and log d ≥ 4 ·H(3/4). For any ∈ (0, 1], there exists a set X ⊂ X (G) such that\n/ √ 2 < ‖xi − xj‖2 ≤ √ 2 · ,\nfor all xi,xj ∈ X , xi 6= xj , and\nlog |X | ≥ log p− 2 k + 1 4 · k log d.\nProof. Without loss of generality, consider a labeling 1, . . . , p of the p vertices in G, such that S and T correspond to vertices 1 and p, respectively. Let\nΩ , { x ∈ {0, 1}p : supp(x) ∈ P(G) } ,\nwhere P(G) is the collection of S-T paths in G. By Lemma 7.4, and for ξ = 3/4, there exists a set Ωξ ⊆ Ω such that\nδH(ωi,ωj) > 1\n2 · k, (32)\n∀ωi,ωj ∈ Ωξ, ωi 6= ωj , and,\nlog |Ωξ| ≥ log p−2k + ( 3 4 · k − 1 ) log d− k ·H ( 3 4 ) ≥ log p−2k + 24 · k · log d− k ·H ( 3 4\n) ≥ log p−2k + 14 · k · log d (33)\nwhere the second and third inequalites hold under the assumptions of the lemma; k ≥ 4 and log d ≥ 4 ·H(3/4). Consider the bijective mapping ψ : Ωξ → Rp defined as\nψ(ω) =\n[√ (1− 2) 2 · ω1, √k · ω2:p−1, √ (1− 2) 2 · ωp ] .\nWe show that the set\nX , {ψ(ω) : ω ∈ Ωξ} .\nhas the desired properties. First, to verify thatX is a subset of X (G), note that ∀ω ∈ Ωξ ⊂ Ω,\nsupp(ψ(ω)) = supp(ω) ∈ P(G), (34)\nand\n‖ψ(ω)‖22 = 2 · (1− 2) 2 + 2 k · p−1∑ i=2 ωi = 1.\nSecond, for all pairs of points xi,xj ∈ X ,\n‖xi − xj‖22 = δH(ωi,ωj) · 2\nk ≤ 2 · k ·\n2\nk = 2 · 2.\nThe inequality follows from the fact that δH(ω,0) = k+ 2 ω1 = 1 and ωp = 1, ∀ω ∈ Ωξ, and in turn\nδH(ωi,ωj) ≤ 2 · k.\nSimilarly, for all pairs xi,xj ∈ X , xi 6= xj ,\n‖xi − xj‖2 = δH(ωi,ωj) · 2 k ≥ 1 2 · k · 2 k = 2 2 ,\nwhere the inequality is due to (32). Finally, the lower bound on the cardinality of X follows immediately from (33) and the fact that |X | = |Ωξ|, which completes the proof."
    }, {
      "heading" : "8. Details in proof of Lemma 1",
      "text" : "We want to show that if\n2 = min { 1,\nC ′ · (1 + β) β2 · log p−2 k + k 4 · log d n\n} ,\nfor an appropriate choice ofC ′ > 0, then the following two conditions (Eq. (13)) are satisfied:\nn · 2 2β2\n(1 + β)\n1 log |X | ≤ 1 4 and log |X | ≥ 4 log 2.\nFor the second inequality, recall that by Lemma 2.2,\nlog |X | ≥ log p− 2 k + 1 4 · k log d > 0. (35)\nUnder the assumptions of Thm. 1 on the parameters k and d (note that p− 2 ≥ k · d by the structure of G),\nlog |X | ≥ log p− 2 k + k 4 · log d ≥ 4 ·H(3/4) ≥ 4 log 2,\nwhich is the desired result.\nFor the first inequality, we consider two cases:\n• First, we consider the case where 2 = 1, i.e.,\n2 = 1 ≤ C ′ · (1 + β) β2 · log p−2 k + k 4 · log d n .\nEquivalently,\nn · 2 2β2\n(1 + β) ≤ 2 · C ′ ·\n( log\np− 2 k + k 4 · log d\n) . (36)\n• In the second case,\n2 = C ′ · (1 + β)\nβ2 · log\np−2 k + k 4 · log d\nn ,\nwhich implies that\nn · 2 2β2\n(1 + β) = 2 · C ′ ·\n( log\np− 2 k + k 4 · log d\n) . (37)\nCombining (36) or (37), with (35), we obtain\nn · 2 2β2\n(1 + β)\n1 log |X | ≤ 2 · C ′ ≤ 1 4\nfor C ′ ≤ 1/8. We conclude that for chosen as in (12), the conditions in (13) hold."
    }, {
      "heading" : "9. Other",
      "text" : "Assumption 1. There exist i.i.d. random vectors z1, . . . , zn ∈ Rp, such that Ezi = 0 and Eziz>i = Ip,\ny = µ+ Σ 1/2zi (38)\nand\nsup x∈Sp−12\n‖z>i x‖ψ2 ≤ K, (39)\nwhere µ ∈ Rp and K > 0 is a constant depending on the distribution of zis."
    } ],
    "references" : [ {
      "title" : "High-dimensional analysis of semidefinite relaxations for sparse principal components",
      "author" : [ "Amini", "Arash", "Wainwright", "Martin" ],
      "venue" : "In Information Theory,",
      "citeRegEx" : "Amini et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Amini et al\\.",
      "year" : 2008
    }, {
      "title" : "High-dimensional analysis of semidefinite relaxations for sparse principal components",
      "author" : [ "Amini", "Arash", "Wainwright", "Martin" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "Amini et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Amini et al\\.",
      "year" : 2009
    }, {
      "title" : "Nonnegative sparse PCA with provable guarantees",
      "author" : [ "Asteris", "Megasthenis", "Papailiopoulos", "Dimitris", "Dimakis", "Alexandros" ],
      "venue" : "In Proceedings of the 31st International Conference on Machine Learning",
      "citeRegEx" : "Asteris et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Asteris et al\\.",
      "year" : 2014
    }, {
      "title" : "Group-sparse model selection: Hardness and relaxations",
      "author" : [ "Baldassarre", "Luca", "Bhan", "Nirav", "Cevher", "Volkan", "Kyrillidis", "Anastasios" ],
      "venue" : "arXiv preprint arXiv:1303.3207,",
      "citeRegEx" : "Baldassarre et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Baldassarre et al\\.",
      "year" : 2013
    }, {
      "title" : "Model-based compressive sensing",
      "author" : [ "R. Baraniuk", "V. Cevher", "M. Duarte", "C. Hegde" ],
      "venue" : "Information Theory, IEEE Transactions on,",
      "citeRegEx" : "Baraniuk et al\\.,? \\Q1982\\E",
      "shortCiteRegEx" : "Baraniuk et al\\.",
      "year" : 1982
    }, {
      "title" : "Introduction to Algorithms",
      "author" : [ "Cormen", "Thomas", "Stein", "Clifford", "Rivest", "Ronald", "Leiserson", "Charles" ],
      "venue" : "McGraw-Hill Higher Education, 2nd edition,",
      "citeRegEx" : "Cormen et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Cormen et al\\.",
      "year" : 2001
    }, {
      "title" : "A direct formulation for sparse PCA using semidefinite programming",
      "author" : [ "d’Aspremont", "Alexandre", "El Ghaoui", "Laurent", "Jordan", "Michael", "Lanckriet", "Gert" ],
      "venue" : "SIAM review,",
      "citeRegEx" : "d.Aspremont et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "d.Aspremont et al\\.",
      "year" : 2007
    }, {
      "title" : "Optimal solutions for sparse principal component analysis",
      "author" : [ "d’Aspremont", "Alexandre", "Bach", "Francis", "Ghaoui", "Laurent El" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "d.Aspremont et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "d.Aspremont et al\\.",
      "year" : 2008
    }, {
      "title" : "Resting-state functional connectivity reflects structural connectivity in the default mode network",
      "author" : [ "Greicius", "Michael D", "Supekar", "Kaustubh", "Menon", "Vinod", "Dougherty", "Robert F" ],
      "venue" : "Cerebral cortex,",
      "citeRegEx" : "Greicius et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Greicius et al\\.",
      "year" : 2009
    }, {
      "title" : "Structured sparse principal component analysis",
      "author" : [ "Jenatton", "Rodolphe", "Obozinski", "Guillaume", "Bach", "Francis" ],
      "venue" : "In International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "Jenatton et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Jenatton et al\\.",
      "year" : 2010
    }, {
      "title" : "Sparse principal components analysis",
      "author" : [ "Johnstone", "Iain", "Lu", "Arthur Yu" ],
      "venue" : "Unpublished manuscript,",
      "citeRegEx" : "Johnstone et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Johnstone et al\\.",
      "year" : 2004
    }, {
      "title" : "Combinatorial selection and least absolute shrinkage via the CLASH algorithm",
      "author" : [ "Kyrillidis", "Anastasios", "Cevher", "Volkan" ],
      "venue" : "In Information Theory Proceedings (ISIT),",
      "citeRegEx" : "Kyrillidis et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Kyrillidis et al\\.",
      "year" : 2012
    }, {
      "title" : "Path coding penalties for directed acyclic graphs",
      "author" : [ "Mairal", "Julien", "Yu", "Bin" ],
      "venue" : "In Proceedings of the 4th NIPS Workshop on Optimization for Machine Learning (OPTâĂŹ11). Citeseer,",
      "citeRegEx" : "Mairal et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Mairal et al\\.",
      "year" : 2011
    }, {
      "title" : "Empirical processes with a bounded ψ-1 diameter",
      "author" : [ "Mendelson", "Shahar" ],
      "venue" : "Geometric and Functional Analysis,",
      "citeRegEx" : "Mendelson and Shahar.,? \\Q2010\\E",
      "shortCiteRegEx" : "Mendelson and Shahar.",
      "year" : 2010
    }, {
      "title" : "Sparse PCA through low-rank approximations",
      "author" : [ "Papailiopoulos", "Dimitris", "Dimakis", "Alex", "Korokythakis", "Stavros" ],
      "venue" : "In Proceedings of the 30th International Conference on Machine Learning,",
      "citeRegEx" : "Papailiopoulos et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Papailiopoulos et al\\.",
      "year" : 2013
    }, {
      "title" : "The WU-Minn human connectome project: An overview",
      "author" : [ "Van Essen", "David", "Smith", "Stephen", "Barch", "Deanna", "Behrens", "Timothy", "Yacoub", "Essa", "Ugurbil", "Kamil", "Consortium", "WUMinn HCP" ],
      "venue" : null,
      "citeRegEx" : "Essen et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Essen et al\\.",
      "year" : 2013
    }, {
      "title" : "Minimax rates of estimation for sparse PCA in high dimensions",
      "author" : [ "Vu", "Vincent", "Lei", "Jing" ],
      "venue" : "In International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "Vu et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Vu et al\\.",
      "year" : 2012
    }, {
      "title" : "Reward-related fMRI activation of dopaminergic midbrain is associated with enhanced hippocampus-dependent long-term memory formation",
      "author" : [ "Wittmann", "Bianca", "Schott", "Björn", "Guderian", "Sebastian", "Frey", "Julietta", "Heinze", "Hans-Jochen", "Düzel", "Emrah" ],
      "venue" : null,
      "citeRegEx" : "Wittmann et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Wittmann et al\\.",
      "year" : 2005
    }, {
      "title" : "Truncated power method for sparse eigenvalue problems",
      "author" : [ "Yuan", "Xiao-Tong", "Zhang", "Tong" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Yuan et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Yuan et al\\.",
      "year" : 2013
    }, {
      "title" : "Sparse principal component analysis",
      "author" : [ "Zou", "Hui", "Hastie", "Trevor", "Tibshirani", "Robert" ],
      "venue" : "Journal of computational and graphical statistics,",
      "citeRegEx" : "Zou et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Zou et al\\.",
      "year" : 2006
    } ],
    "referenceMentions" : [ {
      "referenceID" : 14,
      "context" : "We propose two algorithms for approximating the solution of (3), based on those of (Yuan & Zhang, 2013) and (Papailiopoulos et al., 2013; Asteris et al., 2014) for the sparse PCA problem.",
      "startOffset" : 108,
      "endOffset" : 159
    }, {
      "referenceID" : 2,
      "context" : "We propose two algorithms for approximating the solution of (3), based on those of (Yuan & Zhang, 2013) and (Papailiopoulos et al., 2013; Asteris et al., 2014) for the sparse PCA problem.",
      "startOffset" : 108,
      "endOffset" : 159
    }, {
      "referenceID" : 19,
      "context" : "Related Work There is a large volume of work on algorithms and the statistical analysis of sparse PCA (Johnstone & Lu, 2004; Zou et al., 2006; d’Aspremont et al., 2008; 2007; Johnstone & Lu, 2004; Vu & Lei, 2012; Amini & Wainwright, 2009).",
      "startOffset" : 102,
      "endOffset" : 238
    }, {
      "referenceID" : 7,
      "context" : "Related Work There is a large volume of work on algorithms and the statistical analysis of sparse PCA (Johnstone & Lu, 2004; Zou et al., 2006; d’Aspremont et al., 2008; 2007; Johnstone & Lu, 2004; Vu & Lei, 2012; Amini & Wainwright, 2009).",
      "startOffset" : 102,
      "endOffset" : 238
    }, {
      "referenceID" : 9,
      "context" : "Motivated by a face recognition application, (Jenatton et al., 2010) introduce structured sparse PCA using a regularization that encodes higher-order information about the data.",
      "startOffset" : 45,
      "endOffset" : 68
    }, {
      "referenceID" : 3,
      "context" : "Finally, we note that the idea of pursuing additional structure on top of sparsity is not limited to PCA: Modelbased compressive sensing seeks sparse solutions under a restricted family of sparsity patterns (Baldassarre et al., 2013; Baraniuk et al., 2010; Kyrillidis & Cevher, 2012), while structure induced by an underlying network is found in (Mairal & Yu, 2011) for sparse linear regression.",
      "startOffset" : 207,
      "endOffset" : 283
    }, {
      "referenceID" : 14,
      "context" : "The second relies on approximately solving (3) on a low rank approximation of Σ̂, similar to (Papailiopoulos et al., 2013; Asteris et al., 2014).",
      "startOffset" : 93,
      "endOffset" : 144
    }, {
      "referenceID" : 2,
      "context" : "The second relies on approximately solving (3) on a low rank approximation of Σ̂, similar to (Papailiopoulos et al., 2013; Asteris et al., 2014).",
      "startOffset" : 93,
      "endOffset" : 144
    }, {
      "referenceID" : 2,
      "context" : "and in turn (see (Asteris et al., 2014) for details), as a double maximization over the variables c ∈ Sr−1 and x ∈ R:",
      "startOffset" : 17,
      "endOffset" : 39
    }, {
      "referenceID" : 5,
      "context" : "In the case of DAGs, however, it can be solved using standard algorithms relying on topological sorting in time O(|V |+ |E|) (Cormen et al., 2001), i.",
      "startOffset" : 125,
      "endOffset" : 146
    }, {
      "referenceID" : 14,
      "context" : "of (Papailiopoulos et al., 2013), respectively.",
      "startOffset" : 3,
      "endOffset" : 32
    }, {
      "referenceID" : 8,
      "context" : "Based on recent results on resting state fMRI neural networks, we set the posterior cingulate cortex as a source node S, and the prefrontal cortex as a target node T (Greicius et al., 2009).",
      "startOffset" : 166,
      "endOffset" : 189
    }, {
      "referenceID" : 8,
      "context" : "late cortex and the prefrontal cortex (Greicius et al., 2009).",
      "startOffset" : 38,
      "endOffset" : 61
    }, {
      "referenceID" : 17,
      "context" : "The nucleus accumbens receives input from the hippocampus, and plays an important role in memory consolidation (Wittmann et al., 2005).",
      "startOffset" : 111,
      "endOffset" : 134
    } ],
    "year" : 2015,
    "abstractText" : "We introduce a variant of (sparse) PCA in which the set of feasible support sets is determined by a graph. In particular, we consider the following setting: given a directed acyclic graph G on p vertices corresponding to variables, the non-zero entries of the extracted principal component must coincide with vertices lying along a path in G. From a statistical perspective, information on the underlying network may potentially reduce the number of observations required to recover the population principal component. We consider the canonical estimator which optimally exploits the prior knowledge by solving a non-convex quadratic maximization on the empirical covariance. We introduce a simple network and analyze the estimator under the spiked covariance model. We show that side information potentially improves the statistical complexity. We propose two algorithms to approximate the solution of the constrained quadratic maximization, and recover a component with the desired properties. We empirically evaluate our schemes on synthetic and real datasets.",
    "creator" : "LaTeX with hyperref package"
  }
}