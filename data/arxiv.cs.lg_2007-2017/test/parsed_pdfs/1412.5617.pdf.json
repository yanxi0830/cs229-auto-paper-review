{
  "name" : "1412.5617.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Learning from Data with Heterogeneous Noise using SGD",
    "authors" : [ "Shuang Song", "Kamalika Chaudhuri", "Anand D. Sarwate" ],
    "emails" : [ "shs037@eng.ucsd.edu", "kamalika@cs.ucsd.edu", "asarwate@ece.rutgers.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "The main contribution of this paper is to identify how heterogeneous noise impacts performance. We show that given two datasets with heterogeneous noise, the order in which to use them in standard SGD depends on the learning rate. We propose a method for changing the learning rate as a function of the heterogeneity, and prove new regret bounds for our method in two cases of interest. Experiments on real data show that our method performs better than using a single learning rate and using only the less noisy of the two datasets when the noise level is low to moderate."
    }, {
      "heading" : "1 Introduction",
      "text" : "Modern large-scale machine learning systems often integrate data from several different sources. In many cases, these sources provide data of a similar type (i.e. with the same features) but collected under different circumstances. For example, patient records from different studies of a particular drug may be combined to perform a more comprehensive analysis, or a collection of images with annotations from experts as well as non-experts may be combined to learn a predictor. In particular, data from different sources may be of varying quality. In this paper we adopt a model in which data is observed through heterogeneous noise, where the noise level reflects the quality of the data source. We study how to use stochastic gradient algorithms to learn from data of heterogeneous quality.\nIn full generality, learning from heterogeneous data is essentially the problem of domain adaptation – a challenge for which good and complete solutions are difficult to obtain. Instead, we focus on the special case of heterogeneous noise and show how to use information about the data quality to improve the performance of learning algorithms which ignore this information.\nTwo concrete instances of this problem motivate our study: locally differentially private learning from multiple sites, and classification with random label noise. Differential privacy (Dwork et al.,\n∗Computer Science and Engineering Dept., University of California, San Diego, shs037@eng.ucsd.edu †Computer Science and Engineering Dept., University of California, San Diego, kamalika@cs.ucsd.edu ‡Electrical and Computer Engineering Dept., Rutgers University, asarwate@ece.rutgers.edu\nar X\niv :1\n41 2.\n56 17\nv1 [\ncs .L\nG ]\n1 7\nD ec\n2 01\n2006b,a) is a privacy model that has received significant attention in machine-learning and datamining applications. A variant of differential privacy is local privacy – the learner can only access the data via noisy estimates, where the noise guarantees privacy (Duchi et al., 2012, 2013). In many applications, we are required to learn from sensitive data collected from individuals with heterogeneous privacy preferences, or from multiple sites with different privacy requirements; this results in the heterogeneity of noise added to ensure privacy. Under random classification noise (RCN) (Kearns, 1998), labels are randomly flipped before being presented to the algorithm. The heterogeneity in the noise addition comes from combining labels of variable quality – such as labels assigned by domain experts with those assigned by a crowd.\nTo our knowledge, Crammer et al. (2006) were the first to provide a theoretical study of how to learn classifiers from data of variable quality. In their formulation, like ours, data is observed through heterogeneous noise. Given data with known noise levels, their study focuses on finding an optimal ordering of the data and a stopping rule without any constraint on the computational complexity. We instead shift our attention to studying computationally efficient strategies for learning classifiers from data of variable quality.\nWe propose a model for variable data quality which is natural in the context of large-scale learning using stochastic gradient descent (SGD) and its variants (Bottou, 2010; Bekkerman et al., 2011). We assume that the training data are accessed through an oracle which provides an unbiased but noisy estimate of the gradient of the objective. The noise comes from two sources: the random sampling of a data point, and additional noise due to the data quality. Our two motivating applications – learning with local differential privacy and learning from data of variable quality – can both be modeled as solving a regularized convex optimization problem using SGD. Learning from data with heterogeneous noise in this framework thus reduces to running SGD with noisy gradient estimates, where the magnitude of the added noise varies across iterations.\nMain results. In this paper we study noisy stochastic gradient methods when learning from multiple data sets with different noise levels. For simplicity we consider the case where there are two data sets, which we call Clean and Noisy. We process these data sets sequentially using SGD with learning rate O(1/t). In a future full version of this work we also analyze averaged gradient descent (AGD) with learning rate O(1/ √ t). We address some basic questions in this setup:\nIn what order should we process the data? Suppose we use standard SGD on the union of Clean and Noisy. We show theoretically and empirically that the order in which we should process the datasets to get good performance depends on the learning rate of the algorithm: in some cases we should use the order (Clean,Noisy) and in others (Noisy,Clean).\nCan we use knowledge of the noise rates? We show that using separate learning rates that depend on the noise levels for the clean and noisy datasets improves the performance of SGD. We provide a heuristic for choosing these rates by optimizing an upper bound on the error for SGD that depends on the ratio of the noise levels. We analytically quantify the performance of our algorithm in two regimes of interest. For moderate noise levels, we demonstrate empirically that our algorithm outperforms using a single learning rate and using clean data only.\nDoes using noisy data always help? The work of Crammer et al. (2006) suggests that if the noise level of noisy data is above some threshold, then noisy data will not help. Moreover, when the noise levels are very high, our heuristic does not always empirically outperform simply using the clean data. On the other hand, our theoretical results suggest that changing the learning rate can make noisy data useful. How do we resolve this apparent contradiction?\nWe perform an empirical study to address this question. Our experiments demonstrate that very often, there exists a learning rate at which noisy data helps; however, because the actual noise level may be far from the upper bound used in our algorithm, our optimization may not choose the best learning rate for every data set. We demonstrate that by adjusting the learning rate we can\nstill take advantage of noisy data. For simplicity we, like previous work Crammer et al. (2006), assume that the algorithms know the noise levels exactly. However, our algorithms can still be applied in the presence of approximate knowledge of the noise levels, and our result on the optimal data order only needs to know which dataset has more noise.\nRelated Work. There has been significant work on the convergence of SGD assuming analytic properties of the objective function, such as strong convexity and smoothness. When the objective function is λ-strongly convex, the learning rate used for SGD is O(1/λt) (Nemirovsky and Yudin, 1983; Agarwal et al., 2009; Rakhlin et al., 2012; Moulines and Bach, 2011), which leads to a regret of O(1/λ2t) for smooth objectives. For non-smooth objectives, SGD with learning rate O(1/λt) followed by some form of averaging of the iterates achieves O(1/λt) (Nesterov and Vial, 2008; Nemirovski et al., 2009; Shalev-Shwartz et al., 2009; Xiao, 2010; Duchi and Singer, 2009).\nThere is also a body of literature on differentially private classification by regularized convex optimization in the batch (Chaudhuri et al., 2011; Rubinstein et al., 2012; Kifer et al., 2012) as well as the online (Jain et al., 2012) setting. In this paper, we consider classification with local differential privacy (Wasserman and Zhou, 2010; Duchi et al., 2012), a stronger form of privacy than ordinary differential privacy. Duchi et al. (2012) propose learning a classifier with local differential privacy using SGD, and Song et al. (2013) show empirically that using mini-batches significantly improves the performance of differentially private SGD. Recent work by Bassily et al. (2014) provides an improved privacy analysis for non-local privacy. Our work is an extension of these papers to heterogeneous privacy requirements.\nCrammer et al. (2006) study classification when the labels in each data set are corrupted by RCN of different rates. Assuming the classifier minimizing the empirical 0/1 classification error can always be found, they propose a general theoretical procedure that processes the datasets in increasing order of noise, and determines when to stop using more data. In contrast, our noise model is more general and we provide a polynomial time algorithm for learning. Our results imply that in some cases the algorithm should process the noisy data first, and finally, our algorithm uses all the data."
    }, {
      "heading" : "2 The Model",
      "text" : "We consider linear classification in the presence of noise. We are given T labelled examples (x1, y1), . . . , (xT , yT ), where xi ∈ Rd, and yi ∈ {−1, 1} and our goal is to find a hyperplane w that largely separates the examples labeled 1 from those labeled −1. A standard solution is via the following regularized convex optimization problem:\nw∗ = argmin w∈W\nf(w) := λ 2 ‖w‖2 + 1 T T∑ i=1 `(w, xi, yi). (1)\nHere ` is a convex loss function, and λ2‖w‖ 2 is a regularization term. Popular choices for ` include the logistic loss `(w, x, y) = log(1 + e−yw >x) and the hinge loss `(w, x, y) = max(0, 1− yw>x).\nStochastic Gradient Descent (SGD) is a popular approach to solving (1): starting with an initial w1, at step t, SGD updates wt+1 using the point (xt, yt) as follows:\nwt+1 = ΠW (wt − ηt(λwt +∇`(wt, xt, yt))) . (2)\nHere Π is a projection operator onto the convex feasible set W, typically set to {w : ‖w‖2 ≤ 1/λ} and ηt is a learning rate (or step size) which specifies how fast wt changes. A common choice for the learning rate for the case when λ > 0 is c/t, where c = Θ(1/λ)."
    }, {
      "heading" : "2.1 The Heterogeneous Noise Model",
      "text" : "We propose an abstract model for heterogeneous noise that can be specialized to two important scenarios: differentially private learning, and random classification noise. By heterogeneous noise we mean that the distribution of the noise can depend on the data points themselves. More formally, we assume that the learning algorithm may only access the labeled data through an oracle G which, given a w ∈ Rd, draws a fresh independent sample (x, y) from the underlying data distribution, and returns an unbiased noisy gradient of the objective function ∇f(w), based on the example (x, y):\nE [G(w)] = λw +∇`(w, x, y), E [ ‖G(w)‖2 ] ≤ Γ2. (3)\nThe precise manner in which G(w) is generated depends on the application. Define the noise level for the oracle G as the constant Γ in (3); larger Γ means more noisy data. Finally, to model finite training datasets, we assume that an oracle G may be called only a limited number of times.\nObserve that in this noise model, we can easily use the noisy gradient returned by G to perform SGD. The update rule becomes:\nwt+1 = ΠW (wt − ηtG(wt)) . (4)\nThe SGD estimate is wt+1. In practice, we can implement an oracle such as G based on a finite labelled training set D as follows. We apply a random permutation on the samples in D, and at each invocation, compute a noisy gradient based on the next sample in the permutation. The number of calls to the oracle is limited to |D|. If the samples in D are drawn iid from the underlying data distribution, and if any extraneous noise added to the gradient at each iteration is unbiased and drawn independently, then this process will implement the oracle correctly.\nTo model heterogeneous noise, we assume that we have access to two oracles G1 and G2 implemented based on datasets D1 and D2, which can be called at most |D1| and |D2| times respectively. For j = 1, 2, the noise level of oracle Gj is Γj , and the values of Γ1 and Γ2 are known to the algorithm. In some practical situations, Γ1 and Γ2 will not be known exactly; however, our algorithm in Section 4 also applies when approximate noise levels are known, and our algorithm in Section 3 applies even when only the relative noise levels are known."
    }, {
      "heading" : "2.1.1 Local Differential Privacy",
      "text" : "Local differential privacy (Wasserman and Zhou, 2010; Duchi et al., 2012; Kasiviswanathan et al., 2008) is a strong notion of privacy motivated by differential privacy (Dwork et al., 2006b). An untrusted algorithm is allowed to access a perturbed version of a sensitive dataset through a sanitization interface, and must use this perturbed data to perform some estimation. The amount of perturbation is controlled by a parameter , which measures the privacy risk.\nDefinition 1 (Local Differential Privacy). Let D = (X1, . . . , Xn) be a sensitive dataset where each Xi ∈ D corresponds to data about individual i. A randomized sanitization mechanism M which outputs a disguised version (U1, . . . , Un) of D is said to provide -local differential privacy to individual i, if for all x, x′ ∈ D and for all S ⊆ S,\nPr(Ui ∈ S|Xi = x) ≤ e Pr(Ui ∈ S|Xi = x′). (5)\nHere the probability is taken over the randomization in the sanitization mechanism, and is a parameter that measures privacy risk where smaller means less privacy risk.\nConsider learning a linear classifier from a sensitive labelled dataset while ensuring local privacy of the participants. This problem can be expressed in our noise model by setting the sanitization mechanism as the oracle. Given a privacy risk , for w ∈ Rd, the oracle GDP draws a random labelled sample (x, y) from the underlying data distribution, and returns the noisy gradient of the objective function at w computed based on (x, y) as\nGDP(w) = λw +∇`(w, x, y) + Z, (6)\nwhere Z is independent random noise drawn from the density: ρ(z) ∝ e−( /2)‖z‖. Duchi et al. (2012) showed that this mechanism provides -local privacy assuming analytic conditions on the loss function, bounded data, and that the oracle generates a fresh random sample at each invocation. The following result shows how to set the parameters to fit in our heterogeneous noise model. The full proof is provided in Appendix A.2.\nTheorem 1. If ‖∇`(w, x, y)‖ ≤ 1 for all w and (x, y), then GDP(w) is -local differentially private. Moreover, for any w such that ‖w‖ ≤ 1λ , E[G DP(w)] = λw +∇E(x,y)[`(w, x, y)], and\nE[‖GDP(w)‖2] ≤ 4 + 4(d 2 + d)\n2 .\nProof. (Sketch) The term 4 comes from upper bounding E[‖λw+∇`(w, x, y)‖2] by maxw,x,y ‖λw+ ∇`(w, x, y)‖2 using ‖w‖ ≤ 1/λ and ‖∇`(w, x, y)‖ ≤ 1. The term 4(d2 + d)/ 2 comes from properties of the noise distribution.\nIn practice, we may wish to learn classifiers from multiple sensitive datasets with different privacy parameters. For example, suppose we wish to learn a classifier from sensitive patient records in two different hospitals holding data sets D1 and D2, respectively. The hospitals have different privacy policies, and thus different privacy parameters 1 and 2. This corresponds to a heterogeneous noise model in which we have two sanitizing oracles – GDP1 and GDP2 . For j = 1, 2, GDPj implements a differentially private oracle with privacy parameter j based on dataset Dj and may be called at most |Dj | times."
    }, {
      "heading" : "2.1.2 Random Classification Noise",
      "text" : "In the random classification noise model of Kearns (1998), the learning algorithm is presented with labelled examples (x1, ỹ1), . . . , (xT , ỹT ), where each ỹi ∈ {−1, 1} has been obtained by independently flipping the true label yi with some probability σ. Natarajan et al. (2013) showed that solving\nargmin w\nλ 2 ‖w‖2 + 1 T T∑ i=1 ˜̀(w, xi, ỹi, σ) (7)\nyields a linear classifier from data with random classification noise, where ˜̀ is a surrogate loss function corresponding to a convex loss `:\n˜̀(w, x, y, σ) = (1− σ)`(w, x, y)− σ`(w, x,−y)\n1− 2σ ,\nand σ is the probability that each label is flipped. This problem can be expressed in our noise model using an oracle GRCN which on input w draws a fresh labelled example (x, ỹ) and returns\nGRCN(w) = λw +∇˜̀(w, x, ỹ, σ).\nThe SGD updates in (4) with respect to GRCN minimize (7). If ‖x‖ ≤ 1 and ‖∇`(w, x, y)‖ ≤ 1, we have E [GRCN(w)] = λw + ∇`(w, x, y) and E [ ‖GRCN(w)‖22 ] ≤ 3 + 1/(1− 2σ)2, under the random classification noise assumption, so the oracle GRCN satisfies the conditions in (3) with Γ2 = 3 + 1/(1− 2σ)2.\nIn practice, we may wish to learn classifiers from multiple datasets with different amounts of classification noise (Crammer et al., 2006); for example, we may have a small dataset D1 labeled by domain experts, and a larger noisier dataset D2, labeled via crowdsourcing, with flip probabilities σ1 and σ2. We model this scenario using two oracles – GRCN1 and GRCN2 . For j = 1, 2, oracle GRCNj is implemented based on Dj and flip probability σj , and may be called at most |Dj | times."
    }, {
      "heading" : "3 Data order depends on learning rate",
      "text" : "Suppose we have two oracles GC (for “clean”) and GN (for “noisy”) implemented based on datasets DC, DN with noise levels ΓC,ΓN (where ΓC < ΓN) respectively. In which order should we query the oracle when using SGD? Perhaps surprisingly, it turns out that the answer depends on the learning rate. Below, we show a specific example of a convex optimization problem such that with ηt = c/t, the optimal ordering is to use GC first when c ∈ (0, 1/λ), and the optimal ordering is to use GN first when c > 1/λ.\nLet |DC|+ |DN| = T and consider the convex optimization problem:\nmin w∈W\nλ 2 ‖w‖2 − 1 T T∑ i=1 yiw >xi, (8)\nwhere the points {(xi, yi)} are drawn from the underlying distribution by GC or GN. Suppose G(w) = λw − yx+ Z where Z is an independent noise vector such that E[Z] = 0, E[‖Z‖2] = V 2C if G is GC, and E[‖Z‖2] = V 2N if G is GN with V 2N ≥ V 2C .\nFor our example, we consider the following three variants of SGD: CF and NF for “clean first” and “noisy first” and AO for an “arbitrary ordering”:\n1. CF: For t ≤ |DC|, query GC in the SGD update (4). For t > |DC|, query GN.\n2. NF: For t ≤ |DN|, query GN in the SGD update (4). For t > |DN|, query GC.\n3. AO: Let S be an arbitrary sequence of length T consisting of |DC| C’s and |DN| N’s. In the SGD update (4) in round t, if the t-th element St of S is C, then query GC; else, query GN.\nIn order to isolate the effect of the noise, we consider two additional oracles G′C and G′N; the oracle G′C (resp. G′N) is implemented based on the dataset DC (resp. DN), and iterates over DC (resp. DN) in exactly the same order as GC (resp. GN); the only difference is that for G′C (resp. G′N), no extra noise is added to the gradient (that is, Z = 0). The main result of this section is stated in Theorem 2.\nTheorem 2. Let {wCFt }, {wNFt } and {wAOt } be the sequences of updates obtained by running SGD for objective function (8) under CF, NF and AO respectively, and let {vCFt }, {vNFt } and {vAOt } be the sequences of updates under CF, NF and AO with calls to GC and GN replaced by calls to G′C and G′N. Let T = |DC|+ |DN|.\n1. If the learning rate ηt = c/t where c ∈ (0, 1/λ), then E [ ‖vCFT+1 − wCFT+1‖2 ] ≤ E [ ‖vAOT+1 − wAOT+1‖2 ] .\n2. If the learning rate ηt = c/t where c > 1/λ, then E [ ‖vNFT+1 − wNFT+1‖2 ] ≤ E [ ‖vAOT+1 − wAOT+1‖2 ] .\nProof. Let the superscripts CF, NF and AO indicate the iterates for the CF, NF and AO algorithms. Let w1 denote the initial point of the optimization. Let (x O t , y O t ) be the data used under order O = CF,NF or AO to update w at time t, ZOi be the noise added to the exact gradient by GC or GN, depending on which oracle is used by O at t and wOt be the w obtained under order O at time t. Then by expanding the expression for wt in terms of the gradients, we have\nwOT+1 =w1 T∏ i=1 (1− ηtλ)− T∑ t=1 ηt\n( T∏\ns=t+1\n(1− ηsλ) ) (yOt x O t + Z O t ). (9)\nSimilarly, if v1 = w1, we have\nvOT+1 = w1 T∏ i=1 (1− ηtλ)− T∑ t=1 ηt\n( T∏\ns=t+1\n(1− ηsλ) ) yOt x O t . (10)\nDefine\n∆t = ηt >∏ s=t+1 (1− ηsλ).\nTaking the expected squared difference between (9) from (10), we obtain\nE [ ‖vOT+1 − wOT+1‖2 ] = E ∥∥∥∥∥ T∑ t=1 ηt ( T∏ s=t+1 (1− ηsλ) ) ZOt ∥∥∥∥∥ 2 \n= E ∥∥∥∥∥ T∑ t=1 ∆tZ O t ∥∥∥∥∥ 2  = T∑ t=1 ∆2tE [ ‖ZOt ‖2 ] , (11)\nwhere the second step follows because the ZOi ’s are independent. If ηt = c/t, then\n∆t = c\nt >∏ s=t+1 ( 1− cλ s ) .\nTherefore\n∆2t+1 ∆2t =\n c t+ 1 ∏> s=t+2 ( 1− cλ s ) c\nt\n∏> s=t+1 ( 1− cλ\ns\n)  2 =  t (t+ 1) ( 1− cλ\nt+ 1\n)  2 =  1 1 +\n1− cλ t\n 2 ,\nwhich is smaller than 1 if c < 1/λ, equal to 1 if c = 1/λ, and greater than 1 if c > 1/λ. Therefore ∆t is decreasing if c < 1/λ and is increasing if c > 1/λ. If ∆t is decreasing, then (11) is minimized if E [ ‖ZOt ‖2 ] is increasing; if ∆t is increasing, then (11)\nis minimized if E [ ‖ZOt ‖2 ] is decreasing; and if ∆t is constant, then (11) is the same under any\norder of E [ ‖ZOt ‖2 ] . Therefore for c < 1/λ,\nE [∥∥∥vCFT+1 − wCFT+1∥∥∥2] ≤ E [∥∥∥vAOT+1 − wAOT+1∥∥∥2] ≤ E [∥∥∥vNFT+1 − wNFT+1∥∥∥2] .\nFor c = 1/λ, E [∥∥∥vCFT+1 − wCFT+1∥∥∥2] = E [∥∥∥vAOT+1 − wAOT+1∥∥∥2] = E [∥∥∥vNFT+1 − wNFT+1∥∥∥2] .\nFor c > 1/λ, E [∥∥∥vCFT+1 − wCFT+1∥∥∥2] ≥ E [∥∥∥vAOT+1 − wAOT+1∥∥∥2] ≥ E [∥∥∥vNFT+1 − wNFT+1∥∥∥2] .\nThis result says that arbitrary ordering of the data is worse than sequentially processing one data set after the other except in the special case where c = 1/λ. If the step size is small (c < 1/λ), the SGD should use the clean data first to more aggressively proceed towards the optimum. If the step size is larger (c > 1/λ), then SGD should reserve the clean data for refining the initial estimates given by processing the noisy data."
    }, {
      "heading" : "4 Adapting the learning rate to the noise level",
      "text" : "We now investigate whether the performance of SGD can be improved by using different learning rates for oracles with different noise levels. Suppose we have oracles G1 and G2 with noise levels Γ1 and Γ2 that are implemented based on two datasets D1 and D2. Unlike the previous section, we do not assume any relation between Γ1 and Γ2 – we analyze the error for using oracle G1 followed by G2 in terms of Γ1 and Γ2 to choose a data order. Let T = |D1| + |D2|. Let β1 = |D1|T and β2 = 1− β1 = |D2|T be the fraction of the data coming from G1 and G2, respectively. We adapt the gradient updates in (4) to heterogeneous noise by choosing the learning rate ηt as a function of the noise level. Algorithm 1 shows a modified SGD for heterogeneous learning rates.\nAlgorithm 1 SGD with varying learning rate\n1: Inputs: Oracles G1,G2 implemented by data sets D1, D2. Learning rates c1 and c2. 2: Set w1 = 0. 3: for t = 1, 2, . . . , |D1| do 4: wt+1 = ΠW ( wt − c1t G1(wt)\n) 5: end for 6: for t = |D1|+ 1, |D1|+ 2, . . . , |D1|+ |D2| do 7: wt+1 = ΠW ( wt − c2t G2(wt)\n) 8: end for 9: return w|D1|+|D2|+1.\nConsider SGD with learning rate ηt = c1/t while querying G1 and with ηt = c2/t while querying G2 in the update (4). We must choose an order in which to query G1 and G2 as well as the constants\nc1 and c2 to get the best performance. We do this by minimizing an upper bound on the distance between the final iterate wT+1 and the optimal solution w\n∗ to E[f(w)] where f is defined in (1), and the expectation is with respect to the data distribution and the gradient noise; the upper bound we choose is based on Rakhlin et al. (2012). Note that for smooth functions f , a bound on the distance ‖wT+1 − w∗‖ automatically translates to a bound on the regret f(wT+1)− f(w∗).\nTheorem 3 generalizes the results of Rakhlin et al. (2012) to our heterogeneous noise setting; the proof is in the supplement.\nTheorem 3. If 2λc1 > 1 and if 2λc2 6= 1, and if we query G1 before G2 with learning rates c1/t and c2/t respectively, then the SGD algorithm satisfies\nE [ ‖wT+1 − w∗‖2 ] ≤ 4Γ 2 1\nT · β\n2λc2−1 1 c 2 1 2λc1 − 1\n+ 4Γ22 T · (1− β 2λc2−1 1 )c 2 2 2λc2 − 1 +O\n( 1\nTmin(2,2λc1)\n) . (12)\nProof. (Sketch) Let g(w) be the true gradient ∇f(w) and ĝ(w) be the unbiased noisy gradient provided by the oracle G1 or G2, whichever is queried. By strong convexity of f , we have\nE1,...,t [ ‖wt+1 − w∗‖2 ] ≤ (1− 2ληt)E1,...,t [ ‖wt − w∗‖2 ] + η2t γ 2 t .\nSolving it inductively, with γt = Γ1, ηt = c1/t for t ≤ β1T and γt = Γ2, ηt = c2/t for t > β1T , we have\nE1,...,T [ ‖wT+1 − w∗‖2 ] ≤ β1T∏ i=i0 ( 1− 2λc1 i ) T∏ i=β1T+1 ( 1− 2λc2 i ) E1,...,T [ ‖wi0 − w∗‖2 ] + Γ21\nT∏ i=β1T+1 ( 1− 2λc2 i ) β1T∑ i=i0 c21 i2 β1T∏ j=i+1 ( 1− 2λc1 j )\n+ Γ22 T∑ i=β1T+1 c22 i2 T∏ j=i+1 ( 1− 2λc2 j ) ,\nwhere i0 is the smallest positive integer such that 2ληi0 < 1, i.e, i0 = d2c1λe. Then using 1− x ≤ e−x and upper bounding each term using integrals we get the (12).\nTwo remarks are in order. First, observe that the first two terms in the right hand side dominate the other term. Second, our proof techniques for Theorem 3, adapted from Rakhlin et al. (2012), require that 2λc1 > 1 in order to get a O(1/T ) rate of convergence; without this condition, the dependence on T is Ω(1/T )."
    }, {
      "heading" : "4.1 Algorithm description",
      "text" : "Our algorithm for selecting c1 and c2 is motivated by Theorem 3. We propose an algorithm that selects c1 and c2 by minimizing the quantity B(c1, c2) which represents the highest order terms in Theorem 3:\nB(c1, c2) = 4Γ21β 2λc2−1 1 c 2 1\nT (2λc1 − 1) +\n4Γ22(1− β 2λc2−1 1 )c 2 2\nT (2λc2 − 1) . (13)\nGiven λ, Γ1, Γ2 and β1, we use c ∗ 1 and c ∗ 2 to denote the values of c1 and c2 that minimize B(c1, c2). We can optimize for fixed c2 with respect to c1 by minimizing c21\n2λc1−1 ; this gives c ∗ 1 = 1/λ, and\nc∗21 2λc∗1−1 = 1/λ2, which is independent of β1 or the noise levels Γ1 and Γ2. Minimizing B(c ∗ 1, c2) with respect to c2 can be now performed numerically to yield c ∗ 2 = argminc2 B(c ∗ 1, c2). This yields optimal values of c1 and c2. Now suppose we have two oracles GC, GN with noise levels ΓC and ΓN that are implemented based on datasets DC and DN respectively. Let ΓC < ΓN, and let βC = |DC|\n|DC|+|DN| and βN = |DN|\n|DC|+|DN| be the fraction of the total data in each data set. Define the following functions:\nHCN(c) = 4Γ2Cβ 2λc−1 C\nλ2 +\n4Γ2N(1− β 2λc−1 C )c 2\n2λc− 1 ,\nHNC(c) = 4Γ2Nβ 2λc−1 N\nλ2 +\n4Γ2C(1− β 2λc−1 N )c 2\n2λc− 1 .\nThese represent the constant of the leading term in the upper bound in Theorem 3 for (G1,G2) = (GC,GN) and (G1,G2) = (GN,GC), respectively. Algorithm 2 repeats the process of choosing optimal c1, c2 with two orderings of the data – GC first and GN first – and selects the solution which provides the best bounds (according to the higher order terms of Theorem 3).\nAlgorithm 2 Selecting the Learning Rates\n1: Inputs: Data sets DC and DN accessed through oracles GC and GN with noise levels ΓC and ΓN. 2: Let βC = |DC|\n|DC|+|DN| and βN = |DN| |DC|+|DN| .\n3: Calculate cCN = argmincHCN(c) and cNC = argmincHNC(c). 4: if HCN(cCN) ≤ HNC(cNC) then 5: Run Algorithm 1 using oracles (GC,GN), learning rates c1 = 1λ and c2 = cCN. 6: else 7: Run Algorithm 1 using oracles (GN,GC), learning rates c1 = 1λ and c2 = cNC. 8: end if"
    }, {
      "heading" : "4.2 Regret Bounds",
      "text" : "To provide a regret bound on the performance of SGD with two learning rates, we need to plug the optimal values of c1 and c2 into the right hand side of (13). Observe that as c1 = c2 and c2 = 0 are feasible inputs to (13), our algorithm by construction has a superior regret bound than using a single learning rate only, or using clean data only.\nUnfortunately, the value of c2 that minimizes (13) does not have a closed form solution, and as such it is difficult to provide a general simplified regret bound that holds for all Γ1, Γ2 and β1. In this section, we consider two cases of interest, and derive simplified versions of the regret bound for SGD with two learning rates for these cases.\nWe consider the two data orders (Γ1,Γ2) = (ΓN,ΓC) and (Γ1,Γ2) = (ΓC,ΓN) in a scenario where ΓN/ΓC 1 and both βN and βC are bounded away from 0 and 1. That is, the noisy data is much noisier. The following two lemmas provide upper and lower bounds on B(c∗1, c ∗ 2) in this setting.\nLemma 1. Suppose (Γ1,Γ2) = (ΓN,ΓC) and 0 < βN < 1. Then for sufficiently large ΓN/ΓC, the optimal solution c∗2 to (13) satisfies\n2c∗2λ ∈\n[ 1 + 2 log(ΓN/ΓC) + log log(1/βN)\nlog(1/βN) ,\n1 + 2 log(4ΓN/ΓC) + log log(1/βN)\nlog(1/βN)\n] .\nMoreover, B(c∗1, c ∗ 2) satisfies:\nB(c∗1, c ∗ 2) ≥\n4Γ2C(log( ΓN ΓC ) + 12 log log 1 βN )\nλ2T log( 1βN )\nB(c∗1, c ∗ 2) ≤ 4Γ2C λ2T\n( 4 + 4 + 2 log(ΓNΓC ) + log log( 1 βN )\nlog( 1βN )\n) .\nProof. (Sketch) We prove that for any 2λc2 ≤ 1 + 2 log(ΓN/ΓC)+log log(1/βN)log(1/βN) , B(c ∗ 1, c2) is decreasing with respect to c2 when ΓN/ΓC is sufficiently large; and for any 2λc2 ≥ 1 + 2 log(4ΓN/ΓC)+log log(1/βN)log(1/βN) , B(c∗1, c2) is increasing when ΓN/ΓC is sufficiently large. Therefore the minimum of B(c ∗ 1, c2) is achieved when 2λc2 is in between.\nObserve that the regret bound grows logarithmically with ΓN/ΓC. Moreover, if we only used\nthe cleaner data, then the regret bound would be 4Γ2C λ2βCT\n, which is better, especially for large ΓN/ΓC. This means that using two learning rates with the noisy data first gives poor results at high noise levels.\nOur second bound takes the opposite data order, processing the clean data first.\nLemma 2. Suppose (Γ1,Γ2) = (ΓC,ΓN) and 0 < βC < 1. Let σ = (ΓN/ΓC) −2. Then for sufficiently large ΓN/ΓC, the optimal solution c ∗ 2 to (13) satisfies: 2c ∗ 2λ ∈ [ σ, 8βCσ ] . Moreover, B(c∗1, c ∗ 2) satisfies:\nB(c∗1, c ∗ 2) ≥ 4Γ2C λ2βCT β 8σ/βC C\nB(c∗1, c ∗ 2) ≤ 4Γ2C λ2βCT βσC\n( 1 + σ log(1/βC)\n4\n) .\nProof. (Sketch) Similar as the proof of Lemma 1, we prove that for any 2λc2 ≤ σ, B(c∗1, c2) is decreasing with respect to c2; and for any 2λc2 ≥ 8βCσ, B(c ∗ 1, c2) is increasing when ΓN/ΓC is sufficiently large. Therefore the minimum of B(c∗1, c2) is achieved when 2λc2 is in between.\nIf we only used the clean dataset, then the regret bound would be 4Γ2C λ2βCT , so Lemma 2 yields an\nimprovement by a factor of β (ΓN/ΓC)\n−2\nC\n( 1 + ( ΓN ΓC )−2 log(1/βC) 4 ) . As βC < 1, observe that this factor\nis always less than 1, and tends to 1 as ΓN/ΓC tends to infinity; therefore the difference between the regret bounds narrows as the noisy data grows noisier. We conclude that using two learning rates with clean data first gives a better regret bound than using only clean data or using two learning rates with noisy data first."
    }, {
      "heading" : "5 Experiments",
      "text" : "We next illustrate our theoretical results through experiments on real data. We consider the task of training a regularized logistic regression classifier for binary classification under local differential privacy. For our experiments, we consider two real datasets – MNIST (with the task 1 vs. Rest) and Covertype (Type 2 vs. Rest). The former consists of 60, 000 samples in 784 dimensions, while the latter consists of 500, 000 samples in 54-dimensions. We reduce the dimension of the MNIST dataset to 25 via random projections.\nTo investigate the effect of heterogeneous noise, we divide the training data into subsets (DC, DN) to be accessed through oracles (GC,GN) with privacy parameters ( C, N) respectively. We pick C > N, so GN is noisier than GC. To simulate typical practical situations where cleaner data is rare, we set the size of DC to be βC = 10% of the total data size. We set the regularization parameter λ = 10−3, ΓC and ΓN according to Theorem 1 and use SGD with mini-batching (batch size 50).\nDoes Data Order Change Performance? Our first task is to investigate the effect of data order on performance. For this purpose, we compare three methods – CleanFirst, where all of DC is used before DN, NoisyFirst, where all of DN is used before DC, and Arbitrary, where data from DN ∪DC is presented to the algorithm in a random order.\nThe results are in Figures 1(a) and 1(d). We use C = 10, N = 3. For each algorithm, we plot |f(wT+1)− f(vT+1)| as a function of the constant c in the learning rate. Here f(wT+1) is the function value obtained after T rounds of SGD, and f(vT+1) is the function value obtained after T rounds of SGD if we iterate over the data in the same order, but add no extra noise to the gradient. (See Theorem 2 for more details.) As predicted by Theorem 2, the results show that for c < 1λ ,\nCleanFirst has the best performance, while for c > 1λ , NoisyFirst performs best. Arbitrary performs close to NoisyFirst for a range of values of c, which we expect as only 10% of the data belongs to DC.\nAre Two Learning Rates Better than One? We next investigate whether using two learning rates in SGD can improve performance. We compare five approaches. Optimal is the gold standard where we access the raw data without any intervening noisy oracle. CleanOnly uses only DC with learning rate with the optimal value of c obtained from Section 4. SameClean and SameNoisy use a single value of the constant c in the learning rate for DN ∪DC, where c is obtained by optimizing (13)1 under the constraint that c1 = c2. SameClean uses all of DC before using DN, while SameNoisy uses all of DN before using DC. In Algorithm2, we use Algorithm 2 to set the two learning rates and the data order (DC first or DN first). In each case, we set C = 10, vary N from 1 to 10, and plot the function value obtained at the end of the optimization.\nThe results are plotted in Figures 1(b) and 1(e). Each plotted point is an average of 100 runs. It is clear that Algorithm2, which uses two learning rates, performs better than both SameNoisy and SameClean. As expected, the performance difference diminishes as N increases (that is, the noisy data gets cleaner). For moderate and high N, Algorithm2 performs best, while for low N (very noisy DN), CleanOnly has slightly better performance. We therefore conclude that using two learning rates is better than using a single learning rate with both datasets, and that Algorithm2 performs best for moderate to low noise levels.\nDoes Noisy Data Always Help? A natural question to ask is whether using noisy data always helps performance, or if there is some threshold noise level beyond which we should not use noisy data. Lemma 2 shows that in theory, we obtain a better upper bound on performance when we use noisy data; in contrast, Figures 1(b) and 1(e) show that for low N (high noise), Algorithm2 performs worse than CleanOnly. How do we explain this apparent contradiction?\nTo understand this effect, in Figures 1(c) and 1(f) we plot the performance of SGD using two learning rates (with c1 = 1 λ) against CleanOnly as a function of the second learning rate c2. The figures show that the best performance is attained at a value of c2 which is different from the value predicted by Algorithm2, and this best performance is better than CleanOnly. Thus, noisy data always improves performance; however, the improvement may not be achieved at the learning rate predicted by our algorithm.\nWhy does our algorithm perform suboptimally? We believe this happens because the values of ΓN and ΓC used by our algorithm are fairly loose upper bounds. For local differential privacy, an easy lower bound on Γ is √\n4(d2+d) 2b , where b is the mini-batch size; let c2(L) (resp. c2(U)) be the\nvalue of c2 obtained by plugging in these lower bounds (resp. upper bounds from Theorem 1) to Algorithm 1. Our experiments show that the optimal value of c2 always lies between c2(L) and c2(U), which indicates that the suboptimal performance may be due to the looseness in the bounds.\nWe thus find that even in these high noise cases, theoretical analysis often allows us to identify an interval containing the optimal value of c2. In practice, we recommend running Algorithm 2 twice – once with upper, and once with lower bounds to obtain an interval containing c2, and then performing a line search to find the optimal c2.\n1Note that we plug in separate noise rates for GC and GN in the learning rate calculations."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper we propose a model for learning from heterogeneous noise that is appropriate for studying stochastic gradient approaches to learning. In our model, data from different sites are accessed through different oracles which provide noisy versions of the gradient. Learning under local differential privacy and random classification noise are both instances of our model. We show that for two sites with different noise levels, processing data from one site followed by the other is better than randomly sampling the data, and the optimal data order depends on the learning rate. We then provide a method for choosing learning rates that depends on the noise levels and showed that these choices achieve lower regret than using a common learning rate. We validate these findings through experiments on two standard data sets and show that our method for choosing learning rates often yields improvements when the noise levels are moderate. In the case where one data set is much noisier than the other, we provide a different heuristic to choose a learning rate that improves the regret.\nThere are several different directions towards generalizing the work here. Firstly, extending the results to multiple sites and multiple noise levels will give more insights as to how to leverage large numbers of data sources. This leads naturally to cost and budgeting questions: how much should we pay for additional noisy data? Our results for data order do not depend on the actual noise levels, but rather their relative level. However, we use the noise levels to tune the learning rates for different sites. If bounds on the noise levels are available, we can still apply our heuristic. Adaptive approaches for estimating the noise levels while learning are also an interesting approach for future study.\nAcknowledgements. The work of K. Chaudhuri and S. Song was sponsored by NIH under U54 HL108460 and the NSF under IIS 1253942."
    }, {
      "heading" : "A Appendix",
      "text" : ""
    }, {
      "heading" : "A.1 Mathematical miscellany",
      "text" : "In many cases we would like to bound a summation using an integral.\nLemma 3. For x ≥ 0, we have\nb∑ i=a ix ≤ ∫ b+1 a ixdi = (b+ 1)x+1 − ax+1 x+ 1 (14) b∑ i=a ix ≥ ∫ b a−1 ixdi = bx+1 − (a− 1)x+1 x+ 1 (15)\nFor x < 0 and x 6= −1, we have\nb∑ i=a ix ≤ ∫ b a−1 ixdi = bx+1 − (a− 1)x+1 x+ 1 (16) b∑ i=a ix ≥ ∫ b+1 a ixdi = (b+ 1)x+1 − ax+1 x+ 1 (17)\nFor x = −1, we have b∑ i=a ix ≤ ∫ b a−1 ixdi = log b a− 1 (18)\nb∑ i=a ix ≥ ∫ b+1 a ixdi = log b+ 1 a (19)\nThe sequence {ix} is increasing when x > 0 and is decreasing when x < 0. The proof follows directly from applying standard technique of bounding summation with integral."
    }, {
      "heading" : "A.2 Details from Section 2",
      "text" : "Proof. (Of Theorem 1) Consider an oracle G implemented based on a dataset D of size T . Given any sequence w1, w2, . . . , wT , the disguised version of D output by G is the sequence of gradients G(w1), . . . ,G(wT ). Suppose that the oracle accesses the data in a (random) order specified by a permutation π; for any t, any x, x′ ∈ X , y, y′ ∈ {1,−1}, we have\nρ(G(wt) = g|(xπ(t), yπ(t)) = (x, y)) ρ(G(wt) = g|(xπ(t), yπ(t)) = (x′, y′)) = ρ(Zt = g − λw −∇`(w, x, y)) ρ(Zt = g − λw −∇`(w, x′, y′))\n= e−( /2)‖g−λw−∇`(w,x,y)‖\ne−( /2)‖g−λw−∇`(w,x′,y′)‖ ≤ exp ( ( /2)(‖∇`(w, x, y)‖+ ‖∇`(w, x′, y′)‖) ) ≤ exp ( ) .\nThe first inequality follows from the triangle inequality, and the last step follows from the fact that ‖∇`(w, x, y)‖ ≤ 1. The privacy proof follows.\nFor the rest of the theorem, we consider a slightly generalized version of SGD that includes mini-batch updates. Suppose the batch size is b; for standard SGD, b = 1. For a given t, we call G(wt) b successive times to obtain noisy gradient estimates g1(wt), . . . , gb(wt); these are gradient estimates at wt but are based on separate (private) samples. The SGD update rule is:\nwt+1 = ΠW ( wt −\nηt b\n(g1(wt) + . . .+ gb(wt)) ) .\nFor any i, E[gi(wt)] = λw+E[∇`(w, x, y)], where the first expectation is with respect to the data distribution and the noise, and the second is with respect to the data distribution; the unbiasedness result follows.\nWe now bound the norm of the noisy gradient calculated from a batch. Suppose that the oracle accesses the dataset D in an order π. Then, gi(wt) = λw+∇`(wt, xπ((t−1)b+i), yπ((t−1)b+i)) + Z(t−1)b+i. Expanding on the expression for the expected squared norm of the gradient, we have\nE [∥∥∥∥1b (g1(wt) + . . .+ gb(wt)) ∥∥∥∥2 ] =E ∥∥∥∥∥λw + 1b b∑ i=1 ∇`(wt, xπ((t−1)b+i), yπ((t−1)b+i)) ∥∥∥∥∥ 2 \n+ 2\nb E\n[( λw + 1\nb b∑ i=1 ∇`(wt, xπ((t−1)b+i), yπ((t−1)b+i))\n) · ( b∑ i=1 Z(t−1)b+i )]\n+ 1\nb2 E ∥∥∥∥∥ b∑ i=1 Z(t−1)b+i ∥∥∥∥∥ 2  (20)\nWe now look at the three terms in (20) separately. The first term can be further expanded to:\nE [ ‖λw‖2 ] + E ∥∥∥∥∥ 1b2 b∑ i=1 ∇`(wt, xπ((t−1)b+i), yπ((t−1)b+i)) ∥∥∥∥∥ 2 \n+ 2λw · ( b∑ i=1 E [ ∇`(wt, xπ((t−1)b+i), yπ((t−1)b+i)) ]) (21)\nThe first term in (21) is at most λ2 maxw∈W ‖w‖2, which is at most 1. The second term is at most maxw λ‖w‖ · maxw,x,y ‖∇`(w, x, y)‖ ≤ 1, and the third term is at most 2. Thus, the first term in (20) is at most 4. Notice that this upper bound can be pretty loose compare to the average∥∥∥∥λw + 1b∑bi=1∇`(wt, xπ((t−1)b+i), yπ((t−1)b+i))\n∥∥∥∥2 values seen in experiment. This leads to a loose estimation of the noise level for oracle GDP. To bound the second term in (20), observe that for all i, Z(t−1)b+i is independent of any Z(t−1)b+i′ when i 6= i′, as well as of the dataset. Combining this with the fact that E [Zτ ] = 0 for any τ , we get that this term is 0. To bound the third term in (20), we have:\n1 b2 E ∥∥∥∥∥∑ t∈B Zt ∥∥∥∥∥ 2\n2\n = 1 b2 E ∑ t∈B ‖Zt‖22 + ∑ t∈B,s∈B,t 6=s Zt · Zs  = 1\nb2 ∑ t∈B E [ ‖Zt‖22 ] + 1 b2 ∑ t∈B,s∈B,t 6=s E [Zt] · E [Zs]\n= 1\nb2 ∑ t∈B E [ ‖Zt‖22 ] ,\nwhere the first equality is from the linearity of expectation and the last two equalities is from the fact that Zi is independently drawn zeros mean vector. Because Zt follows ρ(Zt = z) ∝ e−( /2)‖z‖, we have\nρ(‖Zt‖ = x) ∝ xd−1e−( /2)x,\nwhich is a Gamma distribution. For X ∼ Gamma(k, θ), E [X] = kθ and Var (X) = kθ2. Also, by property of expectation, E [ X2 ] = (E [X])2 + Var (X). We then have E [ ‖Zt‖22 ] = 4(d2 + d) 2 and the whole term equals to 4(d2 + d)\n2b .\nCombining the three bounds together, we have a final bound of 4 + 4(d2 + d)\n2b . The lemma follows."
    }, {
      "heading" : "A.3 Proofs from Section 4",
      "text" : "Recall that we have oracles G1,G2 based on data sets D1 and D2. The fractions of data in each data set are β1 =\n|D1| |D1|+|D2| and β2 = |D2| |D1|+|D2| , respectively."
    }, {
      "heading" : "A.3.1 Proof of Theorem 3",
      "text" : "Theorem 3 is a corollary of the following Lemma.\nLemma 4. Consider the SGD algorithm that follows Algorithm 1. Suppose the objective function is λ-strongly convex, and define W = {w : ‖w‖ ≤ B}. If 2λc1 > 1 and i0 = d2c1λe, then we have the following two cases:\n1. If 2λc2 6= 1,\nE [ ‖wt+1 − w∗‖2 ] ≤ ( 4Γ21 β2λc2−11 c 2 1\n2λc1 − 1 + 4Γ22\nc22(1− β 2λc2−1 1 )\n2λc2 − 1\n) · 1 T +O (\n1\nTmin(2λc1,2)\n)\n2. If 2λc2 = 1,\nE [ ‖wt+1 − w∗‖2 ] ≤ ( 4Γ21 β2λc2−11 c 2 1\n2λc1 − 1 + 4Γ22c 2 2 log\n1\nβ1\n) · 1 T +O (\n1\nTmin(2λc1,2)\n)\nWe first begin with a lemma which follows from arguments very similar to those made in Rakhlin et al. (2012).\nLemma 5. Let w∗ be the optimal solution to E[f(w)]. Then,\nE1,...,t [ ‖wt+1 − w∗‖2 ] ≤ (1− 2ληt)E1,...,t [ ‖wt − w∗‖2 ] + η2t γ 2 t .\nwhere the expectation is taken wrt the oracle as well as sampling from the data distribution.\nProof. (Of Lemma 5) By strong convexity of f , we have\nf(w′) ≥ f(w) + g(w)>(w′ − w) + λ 2 ‖w − w′‖2. (22)\nThen by taking w = wt, w ′ = w∗ we have\ng(wt) >(wt − w∗) ≥ f(wt)− f(w∗) +\nλ 2 ‖wt − w∗‖2. (23)\nAnd similarly by taking w′ = wt, w = w ∗, we have\nf(wt)− f(w∗) ≥ λ 2 ‖wt − w∗‖2. (24)\nBy the update rule and convexity of W, we have\nE1,...,t [ ‖wt+1 − w∗‖2 ] = E1,...,t [ ‖ΠW (wt − ηtĝ(wt))− w∗‖2 ] ≤ E1,...,t [ ‖wt − ηtĝ(wt)− w∗‖2\n] = E1,...,t [ ‖wt − w∗‖2 ] − 2ηtE1,...,t [ ĝ(wt) >(wt − w∗) ] η2tE1,...,t [ ‖ĝ(wt)‖2 ] .\nConsider the term E1,...,t [ ĝ(wt) >(wt − w∗) ] , where the expectation is taken over the randomness from time 1 to t. Since wt is a function of the samples used from time 1 to t− 1, it is independent\nof the sample used at t. So we have\nE1,...,t [ ‖wt+1 − w∗‖2 ] ≤ E1,...,t [ ĝ(wt) >(wt − w∗) ]\n= E1,...,t−1 [ Et[ĝ(wt)>(wt − w∗)|wt] ] = E1,...,t−1 [ Et[ĝ(wt)>|wt](wt − w∗)\n] = E1,...,t−1 [ g(wt) >(wt − w∗) ] .\nWe have the following upper bound:\nE1,...,t [ ‖wt+1 − w∗‖2 ] ≤ E1,...,t [ ‖wt − w∗‖2 ] − 2ηtE1,...,t−1 [ g(wt) >(wt − w∗) ]\n+ η2tE1,...,t [ ‖ĝ(wt)‖2 ] .\nBy (23) and the bound E [ ‖ĝ(wt)‖2 ] ≤ γ2t , we have E1,...,t [ ‖wt+1 − w∗‖2 ] ≤ E1,...,t [ ‖wt − w∗‖2 ] − 2ηtE1,...,t−1 [ f(wt)− f(w∗) + λ\n2 ‖wt − w∗‖2\n] + η2t γ 2 t .\nThen by (24) and the fact that wt is independent of the sample used in time t, we have the following recursion:\nE1,...,t [ ‖wt+1 − w∗‖2 ] ≤ (1− 2ληt)E1,...,t [ ‖wt − w∗‖2 ] + η2t γ 2 t .\nProof. (Of Lemma 4) Let g(w) be the true gradient ∇f(w) and ĝ(w) be the unbiased noisy gradient provided by the oracle G1 or G2, whichever is queried. From Lemma 5, we have the following recursion:\nE1,...,t [ ‖wt+1 − w∗‖2 ] ≤ (1− 2ληt)E1,...,t [ ‖wt − w∗‖2 ] + η2t γ 2 t .\nLet i0 be the smallest positive integer such that 2ληi0 < 1, i.e, i0 = d2c1λe. Notice that for fixed step size constant c and λ, i0 would be a fixed constant. Therefore we assume that i0 < βT . Using the above inequality inductively, and substituting γt = Γ1 for t ≤ β1T and γt = Γ2 for t > β1T , we have\nE1,...,T [ ‖wT+1 − w∗‖2 ] ≤ β1T∏ i=i0 (1− 2ληi) T∏ i=β1T+1 (1− 2ληi)E1,...,T [ ‖wi0 − w∗‖2 ] + Γ21\nT∏ i=β1T+1 (1− 2ληi) β1T∑ i=i0 η2i β1T∏ j=i+1 (1− 2ληj)\n+ Γ22 T∑ i=β1T+1 η2i T∏ j=i+1 (1− 2ληj).\nBy substituting ηt = c1 t for D1 and ηt = c2 t for D2, we have\nE1,...,T [ ‖wT+1 − w∗‖2 ] ≤ β1T∏ i=i0 ( 1− 2λc1 i ) T∏ i=β1T+1 ( 1− 2λc2 i ) E1,...,T [ ‖wi0 − w∗‖2 ] + Γ21\nT∏ i=β1T+1 ( 1− 2λc2 i ) β1T∑ i=i0 c21 i2 β1T∏ j=i+1 ( 1− 2λc1 j )\n+ Γ22 T∑ i=β1T+1 c22 i2 T∏ j=i+1 ( 1− 2λc2 j ) .\nApplying the inequality 1− x ≤ e−x to each of the terms in the products, and simplifying, we get:\nE1,...,T [ ‖wT+1 − w∗‖2 ] ≤ e−2λc1 ∑β1T i=i0 1 i e −2λc2 ∑> i=β1T+1 1 i E1,...,T [ ‖wi0 − w∗‖2 ] + Γ21e −2λc2 ∑> i=β1T+1 1 i\nβ1T∑ i=i0 c21 i2 e −2λc1 ∑β1T j=i+1 1 j\n+ Γ22 >∑ i=β1T+1 c22 i2 e −2λc2 ∑> j=i+1 1 j . (25)\nWe would like to bound (25) term by term. A bound we will use later is:\ne2λc2/β1T = 1 + 2λc2 β1T e2λc2/β1T ′ ≤ 1 + 2λc2 β1T e2λc2/β1 , (26)\nwhere the equality is obtained using Taylor’s theorem, and the inequality follows because T ′ is in the range [1,∞). Now we can bound the three terms in (25) separately.\nThe first term in (25): We bound this as follows:\ne −2λc1 ∑β1T i=i0 1 i e −2λc2 ∑> i=β1T+1 1 i E1,...,T [ ‖wi0 − w∗‖2 ] ≤ e−2λc1 log β1T i0 e −2λc2(log 1β1− 1 β1T )E1,...,T [ ‖wi0 − w∗‖2\n] ≤ ( i0 T )2λc1 β 2λ(c2−c1) 1 e 2λc2/β1T (4B2)\n≤ ( i0 T )2λc1 β 2λ(c2−c1) 1 ( 1 + 2λc2 β1T e2λc2/β1 ) 4B2\n= 4B2i0 2λc1β 2λ(c2−c1) 1\n1\nT 2λc1 +O\n( 1\nT 2λc1+1\n) ,\nwhere the first equality follows from (17).The second inequality follows from ‖w‖ ≤ B, ‖w−w′‖ ≤ ‖w‖+ ‖w′‖ ≤ 2B, and bounding expectation using maximum. The third follows from (26).\nThe second term in (25): We bound this as follows:\nΓ21e −2λc2\n∑> i=β1T+1 1 i β1T∑ i=i0 c21 i2 e −2λc1 ∑β1T j=i+1 1 j ≤ Γ21e −2λc2(log 1β1− 1 β1T ) β1T∑ i=i0 c21 i2 e−2λc1 log β1T i+1\n= Γ21β 2λc2 1 e 2λc2/β1T β1T∑ i=i0 c21 i2 ( i+ 1 β1T )2λc1\n= Γ21β 2λ(c2−c1) 1 e 2λc2/β1T c21T −2λc1 β1T∑ i=i0 (i+ 1)2λc1 i2\n≤Γ21β 2λ(c2−c1) 1 e 2λc2/β1T c21T −2λc1 β1T∑ i=i0 4(i+ 1)2λc1−2\n≤4Γ21β 2λ(c2−c1) 1\n( 1 +\n2λc2 β1T\ne2λc2/β1 ) c21T −2λc1 β1T+1∑ i=i0+1 i2λc1−2,\n(27)\nwhere the first inequality follows from (17), the second inequality follows from (1+ 1i ) 2 ≤ (1+ 11) 2 = 4, and the last inequality follows from (26).\nBounding summation using integral following (16) and (14) of Lemma 3, if 2λc1 > 1, the term on the right hand side would be in the order of O(1/T ); if 2λc1 = 1, it would be O(log T/T ); if 2λc1 < 1, it would be O(1/T 2λc1). Therefore to minimize the bound in terms of order, we would choose c1 such that 2λc1 > 1. To get an upper bound of the summation in (27), using (16) of Lemma 3, for 2λc1 < 2,\nβ1T+1∑ j=i0+1 i2λc1−2 = β1T∑ j=i0+1 i2λc1−2 + (β1T + 1) 2λc1−2 ≤ (β1T ) 2λc1−1 2λc1 − 1 +O(T 2λc1−2).\nFor 2λc1 > 2, using (14) of Lemma 3,\nβ1T+1∑ j=i0+1 i2λc1−2 = β1T−1∑ j=i0+1 i2λc1−2 + (β1T ) 2λc1−2 + (β1T + 1) 2λc1−2 ≤ (β1T ) 2λc1−1 2λc1 − 1 +O(T 2λc1−2).\nFinally, for 2λc1 = 2,\nβ1T+1∑ j=i0+1 i2λc1−2 = (β1T + 1)− (i0 + 1) + 1 = β1T +O(1).\nCombining the three cases together, we have\nβ1T+1∑ j=i0+1 i2λc1−2 ≤ (β1T ) 2λc1−1 2λc1 − 1 +O ( T 2λc1−2 ) .\nThis allows us to further upper bound (27):\n4Γ21β 2λ(c2−c1) 1\n( 1 +\n2λc2 β1T\ne2λc2/β1 ) c21T −2λc1 β1T+1∑ i=i0+1 i2λc1−2\n≤ 4Γ21β 2λ(c2−c1) 1\n( 1 +\n2λc2 β1T\ne2λc2/β1 ) c21T −2λc1 ( (β1T ) 2λc1−1\n2λc1 − 1 +O\n( T 2λc1−2 )) = 4Γ21c 2 1β 2λc2−1 1\n2λc1 − 1 · 1 T\n+O ( 1\nT 2\n) +O ( 1\nT 3\n) .\nThe last term in (25): We bound this as follows:\nΓ22 >∑ i=β1T+1 c22 i2 e −2λc2 ∑> j=i+1 1 j ≤ Γ22 >∑ i=β1T+1 c22 i2 e−2λc2 log T i+1\n= Γ22c 2 2T −2λc2 >∑ i=β1T+1 (i+ 1)2λc2 i2 ≤ 4Γ22c22T−2λc2 >∑ i=β1T+1 (i+ 1)2λc2 (i+ 1)2\n= 4Γ22c 2 2T −2λc2 >+1∑ i=β1T+2 i2λc2−2, (28)\nwhere the first inequality follows from (17) and the last inequality from (1 + 1i ) 2 ≤ 4. If 2λc2 6= 1 and 2λc2 ≤ 2, using (16) from Lemma 3,\nT+1∑ j=β1T+2 i2λc2−2 ≤ 1− β 2λc2−1 1 2λc2 − 1 T 2λc2−1.\nIf 2λc2 > 2, using (14) from Lemma 3,\nT+1∑ j=β1T+2 i2λc2−2 = T−1∑ j=β1T i2λc2−2 + T 2λc2−2 + (T + 1)2λc2−2 − (β1T + 1)2λc2−2 − (β1T )2λc2−2\n= 1− β2λc2−11\n2λc2 − 1 T 2λc2−1 +O\n( T 2λc2−2 ) .\nIf 2λc2 = 2,\nT+1∑ j=β1T+2 i2λc2−2 = T+1∑ j=β1T+2 1 = (1− β1)T.\nIn all three cases we have\nT+1∑ j=β1T+2 i2λc2−2 ≤ 1− β 2λc2−1 1 2λc2 − 1 T 2λc2−1 +O ( T 2λc2−2 ) .\nThen (28) can be further upper bounded for 2λc2 6= 1\n4Γ22c 2 2T −2λc2 >+1∑ i=β1T+2 i2λc2−2 ≤ 4Γ22 c22(1− β 2λc2−1 1 ) 2λc2 − 1 · 1 T +O ( 1 T 2 ) . (29)\nIf 2λc2 = 1, we have\nT+1∑ j=β1T+2 i2λc2−2 = T∑ j=β1T+1 i−1 − (β1T + 1)−1 + (T + 1)−1 ≤ log 1 β1 ,\nand then\n4Γ22c 2 2T −2λc2 >+1∑ i=β1T+2 i2λc2−2 ≤ 4Γ22c22 log 1 β1 · 1 T .\nwhich is basically taking the limit as 2λc2 → 1 of the highest order term of (29). Therefore the summation of the three terms is of order O( 1T ) (from the second and third terms), and the constant in the front of the highest order term takes on one of two values:\n1. If 2λc2 6= 1,\n4Γ21 c21β 2λc2−1 1\n2λc1 − 1 + 4Γ22\nc22(1− β 2λc2−1 1 )\n2λc2 − 1 .\n2. If 2λc2 = 1,\n4Γ21 c21β 2λc2−1 1\n2λc1 − 1 + 4Γ22c 2 2 log\n1\nβ1 ."
    }, {
      "heading" : "A.3.2 Proof of Lemma 1",
      "text" : "Proof. (Of Lemma 1) Omitting the constant terms and setting k1 = 2λc1, k2 = 2λc2, we can re-write (13) as 1/T times\nQ(k1, k2) =Γ 2 1\nβk2−11 k 2 1\nk1 − 1 + Γ22 (1− βk2−11 )k22 k2 − 1 , (30)\nwith k∗1 = 2λc ∗ 1 = 2. Observe that in this case, k∗2 ≥ 2. Let x = k2 − 1; then x ≥ 1. Plugging in k∗1 = 2, we can re-write (30) as\nQ(x) = 4Γ21β x 1 + Γ 2 2(1− βx1 )\n( x+ 1\nx + 2\n) . (31)\nTaking the derivative, we see that Q′(x) =− 4Γ21βx1 log(1/β1) + Γ22(1− βx1 ) (\n1− 1 x2\n) + Γ22 ( x+ 1\nx + 2\n) βx1 log(1/β1). (32)\nSuppose\nl = 2 log(Γ1/Γ2) + log log(1/β1)\nlog(1/β1) .\nObserve that βl1 log(1/β1) = Γ22 Γ21 . Plugging x = l in to (32), the first term is −4Γ22, the second term is at most Γ22, and the third term is at most Γ42 Γ21 (l+ 1l + 2). Observe that for any fixed β1, for large enough Γ1/Γ2, l ≥ 1. Thus, the right hand side of (32) is at most: −4Γ22 + Γ22 + Γ42 Γ21 (l + 3). For fixed β1, l grows logarithmically in Γ1/Γ2, and hence, for large enough Γ1/Γ2, Γ22(l+3)\nΓ21 will become\narbitrarily small. Therefore, for large enough Γ1/Γ2, Q ′(l) < 0. Suppose\nu = 2 log(4Γ1/Γ2) + log log(1/β1)\nlog(1/β1) .\nObserve that βu1 log(1/β1) = Γ22\n16Γ21 . Plugging in x = u to (32), the first term reduces to −14Γ 2 2, the\nsecond term is Γ22(1− βu1 )(1− 1u2 ), and the third term is ≥ 0. Observe that as Γ1/Γ2 →∞ with β1 fixed, βu1 → 0 and 1/u2 → 0. Thus, for large enough Γ1/Γ2, Γ22(1−βu1 )(1− 1u2 )→ Γ 2 2, and therefore Q′(u) > 0. Thus, Q′(x) = 0 somewhere between l and u and the first part of the lemma follows. Consider\nx = 2 log(mΓ1/Γ2) + log log(1/β1)\nlog(1/β1)\nwith 1 ≤ m ≤ 4. The first term of (31) is always positive. As for the second term, x+ 1x +2 ≥ x for positive x and βx1 =\nΓ22 m2Γ21 1 log(1/β1) is small when Γ1/Γ2 is sufficiently large. Therefore for sufficiently\nlarge Γ1/Γ2, we have Γ 2 2(1 − βx1 )(x + 1x + 2) ≥ Γ22 2 x, and thus Q(x) ≥ Γ22 2 x, which gives the lower bound. And plugging in x = l gives the upper bound."
    }, {
      "heading" : "A.3.3 Proof of Lemma 2",
      "text" : "Proof. (Of Lemma 2) Let k2 = ; then ≥ 0. Plugging in k∗1 = 2, we can re-write (30) as\nQ( ) = 4Γ21β −1 1 + Γ 2 2(1− β −11 )\n( −1 + + 1\n−1 + + 2\n) . (33)\nTaking the derivative, we obtain the following:\nQ′( ) =− 4Γ21β −11 log(1/β1) + Γ 2 2(1− β −11 )(1−\n1 (1− )2 )− Γ\n2 2 2\n1− β −11 log(1/β1)\n=− β −11 log(1/β1) ( 4Γ21 + Γ22 2\n1−\n) + Γ22(β −1 1 − 1) ( 1 (1− )2 − 1 )\n=− β −11 log(1/β1) ( 4Γ21 + Γ22 2\n1−\n) + Γ22(β −1 1 − 1) (2− ) (1− )2 . (34)\nFor = Γ21 Γ22 ≤ 1, using 1− β1− 1 ≤ (1− ) log(1/β1) and β −1 1 − 1 = (1− β 1− 1 )β −1, this is at most:\n−β −11 log(1/β1) ( 4Γ21 + Γ22 2\n1− − Γ 2 2 (2− ) 1− ) = −2Γ21β −11 log(1/β1).\nThus, at l = Γ21 Γ22 , Q′(l) < 0. Moreover, for ∈ [0, 12 ], 1− β 1− 1 ≥ β1(1− ) log(1/β1). Therefore, Q′( ) is at least:\nQ′( ) ≥ −β −11 log(1/β1) ( 4Γ21 + Γ22 2\n1−\n) + Γ22β 1 log(1/β1)\n(2− ) 1−\n≥ β −11 log(1/β1) ( Γ22β (2− ) 1− − 4Γ21 − Γ22 2 1− ) .\nLet u = 8Γ21 β1Γ22 ; suppose that Γ2/Γ1 is large enough such that u ≤ β1/4. Then, u(2−u)β1−u2 ≥ 15uβ116 , and\nΓ22(u(2− u)β1 − u2) 1− u ≥ 15Γ 2 2uβ1 16(1− β1) ≥ 15Γ 2 1 2(1− β1) ≥ 5Γ21.\nTherefore, Q′(u) > 0, and thus Q( ) is minimized at some ∈ [l, u]. For the second part of the lemma, the upper bound is obtained by plugging in = Γ1Γ2 . For the lower bound, observe that for any ∈ [l, u], Q( ) ≥ 4Γ21β u−1 1 ≥ 4Γ21β Γ22/βΓ 2 1−1 1 ."
    } ],
    "references" : [ {
      "title" : "Information-theoretic lower bounds on the oracle complexity of convex optimization",
      "author" : [ "A. Agarwal", "P.L. Bartlett", "P. Ravikumar", "M.J. Wainwright" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Agarwal et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Agarwal et al\\.",
      "year" : 2009
    }, {
      "title" : "Private empirical risk minimization, revisited",
      "author" : [ "R. Bassily", "A. Thakurta", "A. Smith" ],
      "venue" : "In Proceedings of the IEEE Symposium on Foundations of Computer Science (FOCS",
      "citeRegEx" : "Bassily et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bassily et al\\.",
      "year" : 2014
    }, {
      "title" : "Scaling up Machine Learning, Parallel and Distributed Approaches",
      "author" : [ "R. Bekkerman", "M. Bilenko", "J. Langford" ],
      "venue" : null,
      "citeRegEx" : "Bekkerman et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Bekkerman et al\\.",
      "year" : 2011
    }, {
      "title" : "Large-scale machine learning with stochastic gradient descent",
      "author" : [ "L. Bottou" ],
      "venue" : "Proceedings of COMPSTAT’2010,",
      "citeRegEx" : "Bottou.,? \\Q2010\\E",
      "shortCiteRegEx" : "Bottou.",
      "year" : 2010
    }, {
      "title" : "Differentially private empirical risk minimization",
      "author" : [ "K. Chaudhuri", "C. Monteleoni", "A.D. Sarwate" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Chaudhuri et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Chaudhuri et al\\.",
      "year" : 2011
    }, {
      "title" : "Learning from data of variable quality",
      "author" : [ "K. Crammer", "M. Kearns", "J. Wortman" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Crammer et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Crammer et al\\.",
      "year" : 2006
    }, {
      "title" : "Efficient online and batch learning using forward backward splitting",
      "author" : [ "J. Duchi", "Y. Singer" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Duchi and Singer.,? \\Q2009\\E",
      "shortCiteRegEx" : "Duchi and Singer.",
      "year" : 2009
    }, {
      "title" : "Privacy aware learning",
      "author" : [ "J. Duchi", "M. Jordan", "M. Wainwright" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Duchi et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Duchi et al\\.",
      "year" : 2012
    }, {
      "title" : "Efficient noise-tolerant learning from statistical queries",
      "author" : [ "M. Kearns" ],
      "venue" : "Journal of the ACM,",
      "citeRegEx" : "Kearns.,? \\Q2008\\E",
      "shortCiteRegEx" : "Kearns.",
      "year" : 2008
    }, {
      "title" : "Making gradient descent optimal for strongly convex",
      "author" : [ "O. Shamir", "K. Sridharan" ],
      "venue" : null,
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2008
    }, {
      "title" : "Stochastic convex optimization",
      "author" : [ "S. Shalev-Shwartz", "O. Shamir", "N. Srebro", "K. Sridaran" ],
      "venue" : null,
      "citeRegEx" : "Shalev.Shwartz et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Shalev.Shwartz et al\\.",
      "year" : 2012
    }, {
      "title" : "Stochastic gradient descent with differen",
      "author" : [ "K. Chaudhuri", "Anand D. Sarwate" ],
      "venue" : null,
      "citeRegEx" : "Song et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2009
    }, {
      "title" : "A statistical framework for differential privacy",
      "author" : [ "S. Zhou" ],
      "venue" : "Journal of the American",
      "citeRegEx" : "Wasserman and Zhou.,? \\Q2013\\E",
      "shortCiteRegEx" : "Wasserman and Zhou.",
      "year" : 2013
    }, {
      "title" : "Dual averaging methods for regularized stochastic learning and online",
      "author" : [ ],
      "venue" : null,
      "citeRegEx" : "Xiao.,? \\Q2009\\E",
      "shortCiteRegEx" : "Xiao.",
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "We propose a model for variable data quality which is natural in the context of large-scale learning using stochastic gradient descent (SGD) and its variants (Bottou, 2010; Bekkerman et al., 2011).",
      "startOffset" : 158,
      "endOffset" : 196
    }, {
      "referenceID" : 2,
      "context" : "We propose a model for variable data quality which is natural in the context of large-scale learning using stochastic gradient descent (SGD) and its variants (Bottou, 2010; Bekkerman et al., 2011).",
      "startOffset" : 158,
      "endOffset" : 196
    }, {
      "referenceID" : 3,
      "context" : "To our knowledge, Crammer et al. (2006) were the first to provide a theoretical study of how to learn classifiers from data of variable quality.",
      "startOffset" : 18,
      "endOffset" : 40
    }, {
      "referenceID" : 2,
      "context" : "We propose a model for variable data quality which is natural in the context of large-scale learning using stochastic gradient descent (SGD) and its variants (Bottou, 2010; Bekkerman et al., 2011). We assume that the training data are accessed through an oracle which provides an unbiased but noisy estimate of the gradient of the objective. The noise comes from two sources: the random sampling of a data point, and additional noise due to the data quality. Our two motivating applications – learning with local differential privacy and learning from data of variable quality – can both be modeled as solving a regularized convex optimization problem using SGD. Learning from data with heterogeneous noise in this framework thus reduces to running SGD with noisy gradient estimates, where the magnitude of the added noise varies across iterations. Main results. In this paper we study noisy stochastic gradient methods when learning from multiple data sets with different noise levels. For simplicity we consider the case where there are two data sets, which we call Clean and Noisy. We process these data sets sequentially using SGD with learning rate O(1/t). In a future full version of this work we also analyze averaged gradient descent (AGD) with learning rate O(1/ √ t). We address some basic questions in this setup: In what order should we process the data? Suppose we use standard SGD on the union of Clean and Noisy. We show theoretically and empirically that the order in which we should process the datasets to get good performance depends on the learning rate of the algorithm: in some cases we should use the order (Clean,Noisy) and in others (Noisy,Clean). Can we use knowledge of the noise rates? We show that using separate learning rates that depend on the noise levels for the clean and noisy datasets improves the performance of SGD. We provide a heuristic for choosing these rates by optimizing an upper bound on the error for SGD that depends on the ratio of the noise levels. We analytically quantify the performance of our algorithm in two regimes of interest. For moderate noise levels, we demonstrate empirically that our algorithm outperforms using a single learning rate and using clean data only. Does using noisy data always help? The work of Crammer et al. (2006) suggests that if the noise level of noisy data is above some threshold, then noisy data will not help.",
      "startOffset" : 173,
      "endOffset" : 2296
    }, {
      "referenceID" : 0,
      "context" : "When the objective function is λ-strongly convex, the learning rate used for SGD is O(1/λt) (Nemirovsky and Yudin, 1983; Agarwal et al., 2009; Rakhlin et al., 2012; Moulines and Bach, 2011), which leads to a regret of O(1/λ2t) for smooth objectives.",
      "startOffset" : 92,
      "endOffset" : 189
    }, {
      "referenceID" : 6,
      "context" : "For non-smooth objectives, SGD with learning rate O(1/λt) followed by some form of averaging of the iterates achieves O(1/λt) (Nesterov and Vial, 2008; Nemirovski et al., 2009; Shalev-Shwartz et al., 2009; Xiao, 2010; Duchi and Singer, 2009).",
      "startOffset" : 126,
      "endOffset" : 241
    }, {
      "referenceID" : 4,
      "context" : "There is also a body of literature on differentially private classification by regularized convex optimization in the batch (Chaudhuri et al., 2011; Rubinstein et al., 2012; Kifer et al., 2012) as well as the online (Jain et al.",
      "startOffset" : 124,
      "endOffset" : 193
    }, {
      "referenceID" : 7,
      "context" : "In this paper, we consider classification with local differential privacy (Wasserman and Zhou, 2010; Duchi et al., 2012), a stronger form of privacy than ordinary differential privacy.",
      "startOffset" : 74,
      "endOffset" : 120
    }, {
      "referenceID" : 2,
      "context" : "For simplicity we, like previous work Crammer et al. (2006), assume that the algorithms know the noise levels exactly.",
      "startOffset" : 38,
      "endOffset" : 60
    }, {
      "referenceID" : 0,
      "context" : "When the objective function is λ-strongly convex, the learning rate used for SGD is O(1/λt) (Nemirovsky and Yudin, 1983; Agarwal et al., 2009; Rakhlin et al., 2012; Moulines and Bach, 2011), which leads to a regret of O(1/λ2t) for smooth objectives. For non-smooth objectives, SGD with learning rate O(1/λt) followed by some form of averaging of the iterates achieves O(1/λt) (Nesterov and Vial, 2008; Nemirovski et al., 2009; Shalev-Shwartz et al., 2009; Xiao, 2010; Duchi and Singer, 2009). There is also a body of literature on differentially private classification by regularized convex optimization in the batch (Chaudhuri et al., 2011; Rubinstein et al., 2012; Kifer et al., 2012) as well as the online (Jain et al., 2012) setting. In this paper, we consider classification with local differential privacy (Wasserman and Zhou, 2010; Duchi et al., 2012), a stronger form of privacy than ordinary differential privacy. Duchi et al. (2012) propose learning a classifier with local differential privacy using SGD, and Song et al.",
      "startOffset" : 121,
      "endOffset" : 943
    }, {
      "referenceID" : 0,
      "context" : "When the objective function is λ-strongly convex, the learning rate used for SGD is O(1/λt) (Nemirovsky and Yudin, 1983; Agarwal et al., 2009; Rakhlin et al., 2012; Moulines and Bach, 2011), which leads to a regret of O(1/λ2t) for smooth objectives. For non-smooth objectives, SGD with learning rate O(1/λt) followed by some form of averaging of the iterates achieves O(1/λt) (Nesterov and Vial, 2008; Nemirovski et al., 2009; Shalev-Shwartz et al., 2009; Xiao, 2010; Duchi and Singer, 2009). There is also a body of literature on differentially private classification by regularized convex optimization in the batch (Chaudhuri et al., 2011; Rubinstein et al., 2012; Kifer et al., 2012) as well as the online (Jain et al., 2012) setting. In this paper, we consider classification with local differential privacy (Wasserman and Zhou, 2010; Duchi et al., 2012), a stronger form of privacy than ordinary differential privacy. Duchi et al. (2012) propose learning a classifier with local differential privacy using SGD, and Song et al. (2013) show empirically that using mini-batches significantly improves the performance of differentially private SGD.",
      "startOffset" : 121,
      "endOffset" : 1039
    }, {
      "referenceID" : 0,
      "context" : "When the objective function is λ-strongly convex, the learning rate used for SGD is O(1/λt) (Nemirovsky and Yudin, 1983; Agarwal et al., 2009; Rakhlin et al., 2012; Moulines and Bach, 2011), which leads to a regret of O(1/λ2t) for smooth objectives. For non-smooth objectives, SGD with learning rate O(1/λt) followed by some form of averaging of the iterates achieves O(1/λt) (Nesterov and Vial, 2008; Nemirovski et al., 2009; Shalev-Shwartz et al., 2009; Xiao, 2010; Duchi and Singer, 2009). There is also a body of literature on differentially private classification by regularized convex optimization in the batch (Chaudhuri et al., 2011; Rubinstein et al., 2012; Kifer et al., 2012) as well as the online (Jain et al., 2012) setting. In this paper, we consider classification with local differential privacy (Wasserman and Zhou, 2010; Duchi et al., 2012), a stronger form of privacy than ordinary differential privacy. Duchi et al. (2012) propose learning a classifier with local differential privacy using SGD, and Song et al. (2013) show empirically that using mini-batches significantly improves the performance of differentially private SGD. Recent work by Bassily et al. (2014) provides an improved privacy analysis for non-local privacy.",
      "startOffset" : 121,
      "endOffset" : 1187
    }, {
      "referenceID" : 0,
      "context" : "When the objective function is λ-strongly convex, the learning rate used for SGD is O(1/λt) (Nemirovsky and Yudin, 1983; Agarwal et al., 2009; Rakhlin et al., 2012; Moulines and Bach, 2011), which leads to a regret of O(1/λ2t) for smooth objectives. For non-smooth objectives, SGD with learning rate O(1/λt) followed by some form of averaging of the iterates achieves O(1/λt) (Nesterov and Vial, 2008; Nemirovski et al., 2009; Shalev-Shwartz et al., 2009; Xiao, 2010; Duchi and Singer, 2009). There is also a body of literature on differentially private classification by regularized convex optimization in the batch (Chaudhuri et al., 2011; Rubinstein et al., 2012; Kifer et al., 2012) as well as the online (Jain et al., 2012) setting. In this paper, we consider classification with local differential privacy (Wasserman and Zhou, 2010; Duchi et al., 2012), a stronger form of privacy than ordinary differential privacy. Duchi et al. (2012) propose learning a classifier with local differential privacy using SGD, and Song et al. (2013) show empirically that using mini-batches significantly improves the performance of differentially private SGD. Recent work by Bassily et al. (2014) provides an improved privacy analysis for non-local privacy. Our work is an extension of these papers to heterogeneous privacy requirements. Crammer et al. (2006) study classification when the labels in each data set are corrupted by RCN of different rates.",
      "startOffset" : 121,
      "endOffset" : 1350
    }, {
      "referenceID" : 7,
      "context" : "1 Local Differential Privacy Local differential privacy (Wasserman and Zhou, 2010; Duchi et al., 2012; Kasiviswanathan et al., 2008) is a strong notion of privacy motivated by differential privacy (Dwork et al.",
      "startOffset" : 56,
      "endOffset" : 132
    }, {
      "referenceID" : 7,
      "context" : "Duchi et al. (2012) showed that this mechanism provides -local privacy assuming analytic conditions on the loss function, bounded data, and that the oracle generates a fresh random sample at each invocation.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 8,
      "context" : "2 Random Classification Noise In the random classification noise model of Kearns (1998), the learning algorithm is presented with labelled examples (x1, ỹ1), .",
      "startOffset" : 74,
      "endOffset" : 88
    }, {
      "referenceID" : 8,
      "context" : "2 Random Classification Noise In the random classification noise model of Kearns (1998), the learning algorithm is presented with labelled examples (x1, ỹ1), . . . , (xT , ỹT ), where each ỹi ∈ {−1, 1} has been obtained by independently flipping the true label yi with some probability σ. Natarajan et al. (2013) showed that solving",
      "startOffset" : 74,
      "endOffset" : 313
    }, {
      "referenceID" : 5,
      "context" : "In practice, we may wish to learn classifiers from multiple datasets with different amounts of classification noise (Crammer et al., 2006); for example, we may have a small dataset D1 labeled by domain experts, and a larger noisier dataset D2, labeled via crowdsourcing, with flip probabilities σ1 and σ2.",
      "startOffset" : 116,
      "endOffset" : 138
    } ],
    "year" : 2014,
    "abstractText" : "We consider learning from data of variable quality that may be obtained from different heterogeneous sources. Addressing learning from heterogenous data in its full generality is a challenging problem. In this paper, we adopt instead a model in which data is observed through heterogeneous noise, where the noise level reflects the quality of the data source. We study how to use stochastic gradient algorithms to learn in this model. Our study is motivated by two concrete examples where this problem arises naturally: learning with local differential privacy based on data from multiple sources with different privacy requirements, and learning from data with labels of variable quality. The main contribution of this paper is to identify how heterogeneous noise impacts performance. We show that given two datasets with heterogeneous noise, the order in which to use them in standard SGD depends on the learning rate. We propose a method for changing the learning rate as a function of the heterogeneity, and prove new regret bounds for our method in two cases of interest. Experiments on real data show that our method performs better than using a single learning rate and using only the less noisy of the two datasets when the noise level is low to moderate.",
    "creator" : "LaTeX with hyperref package"
  }
}