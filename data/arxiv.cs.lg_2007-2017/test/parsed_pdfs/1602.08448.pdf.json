{
  "name" : "1602.08448.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Simple Bayesian Algorithms for Best-Arm Identification",
    "authors" : [ "Daniel Russo" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "the best among a finite set of options or designs. An experimenter sequentially chooses designs to measure and observes noisy signals of their quality with the goal of confidently identifying the best design after a small number of measurements. This paper proposes three simple and intuitive Bayesian algorithms for adaptively allocating measurement effort, and formalizes a sense in which these seemingly naive rules are the best possible. One proposal is top-two probability sampling, which computes the two designs with the highest posterior probability of being optimal, and then randomizes to select among these two. One is a variant of top-two sampling which considers not only the probability a design is optimal, but the expected amount by which its quality exceeds that of other designs. The final algorithm is a modified version of Thompson sampling that is tailored for identifying the best design. We prove that these simple algorithms satisfy a sharp optimality property. In a frequentist setting where the true quality of the designs is fixed, one hopes the posterior definitively identifies the optimal design, in the sense that that the posterior probability assigned to the event that some other design is optimal converges to zero as measurements are collected. We show that under the proposed algorithms this convergence occurs at an exponential rate, and the corresponding exponent is the best possible among all allocation rules."
    }, {
      "heading" : "1 Introduction",
      "text" : "This paper considers the optimal adaptive allocation of measurement effort in order to identify the best among a finite set of options or designs. An experimenter sequentially chooses designs to measure and observes independent noisy signals of their quality. The goal is to allocate measurement effort intelligently so that the best design can be identified confidently after a small number of measurements. Just as the multi-armed bandit problem crystallizes the tradeoff between exploration and exploitation in sequential decision-making, this “pure–exploration” problem crystallizes the challenge of efficiently gathering information before committing to a final decision. It serves as a fundamental abstraction of issues faced in many practical settings. For example:\n• Efficient A/B/C Testing: An e-commerce platform is considering a change to its website and would like to identify the best performing candidate among many potential new designs. To do this, the platform runs an experiment, displaying different designs to different users who visit the site. How should the platform decide what percentage of traffic to allocate to each website design?\n• Simulation Optimization: An engineer would like to identify the best performing aircraft design among several proposals. She has access to a realistic simulator through which she can assess the quality of the designs, but each simulation trial is very time consuming and produces only noisy output. How should she allocate simulation effort among the designs?\nar X\niv :1\n60 2.\n08 44\n8v 3\n[ cs\n.L G\n] 7\nD ec\n2 01\n6\n• Design of Clinical Trials: A medical research organization would like to find the most effective treatment out of several promising candidates. They run a clinical trail in which they experiment with the treatments. The results of the study may influence practice for many years to come, and so it is worth reaching a definitive conclusion. At the same time, clinical trails are extremely expensive, and careful experimentation can help to mitigate the associated costs.1 Multi-armed bandit models of clinical trails date back to Thompson [1933], but bandit algorithms lack statistical power in detecting the best treatment at the end of the trial [Villar et al., 2015]. Can we develop adaptive rules with better performance?\nWe study Bayesian algorithms for adaptively allocating measurement effort. Each begins with a prior distribution over the unknown quality of the designs. The experimenter learns as measurements are gathered, and beliefs are updated to form a posterior distribution. This posterior distribution gives a principled mechanism for reasoning about the uncertain quality of designs, and for assessing the probability any given design is optimal. By formulating this problem as a Markov decision process whose state-space tracks posterior beliefs about the true quality of each design, dynamic programming could in principle be used to optimize many natural measures of performance. Unfortunately, computing or even storing an optimal policy is usually infeasible due to the curse of dimensionality. Instead, this work proposes three simple and intuitive rules for adaptively allocating measurement effort, and by characterizing fundamental limits on the performance of any algorithm, formalizes a sense in which these seemingly naïve rules are the best possible.\nThe first algorithm we propose is called top–two probability sampling. It computes at each time-step the two designs with the highest posterior probability of being optimal. It then randomly chooses among them, selecting the design that appears most likely to be optimal with some fixed probability, and selecting the second most likely otherwise. Beliefs are updated as observations are collected, so the top-two designs change over time. The long run fraction of measurement effort allocated to each design depends on the true quality of the designs, and the distribution of observation noise. Top–two value sampling proceeds in a similar manner, but in selecting the top-two designs it considers not only the probability a design is optimal, but the expected amount by which its quality exceeds that of other designs. The final algorithm we propose is a toptwo sampling version of the Thompson sampling algorithm for multi-armed bandits. Thompson sampling has attracted a great deal of recent interest in both academia and industry [Scott, 2016, Tang et al., 2013, Graepel et al., 2010, Chapelle and Li, 2011, Agrawal and Goyal, 2012, Kauffmann et al., 2012, Gopalan et al., 2014, Russo and Van Roy, 2014], but it is designed to maximize the cumulative reward earned while sampling. As a result, in the long run it allocates almost all effort to measuring the estimated-best design, and requires a huge number of total measurements to certify that none of the alternative designs offer better performance. We introduce a natural top-two variant of Thompson sampling that avoids this issue and as a result offers vastly superior performance for the best-arm identification problem.\nRemarkably, these simple heuristic algorithms satisfy a strong optimality property. Our analysis focuses on frequentist consistency and rate convergence of the posterior distribution, and therefore takes place in a setting where the true quality of the designs is fixed, but unknown to the experimenter. One hopes that as measurements are collected the posterior distribution definitively identifies the true best design, in the sense that the posterior probability assigned to the event that some other design is optimal converges to zero. We show that under the proposed algorithms this convergence occurs at an exponential rate, and the corresponding exponent is essentially the best\n1Interpreted the context of clinical trials, this paper’s results are stated in terms of the number of patients required to reach a confided conclusion of the best treatment. However, we will see that optimal rules from this perspective also allocate fewer patients to very poor treatments, potentially leading to more ethical trials [Berry, 2004].\npossible among all allocation rules."
    }, {
      "heading" : "1.1 Main Contributions",
      "text" : "This paper makes both algorithmic and theoretical contributions. On the algorithmic side, we develop three new adaptive measurement rules. The top-two Thompson sampling rule, in particular, could have an immediate impact in application areas where Thompson sampling is already in use. For example, there are various reports of Thompson sampling being used in A/B testing [Scott, 2016] and in clinical trials [Berry, 2004]. But practitioners in these domains typically hope to commit to a decision after definitive period of experimentation, and top-two Thompson sampling can greatly reduce the number of measurements required to do so. In addition, because of their simplicity, the proposed allocation rules can be easily adapted to treat problems beyond the scope of this paper’s problem formulation. See Section 7 for examples.\nThe paper also makes several theoretical contributions. Most importantly, it is of broad scientific interest to understand when very simple measurement strategies are the best possible. This paper provides a sharp result of this type by proving that three top-two sampling rules attain an optimal rate of posterior convergence across a broad class of problems. In establishing this result, we exactly characterize the optimal rate of posterior convergence attainable by an adaptive algorithm, and provide interpretable bounds on this rate when measurement distributions are sub-Gaussian. The analysis also provides several intermediate results which may be of independent interest, including establishing consistency and exponential rates of convergence for posterior distributions with nonconjugate priors and under adaptive measurement rules."
    }, {
      "heading" : "1.2 Related Literature",
      "text" : "Sequential Bayesian Best-Arm Identification. There is a sophisticated literature on algorithms for Bayesian multi-armed bandit problems. In discounted bandit problems with independent arms, Gittins indices characterize the Bayes optimal policy [Gittins and Jones, 1974, Gittins, 1979]. Moreover, a variety of simpler Bayesian allocation rules have been developed, including Bayesian upper-confidence bound algorithms [Kaufmann et al., 2012, Srinivas et al., 2012, Kaufmann, 2016], Thompson sampling [Agrawal and Goyal, 2012, Korda et al., 2013, Gopalan et al., 2014, Johnson et al., 2015], information-directed sampling [Russo and Van Roy, 2014], the knowledge gradient [Ryzhov et al., 2012], and optimistic Gittins indices [Gutin and Farias, 2016]. These heuristic algorithms can be applied effectively to complicated learning problems beyond the specialized settings in which the Gittins index theorem holds, have been shown to have strong performance in simulation, and have theoretical performance guarantees. In several cases, they are known to attain sharp asymptotic limits on the performance of any adaptive algorithm due to Lai and Robbins [1985].\nThe pure-exploration problem studied in this paper is not nearly as well understood. Recent work has cast this problem in a decision-theoretic framework [Chick and Gans, 2009]. However, because the conditions required for the Gittins index theorem do not hold, computing an optimal policy via dynamic programming is generally infeasible due to the curse of dimensionality. Papers in this area typically focus on problems with Gaussian observations and priors. They formulate simpler problems that can be solved exactly – like a problem where only a single measurement can be gathered [Gupta and Miescke, 1996, Frazier et al., 2008, Chick et al., 2010] or a continuous-time problem with only two alternatives [Chick and Frazier, 2012] – and then extend those solutions heuristically to build measurement and stopping rules in more general settings.\nFor problems with Gaussian priors and noise distributions, the expected-improvement (EI) algorithm is a popular Bayesian approach to sequential information-gathering. Interesting recent\nwork by Ryzhov [2016] studies the long run distribution of measurement effort allocated by the expected-improvement and shows this is related to the optimal computing budget allocation of Chen et al. [2000]. This contribution is very similar in spirit to this paper, as it relates the longrun behavior of a simple Bayesian measurement strategy to a notion of an approximately optimal allocation. Unfortunately, EI cannot match the performance guarantees in this paper. In fact, under EI the posterior converges only at a polynomial rate, instead of the exponential rate attained by the algorithms proposed here. See appendix C for a more precise discussion.\nClassical Ranking and Selection. The problem of identifying the best system has been studied for many decades under the names ranking and selection or ordinal optimization. See Kim and Nelson [2006] and Kim and Nelson [2007] for reviews. Part of this literature focuses on a problem called subset-selection, where the goal is not to identify the best-design, but to find a fairly small subset of designs that is guaranteed to contain the best design. Beginning with Bechhofer [1954], many papers have focused on an indifference zone formulation, where, for user-specified , δ > 0, the goal is to guarantee with probability at least 1 − δ the algorithm returns a design within of optimal. Assuming measurement noise is Gaussian with known variance σ2, one can guarantee this indifference-zone criterion by gathering O ( (σk/ 2) log(k/δ) ) total measurements, divided equally among the k designs, and then returning the design with highest empirical-mean. For the case of unknown variances, Rinott [1978] proposes a two stage procedure, where the first stage is used to estimate the variance of each population, and the number of samples collected from each design in the second stage is scaled by its estimated standard deviation. In the machine learning literature, Even-Dar et al. [2002] shows that when measurement noise is uniformly bounded, the indifferencezone criterion is satisfied by a sequential elimination strategy that uses only O ( (k/ 2) log(1/δ)\n) samples on average. Mannor and Tsitsiklis [2004] provide a matching lower bound. Similar to minimax bounds, this shows the upper bound of Even-Dar et al. [2002] is tight, up to a constant factor, for a certain worst case problem instance. Since Paulson [1964], many authors have sought to reduce the number of samples required on easier problem instances by designing algorithms that sequentially eliminate arms once they are determined to be suboptimal with high confidence. See the recent work of Frazier [2014] and the references therein. However, in a sense described below, Jennison et al. [1982] show formally that there are problems with Gaussian observations where any sequential-elimination algorithm will require substantially more samples than optimal adaptive allocation rules. See Section 7 for modified top-two sampling algorithms designed for an indifference zone criterion.\nThe asymptotic complexity of best-arm identification. We described attainable rates of performance on a worst-case problem instance characterized by Even-Dar et al. [2002] and Mannor and Tsitsiklis [2004]. A great deal of work has sought “problem dependent” bounds, which reveal that the best-arm can be identified more rapidly when the true problem instance is easier. This is the case, for example, when some arms are of very low quality, and can be distinguished from the best using a small number of measurements. Asymptotic measures of the complexity of best-arm identification appear to have been derived independently in statistics [Chernoff, 1959, Jennison et al., 1982], simulation optimization [Glynn and Juneja, 2004], and, concurrently with this paper, in the machine learning literature [Garivier and Kaufmann, 2016]. Each of these papers studies a slightly different objective, but each captures a notion of the number of samples required to identify the best-arm as a function of the problem instance – i.e. as a function the number of designs, each design’s true quality, and the distribution of measurement noise.\nGlynn and Juneja [2004] build on the optimal-computing-budget allocation (OCBA) of Chen et al. [2000] to provide a rigorous large-deviations derivation of the optimal fixed allocation. In particular, assuming the design with the highest empirical mean is returned, there is a fixed allocation under which the probability of incorrect selection decays exponentially, and the exponent is optimal under all fixed-allocation rules. The setting studied by this paper is often called the “fixed-budget” setting in the recent multi-armed bandit literature. Unfortunately, it may be difficult to implement the allocation in Glynn and Juneja [2004] without additional prior knowledge. Later work by Glynn and Juneja [2015] provides a substantial discussion of this issue.\nThis paper was highly influenced by a classic paper by Chernoff [1959] on the sequential design of experiments for binary hypothesis testing. Chernoff’s asymptotic derivations give great insight best-arm identification, which can be formulated as a multiple-hypothesis testing problem with sequentially chosen experiments, but surprisingly this connection does not seem to be discussed in the literature. Chernoff looks at a different scaling than Glynn and Juneja [2004]. Rather than take the budget of available measurements to infinity, he allows the algorithm to stop and declare the hypothesis true or false at any time, but takes the cost of gathering measurements to zero while the cost of an incorrect terminal decision stays fixed. He constructs rules that minimize expected total costs in this limit. Chernoff makes restrictive technical assumptions, some of which have been removed in subsequent work [Albert, 1961, Kiefer and Sacks, 1963, Keener, 1984, Nitinawarat et al., 2013, Naghshvar et al., 2013].\nJennison et al. [1982] study an indifference zone formulation of the problem of identifying the best-design. Like Chernoff [1959], they allow the algorithm to stop and return an estimate of the best-arm at any time, but rather than penalize incorrect decisions, they require that the probability correct selection (PCS) exceeds 1 − δ > 0 for every problem instance. Intuitively, the expected number of samples required by an algorithm satisfying this PCS constraint must tend to infinity as δ → 0. In the case of Gaussian measurement noise, Jennison et al. [1982] characterize the optimal asymptotic scaling of expected number of samples in this limit. The recent multi-armed bandit literature refers to this formulation as the “fixed-confidence” setting.\nA large body of work in the recent machine learning literature has sought to characterize various notions of the complexity of best-arm identification [Even-Dar et al., 2002, Mannor and Tsitsiklis, 2004, Audibert and Bubeck, 2010, Gabillon et al., 2012, Karnin et al., 2013, Jamieson and Nowak, 2014]. However, upper and lower bounds match only up to constant or logarithmic factors, and only for particular hard problem instances. Substantial progress was presented by Kaufmann and Kalyanakrishnan [2013] and Kaufmann et al. [2014], who seek to exactly characterize the asymptotic complexity of identifying the best arm in both the fixed-budget and fixed-confidence settings. Still, the upper and lower bounds presented there do not match. A short abstract of the current paper appeared in the 2016 Conference on Learning Theory. In the same conference, independent work by Garivier and Kaufmann [2016] provided matching upper and lower bounds on the complexity of identifying the best arm in the “fixed-confidence” setting. Like the present paper, but unlike Jennison et al. [1982], these results apply whenever observation distributions are in the exponential family and do not require an indifference zone.\nThe current paper looks at a different measure. We study a frequentist setting in which the true quality of each design is fixed, and characterize the rate of posterior convergence attainable for each problem instance. We also describe, as a function of the problem instance, the long-run fraction of measurement effort allocated to each design by any algorithm attaining this rate of convergence. These asymptotic limits turn out to be closely related to some of the aforementioned results. In particular, the optimal exponent given in Subsection 6.4 mirrors the complexity measure of Chernoff [1959]. This exponent is then simplified into a form that mirrors one derived by Glynn and Juneja [2004], and, for Gaussian distributions, one derived by Jennison et al. [1982]. While the\ncomplexity measure we derive is similar to past work, the proposed algorithms differ substantially. The allocation rules proposed by Chernoff [1959], Jennison et al. [1982] and Glynn and Juneja [2004] are essentially developed as a means of proving certain rates are attainable asymptotically, and as Chernoff [1975] writes, “sidestep the issue of how to experiment in the early stages.” By contrast, we show simple and natural adaptive rules automatically reach a notion of asymptotically optimal performance. See Subsection 6.5 for a more precise discussion of these approaches."
    }, {
      "heading" : "2 Problem Formulation",
      "text" : "Consider the problem of efficiently identifying the best among a finite set of designs based on noisy sequential measurements of their quality. At each time n ∈ N, a decision-maker chooses to measure the design In ∈ {1, ..., k}, and observes a measurement Yn,In . The measurement Yn,i ∈ R associated with design i and time n is drawn from a fixed, unknown, probability distribution, and the vector Yn , (Yn,1, ..., Yn,k) is drawn independently across time. The decision-maker chooses a policy, or adaptive allocation rule, which is a (possibly randomized) rule for choosing a design In to measure as a function of past observations I1, Y1,I1 , ...In−1, Yn−1,In−1 . The goal is to efficiently identify the design with highest mean.\nWe will restrict attention to problems where measurement distributions are in the canonical one dimensional exponential family. The marginal distribution of the outcome Yn,i has density p(y|θ∗i ) with respect to a base measure ν, where θ∗i ∈ R is an unknown parameter associated with design i. This density takes the form\np(y|θ) = b(y) exp{θT (y)−A(θ)} θ ∈ R (1)\nwhere b, T , and A are known functions, and A(θ) is assumed to be twice differentiable. We will assume that T is a strictly increasing function so that µ(θ) , ´ yp(y|θ)dν(y) is a strictly increasing function of θ. Many common distributions can be written in this form, including Bernoulli, normal (with known variance), Poisson, exponential, chi-squared, and Pareto (with known minimal value).\nThroughout the paper, θ∗ , (θ∗1, ..., θ∗k) will denote the unknown true parameter vector, and θ and θ′ will be used to denote possible alternative parameter vectors. Let I∗ = arg max1≤i≤k θ∗i denote the unknown best design. We will assume throughout that θ∗i 6= θ∗j for i 6= j so that I∗ is unique, although this can be relaxed by considering an indifference zone formulation where the goal is to identify an –optimal design, for some specified tolerance level > 0.\nPrior and Posterior Distributions. The policies studied in this paper make use of a prior distribution Π1 over a set of possible parameters Θ that contains θ∗. Based on a sequence of observations (I1, Y1,I1 , ..., In−1, Yn−1,In−1), beliefs are updated to attain a posterior distribution Πn. We assume Π1 has density π1 with respect to Lebesgue measure. In this case, the posterior distribution Πn has corresponding density\nπn(θ) = π1(θ)Ln−1(θ)´\nΘ π1(θ′)Ln−1(θ′)dθ′ n ≥ 2, (2)\nwhere\nLn−1(θ) = n−1∏ l=1 p(Yl,Il |θIl)\nis the likelihood function. While this formulation enforces some technical restrictions to facilitate theoretical analysis, it allows for very general prior distributions, and in particular allows for the quality of different designs to be correlated under the priors.\nOptimal Action Probabilities. Let\nΘi , { θ ∈ Θ ∣∣∣∣θi > maxj 6=i θj }\ndenote the set of parameters under which design i is optimal, and let\nαn,i , Πn(Θi) = ˆ\nΘi\nπn(θ)dθ (3)\ndenote the posterior probability assigned to the event that action i is optimal. Our analysis will focus on Πn(ΘcI∗) = 1−αI∗ , which is the posterior probability assigned to the event that an action other than I∗ is optimal. The next section will introduce policies under which Πn(ΘcI∗) → 0 as n→∞, and the rate of convergence is essentially optimal.\nFurther Notation. Before proceeding, we introduce some further notation. Let Fn denote the sigma algebra generated by (I1, Y1,I1 , ...In, Yn,In). For all i ∈ {1, ..., k} and n ∈ N, define\nψn,i , P(In = i|Fn−1) Ψn,i , n∑ `=1 ψ`,i ψn,i , n −1Ψn,i.\nEach of these measures the effort allocated to design i up to time n."
    }, {
      "heading" : "3 Algorithms",
      "text" : "This section proposes three algorithms for allocating measurement effort. Each depends on a tuning parameter β > 0, which will sometimes be set to a default value of 1/2. Each algorithm is based on the same high level principle. At every time step, each algorithm computes an estimate Î ∈ {1, ..., k} of the optimal design, and measures that with probability β. Otherwise, we consider a counterfactual: in the (possibly unlikely) event that Î is not the best design, which alternative Ĵ 6= Î is most likely to be the best design? With probability 1 − β, the algorithm measures the alternative Ĵ . The algorithms differ in how they compute Î and Ĵ . The most computationally efficient is the modified version of Thompson sampling, under which Î and and Ĵ are themselves randomly sampled from a probability distribution.\nWe will see that asymptotically all three algorithms allocate faction β of measurement effort to measuring the estimated-best design, and the remaining fraction to gathering evidence about alternatives. The algorithms adjust how measurement effort is divided among these alternative designs as evidence is gathered, allocating less effort to measuring clearly inferior designs and greater effort to measuring designs that are more difficult to distinguish from the best."
    }, {
      "heading" : "3.1 Top-Two Probability Sampling (TTPS)",
      "text" : "With probability β, the top-two probability sampling (TTPS) policy plays the action În = arg maxi αn,i which, under the posterior, is most likely to be optimal. When the algorithm does not play În, it plays the most likely alternative Ĵn = arg maxj 6=În αn,j , which is the action that is second most likely to be optimal under the posterior. Put differently, the algorithm sets ψn,În = β, and ψn,Ĵn = 1−β."
    }, {
      "heading" : "3.2 Top-Two Value Sampling (TTVS)",
      "text" : "We now propose a variant of top-two sampling that considers not only the probability a design is optimal, but the expected amount by which its quality exceeds that of other designs. In particular, we will define below a measure Vn,i of the value of design i under the posterior distribution at time n. Top-two value sampling computes the top-two designs under this measure: În = arg maxi Vn,i and Ĵn = arg maxj 6=În Vn,j . It then plays the top design În with probability β and the best alternative Ĵn otherwise. As observations are gathered, beliefs are updated and so the top two designs change over time. The measure of value Vn,i is defined below.\nThe definition of TTVS depends on a choice of (utility) function u : θ 7→ R, which encodes a measure of the value of discovering a design with quality θi. Two natural choices of u are u(θ) = θ and u(θ) = µ(θ). The paper’s theoretical results allow u to be a general function, but we assume that it is continuous and strictly increasing. For a given choice of u, and any i ∈ {1, ..., k}, the function\nvi(θ) = max j u(θj)−max j 6=i u(θj) = { 0 if θ /∈ Θi u(θi)−maxj 6=i u(θj) if θ ∈ Θi\nprovides a measure of the value of design i when the true parameter is θ. It captures the improvement in decision quality due to design i’s inclusion in the choice set. Let\nVn,i = ˆ\nΘ\nvi(θ)πn(θ)dθ = ˆ\nΘi\nvi(θ)πn(θ)dθ (4)\ndenote the expected value of vi(θ) under the posterior distribution at time n. This can be viewed as the option-value of design i: it is the expected additional value of having the option to choose design i when it is revealed to be the best design. Note that the integral (4) defining Vn,i is a weighted version of the integral defining αn,i. The paper will formalize a sense in which Vn,i and αn,i are asymptotically equivalent as n → ∞, and as a result the asymptotic analysis of top-two value sampling essentially reduces to the analysis of top-two probability sampling."
    }, {
      "heading" : "3.3 Thompson Sampling",
      "text" : "Thompson sampling is an old and popular heuristic for multi-armed problems. The algorithm simply samples actions according to the posterior probability they are optimal. In particular, it selects action i with probability ψn,i = αn,i, where αn,i denotes the probability action i is optimal under under a parameter drawn from the posterior distribution.\nThompson sampling can have very poor asymptotic performance for the best arm identification problem. Intuitively, this is because once it estimates that a particular arm is the best with reasonably high probability, it selects that arm in almost all periods at the expense of refining its knowledge of other arms. If αn,i = .95, then the algorithm will only select an action other than i roughly once every 20 periods, greatly extending the time it takes until αn,i > .99. This insight can be made formal; our results imply that Thompson sampling attains a only attains a polynomial, rather exponential, rate of posterior convergence. A similar reasoning applies to other multi-armed bandit algorithms. The work of Bubeck et al. [2009] shows formally that algorithms satisfying regret bounds of order log(n) are necessarily far from optimal for the problem of identifying the best arm.\nWith this in mind, it is natural to consider a modification of Thompson sampling that simply restricts the algorithm from sampling the same action too frequently. One version of this idea is proposed below."
    }, {
      "heading" : "3.4 Top-Two Thompson Sampling (TTTS)",
      "text" : "This section proposes top-two Thompson sampling (TTTS), which modifies standard Thompson sampling by adding a re-sampling step. As with TTPS and TTVS, this algorithm depends on a tuning parameter β > 0 that will sometimes be set to a default value of 1/2.\nAs in Thompson sampling, at time n, the algorithm samples a design I ∼ αn. Design I is measured with probability β, but, in order to prevent the algorithm from exclusively focusing on one action, with probability 1− β, an alternative design is measured. To generate this alternative, the algorithm continues sampling designs J ∼ αn until the first time J 6= I. This can be viewed as a top-two sampling algorithm, where the top-two are chosen by executing Thompson sampling until two distinct designs are drawn.\nUnder top-two Thompson sampling, the probability of measuring design i at time n is\nψn,i = αn,i β + (1− β)∑ j 6=i αn,j 1− αn,j  . This expression simplifies as the algorithm definitively identifies the best design. As αn,I∗ → 1, ψn,I∗ → β, and for each i 6= I∗,\nψn,i 1− ψn,I∗ ∼ αn,i1− αn,I∗ .\nIn this limit, the true best design is sampled with probability β. The probability i is sampled given I∗ is not is equal to the posterior probability i is optimal given I∗ is not."
    }, {
      "heading" : "3.5 Computing and Sampling According to Optimal Action Probabilities",
      "text" : "Here we provide some insight into how to efficiently implement the proposed top-two rules in important problem classes. We begin with top-two Thompson sampling, which is often the easiest to implement. Note that given an ability to sample from Πn, it is easy to sample from the posterior distribution over the optimal design αn. In particular, if θ̂ ∼ Πn is drawn randomly from the posterior, then arg maxi θ̂i is a random sample from αn. Either through the choice of conjugate prior distributions, or through the use of Markov chain Monte Carlo, it is possible to efficiently sample from the posterior for many interesting models. Algorithm 1 shows how to directly sample an action according to TTTS by sampling from the posterior distribution. It is worth highlighting that this algorithm does not require computing or approximating the distribution αn.\nAlgorithm 1 Top-Two Thompson Sampling (β)\n1: Sample θ̂ ∼ Πn and set I ← arg maxi θ̂i . Apply Thompson sampling 2: Sample B ∼ Bernoulli(β) 3: if B = 1 then . Occurs with probability β. 4: Play I 5: else 6: repeat 7: Sample θ̂ ∼ Πn and set J ← arg maxj θ̂j . Repeat Thompson sampling 8: until J 6= I 9: Play J\n10: end if\nThe optimal action probabilities αn,i and values Vn,i are defined by k-dimensional integrals, which may be difficult to compute in general even if the posterior Πn has a closed form. Algorithm 2 shows how to approximate αn,i and Vn,i using samples θ1 . . .θM , which enables efficient approximations to TTPS and TTVS whenever posterior samples can be efficiently generated.\nAlgorithm 2 SampleApprox(K,M, u,θ1, . . . ,θM ) 1: Si ← {m|i = arg maxj θmj } ∀i ∈ {1, ..,K} 2: α̂i ← |Si|/m ∀i ∈ {1, ..K} 3: V̂i ←M−1 ∑ m∈Si ( u(θmi )−maxj 6=i u(θmj ) ) ∀i ∈ {1, ..,K}\n4: return α̂, V̂\nThankfully, the computation of αn,i and Vn,i simplifies when the algorithm begins with an independent prior over the qualities θ1, ...θk of the k designs. To understand this fact, suppose X1, ..., Xk ∈ R are independently distributed and continuous random variables. Then\nP(X1 = max i Xi) =\nˆ\nx∈R\nf1(x) k∏ j=2 Fj(x)dx (5)\nwhere f1 denotes the PDF of X1 and F2, ..., FK are the CDFs of X2, .., Xk. In particular, P(X1 = maxiXi) can be computed by solving a 1-dimensional integral. Based on this insight, Appendix B provides an efficient implementation of TTPS and TTVS for a problem with independent Beta priors and binary observations. That implementation approximates integrals like (5) using quadrature with n points, and has the time and space complexity that scale as O(kn)."
    }, {
      "heading" : "4 A Numerical Experiment",
      "text" : "Some of the paper’s main insights are reflected in a simple numerical experiment. Consider a problem where observations are binary Yn,i ∈ {0, 1}, and the unknown vector θ∗ = (.1, .2, .3, .4, .5) defines the true success probability of each design. Each algorithm begins with an independent uniform prior over the components of θ∗. The experiment compares the performance of top-two probability sampling (TTPS), top-two value sampling (TTVS)2, and top-two Thompson sampling (TTTS) with β = 1/2 against Thompson sampling and a uniform allocation rule which allocates equal measurement effort (ψn,i = 1/5) to each design. The uniform allocation is an especially natural benchmark, as it is the most commonly used strategy in practice.\nFigure 1 displays the average number of measurements required for the posterior to reach a given confidence level. In particular, the experiment tracks the first time when maxi αn,i ≥ c for various confidence levels c ∈ (0, 1). Figure 1 displays the average number of measurements required for each algorithm to reach each fixed confidence level, where the average was taken over 100 trials in Panel (a) and 500 in Panel (b). Even for this simple problem with five designs, the proposed algorithms can reach the same confidence level using fewer than half the measurements required by a uniform allocation rule. While all the top-two rules attain the same asymptotic rate of convergence, we can see that top-two probability sampling is slightly outperformed in this experiment. Panel (a) compares Thompson sampling to Top-Two Thompson sampling. TS appears to reach low confidence levels as rapidly as top-two TS, but as suggested in Subsection 3.3, is very slow to reach high levels of confidence. It requires over than 60% more measurements to reach\n2TTVS is executed with the utility function u(θ) = θ\nconfidence .95 and over 250% more measurements to reach confidence .99. TS requires an onerous number of measurements to reach confidence .999, and so we omit this experiment.\nFigure 2 provides insight into how the proposed algorithms differ from the uniform allocation. It displays the distribution of measurements and posterior beliefs at the first time when a confidence level of .999 is reached. Again, all results are averaged across 500 trials. Panel (a) displays the average number of measurements collected from each design. It is striking that although TTTS, TTPS, and TTVS seem quite different, they all settle on essentially the same distribution of measurement effort. Because β = 1/2, roughly one half of the measurements are collected from I∗ = 5. Moreover, fewer measurements are collected from designs that are farther from optimal, and most of the remaining half of measurement effort is allocated to design 4. Notice that using the same number of noisy samples it is much more difficult certify that θ∗4 < θ∗5 than that θ∗1 < θ∗5, both because θ∗4 is closer to θ∗5, and because observations from a Bernoulli distribution with parameter .4 have higher variance than under a Bernoulli distribution with parameter .1.\nPanel (b) investigates the posterior probability αn,i assigned to the event that design i is optimal. To make the insights more transparent, these are plotted on log-scale, where the value log(1/αn,i) can roughly be interpreted as the magnitude of evidence that alternative i is not optimal. By using an equal allocation of measurement effort across the designs, the uniform sampling rule gathers an enormous amount of evidence to rule out design 1, but an order of magnitude less evidence to rule out design 4. Instead of allocating measurement effort equally across the alternatives, TTTS, TTPS, and TTVS appear to exactly adjust measurement effort to gather equal evidence that each of the first four designs is not optimal.\nIntuitively, in the long run each of the proposed algorithms will allocate measurement effort to design 5–the true best design–and to whichever other designs could most plausibly be optimal. If too much measurement effort has been allocated to a particular design, then the posterior will indicate that it is clearly suboptimal, and effort will be allocated elsewhere until a similar amount of evidence has been gathered about other designs. In this way, measurement effort is automatically adjusted to the appropriate level."
    }, {
      "heading" : "5 Main Theoretical Results",
      "text" : "Our main theoretical results concern the frequentist consistency and rate of convergence of the posterior distribution. Recall that\nΠn(ΘcI∗) = ∑ i 6=I∗ αn,i\ncaptures the posterior mass assigned to the event that an action other than I∗ is optimal. One hopes that Πn(ΘcI∗) → 0 as the number of observations n tends to infinity, so that the posterior distribution converges on the truth. We will show that under the TTTS, TTPS, and TTVS allocation rules, Πn(ΘcI∗) converges to zero an exponential rate and that the exponent governing the rate of convergence is nearly the best possible.\nTo facilitate theoretical analysis, we will make three additional boundedness assumptions, which are assumed throughout all formal proofs. This rules out some cases of interest, such the use of multivariate Gaussian prior. However, we otherwise allow for quite general correlated priors, expressed in terms of a density over a compact set. Assumption 1 is used only in establishing posterior concentration results, and it is likely that these can be established under less restrictive technical conditions.\nAssumption 1. The parameter space is a bounded open hyper-rectangle Θ = (θ, θ)k, the prior density is uniformly bounded with\n0 < inf θ∈Θ π1(θ) < sup θ∈Θ π1(θ) <∞,\nand the log-partition function has bounded first derivative with supθ∈[θ,θ] |A ′(θ)| <∞.\nThe paper’s main results, as stated in the next theorem, characterize the rate of posterior convergence under the proposed algorithms, formalize a sense in which this is the fastest possible rate, and bound the impact of the tuning parameter β ∈ (0, 1). The statement depends on distribution-dependent constants Γ∗β > 0 and Γ∗ > 0 that will be explicitly characterized in Section 6.\nThe first part of the theorem shows that there is an exponent Γ∗ > 0 such that Πn(ΘcI∗) cannot converge to zero at a rate faster than e−nΓ∗ under any allocation rule, and shows that TTPS, TTVS and TTTS attain this optimal rate of convergence when the tuning parameter β is set optimally.\nThe remainder of the theorem investigates the role of the tuning parameter β ∈ (0, 1). Part 2 shows that there is an exponent Γ∗β > 0 such that Πn(ΘcI∗)→ 0 at rate e\n−nΓ∗β under TTPS, TTVS, or TTTS with parameter β, and this is shown to be optimal among a restricted class of allocation rules. In particular, we observe that β controls the fraction of measurement effort allocated to the true best design I∗, in the sense that ψn,I∗ → β as n→∞ under each of the proposed algorithms. A lower bound establishes that no algorithm that allocates a faction β of overall effort to measuring I∗ can converge at rate faster than e−nΓ ∗ β . In this sense, while a tuning parameter controls the long-run measurement effort allocated to the true best design, TTPS, TTVS, and TTTS all automatically adjust how the remaining the measurement effort is allocated among the k− 1 suboptimal designs in an asymptotically optimal manner. The final part of the theorem shows that Γ∗β is close to the largest possible exponent Γ∗ whenever β is close to the optimal value. The choice of β = 1/2 is particularly robust: Γ∗1/2 is never more than a factor of 2 away from the optimal exponent.\nTheorem 1. There exist constants {Γ∗β > 0 : β ∈ (0, 1)} such that Γ∗ = maxβ Γ∗β exists, β∗ = arg maxβ Γ∗β is unique, and the following properties are satisfied with probability 1:\n1. Under TTTS, TTPS, or TTVS with parameter β∗,\nlim n→∞ − 1 n log Πn(ΘcI∗) = Γ∗.\nUnder any adaptive allocation rule,\nlim sup n→∞ − 1 n log Πn(ΘcI∗) ≤ Γ∗.\n2. Under TTTS, TTPS, or TTVS with parameter β ∈ (0, 1),\nlim n→∞ − 1 n log Πn(ΘcI∗) = Γ∗β and limn→∞ψn,I∗ = β.\nUnder any adaptive allocation rule,\nlim sup n→∞ − 1 n log Πn(ΘcI∗) ≤ Γ∗β on any sample path with limn→∞ψn,I∗ = β.\n3. Γ∗ ≤ 2Γ∗1 2 and\nΓ∗ Γ∗β ≤ max\n{ β∗\nβ , 1− β∗ 1− β\n} .\nThis theorem is established in a sequence of results in Section 6. The lower bounds in parts 1 and 2 are given respectively in Propositions 5 and 6. Proposition 7 shows the top-two rules attain these optimal exponents. Part 3 is stated as Lemma 2 in Section 6."
    }, {
      "heading" : "5.1 An upper bound on the error exponent",
      "text" : "Before proceeding, we will state an upper bound on the error exponent when β = 1/2 that is closely related to complexity terms that have appeared in the literature on best–arm identification (e.g. Audibert and Bubeck [2010]). This bound depends on the gaps between the means of the different observation distributions.\nWe say that a real valued random variable X is σ–sub–Gaussian if E [exp{λ(X −E[X])}] ≤ exp { λ2σ2\n2\n} so that the moment generating function of X − E[X] is dominated by that of a zero\nmean Gaussian random variable with variance σ2. Gaussian random variables are sub-Gaussian, as are uniformly bounded random variables. The next result applies to both Bernoulli and Gaussian distributions, as each can be parameterized with sufficient statistic T (y) = y.\nProposition 1. Suppose the exponential family distribution is parameterized with T (y) = y and that each θ ∈ [θ, θ], if Y ∼ p(y|θ), then Y is sub-Gaussian with parameter σ. Then\nΓ∗1 2 ≥ 1 16σ2 ∑ i 6=I∗ ∆−2i\nwhere for each i ∈ {1, ..., k}, ∆i = E[Yn,I∗ ]−E[Yn,i]\nis the difference between the mean under θ∗I∗ and the mean under θ∗i .\nThis shows that Πn(ΘcI∗) decays at asymptotic rate faster than exp{− nmini ∆2i\n16kσ2 }, so convergence is rapid when there is a large gap between the means of different designs. In fact, Proposition 1 replaces the dependence on (1/k) times the smallest gap ∆i with a dependence on (∑k i=2 ∆−2i )−1 , which captures the average inverse gap. This rate is attained only by an intelligent adaptive algorithm which allocates more measurement effort to designs that are nearly optimal and less to designs that are clearly suboptimal. In fact, the next result shows that the asymptotic performance of uniform allocation rule depends only on the smallest gap mini 6=I∗ ∆2i , and therefore even if some designs could be quickly ruled out, the algorithm can’t leverage this to attain a faster rate of convergence.\nProposition 2. If Yn,I∗ ∼ N (0, σ2) and Yn,i ∼ N (−∆i, σ2) for each i 6= I∗,\nlim n→∞ − 1 n log Πn(ΘcI∗) = −nmini ∆2i 4kσ2\nunder a uniform allocation rule which sets ψn,i = 1/k for each i and n."
    }, {
      "heading" : "6 Analysis",
      "text" : ""
    }, {
      "heading" : "6.1 Asymptotic Notation.",
      "text" : "To simplify the presentation, it is helpful to introduce additional asymptotic notation. We say two sequences an and bn taking values in R are logarithmically equivalent, denoted by an\n.= bn, if 1 n log( an bn\n) → 0 as n → ∞. This notation means that an and bn are equal up to first order in the exponent. With this notation, Theorem 1 implies the top-two sampling rules with parameter β attain the convergence rate Πn(ΘcI∗) .= e−nΓ ∗ β . This is an equivalence relation, in the sense that if an .= bn and bn .= cn then an .= cn. Note that an + bn .= max{an, bn}, so that the sequence with\nthe largest exponent dominates. In addition for any positive constant c, can .= an, so that constant multiples of sequences are equal up to first order in the exponent. When applied to sequences of random variables, these relations are understood to apply almost surely.\nIt is natural to wonder whether the proposed algorithms asymptotically minimize expressions like ∑ i 6=I∗(θ∗I∗ − θi)αn,i, which account for how far some designs are from optimal. We note in\npassing, that ∑ i 6=I∗ ciαn,i .= max i 6=I∗ αn,i\nfor any positive costs ci > 0, and so any such performance measures are equal to first order in the exponent. Similar observations have been used to justify the study of the probability of incorrect selection, rather than notions of the expected cost of an incorrect decision [Glynn and Juneja, 2004, Audibert and Bubeck, 2010]."
    }, {
      "heading" : "6.2 Posterior Consistency",
      "text" : "The next proposition provides a consistency and anti-consistency result for the posterior distribution. The first part says that if design i receives infinite measurement effort, the marginal posterior distribution of its quality concentrates around the true value θ∗i . The second part says that when restricted to designs that are not measured infinitely often, the posterior does not concentrate around any value. The posterior converges to the truth as infinite evidence is collected, but nothing can be ruled out with certainty based on finite evidence.\nProposition 3. With probability 1, for any i ∈ {1, .., k} if Ψn,i →∞, then, for all > 0\nΠn({θ ∈ Θ|θi /∈ (θ∗i − , θ∗i + )})→ 0.\nIf I = {i ∈ {1, ..., k}| limn→∞Ψn,i <∞} is nonempty, then\ninf n∈N Πn({θ ∈ Θ|θi ∈ (θ′i, θ′′i ) ∀i ∈ I}) > 0\nfor any collections of open intervals (θ′i, θ′′i ) ⊂ (θ, θ) ranging over i ∈ I.\nThis result is the key to establishing that αn,I∗ → 1 under each of the proposed algorithm. The next subsection gives a more refined result that allows us to to characterize the rate of convergence."
    }, {
      "heading" : "6.3 Posterior Large Deviations",
      "text" : "This section provides an asymptotic characterization of posterior probabilities Πn(Θ̃) for any open set Θ̃ ⊂ Θ and under any adaptive measurement strategy. The characterization depends on the notion of Kullback-Leibler divergence. For two parameters θ, θ′ ∈ R, the log-likelihood ratio, log (p(y|θ)/p(y|θ′)), provides a measure of the amount of information y provides in favor of θ over θ′. The Kullback-Leibler divergence\nd(θ||θ′) , ˆ log ( p(y|θ) p(y|θ′) ) p(y|θ)dν(y).\nis the expected value of the log-likelihood under observations drawn p(y|θ). Then, if the design to measure is chosen by sampling from a probability distribution ψ over {1, .., k},\nDψ(θ||θ′) , k∑ i=1 ψid(θi||θ′i)\nis the average Kullback-Leibler divergence between θ and θ′ under ψ. Under the algorithms we consider, the effort allocated to measuring design i, ψn,i , P(In =\ni|Fn−1), changes over time as data is collected. Recall that ψn,i , n−1 ∑n `=1 ψ`,i captures the fraction of overall effort allocated to measuring design i over the first n periods. Under an adaptive allocation rule, ψn is function of the history (I1, Y1,I1 , ...In−1, Yn−1,In−1) and is therefore a random variable. Given that measurement effort has been allocation according to ψn, Dψn(θ\n∗||θ) quantifies the average information acquired that distinguishes θ from the true parameter θ∗. The following proposition relates the posterior mass assigned to Θ̃ to infθ∈Θ̃Dψn(θ\n∗||θ), which captures the element in Θ̃ that is hardest to distinguish from θ∗ based on samples from ψn.\nProposition 4. For any open set Θ̃ ⊂ Θ,\nΠn(Θ̃) .= exp { −n inf\nθ∈Θ̃ Dψn\n(θ∗||θ) } .\nTo understand this result, consider a simpler setting where the algorithm measures design i in every period, and consider some θ with θi 6= θ∗i . Then the log-ratio of posteriors densities\nlog ( πn(θ) πn(θ∗) ) = log ( π1(θ) π1(θ∗) ) + n−1∑ `=1 log ( p(Y`,i|θi) p(Y`,i|θ∗i ) )\ncan be written as the sum of the log-prior-ratio and the log-likelihood-ratio. The log-likelihood ratio is negative drift random walk: it is the sum of n− 1 i.i.d terms, each of which has mean\nE [ log ( p(Y1,i|θi) p(Y1,i|θ∗i ) )] = E [ − log ( p(Y1,i|θ∗i ) p(Y1,i|θi) )] = −d(θ∗i ||θi).\nTherefore, by the law of large numbers, as n→∞, n−1 log (πn(θ)/πn(θ∗))→ −d(θ∗i ||θi), or equivalently, the ratio of the posterior densities decays exponentially as\nπn(θ) πn(θ∗) .= exp{−nd(θ∗i ||θi}.\nThis calculation can be carried further to show that if the designs measured (I1, I2, I3, ...) are drawn independently of the observations (Y1,Y2,Y3, ...) from a fixed probability distribution ψ, then\nπn(θ) πn(θ∗) .= exp {−nDψ(θ∗||θ)} . (6)\nNow, by a Laplace approximation, one might expect that the integral ´\nΘ̃ πn(θ)dθ is extremely well approximated by integrating around a vanishingly small ball around the point\nθ̂ = arg min θ∈Θ̃\nDψ(θ∗||θ).\nThese are the main ideas behind Proposition 4, but there are several additional technical challenges involved in a rigorous proof. First, we need that a property like (6) holds when the allocation rule is adaptive to the data. Next, convergence of the integral of the posterior density requires a form of uniform convergence in (6). Finally, since ψn changes over time, the point arg min\nθ∈Θ̃ Dψn\n(θ∗||θ)\nchanges over time and basic Laplace approximations don’t directly apply."
    }, {
      "heading" : "6.4 Characterizing the Optimal Allocation",
      "text" : "Throughout this paper, an experimenter wants to gather enough evidence to certify that I∗ is optimal, but since she does not know θ∗, she does not know which measurements will provide the most information. To characterize the optimal exponent Γ∗, however, it is useful to consider the easier problem of gathering the most effective evidence when θ∗ is known. We can cast this as a game between two players:\n• An experimenter, who knows the true parameter θ∗, chooses a (possibly adaptive) measurement rule.\n• A referee observes the resulting sequence of observations (I1, Y1,I1 , ..., In−1, Yn−1,In−1) and computes posterior beliefs (αn,1, .., αn,k) according to Bayes rule (2, 3).\n• How can the experimenter gather the most compelling evidence? A rule which is optimal asymptotically should maximize the rate at which αn,I∗ → 1 as n→∞.\nIn order to drive the posterior probability αn,I∗ to 1, the decision-maker must be able to rule out all parameters in ΘcI∗ under which the optimal action is not I∗. Our analysis shows that the posterior probability assigned to ΘcI∗ is dominated by the parameter that is hardest to distinguish from θ∗ under ψn. In particular, by Proposition 4,\nΠn(ΘcI∗) .= exp { −n ( min θ∈Θc\nI∗ Dψn\n(θ∗||θ) )}\nas n→∞. Therefore, the solution to the max-min problem\nmax ψ min θ∈Θc I∗ Dψ(θ∗||θ) (7)\nrepresents an asymptotically optimal allocation rule. As highlighted in the literature review, the max-min problem (7) closely mirrors the main sample complexity term in Chernoff’s classic paper on the sequential design of experiments (Chernoff [1959]).\nSimplifying the optimal exponent. Thankfully, the best-arm identification problem has additional structure which allows us to simplify the optimization problem (7). Much of our analysis involves the posterior probability probability assigned to the event some action i 6= I∗ is optimal. This can be difficult to evaluate, since the set of parameter vectors under which i is optimal\nΘi = {θ ∈ Θ|θi ≥ θ1, ...θi ≥ θk}\ninvolves k separate constraints. Consider instead a simpler problem of comparing the parameter θ∗i against θ∗I∗ . For each i 6= I∗ define the set\nΘi , {θ ∈ Θ|θi ≥ θI∗} ⊃ Θi\nunder which the value at i exceeds that at I∗. Since, ignoring the boundary of the set, ΘcI∗ = ∪i 6=I∗Θi,\nmax i 6=I∗ Πn(Θi) ≤ Πn(ΘcI∗) ≤ kmax i 6=I∗ Πn(Θi)\nand therefore Πn(ΘcI∗)\n.= max i 6=I∗ Πn(Θi). (8)\nThis yields an analogue of (7) that will simplify our subsequent analysis. Combining (8) with Proposition 4 shows the solution to the max-min problem\nΓ∗ , max ψ min i 6=I∗ min θ∈Θi Dψ(θ∗||θ) (9)\nrepresents an asymptotically optimal allocation rule. Because the set Θi involves only a constraints on θi and θI∗ , we can derive an expression the inner minimization problem over θ in terms of the measurement effort allocated to i and I∗. Define\nCi(β, ψ) , min x∈R βd(θ∗I∗ ||x) + ψd(θ∗i∗ ||x). (10)\nThe next lemma shows that the function Ci arises as the solution to the minimization problem over θ ∈ Θi in (9). It also shows that the minimum in (10) is attained by a parameter θ under which the mean observation is a weighted combination of the means under θ∗I∗ and θ∗i . Recall that, for an exponential family distribution A′(θ) = ´ T (y)p(y|θ)dν(y) is the mean observation of the sufficient statistic T (y) under θ.\nLemma 1. For any i ∈ {1, .., k} and probability distribution ψ over {1, ..., k}\nmin θ∈Θi\nDψ(θ∗||θ) = Ci(ψI∗ , ψi)"
    }, {
      "heading" : "In addition, each Ci is a strictly increasing concave function satisfying",
      "text" : "Ci(ψI∗ , ψi) = ψI∗d(θ∗I∗ ||θ) + ψid(θ∗i ||θ),\nwhere θ ∈ [θ∗i , θ∗I∗ ] is the unique solution to\nA′(θ) = ψI ∗A′(θ∗I∗) + ψiA′(θ∗i )\nψI∗ + ψi .\nLemma 1 and equation (9) immediately imply\nΓ∗ = max ψ min i 6=I∗ Ci(ψI∗ , ψi). (11)\nThe function Ci(β, ψ) captures the effectiveness with which one can certify θ∗I∗ ≥ θ∗i using an allocation rule that measures actions I∗ and i with respective frequencies β and ψ. Naturally, it is an increasing function of the measurement effort (β, ψ) allocated to designs I∗ and i. For given β and ψ, Ci(β, ψ) ≥ Cj(β, ψ) when θ∗i ≤ θ∗j , reflecting that θ∗i is easier to distinguish from θ∗I∗ than θ∗j .\nExample 1. (Gaussian Observations) Suppose each outcome distribution p(y|θ∗i ) is Gaussian with unknown mean θ∗i . Then direct calculation using Lemma 1 shows\nCi(β, ψi) = (\nβψi β + ψi ) (θ∗I∗ − θ∗i )2 2 .\nTo understand this formula, imagine we use a deterministic allocation rule that collects nβ and nψi observations from I∗ and i. Let XI∗ and Xi denote the respective sample means. The empirical difference is normally distributed XI∗ − Xi ∼ N ( ∆, σ2/n ) where ∆ = θ∗I∗ − θ∗i and σ2 = 1/β + 1/ψi = (β + ψi)/(βψi). Standard Gaussian tail bounds imply that as n → ∞, P(XI∗ −Xi < 0) .= exp(−n/2(σ∆)2), and so Ci(β, ψi) appears to characterize the probability of error.\nThe next proposition formalizes the derivations in this section, and states that the solution to the above maximization problem attains the optimal error exponent. Recall that ψn,i , P(In = i|Fn−1) denotes the measurement effort assigned design i at time n.\nProposition 5. Let ψ∗ denote the optimal solution to the maximization problem (11). If ψn = ψ∗ for all n, then\nΠn(ΘcI∗) .= exp{−nΓ∗}.\nMoreover under any other adaptive allocation rule,\nlim sup n→∞ − 1 n log Πn(ΘcI∗) ≤ Γ∗.\nThis shows that under the fixed allocation rule ψ∗ error decays as e−nΓ∗ , and that no faster rate of decay is possible, even under an adaptive allocation.\nAn Optimal Constrained Allocation. Because the algorithms studied in this paper always allocate β–fraction of their samples to measuring I∗ in the long run, they may not exactly attain the optimal error exponent. To make rigorous claims about their performance, consider a modified version of the error exponent (11) given by the constrained max-min problem\nΓ∗β , max ψ:ψI∗=β min i 6=I∗ Ci(β, ψi). (12)\nThis optimization problem yields the optimal allocation subject to a constraint that β–fraction of the samples are spent on I∗. The next subsection will show that TTTS, TTPS, and TTVS attain the error exponent Γ∗β. The next proposition formalizes that the solution to this optimization problem represents an optimal constrained allocation. In addition, it shows that the solution is the unique feasible allocation under which Ci(β, ψi) is equal for all suboptimal designs i 6= I∗. To understand this result, consider the case where there are three designs and θ∗1 > θ∗2 > θ∗3. If ψ2 = ψ3, then C2(β, ψ2) < C3(β, ψ3), reflecting that it is more difficult to certify that θ∗2 ≤ θ∗I∗ than θ∗3 ≤ θ∗I∗ . The next proposition shows it is optimal to decrease ψ2 and increase ψ1, until the point when C2(β, ψ2) = C3(β, ψ3). Instead of allocating equal measurement effort to each alternative, it is optimal to adjust measurement effort to gather equal evidence to rule out each suboptimal alternative. The results in this proposition are closely related to those in Glynn and Juneja [2004], in which large deviations rate functions take the place of the functions Ci.\nProposition 6. The solution to the optimization problem (12) is the unique allocation ψ∗ satisfying ψ∗I∗ = β and\nCi(β, ψi) = Cj(β, ψj) ∀ i, j 6= I∗.\nIf ψn = ψ∗ for all n, then Πn(ΘcI∗) .= exp{−nΓ∗β}.\nMoreover under any other adaptive allocation rule, if ψn,I∗ → β then\nlim sup n→∞ − 1 n log Πn(ΘcI∗) ≤ Γ∗β\nalmost surely.\nThe following lemma relates the constrained exponent Γ∗β to Γ∗.\nLemma 2. For β∗ = arg maxβ Γ∗β and any β ∈ (0, 1),\nΓ∗ Γ∗β ≤ max\n{ β∗\nβ , 1− β∗ 1− β\n} .\nTherefore Γ∗ ≤ 2Γ∗1/2."
    }, {
      "heading" : "6.5 Optimal Adaptive Allocation",
      "text" : "Two Stage Procedures. While the last subsection describes an asymptotically optimal exploration strategy, implementing this strategy requires knowledge of the parameter vector θ∗. One simple approach to attaining the rate (11) is to split the experiment into two phases. For the first o(n) periods the algorithm selects actions uniformly at random, after which it constructs a point estimate θ̂ of θ∗. In the second phase, it solves the optimization problem (11) with θ̂ in place of of θ∗, and follows that allocation for the remaining periods. In the design of sequential experiments, this idea dates back to Kiefer and Sacks [1963], who builds on the work of Chernoff [1959]. For the problem of best arm identification, it dates back at least to Jennison et al. [1982].\nThese two-stage rules can be shown to attain the optimal large deviations rates described in the previous section. But they also have substantial practical limitations, which were discussed explicitly in early papers. Jennison et al. [1982] writes their proposed procedures “typically...do not have good small sample size properties. A better procedure would have several stages and a more sophisticated sampling rule.” In a 1975 review of the sequential design of experiments, Chernoff [1975] notes that asymptotic approaches to the optimal sequential design of experiments had been fairly successful in circumventing the need to compute Bayesian optimal designs via dynamic programming, but “the approach is very coarse for moderate sample size problems.” He writes that two-stage procedures of Kiefer and Sacks [1963], “sidestep the issue of how to experiment in the early stages,” while constructing the optimal allocations based on point estimates “treats estimates of θ based on a few observations with as much respect as that based on many observations.”\nConvergence of Top-Two Algorithms. Instead of attempting to directly solve the optimization problem (11), this paper focuses on simple and intuitive sequential strategies. These algorithms have the potential to explore much more intelligently in early stages, as they carefully measure and reason about uncertainty. While they ostensibly have no connection to the derivations earlier in this section, we establish that remarkably all three automatically converge to the unknown optimal allocation. This is shown formally in the next result.\nWe are now ready to establish the paper’s main claim, which shows that TTTS, TTPS, and TTVS each attain the error exponent Γ∗β.\nProposition 7. Under the TTTS, TTPS, or TTVS algorithm with parameter β > 0, ψn → ψβ, where ψβ is the unique allocation with ψβI∗ = β satisfying\nCi(β, ψβi ) = Cj(β, ψ β j ) ∀i, j 6= I ∗.\nTherefore, Πn(ΘcI∗) .= e−nΓ ∗ β .\nTo understand this result, imagine that n is very large, and ψn,I∗ ≈ β. If the algorithm has allocated too much measurement effort to a suboptimal action i, with ψn,i > ψ β i + δ for a constant δ > 0, then it must have allocated too little measurement effort to at least one other suboptimal\ndesign j 6= i. Since much less evidence has been gathered about j than i, we expect αj,n >> αj,i. When this occurs, TTTS, TTPS and TTVS essentially never sample action i until the average effort ψn,i allocated to design i dips back down toward ψ β i . This seems to suggest that the algorithm cannot allocate too much effort to any alternative, but that in turn implies that it never allocates too little effort to measuring any alternative."
    }, {
      "heading" : "6.6 Asymptotics of the Value Measure",
      "text" : "The proof for top-two value sampling relies on the following lemma, which shows that the posteriorvalue of any suboptimal design is logarithmically equivalent to its probability of being optimal.\nLemma 3. For any i 6= I∗, Vn,i .= αn,i\nNote that by this lemma, Πn(ΘcI∗) = ∑ i 6=I∗ αn,i .= ∑ i 6=I∗ Vn,i,\nand so all of the asymptotic results in this could be reformulated as statements concerning the value assigned to suboptimal alternatives under the posterior.\nThe lemma is not so surprising, as Vn,i = ´ Θi vi(θ)πn(θ)dθ differs from αn,i = ´\nΘi πn(θ)dθ only because of the function vi(θ). The πn(θ) term dominates this integral as n→∞, since it tends to zero at an exponential rate in n whereas vi(θ) is a fixed function of n."
    }, {
      "heading" : "7 Extensions and Open Problems",
      "text" : "This paper studies efficient adaptive allocation of measurement effort for identifying the best among a finite set of options or designs. We propose three simple Bayesian algorithms. Each is a variant of what we call top-two sampling, which, at each time-step, measures one of the two designs that appear most promising given current evidence. Surprisingly, these seemingly naive algorithms are shown to satisfy a strong asymptotic optimality property.\nTop two sampling appears to be a general design principle that can be extended to address a variety of problems beyond to the scope of this paper. To spur research in this area, we briefly discuss a number of extensions and open questions below.\nTop-Two Sampling Via Constrained MAP Estimation. Here we present a version of toptwo sampling that uses MAP estimation. This can simplify computations, as MAP estimates can be computed without solving for the normalizing constant of the posterior density πn(θ). Consider the following procedure for selecting a design at time n:\n1. Compute θ̂ ∈ arg maxθ∈Θ πn(θ) and set În = arg maxi θ̂i.\n2. Compute θ̂′ ∈ arg maxθ∈Θc În πn(θ) and set Ĵn = arg maxi θ̂′i.\n3. Play (În, Ĵn) with respective probabilities (β, 1− β).\nThe first step uses MAP estimation to make a prediction În of the best design, while the second uses constrained MAP estimation to identify the alternative design that is most likely to be optimal when În is not. Many of the asymptotic calculations in the previous section appear to extend to this algorithm, but proving this formally is left as an open problem.\nIndifference Zone Criterion. Suppose our goal is to confidently identify an –optimal arm, for a user specified indifference parameter > 0. Much of the paper investigates the set of parameters Θi under which arm i is optimal, and studies the rate at which Πn(ΘI∗)→ 1. Now, let us instead consider the set of parameters\nΘ ,i = {θ|θi ≥ max j θj − }\nunder which i is –optimal. It is easy to develop a variety of modified top-two sampling rules under which maxi Πn(Θ ,i) → 1 rapidly. For example, we can extend TTPS as follows: set În = arg maxi Πn(Θ ,i). Define Ĵn = arg maxj 6=În Πn(θ|θj = maxi θi & θj > θÎn + ) to be the alternative design that is most likely to be optimal and offer an –improvement over În. A top-two Thompson sampling approach might instead continue sampling θ ∼ Πn until maxi θi > θÎn + and then set Jn = arg maxi θi.\nTop m–arm identification. Suppose now that our goal is to identify the top m < k designs. Consider choosing a design to measure at time n by the following steps:\n1. Sample θ ∼ Πn and compute the top m designs under θ.\n2. Continue sampling θ′ ∼ Πn until the top m designs under θ′ differ from those under θ.\n3. Identify the set of designs that are in the top m under θ or under θ′, but not under both. Choose a design to measure by sampling one uniformly at random from this set.\nThis is the natural extension of top-two Thompson sampling to the top-m arm problem. In fact, when m = 1, this is exactly TTTS with β = 1/2. I conjecture that like the case where m = 1, this algorithm attains a rate of posterior convergence within a factor of 2 of optimal for general m. The optimal exponent for this problem can be calculated by mirroring the steps in Subsection 6.4.\nExtremely Correlated Designs. While our results apply in the case of correlated priors, the proposed algorithms may be wasteful when there are a large number of designs whose qualities are extremely correlated. As an example, consider an extension of our techniques to a pure-exploration variant of a linear bandit problem. Here we associate each action i with a feature vector xi ∈ Rd and seek an action that maximizes xTi θ. The vector θ ∈ Rd is unknown, but we begin with a prior θ ∼ N(0, I) and see noisy observations of xTi θ whenever action i is selected. To apply top-two sampling to this problem, we should modify the algorithm’s second step. For example, under toptwo Thompson sampling, we usually begin drawing a design according to î ∼ αn, and then continue drawing designs ĵ ∼ αn until î 6= ĵ. These are played with respective probabilities (β, 1− β). But even if î 6= ĵ, their features may be nearly identical. A more natural extension of top-two Thompson sampling would modify the second step, and continue sampling ĵ ∼ αn, until a sufficiently different action is drawn – for example until the angle between xĵ and xî exceeds a threshold.\nTuning β. The most glaring gap in this work may be arbitrary choice of tuning parameter β. Optimal asymptotic rates can be attained by adjusting this parameter over time by solving for an optimal allocation as in (11). It is an open problem to instead develop simple algorithms that set β automatically through value of information calculations, or avoid the need for such a parameter altogether.\nAdaptive Stopping. This paper proposed only an allocation rule, which determines the sequence of measurements to draw, but this can be coupled with a rule that determines when to stop sampling. One natural stopping rule in a Bayesian framework is to stop when maxi αn,i > 1 − δ for some δ > 0. Let τδ be a random variable indicating the stopping time under constraint δ. Since 1 −maxi αn,i .= e−nΓ ∗ β under top-two sampling, our results imply that for each sample path τδ ∼ Γ∗β log(1/δ) as δ → 0. It is natural to conjecture that E[τδ] ∼ Γ∗β log(1/δ) as well. This closely mirrors optimal results in Chernoff [1959], Jennison et al. [1982] and Kaufmann [2016]. Does this rule also yield a frequentist probability of incorrect selection that is O(δ) as δ → 0? More generally, an open problem is to show that when combined with an appropriate stopping rule, top-two sampling schemes nearly minimize the expected number of samples E[τδ] as in Jennison et al. [1982] or Kaufmann [2016]."
    }, {
      "heading" : "A Outline",
      "text" : "This technical appendix is organized as follows.\n1. Section B describes a numerical algorithm that can be used to implement TTPS.\n2. Section C provides a more precise discussion of related work by Ryzhov [2016].\n3. The theoretical analysis begins in Section D. There we begin by noting some basic facts of exponential family distributions, as well as some results relating martingales to their quadratic variation process.\n4. Section E establishes results related to the concentration of the posterior distribution, including the proofs of Prop. 3, Prop. 4, and Lemma 3.\n5. Section F studies and simplifies the optimal exponents Γ∗ and Γ∗β, including the proofs of Lemma 1, Prop. 6, Lemma 2, Prop. 1, and Prop. 2.\n6. We conclude with Section G, which studies the top-two allocation rules and provides a proof of Prop. 7."
    }, {
      "heading" : "B An Implementation of TTPS",
      "text" : "This section describes an implementation of the top-two probability sampling for a problem with a Beta prior and binary observations. In this problem, measurements are binary with success probability given by P(Yn,i = 1) = θ∗i . The algorithm begins with an independent prior, under which the ith component of θ follows a Beta distribution with parameters (λ1i , λ2i ). When λ2i = λ2i = 1, this specifies a uniform prior over [0, 1]. This prior distribution can be easily updated to form a posterior distribution according to the update rule given in line 19 of Algorithm 3.\nThis algorithm uses quadrature to approximate the integral defining αn,i. To understand this implementation, consider a random vector (X1, .., XK) whose components are independently distributed with Xi ∼ Beta(λ1i , λ2i ). Then, the probability component i is maximal can be computed according to\nP(Xi = max j Xj) =\nˆ\nx∈R\nP(∩j 6=i{Xj ≤ x})P(Xi = dx)\n= ˆ\nx∈R\n∏ j 6=i P(Xj ≤ x) P(Xi = dx) = ˆ\nx∈R\n K∏ j=1 P(Xj ≤ x)  /P(Xi ≤ x) P(Xi = dx).\nAlgorithm 3 takes as input a vector of x consisting of M points in (0, 1) and approximates the above integral using quadrature at these points. The algorithm computes and updates the posterior PDF and CDF of θi in an M dimensional vectors fi and Fi. It also stores and updates a vector F = ∏K i=1 Fi,m, where Fm is the posterior probability all the designs have quality below xm. Using these quantities, the posterior probability design i is optimal is approximated by a sum in line 8. Lines 11-15 select an action according to TTPS and lines 18-21 update the stored statistics of the posterior using Bayes rule. The algorithm continues for N time steps, and upon stopping returns the posterior parameters λ1 and λ2, which summarize all evidence gathered throughout the measurement process. The algorithm has O(NKM) space and time complexity. It is worth noting that most operations in this algorithm can be implemented in a “vectorized” fashion in languages like MATLAB, NumPy, and Julia.\nAlgorithm 3 BernoulliTTPS(β,K,M,N,λ1,λ2,x) 1: \\\\Initialize: 2: fi,m ← Beta.pdf(xm|λ1i , λ2i ) ∀i,m 3: Fi,m ← Beta.cdf(xm|λ1i , λ2i ) ∀i,m 4: Fm ← ∏ i Fi,m ∀m\n5: 6: for n = 1 . . . N do 7: \\\\Compute Optimal Action Probabilities: 8: αi ← ∑ m fi,mFm/Fi,m ∀i\n9: 10: \\\\Act and Observe: 11: J1 ← arg maxi αi 12: J2 ← arg maxi 6=J1 αi 13: Sample B ∼ Bernoulli(β) 14: I ← BJ1 + (1−B)J2. 15: Play I and Observe Yn,I ∈ {0, 1}. 16: 17: \\\\Update Statistics: 18: (λ1I , λ2I)← (λ1I , λ2I) + (Yn,I , 1− Yn,I) 19: Fm ← (Fm/FI,m)× Beta.cdf(xm|λ1I , λ2I) ∀m 20: FI,m ← Beta.cdf(xm|λ1I , λ2I) ∀m 21: fI,m ← Beta.pdf(xm|λ1I , λ2I) ∀m 22: end for 23: return V ,λ1,λ2"
    }, {
      "heading" : "C Discussion of the Expected Improvement Algorithm",
      "text" : "Here, we briefly discuss interesting recent results of Ryzhov [2016]. He studies a setting with an uncorrelated Gaussian prior, and Gaussian observation noise Yn,i ∼ N(θi, σ2i ). To simplify our discussion, let us restrict attention to the case of common variance σ1 = ... = σk = σ. Ryzhov [2016] shows that under the the expected-improvement algorithm, in the limit as n→∞∑\ni 6=I∗ Ψn,i = O(logn) (13)\nand Ψn,i(θ∗I − θi)2 ∼ Ψn,j(θ∗I − θj)2 ∀i, j 6= I∗ (14)\nRecall that Ψn,i = ∑n `=1 ψn,i denotes the total measurement effort allocated to design i. The sampling ratios (14) are the ratios suggested in the optimal computing budget allocation of Chen et al. [2000]. This work therefore establishes an interesting link between EI and OCBA, which appear quite different on the surface.\nUnfortunately, property (13) is not suggested by the OCBA, and implies that Πn(ΘcI∗) cannot tend to zero at an exponential rate. To see this precisely, assume without loss of generality that I∗ = 1. Then (13) implies ψn → ei ≡ (1, 0, 0, ..., 0). It is easy to show that minθ∈Θc1 Dei(θ\n∗||θ) = 0 and therefore, by Proposition 4,\nlim n→∞ −n−1 log Πn(Θc1) = 0.\nIt is also worth noting that the sampling ratios in (14) are not actually optimal for any finite number of designs k. Specifying our calculations as in Example 1, one can show that under an optimal fixed allocation (ψi, ..., ψk),\n(θ∗I∗ − θ∗i )2\n1/ψI∗ + 1/ψi =\n(θ∗I∗ − θ∗j )2\n1/ψI∗ + 1/1/ψj ∀i, j 6= I∗.\nThese calculations match those in Glynn and Juneja [2004] and Jennison et al. [1982]. As a result, there is no problem with finite k for which the sampling ratios in (14) are optimal. One can show, in fact, that any optimal multi-armed bandit algorithm that attains the lower bound of Lai and Robbins [1985] also satisfies equations (13) and (14). The main innovation in this paper is to show how to build on such bandit algorithms to attain near-optimal rates for the best-arm identification problem.\nRyzhov [2016] also studies the knowledge gradient policy, which could offer improved performance as (13) no longer holds, but shows that as n→∞\nΨn,i(θ∗I − θi) ∼ Ψn,j(θ∗I − θj) ∀i, j 6= I∗,\nwhich could be very far from the optimal sampling proportions."
    }, {
      "heading" : "D Preliminaries",
      "text" : "This section presents some basic results which will be used in the subsequent analysis. First, unless clearly specified, all statements about random variables are meant to hold with probability 1. So for sequences of random variables {Xn} and {Yn}, if we say that Xn →∞ whenever Yn →∞, this means that the set {ω : Yn(ω)→∞, Xn(ω) 9∞} has measure zero.\nFacts about the exponential family. The log partition function A(θ) is strictly convex and differentiable, with\nA′(θ) = ˆ T (y)p(y|θ)dν(y) (15)\nequal to the mean under θ. The Kullback-Leibler divergence is equal to\nd(θ||θ′) = (θ − θ′)A′(θ)−A(θ) +A(θ′) (16)\nand satisfies\nθ′′ > θ′ ≥ θ =⇒ d(θ||θ′′) > d(θ||θ′) (17) θ′′ < θ′ ≤ θ =⇒ d(θ||θ′′) < d(θ||θ′). (18)\nFinally, since [θ, θ] is bounded, and we have assumed supθ∈[θ,θ] |A ′(θ)| <∞,\nsup θ∈[θ,θ] |A(θ)| <∞ and sup θ,θ′∈[θ,θ]\nd(θ||θ′) <∞. (19)\nThis effectively guarantees no single observation can provide enough information to completely rule out a parameter.\nSome martingale convergence results. The next fact relates the behavior of a martingaleMn to its quadratic variation 〈M〉n.\nFact 1. (Williams [1991], 12.13-12.14) Let {Mn} be a square-integrable martingale adapted to the filtration {Hn} and let\n〈M〉n = n∑ `=1 E[(M` −M`−1)2 |H`−1]\ndenote the corresponding quadratic variation process. Then\nMn 〈M〉n →∞\nalmost surely if 〈M〉n →∞ and limn→∞Mn exists and is finite almost surely if limn→∞〈M〉n <∞.\nThe next lemma is crucial to our analysis. To draw the connection with our setting, imagine an adaptive-randomized rule is used to determine when to draw samples from a population. Here Yn ∈ R denotes the sample at time n, Xn ∈ {0, 1} indicates whether the sample was measured, and Zn ∈ [0, 1] determines the probability of measurement conditioned on the past. This lemma provides a law of large numbers when measurement effort ∑n `=1 Z` tends to infinity, but shows that\nif measurement effort is finite then ∑∞ `=1X`Y` is also finite; in this sense the observations collected from Yn are inconclusive when measurement effort is finite.\nLemma 4. Let {Yn} be an i.i.d sequence of real-valued random variables with finite variance and let {Xn} be a sequence of binary random variables. Suppose each sequence is adapted to the filtration {Hn}, and define Zn = P(Xn = 1|Hn−1). If, conditioned on Hn−1, each Yn is independent of Xn, then with probability 1,\nlim n→∞ n∑ `=1 Z` =∞ =⇒ lim n→∞ ∑n `=1X`Y`∑n `=1 Z` = E[Y1]\nand lim n→∞ n∑ `=1 Z` <∞ =⇒ sup n∈N ∣∣∣∣∣ n∑ `=1 X`Y` ∣∣∣∣∣ <∞. Proof. Let µ = E[Y1] and σ2 = E[(Y1 − E[Y1])2] denote the mean and variance of each Yn. Define the martingale\nMn = n∑ `=1 (X`Y` − Z`µ)\nwith M0 = 0 and put Sn = ∑n `=1 Z`. This martingale has quadratic variation\n〈M〉n = n∑ `=1 E[(M` −M`−1)2|H`−1]\n= n∑ `=1 E[(X`(Y` − µ) + (Y` − Z`)µ)2 |H`−1]\n= n∑ `=1 Z`σ 2 + n∑ `=1 Z`(1− Z`)µ2 ≤ (σ2 + µ2)Sn.\nWe use the shorthand S∞ = limn→∞ Sn and 〈M〉∞ = limn→∞〈M〉n. Suppose S∞ <∞ so 〈M〉∞ <∞. By Fact 1, limn→∞Mn exists and is finite almost surely, which\nimplies supn∈N |Mn| <∞. Since | ∑n `=1X`Y`| ≤ |Mn|+ |µS∞|, this shows supn∈N | ∑n `=1X`Y`| <∞ as desired. Now, suppose S∞ = ∞. If 〈M〉∞ < ∞, then again by Fact 1, limn→∞Mn < ∞ and it is immediate that S−1n Mn → 0. However, if 〈M〉∞ =∞ then Mn 〈M〉n → 0,\nwhich implies S−1n Mn → 0 since Sn ≥ (σ2 + µ2)〈M〉n.\nTaking Yn = 1 in the lemma above yields Levy’s extension of the Borel–Cantelli lemmas (Williams [1991], 12.15). Specialized to our setting, this result relates the long run measurement effort Ψn,i = ∑n `=1 ψn,i to the number of times alternative i is actually measured ∑n `=1 1(In = i).\nCorollary 1. For i ∈ {1, ..., k}, set Sn,i = ∑n `=1 1(In = i). Then, with probability 1,\nΨn,i →∞ ⇐⇒ Sn,i →∞\nand Ψn,i →∞ =⇒\nSn,i Ψn,i → 1.\nProof. Apply Lemma 4 with Yn = 1, Xn = 1(In = 1), and Hn = Fn. Then Zn = ψn,i by definition."
    }, {
      "heading" : "E Posterior Concentration and anti-Concentration",
      "text" : ""
    }, {
      "heading" : "E.1 Uniform Convergence of the Log-Likelihood",
      "text" : "We study the log-likelihood\nΛn(θ∗||θ) , log ( Ln(θ∗) Ln(θ) ) = n∑ `=1 log ( p(Y`,I` |θ∗I`) p(Y`,I` |θI`) ) and the log-likelihood from observations of design i\nΛn,i(θ∗i ||θi) , n∑ `=1 1(In = i) log ( p(Yn,i|θ∗i ) p(Yn,i|θi) ) .\nA Doob-decomposition expresses Λn,i(θi) = An(θi) + Mn(θi) as the sum of an Fn−1 predictable process An(θi) and a MartingaleMn(θi). Moreover, an easy calculation shows An(θi) = Ψn,id(θ∗i ||θi) and Mn(θi) = Λn,i(θ∗i ||θi) − Ψn,id(θ∗i ||θi). Applying Lemma 4 shows Ψ−1n,iMn(θi) → 0 if Ψn,i → ∞, which shows the log-likelihood ratio tends to infinity at rate Ψn,id(θ∗i ||θi). The next lemma strengthens this, and provides a link between these quantities that holds uniformly in θi.\nLemma 5. With probability 1, if Ψn,i →∞ then\nsup θi∈[θ,θ] Ψ−1n,i |Λn,i(θ ∗ i ||θi)−Ψn,id(θ∗i ||θi)| → 0,\nand if limn→∞Ψn,i <∞ then\nsup θi∈[θ,θ] sup n∈N |Λn,i(θi)|+ |Ψn,id(θ∗i ||θi)| <∞.\nProof. Define ξn , T (Yn,i) − E[T (Yn,i)] and Xn , 1(In = i). Note that E[ξn|Fn−1] = 0, E[Xn|Fn−1] = ψn,i, and, conditioned on Fn−1, Xn is independent of ξn. Using the form of the exponential family density given in equation (1), and the form of the KL-divergence given in equation (16), the log-likelihood ratio can be written as\nlog ( p(Yn,i|θ∗i ) p(Yn,i|θi) ) = (θ∗i − θi)T (Yn,i)− (A(θ∗i )−A(θi))\n= d(θ∗i ||θi) + (θ∗i − θi) (T (Yn,i)−E[T (Yn,i)]) = d(θ∗i ||θi) + (θ∗i − θi)ξn\nTherefore,\nΛn,i(θ∗i ||θi)−Ψn,id(θ∗i ||θi) = n∑ `=1 X` log ( p(Y`,i|θ∗i ) p(Y`,i|θi) ) − n∑ `=1 ψ`,id(θ∗i ||θi)\n= n∑ `=1 (X` − ψ`,i)d(θ∗i ||θi) + n∑ `=1 X`ξ`(θ∗i − θi).\nHere |θ∗i − θi| ≤ θ − θ ≡ C2 is bounded uniformly. Similarly, as shown in Appendix D, d(θ∗i ||θi) is bounded uniformly in θi by\nC1 ≡ max θ′∈[θ,θ] d(θ∗i ||θ′i) <∞.\nThis implies,\n|Λn,i(θi)−Ψn,id(θ∗i ||θi)| ≤ C1 ∣∣∣∣∣ n∑ `=1 (X` − ψ`,i) ∣∣∣∣∣+ C2 ∣∣∣∣∣ n∑ `=1 X`ξ` ∣∣∣∣∣ (20) |Λn,i(θi)| ≤ C1Ψn,i + C1\n∣∣∣∣∣ n∑ `=1 (X` − ψ`,i) ∣∣∣∣∣+ C2 ∣∣∣∣∣ n∑ `=1 X`ξ` ∣∣∣∣∣ . (21) Since E[ξ2n] < ∞, the result then follows by applying Lemma 4 and Corollary 1. In particular, when Ψn,i →∞,\nlim n→∞ Ψ−1n,i n∑ `=1 (X` − ψ`,i) = 0 and lim n→∞ Ψ−1n,i n∑ `=1 X`ξ` = 0\nWhen limn→∞Ψn,i <∞,\nsup n∈N ∣∣∣∣∣ n∑ `=1 (X` − ψ`,i) ∣∣∣∣∣ <∞ and supn∈N ∣∣∣∣∣ n∑ `=1 X`ξ` ∣∣∣∣∣ <∞. It is also immediate that d(θ∗i ||θi)Ψn,i ≤ C1Ψn,i 9∞, which by (21) implies the second part of the result.\nA corollary of the previous lemma relates the log-likelihood ratio Λn(θ∗||θ) to the KullbackLeibler divergence Dψn(θ ∗||θ).\nCorollary 2. With probability 1,\nsup θ∈Θ |n−1Λn(θ∗||θ)−Dψn(θ ∗||θ)| → 0\nProof.\n∣∣∣n−1Λn(θ∗||θ)−Dψn(θ∗||θ)∣∣∣ = ∣∣∣∣∣n−1 k∑ i=1 (Λn,i(θ∗i ||θi)−Ψn,id(θ∗i ||θi)) ∣∣∣∣∣\n≤ k∑ i=1 n−1|Λn,i(θ∗i ||θi)−Ψn,id(θ∗i ||θi)|.\nLemma 5 implies sup\nθi∈[θ,θ] n−1|Λn,i(θ∗i ||θi)−Ψn,id(θ∗i ||θi)| → 0,\nwhich completes the proof."
    }, {
      "heading" : "E.2 Posterior Consistency: Proof of Prop. 3",
      "text" : "Proposition 3. For any i ∈ {1, .., k} if Ψn,i →∞, then, for all > 0\nΠn({θ ∈ Θ|θi /∈ (θ∗i − , θ∗i + )})→ 0,\nwith probability 1. If I = {i ∈ {1, ..., k}| limn→∞Ψn,i <∞} is nonempty, then\ninf n∈N Πn({θ ∈ Θ|θi ∈ (θ′i, θ′′i ) ∀i ∈ I}) > 0\nfor any collections of open intervals (θ′i, θ′′i ) ⊂ (θ, θ) ranging over i ∈ I.\nBecause we don’t assume an independent prior across the designs, Π1 is not a product measure and therefore neither is Πn. This makes it challenging to reason about the marginal posterior of each design, which is required for Proposition 3. Thankfully, since the prior density is bounded, Πn behaves like a product measure. Note that the likelihood function can be written as the product of k terms:\nLn(θ) = k∏ i=1 Ln,i(θi)\nwhere Ln,i(θi) , ∏ `≤n I`=i p(Y`,1|θi)\nwith the convention that Ln,i(θi) = 1 when ∑n `=1 1(I` = i) = 0. Therefore Ln(θ) forms the density of a product measure. By normalizing, this induces a probability measure over Θ,\nLn(Θ̃) , ´\nΘ̃ Ln(θ)dθ´ Θ Ln(θ)dθ\nΘ̃ ⊂ Θ,\nwhich, as we argue in the next lemma, behaves like the posterior Πn.\nLemma 6. For any set Θ̃ ⊂ Θ,\nC−1Ln(Θ̃) ≤ Πn+1(Θ̃) ≤ CLn(Θ̃),\nwhere C = supθ∈Θ π1(θ)infθ∈Θ π1(θ) <∞\nis independent of n and Θ̃.\nProof. This follows immediately by bounding π1(θ) from above and below in the relation\nΠn+1(Θ̃) = ´\nΘ̃ π1(θ)Ln(θ)dθ´ Θ π1(θ)Ln(θ)dθ .\nWe can now prove Proposition 3.\nProof of Proposition 3. We begin with the first part of the result. For simplicity of notation, we focus on the upper interval Θ̃ = {θ ∈ Θ : θi > θ∗i + }, but results follow identically for the lower interval. We want to show Πn(Θ̃) → 0, which occurs if and only if Ln(Θ̃) → 0. Since Ln is a product measure,\nLn(Θ̃) =\n´ θ θ∗i +\nLn,i(θ)dθ ´ θ θ Ln,i(θ)dθ =\n´ θ θ∗i +\n(Ln,i(θ)/Ln,i(θ∗i )) dθ´ θ θ (Ln,i(θ)/Ln,i(θ ∗ i )) dθ =\n´ θ θ∗i +\nexp{−Λn,i(θ∗i ||θ)}dθ´ θ θ exp{−Λn,i(θ ∗ i ||θ)}dθ\n(22)\nwhere Λn,i(θ∗i ||θi) = log(Ln,i(θ∗i )/Ln,i(θi). By Lemma 5, with probability 1 there is a sequence an → 0 such that |Λn,i(θ∗i ||θ)−Ψn,id(θ∗i ||θ)| ≤ an for all θ. Then, for bn = ean/e−an → 1, one has\nLn(Θ̃) ≤ bn ´ θ θ∗i +\nexp{−Ψn,id(θ∗i ||θ)}dθ´ θ θ exp{−Ψn,id(θ ∗ i ||θ)}dθ\n≤ bn ´ θ θ∗i +\nexp{−Ψn,id(θ∗i ||θ)}dθ´ θ∗i + /2 θ∗i exp{−Ψn,id(θ∗i ||θ)}dθ .\nThe integral in the numerator is upper bounded by (θ − θ∗i − ) exp{−Ψn,id(θ∗i ||θ∗i + ) while the integral in the denominator is lower bounded by ( /2) exp{−Ψn,id(θ∗i ||θ∗i + /2)}. This shows\nLn(Θ̃) ≤ c0bn exp{−Ψn,i (d(θ∗i ||θ∗i + )− d(θ∗i ||θ∗i + /2))} → 0\nwhere c0 = 2 −1(θ − θ∗i − ). The second part of the claim follows from the lower bound in Lemma 6 of\nΠn+1({θ ∈ Θ|θi ∈ (θ′, θ′′) ∀i ∈ I}) ≥ C−1Ln({θ ∈ Θ|θi ∈ (θ′i, θ′′i ) ∀i ∈ I}) (23) = C−1 ∏ i∈I Ln({θ ∈ Θ|θi ∈ (θ′i, θ′′i )}). (24)\nAs in (22),\nLn({θ ∈ Θ|θi ∈ (θ′i, θ′′i )}) = ´ θ′′ θ′ exp{−Λn,i(θ ∗ i ||θ)}dθ´ θ\nθ exp{−Λn,i(θ ∗ i ||θ)}dθ\n.\nWhen limn→∞Ψn,i <∞, Lemma 5 shows that for each i ∈ I,\nsup θi∈[θ,θ] sup n∈N |Λn,i(θ∗i ||θi)| <∞.\nThis implies inf n Ln({θ ∈ Θ|θi ∈ (θ′i, θ′′i )}) > 0\nand establishes the claim."
    }, {
      "heading" : "E.3 Large Deviations: Proof of Proposition 4",
      "text" : "All statements in this section hold when observations are drawn under the parameter θ∗. Since θ∗ is fixed throughout, we simplify notation and write\nWn(θ) , Dψn(θ ∗||θ).\nNote thatWn(θ∗) = 0. As shown in the next lemma n−1 log (πn(θ)/πn(θ∗))−Wn(θ)→ 0 uniformly in θ.\nLemma 7. With probability 1,\nsup θ∈Θ n−1 ∣∣∣∣log(πn(θ∗)πn(θ) ) −Wn(θ) ∣∣∣∣→ 0. Proof. We have\nlog ( πn(θ∗) πn(θ) ) −Wn(θ) = log ( π1(θ∗) π1(θ) ) + (Λn−1(θ∗||θ)−Wn−1(θ)) + (Wn−1(θ)−Wn(θ)) .\nSince infθ∈Θ π1(θ) > 0 and supθ∈Θ π1(θ) < ∞, n−1 log (π1(θ)/π1(θ∗)) → 0 uniformly in θ. By Corollary 2, n−1 (Λn−1(θ)−Wn−1(θ)) → 0 uniformly as well. Finally, by equation (19), n−1(Wn(θ)−Wn−1(θ)) ≤ n−1 maxi d(θ∗i ||θi)→ 0 uniformly in θ.\nThe remaining proof of Proposition 4 follows from a sequence of lemmas. The next observes a form of uniform continuity of Wn that follows from the uniform bound on A′(θ) in Assumption 1.\nLemma 8. For all > 0, there exists δ > 0 such that for θ,θ′ ∈ Θ\n‖θ − θ′‖∞ ≤ δ =⇒ sup n∈N |Wn(θ)−Wn(θ′)| ≤ .\nProof. We have that\n|Wn(θ)−Wn(θ′)| ≤ max 1≤i≤k |d(θ∗i ||θi)− d(θ∗i ||θ′i)|\n= max 1≤i≤k ∣∣(θ′i − θi)A′(θ∗i ) +A(θi)−A(θ′i)∣∣ ≤ 2Cδ\nwhere C = supθ∈(θ,θ) |A ′(θ)| <∞.\nLemma 9. For any open set Θ̃ ⊂ Θ, ˆ\nθ∈Θ̃\nπn(θ) πn(θ∗)\ndθ .= ˆ\nθ∈Θ̃\nexp{−nWn(θ)}dθ.\nProof. By Corollary 2, we can fix a sequence n ≥ 0 with n → 0 such that,\nexp{−n(Wn(θ) + n)} ≤ πn(θ) πn(θ∗) ≤ exp{−n(Wn(θ)− n)}.\nIntegrating over Θ̃ yields,\nexp{−n n} ˆ\nΘ̃\nexp{−nWn(θ)}dθ ≤ ˆ\nΘ̃\nπn(θ) πn(θ∗)\ndθ ≤ exp{n n} ˆ\nΘ̃\nexp{−nWn(θ)}dθ.\nTaking the logarithm of each side implies\n1 n ∣∣∣∣∣∣∣log ˆ\nΘ̃\nπn(θ) πn(θ∗)\ndθ − log ˆ\nΘ̃\nexp{−nWn(θ)}dθ ∣∣∣∣∣∣∣ ≤ n → 0.\nLemma 10. For any open set Θ̃ ⊂ Θ, ˆ\nθ∈Θ̃\nexp{−nWn(θ)}dθ .= exp{−n inf\nθ∈Θ̃ Wn(θ)}\nProof. Let θ̂n be a point in the closure of Θ̃, satisfying\nWn(θ̂n) = inf θ∈Θ̃ Wn(θ).\nSuch a point always exists, since Wn is continuous, and the closure of Θ̃ is compact. Let\nγn , ˆ\nθ∈Θ̃\nexp{−nWn(θ)}dθ.\nOur goal is to show 1 n log(γn) +Wn(θ̂n)→ 0.\nWe have γn ≤ Vol(Θ̃) exp{−nWn(θ̂n)} where for any Θ′ ⊂ Θ, Vol(Θ′) = ´\nΘ̃ dθ ∈ (0,∞) denotes the volume of Θ. This shows\nlim sup n→∞ ( 1 n log(γn) +Wn(θ̂n) ) ≤ 0.\nWe now show the reverse. Fix an arbitrary > 0. By Lemma 8, there exists δ > 0 such that\n|Wn(θ)−Wn(θ̂n)| ≤ ∀n ∈ N\nfor any θ ∈ Θ with ‖θ − θ̂n‖∞ ≤ δ.\nNow, choose a finite δ–cover O of Θ̃ in the norm ‖·‖∞. Remove any set in O that does not intersect Θ̃. Then, for each o ∈ O,\nVol(o ∩ Θ̃) > 0 =⇒ Cδ , min o∈O Vol(o ∩ Θ̃) > 0.\nChoose on ∈ O with θ̂n ∈ closure(on). Then, for every θ ∈ on, Wn(θ) ≤Wn(θ̂n) + . This shows\nγn ≥ ˆ o exp{−nWn(θ}dθ ≥ Cδ exp{−n(Wn(θ̂n)− )).\nTaking the logarithm of both sides implies\n1 n log(γn) +Wn(θ̂n) ≥ Cδ n − → − .\nSince was chosen arbitrarily, this shows\nlim inf n→∞ ( 1 n log(γn) +Wn(θ̂n) ) ≥ 0,\nand completes the proof.\nWe now complete the proof of Proposition 4.\nProof of Proposition 4. We begin with a simple observation. For any sequences of real numbers {an}, {bn}, and {ãn}, {b̃n}, if an .= ãn and bn .= b̃n ∈ R, then an/bn\n.= ãn/b̃n. Therefore, we have\nΠn(Θ̃) = Πn(Θ̃) Πn(Θ)\n= ´\nΘ̃ πn(θ)dθ´ Θ πn(θ)dθ\n= ´ Θ̃ (πn(θ)/πn(θ ∗))dθ´\nΘ (πn(θ)/πn(θ∗))dθ .= exp{−n infθ∈Θ̃Wn(θ)} exp{−n infθ∈ΘWn(θ)}\nwhere the final equality follows from the previous two lemmas. Since Wn(θ) ≥ 0 and Wn(θ∗) = 0, exp{−n infθ∈ΘWn(θ)} = 1."
    }, {
      "heading" : "E.4 Large Deviations of the Value Measure: Proof of Lemma 3",
      "text" : "Lemma 3. For any i 6= I∗, Vn,i .= αn,i.\nProof. First, since\nVn,i = ˆ\nΘi\nvi(θ)πn(θ)dθ ≤ (u(θ)− u(θ)) ˆ\nΘi\nπn(θ)dθ = (u(θ)− u(θ))αn,i\nit is immediate that lim sup n→∞ n−1(log Vn,i − logαn,i) ≤ 0. (25)\nThe other direction more subtle. Define Θi,δ ⊂ Θi by\nΘi,δ = {θ ∈ Θ : θi ≥ max j 6=i θj + δ}.\nFor any θ ∈ Θi,δ, vi(θ) ≥ Cδ where\nCδ ≡ min θ∈[θ,θ] u(θ + δ)− u(θ) > 0.\nBecause u(θ + δ) − u(θ) is continuous and is strictly positive for each θ, this minimum exists and the objective value is strictly positive. Then\nVn,i ≥ ˆ\nΘi,δ\nvi(θ)πn(θ)dθ ≥ Cδ ˆ\nΘi,δ\nπn(θ)dθ = CδΠn(Θi,δ) ∀δ > 0.\nCombining this with Proposition 4 shows\nlim inf n→∞ 1 n (log Vn,i−logαn,i) ≥ lim inf n→∞ 1 n (log Πn(Θi,δ)−log Πn(Θi)) = − min θ∈Θi,δ Dψn (θ∗||θ)−min θ∈Θi Dψn (θ∗||θ).\nThe final term can be made arbitrarily small by taking δ → 0. Precisely, by Lemma 8, for any > 0, there exists δ > 0 such that for all n ∈ N and θ,θ′ ∈ Θ satisfying ‖θ − θ′‖∞ ≤ δ ,\nDψn (θ∗||θ) ≤ .\nTherefore, for each > 0 one can choose δ > 0 such that\nmin θ∈Θi,δ Dψn (θ∗||θ) ≤ min θ∈Θi Dψn (θ∗||θ) + .\nThis shows lim inf n−1(log Vn,i − logαn,i) ≥ − for all > 0, and hence\nlim inf n→∞\nn−1(log Vn,i − logαn,i) ≥ 0."
    }, {
      "heading" : "F Simplifying and Bounding the Error Exponent",
      "text" : ""
    }, {
      "heading" : "F.1 Proof of Lemma 1",
      "text" : "To begin, we restate the results of Lemma 1 in the order in which they will be proved. Recall, from Section D that A(θ) is increasing and strictly convex, and, by (15), A′(θ) is the mean observation under θ.\nLemma 1. Define for each i 6= I∗,ψ ≥ 0,\nCi(β, ψ) , min x∈R βd(θ∗I∗ ||x) + ψd(θ∗i ||x). (26)\n(a) For any i 6= I∗ and probability distribution ψ over {1, ..., k}\nmin θ∈Θi\nDψ(θ∗||θ) = Ci(ψI∗ , ψi).\nwhere Θi , {θ ∈ Θ|θi ≥ θI∗}. (b) Each Ci is a concave function. (c) The unique solution to the minimization problem (26) is θ ∈ R satisfying\nA′(θ) = ψI ∗A′(θ∗I∗) + ψiA′(θ∗i )\nψI∗ + ψi .\nTherefore, Ci(ψI∗ , ψi) = ψI∗d(θ∗I∗ ||θ) + ψid(θ∗i ||θ).\n(d) Each Ci is a strictly increasing function.\nProof. (a)\nmin θ∈Θi Dψ(θ∗||θ) = min θ∈Θ:θi≥θI∗ k∑ j=1 ψn,jd(θ∗j ||θj)\n= min θ≥θi≥θI∗≥θ\nψI∗d(θ∗I∗ ||θI∗) + ψid(θ∗i ||θi) + ∑\nj /∈{i,I∗} min θj ψn,jd(θ∗j ||θj)\n= min θ≥θi≥θI∗≥θ ψI∗d(θ∗I∗ ||θI∗) + ψid(θ∗i ||θi)\nwhere the last equality uses that the minimum occurs when θj = θ∗j for j /∈ {I∗, i}, and this is feasible for any choice of (θi, θI∗). Then, by the monotonicity properties of KL-divergence (see Section D, equation (17)), there is always a minimum with θi = θI∗ . Therefore this objective value is equal to\nmin θ∈[θ,θ] ψI∗d(θ∗I∗ ||θ) + ψid(θ∗i ||θ) = min x∈R ψI∗d(θ∗I∗ ||x) + ψid(θ∗i ||x) = Ci(ψI∗ , ψi).\n(b) Ci is the minimum over a family of linear functions and therefore is concave (See Chapter 3.2 of Boyd and Vandenberghe [2004]). In particular Ci(β, ψ) = minx∈R g((β, ψ);x) where g((β, ψ);x) = βd(θ∗I∗ ||x) + ψd(θ∗i ||x) is linear in (β, ψ).\n(c) Direct calculation using the formula for KL divergence in exponential families (see (16) in Section D) shows\nβd(θ∗I∗ ||x) + ψid(θ∗i ||x) = (β + ψi)A(x)− (βA′(θ∗I∗) + ψiA′(θ∗i ))x+ f(β, θ∗I∗ , ψi, θ∗i )\nwhere f(β, θ∗I∗ , ψi, θ∗i ) captures terms that are independent of x. Setting the derivative with respect to x to zero yields the result since A(x) is strictly convex.\n(d) We will show Ci is strictly increasing in the second argument. The proof that it is strictly increasing in its first argument follows by symmetry. Set\nf(ψi, x) = βd(θ∗I∗ ||x) + ψid(θ∗i ||x)\nso that Ci(β, γi) = minx∈R f(ψi, x). Since KL divergences are non-negative, f(ψi, x) is weakly increasing in ψi. To establish the claim, fix two nonnegative numbers ψ′ < ψ′′. Let x′ = arg minx f(ψ′, x) and x′′ = arg minx f(ψ′′, x). By part (c), these are unique and x′ < x′′. Then\nf(ψ′, x′) < f(ψ′, x′′) ≤ f(ψ′′, x′′)\nwhere the first inequality uses that x′ 6= x′′ and x′ is a unique minimum and the second uses the f is non-decreasing."
    }, {
      "heading" : "F.2 Proof of Proposition 6",
      "text" : "We will begin by restating Proposition 6.\nProposition 6. The solution to the optimization problem (12) is the unique allocation ψ∗ satisfying ψ∗I∗ = β and\nCi(β, ψi) = Cj(β, ψj) ∀ i, j 6= I∗. (27)\nIf ψn = ψ∗ for all n, then Πn(ΘcI∗) .= exp{−nΓ∗β}.\nMoreover under any other adaptive allocation rule, if ψn,I∗ → β as n→∞ then\nlim sup n→∞ − 1 n log Πn(ΘcI∗) ≤ Γ∗β\nalmost surely.\nProof. By Lemma 1, each function Ci is continuous, and therefore mini 6=I∗ Ci(β, ψi) is continuous in (ψi : i 6= I∗). Since continuous functions on a compact space attain their minimum, there exists an optimal solution ψ∗ to (12), which satisfies\nmin i 6=I∗ Ci(β, ψ∗i ) = max ψ:ψI∗=β min i 6=I∗ Ci(β, ψi).\nSuppose ψ∗ does not satisfy (27), so for some j 6= I∗,\nCj(β, ψ∗j ) > min i 6=I∗ Ci(β, ψ∗i ).\nThis yields a contradiction. Consider a new vector ψ with ψ j = ψ∗j − and ψ i = ψ∗i + /(k − 2) for each i /∈ {I∗, j}. For sufficiently small , one has\nCj(β, ψ j) > min i 6=I∗ Ci(β, ψ i ) > min i 6=I∗ Ci(β, ψ∗i )\nand so ψ attains a higher objective value. To show the solution to (27) must be unique, imagine ψ and ψ′ both satisfy (27) and ψI∗ = ψ′I∗ = β. If ψj > ψ′j for some j, then Cj(β, ψj) > Cj(β, ψ′j) since Cj is strictly increasing. But by (27) this implies that Cj(β, ψj) > Cj(β, ψ′j) for every j 6= I∗, which implies ψj > ψ′j for every j, and contradicts that that ∑ j 6=I∗ ψj = ∑ j 6=I∗ ψ ′ j = 1− β.\nThe remaining claims follow immediately from Propoosition 4 and Lemma 1, which together show that under any adaptive allocation rule\nΠn(ΘcI∗) .= exp{−nmin\ni 6=I∗ Ci(ψn,I∗ , ψn,i)}.\nThis implies that if ψn = ψ∗ for all n, then Πn(ΘcI∗) .= exp{−nΓ∗β}. Similarly, by the continuity of each Ci, if ψn,I∗ → β, then\nΠn(ΘcI∗) .= exp{−nmin i 6=I∗ Ci(β, ψn,i)} ≥ exp{−nΓ∗β}\nwhich establishes the final claim."
    }, {
      "heading" : "F.3 Proof of Lemma 2",
      "text" : "Recall, the notation\nΓ∗ = max ψ min i 6=I∗ Ci(ψI∗ , ψi) Γ∗β , max ψ:ψI∗=β min i 6=I∗ Ci(β, ψi)\nwhere Ci(β, ψ) = min\nx∈R βd(θ∗I∗ ||x) + ψd(θ∗i∗ ||x).\nLemma 2. For β∗ = arg maxβ Γ∗β and any β ∈ (0, 1),\nΓ∗ Γ∗β ≤ max\n{ β∗\nβ , 1− β∗ 1− β\n} .\nTherefore Γ∗ ≤ 2Γ∗1/2 Proof. Define for each non-negative vector ψ,\nf(ψ) = min i 6=I∗ Ci(ψI∗ , ψi)\nThe optimal exponent Γ∗ is the maximum of f(ψ) over probability vectors ψ. Here, we instead define f for all non-negative vectors, and proceed by varying the total budget of measurement effort available ∑k i=1 ψi.\nBecause each Ci is non-decreasing (see Lemma 1), f is non-decreasing. Since the minimum over x in the definition of Ci only depends on the relative size of the components of ψ, f is homogenous of degree 1. That is f(cψ) = cf(ψ) for all c ≥ 1. For each c1, c2 > 0 define\ng(c1, c2) = max{f(ψ) : ψI∗ = c1, ∑ i 6=I∗ ψi ≤ c2,ψ ≥ 0}.\nThe function g inherits key properties of f ; it is also non-decreasing and homogenous of degree 1. We have\nΓ∗β = max{f(ψ) : ψI∗ = β, k∑ i=1 ψi = 1,ψ ≥ 0}\n= max{f(ψ) : ψI∗ = β, ∑ i 6=I∗ ψi ≤ 1− β,ψ ≥ 0}\n= g(β, 1− β)\nwhere the second equality uses that f is non-decreasing. Similarly, Γ∗ = g(β∗, 1− β∗). Setting\nr := max { β∗\nβ , 1− β∗ 1− β } implies rβ ≥ β∗ and r(1− β) ≥ 1− β∗. Therefore\nrΓ∗β = rg(β, 1− β) = g(rβ, r(1− β)) ≥ g(β∗, 1− β∗) = Γ∗."
    }, {
      "heading" : "F.4 Sub-Gaussian Bound: Proof of Proposition 1",
      "text" : "The proof of Proposition 1 relies on the following variational form of Kullback–Leibler divergence, which is given in Theorem 5.2.1 of Robert Gray’s textbook Entropy and Information Theory Gray [2011].\nFact 2. Fix two probability measures P and Q defined on a common measureable space (Ω,F). Suppose that P is absolutely continuous with respect to Q. Then\nD (P||Q) = sup X\n{ EP[X]− log EQ[eX ] } ,\nwhere the supremum is taken over all random variables X such that the expectation of X under P is well defined, and eX is integrable under Q.\nWhen comparing two normal distributions N (θ, σ2) and N (θ′, σ2) with common variance, the KL-divergence can be expressed as d(θ||θ′) = (θ − θ′)2/(2σ2). We follow Russo and Zou [2015] in deriving the following corollary of Fact 2, which provides and analogous lower bound on the KL-divergences when distributions are sub-Gaussian. Recall that, µ(θ) = ´ yp(y|θ)dν(y) denotes the mean observation under θ.\nCorollary 3. Fix any θ, θ′ ∈ [θ, θ]. If when Y ∼ p(y|θ′), Y is sub-Gaussian with parameter σ, then,\nd(θ||θ′) ≥ (µ(θ)− µ(θ ′))2\n2σ2 Proof. Consider two alternate probability distributions for a random variable Y , one where Y ∼ p(y|θ) and one where Y ∼ p(y|θ′) We apply Fact 2 where X = λ(Y −Eθ′ [Y ]), P is the probability measure when Y ∼ p(y|θ) and Q is the measure when Y ∼ p(y|θ′). By the sub-Gaussian assumption log Eθ′ [exp{X}] ≤ λ2σ2/2. Therefore, Fact 2 implies\nd(θ||θ′) ≥ λ(Eθ[X])− λ2σ2\n2 = λ(Eθ[Y ]−Eθ ′ [Y ])− λ\n2σ2\n2 .\nThe result follows by choosing λ = (Eθ[Y ]−Eθ′ [Y ])/σ2 which minimizes the right hand side.\nWe are now ready to prove Proposition 1. Recall that in an exponential family, A′(θ) =´ T (y)p(y|θ)dν(y), so if T (y) = y then A′(θ) = µ(θ).\nProof of Proposition 1. By Lemma 1,\nΓ∗1/2 = max ψ:ψI∗=1/2 min i 6=I∗ Ci(1/2, ψi)\nLet µI∗ = A′(θ∗I∗) and µi = A′(θ∗i ) denote the means of designs I∗ and i so ∆i = µI∗ − µi. By Lemma 1, Ci(1/2, ψi) = (1/2)d(θ∗I∗ ||θ) + ψid(θ∗i ||θ). where θ is the unique parameter with mean\nA′(θ) = (1/2)µI ∗ + ψiµi\n1/2 + ψi .\nFor ψi ≤ 1/2, A′(θ) ≥ µI\n∗ + µi 2 = µi + ∆i/2.\nNow, using Corollary 3 and the non-negativity of KL-divergence\nCi(1/2, ψi) ≥ ψid(θ∗i ||θ) ≥ ψi(µi − µi + ∆i/2)2 2σ2 = ψi∆2i 8σ2 .\nChoosing ψI∗ = 1/2, and ψi ∝ ∆−2i , so\nψi = 1 2  k∑ j−2 ∆−2j −1 ∆−2i yields\nmin i 6=I∗\nCi(1/2, ψi) ≥ 1 16σ2 ∑k 2 ∆−2j ."
    }, {
      "heading" : "F.5 Convergence of Uniform Allocation: Proof of Proposition 2",
      "text" : "Proof. Without loss of generality, assume the problem is parameterized so that the mean of design i is θ∗i By Proposition 5, we have\nΠn(ΘcI∗) .= exp{−nmin i 6=I∗ Ci(k−1, k−1)}\nBy Lemma 1, Ci(k−1, k−1) = k−1d(θ∗I∗ ||θ) + k−1d(θ∗i ||θ)\nwhere θ = (θ∗I∗ + θ∗i )/2. Therefore, using the formula for the KL-divergence of standard Gaussian random variables\nCi(k−1, k−1) = (θ∗I∗ − θ)2 2σ2 + (θ∗i − θ)2 2σ2 = (θ∗I∗ − θ∗i )2 4σ2 = ∆2i 4σ2 .\nG Analysis of the Top-Two Allocation Rules: Proof of Proposition 7\nProposition 7. Under the TTTS, TTPS, or TTVS algorithm with parameter β > 0, ψn → ψβ, where ψβ is the unique allocation with ψβI∗ = β satisfying\nCi(β, ψβi ) = Cj(β, ψ β j ) ∀i, j 6= I ∗. (28)\nTherefore, Πn(ΘcI∗) .= e−nΓ ∗ β . (29)\nBecause each Ci is continuous, if ψn → ψβ then Ci(ψn,I∗ , ψn,i) → Ci(β, ψ β i ) for all i 6= I∗. Equation (29) then follows by invoking Proposition 6, which establishes the optimality of the allocation ψβ.\nThe remainder of this section establishes that ψn → ψβ almost surely the proposed top-two rules. The proof is broken into a number of steps. In order to provide a nearly unified treatment of the three algorithms, we begin with several results that hold for any allocation rule.\nG.1 Results for a general allocation rule\nAs in other sections, all arguments here hold for any sample path (up to a set of measure zero). The first result provides a sufficient condition under which ψn → ψβ. Roughly speaking, if ψn,j ≥ ψ β j +δ, then too much measurement effort has been allocated to design j relative to the optimal proportion ψβj . Algorithms satisfying (30) allocate negligible measurement effort to such designs, and therefore the average measurement effort they receive must decrease toward the optimal proportion.\nLemma 11 (Sufficient condition for optimality). Consider any adaptive allocation rule. If ψn,I∗ → β and ∑\nn∈N ψn,j1(ψn,j ≥ ψ β j + δ) <∞ ∀ j 6= I ∗, δ > 0, (30)\nthen ψn → ψβ.\nProof. Fix a sample path for which ψn,I∗ → β, and (30) holds. Fix some j 6= I∗. We first show lim inf n→∞\nψn,j ≤ ψ∗j . Suppose otherwise. Then, with positive probability, for some δ > 0, there exists N such that for all n ≥ N , ψn,j ≥ ψ∗j + δ. But then,\n∑ n∈N ψn,j = N∑ n=1 ψn,j + ∞∑ n=N+1 1(ψn,j ≥ ψ∗j + δ)ψn,j <∞.\nBut since ψn,j = ∑n `=1 ψn,j/n this implies ψn,j → 0.\nNow, we show lim sup n→∞ ψn,j ≤ ψ∗j . Proceeding by contradiction again, suppose otherwise. Then, with positive probability\nlim sup n→∞ ψn,j > ψ β j & lim infn→∞ ψn,j ≤ ψ β j .\nOn any sample path where this occurs, for some δ > 0, there exists an infinite sequence of times N1 < N2 < N3 < ... such that ψN`,j ≥ ψ β j + 2δ when ` is odd and ψN`,j ≤ ψ β j + δ when ` is even.\nThis can only occur if, ∑ n∈N ψn,j1(ψn,j ≥ ψ∗j + δ) =∞,\nwhich violates the hypothesis. Together with the hypothesis that ψn,I∗ → β, this implies that for all i ∈ {1, ..., k}, lim sup\nn→∞ ψn,i ≤ ψβi . But since ∑ i ψn,i = ∑ i ψ β i , this implies ψn → ψβ.\nThe next lemma will be used to establish that (30) holds for each of the proposed algorithms. It shows that if too much measurement effort has been allocated to some design i 6= I∗, in the sense that ψn,i > ψ β i + δ for a constant δ > 0, then αn,i is exponentially small compared maxj 6=I∗ αn,j .\nLemma 12 (Over-allocation implies negligible probability). Fix any δ > 0 and j 6= I∗. With probability 1, under any allocation rule, if ψn,I∗ → β, there exists δ′ > 0 and a sequence n with n → 0 such that for any n ∈ N,\nψn,j ≥ ψ β j + δ =⇒ αn,j maxi 6=I∗ αn,i ≤ e−n(δ′+ n).\nProof. Since Πn(ΘcI∗) = ∑ i 6=I∗ αn,i, Πn(ΘcI∗)\n.= maxi 6=I∗ αn,i. Then, by invoking Proposition (6), since ψn,I∗ → β,\nlim sup n→∞ − 1 n\nlog (\nmax i 6=I∗ αn,i\n) ≤ Γ∗β.\nRecall the definition Θi , {θ|θi ≥ θI∗}. Now, by Proposition 4 and Lemma 1,\nαn,j = Πn(Θj) ≤ Πn(Θj) .= exp{−nCj(ψn,I∗ , ψn,j)} .= exp{−nCj(β, ψn,j)}.\nCombining these equations implies that there exists a non-negative sequence n → 0 with\nαn,j maxi 6=I∗ αn,i ≤ exp{−n(Cj(β, ψn,j)− n/2)} exp{−n(Γ∗β + n/2)} = exp\n{ −n ( (Cj(β, ψn,j)− Γ∗β)− n )} Since Cj(β, ψj) is strictly increasing in ψj (See lemma 1) and Cj(β, ψβj ) = Γ∗β, there exists some δ′ > 0 such that\nψn,j ≥ ψ β j + δ =⇒ Cj(β, ψn,j)− Γ ∗ β > δ ′.\nThe next result builds on Proposition 3. It shows that the quality of any design which receives infinite measurement effort is identified to arbitrary precision. On the other hand, for designs receiving finite measurement effort, there is always nonzero probability under the posterior that one of them significantly exceeds the highest quality that has been confidently identified. Therefore, αn,i and Vn,i remain bounded away from 0 for designs that receive finite measurement effort. This result will be used to show that all designs receive infinite measurement effort under the proposed top-two allocation rules, and as a result the posterior converges on the truth asymptotically. Lemma 13 (Implications of finite measurement). Let\nI = {i ∈ {1, .., k} : ∞∑ n=1 ψn,i <∞}\ndenote the set of designs to which a finite amount of measurement effort is allocated. Then, for any i /∈ I Πn ({θ : θi ∈ (θ∗i − , θ∗i + ))→ 1, (31) and if I is empty\nVn,i → { 0 if i 6= I∗\nvI∗(θ∗) > 0 if i = I∗ and αn,i →\n{ 0 if i 6= I∗\n1 if i = I∗.\nIf I is nonempty, then for every i ∈ I,\nlim inf n→∞ αn,i > 0 and lim inf n→∞ Vn,i > 0.\nProof. Equation (31) is implied by by Proposition 3. Now, set\nΘi, = {θ ∈ Θ : θi ≥ max j 6=i θj + }\nto be the set of parameters under which the quality of design i exceeds that of all others by at least . Let ρ∗ = maxi/∈I θ∗i denote the quality of the best design among those that are sampled infinitely often, and choose > 0 small enough that ρ∗ + 2 < θ. For i ∈ I, we have\nΠn(Θi, ) ≥ Πn(A)−Πn (B)\nfor A ≡ {θ|θi ≥ ρ∗ + 2 & θj < ρ∗ ∀j ∈ I \\ {i}}\ndefined to be parameters under which θi ≥ ρ∗ + 2 but none of the other designs in I exceed ρ∗, and\nB ≡ {θ : max i/∈I\nθi ≥ ρ∗ + }\ndefined to be the parameter vectors under which there is no design in Ic with quality exceeding ρ∗ + . By (31), Πn (B)→ 0, but by the second part of Proposition 3, the set of parameters A cannot be completely ruled based on a finite amount of measurement effort, and\ninf n∈N Πn(A) > 0.\nTogether this shows lim inf n→∞ Πn(Θi, ) > 0, which implies the result.\nG.2 Results specific to the proposed algorithms\nWe now leverage the general results of the previous subsection to show ψ → ψβ under each proposed top-two allocation rule. Proofs are provided separately for each of the three algorithms, but they follow a similar structure. In the first step, we use Lemma 13 to argue that ψn,I∗ → β almost surely. The proof then uses Lemma 12 to show (30) holds, which by Lemma 11 is sufficient to establish that ψn → ψβ."
    }, {
      "heading" : "G.2.1 Top-Two Thompson Sampling",
      "text" : "Recall that under top-two Thompson sampling, for every i ∈ {1, ..., k},\nψn,i = αn,i β + (1− β)∑ j 6=i αn,j 1− αn,j  ."
    }, {
      "heading" : "Proof for TTTS.",
      "text" : "Step 1: Show ψn,I∗ → β. To begin, we show ∑ n∈N ψn,i =∞ for each design i. Suppose otherwise.\nLet I = {i ∈ {1, .., k} : ∑∞\n1 ψn,i < ∞} be the set of designs to which finite measurement effort is allocated. Under the TTTS sampling rule, ψn,i ≥ βαn,i. Therefore, by Lemma 13, if i ∈ I then lim inf n→∞ αn,i > 0, which implies ∑ n∈N ψn,i =∞, a contradiction.\nSince ∑∞\n1 ψn,i = ∞ for all i, by applying Lemma 13 we conclude that αn,I∗ → 1. For TTTS, this implies ψn,I∗ → β.\nStep 2: Show (30) holds. By Lemma 11, it is enough to show that (30) holds under TTTS. Let În = arg maxi αn,i, and Ĵn = arg maxi 6=În αn,i. Since αn,I∗ → 1, for each sample path there is a finite time τ < ∞ such that for all n ≥ τ , În = I∗ and therefore Ĵn = arg maxi 6=I∗ αn,i. Under TTTS,\nψn,i ≤ βαn,i + (1− β) αn,i αn,Jn ≤ αn,i αn,Jn ,\nwhere the first inequality follows since\n∑ j 6=i αn,j 1− αn,j ≤ ∑ j 6=i αn,i 1− αn,În ≤ ∑ j 6=i αn,j αn,Ĵn ≤ 1 αn,Ĵn .\nFor n ≥ τ , this means ψn,i ≤ αn,i/(maxj 6=I∗ αn,i) for any i 6= I∗. By Lemma 12, there is a constant δ′ > 0 and a sequence n → 0 such that\nψn,i ≥ ψ β i + δ =⇒ αn,i maxj 6=I∗ αn,j ≤ e−n(δ′− n).\nTherefore for all i 6= I∗ ∑ n≥τ ψn,i1(ψn,i ≥ ψ β i + δ) ≤ ∑ n≥τ e−n(δ ′− n) <∞."
    }, {
      "heading" : "G.2.2 Top-Two Probability Sampling",
      "text" : "Recall that top-two probability sampling sets ψn,În = β and ψn,Ĵn = 1−β where În = arg maxi αn,i and Ĵn = arg maxj 6=În αn,i are the two designs with the highest posterior probability of being optimal."
    }, {
      "heading" : "Proof for TTPS.",
      "text" : "Step 1: Show ψn,I∗ → β. To begin, we show ∑ n∈N ψn,i =∞ for each design i. Suppose otherwise.\nLet I = {i ∈ {1, .., k} : ∑∞\n1 ψn,i < ∞} be the set of designs to which finite measurement effort is allocated. Proceeding by contradiction, suppose I is nonempty. By Lemma 13, there is a time τ and some probability α′ > 0 such that αn,i > α′ for all n ≥ τ and i ∈ I. However, because of the assumption that θ∗i 6= θ∗j , for i 6= j, I = arg maxi/∈I θ∗i is unique. By (31), the algorithm identifies arg maxi/∈I θ∗i with certainty, and αn,i → 0 for every i /∈ I except for I. This means there is a time τ ′ > τ such that for n ≥ τ ′\nαn,i > α ′ if i ∈ I αn,i ≤ α′ if i /∈ I and i 6= I.\nWhen this occurs at least one of the two designs with highest probability αn,i of being optimal must be in the set I, which implies designs in I receive infinite measurement effort, yielding a contradiction.\nSince ∑∞\n1 ψn,i = ∞ for all i, Lemma 13 implies αn,I∗ → 1. Therefore, there is a finite time τ such that În , arg maxi αn,i = I∗ for all n ≥ τ . By the definition of the algorithm ψn,În = β, and so ψn,I∗ = β for all n ≥ τ . We conclude that ψn,I∗ → β.\nStep 2: Show (30) holds. As argued above, for each sample path there is a finite time τ < ∞ such that for all n ≥ τ , În = I∗ and therefore Ĵn = arg maxi 6=I∗ αn,i. By Lemma 12, one can choose τ ′ ≥ τ such that for all n ≥ τ ′,\nψn,j ≥ ψ β j + δ =⇒ αn,j < max\ni 6=I∗ αn,i\nand therefore by definition Ĵn 6= j. This concludes the proof, as it shows that for each sample path there is a finite time τ ′ after which TTPS never allocates any measurement effort to design j when ψn,j ≥ ψ β j + δ."
    }, {
      "heading" : "G.2.3 Top-Two Value Sampling",
      "text" : "Recall that top-two value sampling sets and ψn,În = β and ψn,Ĵn = 1− β where În = arg maxi Vn,i and Ĵn = arg maxj 6=În Vn,i are the two designs with the highest posterior value.\nProof for TTVS. Step 1: Show ψn,I∗ → β. The proof is essentially identical to that for TTPS.To begin, we show ∑ n∈N ψn,i = ∞ for each design i. Suppose otherwise. Let I = {i ∈ {1, .., k} :∑∞\n1 ψn,i < ∞} be the set of designs to which finite measurement effort is allocated. Proceeding by contradiction, suppose I is nonempty. By Lemma 13, there is a time τ and some v > 0 such that Vn,i > v for all n ≥ τ and i ∈ I. However, because of the assumption that θ∗i 6= θ∗j , for i 6= j, I = arg maxi/∈I θ∗i is unique3. By (31), the algorithm identifies arg maxi/∈I θ∗i with certainty, and\n3If the arg-max is not unique, then one could show that Vn,i → 0 for all i /∈ I, and that therefore there is a finite time after both of the top-two designs are always in the set I, yielding a contradiction\nVn,i → 0 for every i /∈ I except for I. Then there is a time τ ′ > τ such that for n ≥ τ ′\nVn,i > v if i ∈ I Vn,i ≤ v if i /∈ I and i 6= I∗.\nWhen this occurs at least one of the two designs with highest value Vn,i must be in the set I, which implies designs in I receive infinite measurement effort, yielding a contradiction.\nSince ∑∞\n1 ψn,i =∞ for all i, Lemma 13 implies Vn,I∗ → vI∗(θ∗) > 0 and Vn,i → 0 for all i 6= I∗. Therefore, there is a finite time τ such that arg maxi Vn,i = I∗ for all τ ≥ n. By the definition of the algorithm arg maxi Vn,i is sampled with probability β, and so ψn,I∗ = β for all n ≥ τ . We conclude that ψn,I∗ → β.\nStep 2: Show (30) holds. Again, the proof is essentially identical to that for TTPS. As argued above, for each sample path there is a finite time τ < ∞ such that for all n ≥ τ , În = I∗ and therefore Ĵn = arg maxi 6=I∗ Vn,i. By Lemma 3, Vn,i\n.= αn,i. Combining this with Lemma 12 shows one can choose τ ′ ≥ τ such that for all n ≥ τ ′,\nψn,j ≥ ψ β j + δ =⇒ Vn,j < max\ni 6=I∗ Vn,i\nand therefore by definition Ĵn 6= j. This concludes the proof, as it shows that for each sample path there is a finite time τ ′ after which TTVS never allocates any measurement effort to design j 6= I∗ when ψn,j ≥ ψ β j + δ."
    } ],
    "references" : [ {
      "title" : "Analysis of Thompson sampling for the multi-armed bandit problem",
      "author" : [ "S. Agrawal", "N. Goyal" ],
      "venue" : "In Proceedings of the 21st Annual Conference on Learning Theory (COLT),",
      "citeRegEx" : "Agrawal and Goyal.,? \\Q2012\\E",
      "shortCiteRegEx" : "Agrawal and Goyal.",
      "year" : 2012
    }, {
      "title" : "The sequential design of experiments for infinitely many states of nature",
      "author" : [ "A.E. Albert" ],
      "venue" : "The Annals of Mathematical Statistics,",
      "citeRegEx" : "Albert.,? \\Q1961\\E",
      "shortCiteRegEx" : "Albert.",
      "year" : 1961
    }, {
      "title" : "Best arm identification in multi-armed bandits",
      "author" : [ "J.-Y. Audibert", "S. Bubeck" ],
      "venue" : "In COLT-23th Conference on Learning Theory-2010, pages 13–p,",
      "citeRegEx" : "Audibert and Bubeck.,? \\Q2010\\E",
      "shortCiteRegEx" : "Audibert and Bubeck.",
      "year" : 2010
    }, {
      "title" : "A single-sample multiple decision procedure for ranking means of normal populations with known variances",
      "author" : [ "R.E. Bechhofer" ],
      "venue" : "The Annals of Mathematical Statistics,",
      "citeRegEx" : "Bechhofer.,? \\Q1954\\E",
      "shortCiteRegEx" : "Bechhofer.",
      "year" : 1954
    }, {
      "title" : "Bayesian statistics and the efficiency and ethics of clinical trials",
      "author" : [ "D.A. Berry" ],
      "venue" : "Statistical Science,",
      "citeRegEx" : "Berry.,? \\Q2004\\E",
      "shortCiteRegEx" : "Berry.",
      "year" : 2004
    }, {
      "title" : "Pure exploration in multi-armed bandits problems",
      "author" : [ "S. Bubeck", "R. Munos", "G. Stoltz" ],
      "venue" : "In Algorithmic Learning Theory,",
      "citeRegEx" : "Bubeck et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Bubeck et al\\.",
      "year" : 2009
    }, {
      "title" : "An empirical evaluation of Thompson sampling",
      "author" : [ "O. Chapelle", "L. Li" ],
      "venue" : "In Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Chapelle and Li.,? \\Q2011\\E",
      "shortCiteRegEx" : "Chapelle and Li.",
      "year" : 2011
    }, {
      "title" : "Simulation budget allocation for further enhancing the efficiency of ordinal optimization",
      "author" : [ "C.-H. Chen", "J. Lin", "E. Yücesan", "S.E. Chick" ],
      "venue" : "Discrete Event Dynamic Systems,",
      "citeRegEx" : "Chen et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2000
    }, {
      "title" : "Sequential design of experiments",
      "author" : [ "H. Chernoff" ],
      "venue" : "The Annals of Mathematical Statistics,",
      "citeRegEx" : "Chernoff.,? \\Q1959\\E",
      "shortCiteRegEx" : "Chernoff.",
      "year" : 1959
    }, {
      "title" : "Approaches in sequential design of experiments. A Survey of Statistical Design and Linear Models (Edited by J. N",
      "author" : [ "H. Chernoff" ],
      "venue" : null,
      "citeRegEx" : "Chernoff.,? \\Q1975\\E",
      "shortCiteRegEx" : "Chernoff.",
      "year" : 1975
    }, {
      "title" : "Sequential sampling with economics of selection procedures",
      "author" : [ "S.E. Chick", "P. Frazier" ],
      "venue" : "Management Science,",
      "citeRegEx" : "Chick and Frazier.,? \\Q2012\\E",
      "shortCiteRegEx" : "Chick and Frazier.",
      "year" : 2012
    }, {
      "title" : "Economic analysis of simulation selection problems",
      "author" : [ "S.E. Chick", "N. Gans" ],
      "venue" : "Management Science,",
      "citeRegEx" : "Chick and Gans.,? \\Q2009\\E",
      "shortCiteRegEx" : "Chick and Gans.",
      "year" : 2009
    }, {
      "title" : "Sequential sampling to myopically maximize the expected value of information",
      "author" : [ "S.E. Chick", "J. Branke", "C. Schmidt" ],
      "venue" : "INFORMS Journal on Computing,",
      "citeRegEx" : "Chick et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Chick et al\\.",
      "year" : 2010
    }, {
      "title" : "Pac bounds for multi-armed bandit and markov decision processes",
      "author" : [ "E. Even-Dar", "S. Mannor", "Y. Mansour" ],
      "venue" : "In Computational Learning Theory,",
      "citeRegEx" : "Even.Dar et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Even.Dar et al\\.",
      "year" : 2002
    }, {
      "title" : "A fully sequential elimination procedure for indifference-zone ranking and selection with tight bounds on probability of correct selection",
      "author" : [ "P.I. Frazier" ],
      "venue" : "Operations Research,",
      "citeRegEx" : "Frazier.,? \\Q2014\\E",
      "shortCiteRegEx" : "Frazier.",
      "year" : 2014
    }, {
      "title" : "A knowledge-gradient policy for sequential information collection",
      "author" : [ "P.I. Frazier", "W.B. Powell", "S. Dayanik" ],
      "venue" : "SIAM Journal on Control and Optimization,",
      "citeRegEx" : "Frazier et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Frazier et al\\.",
      "year" : 2008
    }, {
      "title" : "Best arm identification: A unified approach to fixed budget and fixed confidence",
      "author" : [ "V. Gabillon", "M. Ghavamzadeh", "A. Lazaric" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Gabillon et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Gabillon et al\\.",
      "year" : 2012
    }, {
      "title" : "Optimal best arm identification with fixed confidence",
      "author" : [ "Aurélien Garivier", "Emilie Kaufmann" ],
      "venue" : "In Conference on Learning Theory (COLT),",
      "citeRegEx" : "Garivier and Kaufmann.,? \\Q2016\\E",
      "shortCiteRegEx" : "Garivier and Kaufmann.",
      "year" : 2016
    }, {
      "title" : "Bandit processes and dynamic allocation indices",
      "author" : [ "J.C. Gittins" ],
      "venue" : "J. R. Statist. Soc. B,",
      "citeRegEx" : "Gittins.,? \\Q1979\\E",
      "shortCiteRegEx" : "Gittins.",
      "year" : 1979
    }, {
      "title" : "A dynamic allocation index for the sequential design of experiments",
      "author" : [ "J.C. Gittins", "D.M. Jones" ],
      "venue" : "In J. Gani, editor, Progress in Statistics,",
      "citeRegEx" : "Gittins and Jones.,? \\Q1974\\E",
      "shortCiteRegEx" : "Gittins and Jones.",
      "year" : 1974
    }, {
      "title" : "A large deviations perspective on ordinal optimization",
      "author" : [ "P. Glynn", "S. Juneja" ],
      "venue" : "In Simulation Conference,",
      "citeRegEx" : "Glynn and Juneja.,? \\Q2004\\E",
      "shortCiteRegEx" : "Glynn and Juneja.",
      "year" : 2004
    }, {
      "title" : "Ordinal optimization-empirical large deviations rate estimators, and stochastic multi-armed bandits",
      "author" : [ "P. Glynn", "S. Juneja" ],
      "venue" : "arXiv preprint arXiv:1507.04564,",
      "citeRegEx" : "Glynn and Juneja.,? \\Q2015\\E",
      "shortCiteRegEx" : "Glynn and Juneja.",
      "year" : 2015
    }, {
      "title" : "Thompson sampling for complex online problems",
      "author" : [ "A. Gopalan", "S. Mannor", "Y. Mansour" ],
      "venue" : "In Proceedings of The 31st International Conference on Machine Learning,",
      "citeRegEx" : "Gopalan et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Gopalan et al\\.",
      "year" : 2014
    }, {
      "title" : "Web-scale Bayesian click-through rate prediction for sponsored search advertising in Microsoft’s Bing search engine",
      "author" : [ "T. Graepel", "J.Q. Candela", "T. Borchert", "R. Herbrich" ],
      "venue" : "In Proceedings of the 27th International Conference on Machine Learning",
      "citeRegEx" : "Graepel et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Graepel et al\\.",
      "year" : 2010
    }, {
      "title" : "Entropy and information theory",
      "author" : [ "R.M. Gray" ],
      "venue" : null,
      "citeRegEx" : "Gray.,? \\Q2011\\E",
      "shortCiteRegEx" : "Gray.",
      "year" : 2011
    }, {
      "title" : "Bayesian look ahead one-stage sampling allocations for selection of the best population",
      "author" : [ "S.S. Gupta", "K.J. Miescke" ],
      "venue" : "Journal of statistical planning and inference,",
      "citeRegEx" : "Gupta and Miescke.,? \\Q1996\\E",
      "shortCiteRegEx" : "Gupta and Miescke.",
      "year" : 1996
    }, {
      "title" : "Optimistic gittins indices",
      "author" : [ "E. Gutin", "V. Farias" ],
      "venue" : "In Advances In Neural Information Processing Systems,",
      "citeRegEx" : "Gutin and Farias.,? \\Q2016\\E",
      "shortCiteRegEx" : "Gutin and Farias.",
      "year" : 2016
    }, {
      "title" : "Best-arm identification algorithms for multi-armed bandits in the fixed confidence setting",
      "author" : [ "K. Jamieson", "R. Nowak" ],
      "venue" : "In Information Sciences and Systems (CISS),",
      "citeRegEx" : "Jamieson and Nowak.,? \\Q2014\\E",
      "shortCiteRegEx" : "Jamieson and Nowak.",
      "year" : 2014
    }, {
      "title" : "Asymptotically optimal procedures for sequential adaptive selection of the best of several normal means",
      "author" : [ "C. Jennison", "I.M. Johnstone", "B.W. Turnbull" ],
      "venue" : "Statistical decision theory and related topics III,",
      "citeRegEx" : "Jennison et al\\.,? \\Q1982\\E",
      "shortCiteRegEx" : "Jennison et al\\.",
      "year" : 1982
    }, {
      "title" : "Almost optimal exploration in multi-armed bandits",
      "author" : [ "Z. Karnin", "T. Koren", "O. Somekh" ],
      "venue" : "In Proceedings of the 30th International Conference on Machine Learning",
      "citeRegEx" : "Karnin et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Karnin et al\\.",
      "year" : 2013
    }, {
      "title" : "Thompson sampling: an asymptotically optimal finite time analysis",
      "author" : [ "E. Kauffmann", "N. Korda", "R. Munos" ],
      "venue" : "In International Conference on Algorithmic Learning Theory,",
      "citeRegEx" : "Kauffmann et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Kauffmann et al\\.",
      "year" : 2012
    }, {
      "title" : "On bayesian index policies for sequential resource allocation",
      "author" : [ "E. Kaufmann" ],
      "venue" : "arXiv preprint arXiv:1601.01190,",
      "citeRegEx" : "Kaufmann.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kaufmann.",
      "year" : 2016
    }, {
      "title" : "Information complexity in bandit subset selection",
      "author" : [ "E. Kaufmann", "S. Kalyanakrishnan" ],
      "venue" : "In Conference on Learning Theory,",
      "citeRegEx" : "Kaufmann and Kalyanakrishnan.,? \\Q2013\\E",
      "shortCiteRegEx" : "Kaufmann and Kalyanakrishnan.",
      "year" : 2013
    }, {
      "title" : "On Bayesian upper confidence bounds for bandit problems",
      "author" : [ "E. Kaufmann", "O. Cappé", "A. Garivier" ],
      "venue" : "In Conference on Artificial Intelligence and Statistics (AISTATS),",
      "citeRegEx" : "Kaufmann et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Kaufmann et al\\.",
      "year" : 2012
    }, {
      "title" : "On the complexity of best arm identification in multiarmed bandit models",
      "author" : [ "E. Kaufmann", "O. Cappé", "A. Garivier" ],
      "venue" : "arXiv preprint arXiv:1407.4443,",
      "citeRegEx" : "Kaufmann et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kaufmann et al\\.",
      "year" : 2014
    }, {
      "title" : "Second order efficiency in the sequential design of experiments",
      "author" : [ "R. Keener" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "Keener.,? \\Q1984\\E",
      "shortCiteRegEx" : "Keener.",
      "year" : 1984
    }, {
      "title" : "Asymptotically optimum sequential inference and design",
      "author" : [ "J. Kiefer", "J. Sacks" ],
      "venue" : "The Annals of Mathematical Statistics,",
      "citeRegEx" : "Kiefer and Sacks.,? \\Q1963\\E",
      "shortCiteRegEx" : "Kiefer and Sacks.",
      "year" : 1963
    }, {
      "title" : "Selecting the best system",
      "author" : [ "S.-H. Kim", "B.L. Nelson" ],
      "venue" : "Handbooks in operations research and management science,",
      "citeRegEx" : "Kim and Nelson.,? \\Q2006\\E",
      "shortCiteRegEx" : "Kim and Nelson.",
      "year" : 2006
    }, {
      "title" : "Recent advances in ranking and selection. In Proceedings of the 39th conference on Winter simulation: 40 years! The best is yet to come, pages 162–172",
      "author" : [ "S.-H. Kim", "B.L. Nelson" ],
      "venue" : null,
      "citeRegEx" : "Kim and Nelson.,? \\Q2007\\E",
      "shortCiteRegEx" : "Kim and Nelson.",
      "year" : 2007
    }, {
      "title" : "Thompson sampling for one-dimensional exponential family bandits",
      "author" : [ "N. Korda", "E. Kaufmann", "R. Munos" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Korda et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Korda et al\\.",
      "year" : 2013
    }, {
      "title" : "Asymptotically efficient adaptive allocation rules",
      "author" : [ "T.L. Lai", "H. Robbins" ],
      "venue" : "Advances in applied mathematics,",
      "citeRegEx" : "Lai and Robbins.,? \\Q1985\\E",
      "shortCiteRegEx" : "Lai and Robbins.",
      "year" : 1985
    }, {
      "title" : "The sample complexity of exploration in the multi-armed bandit problem",
      "author" : [ "S. Mannor", "J.N. Tsitsiklis" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Mannor and Tsitsiklis.,? \\Q2004\\E",
      "shortCiteRegEx" : "Mannor and Tsitsiklis.",
      "year" : 2004
    }, {
      "title" : "Active sequential hypothesis testing",
      "author" : [ "M. Naghshvar", "T. Javidi" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "Naghshvar and Javidi,? \\Q2013\\E",
      "shortCiteRegEx" : "Naghshvar and Javidi",
      "year" : 2013
    }, {
      "title" : "Controlled sensing for multihypothesis testing",
      "author" : [ "S. Nitinawarat", "G. K Atia", "V.V. Veeravalli" ],
      "venue" : "IEEE Transactions on Automatic Control,",
      "citeRegEx" : "Nitinawarat et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Nitinawarat et al\\.",
      "year" : 2013
    }, {
      "title" : "A sequential procedure for selecting the population with the largest mean from k normal populations",
      "author" : [ "E. Paulson" ],
      "venue" : "The Annals of Mathematical Statistics,",
      "citeRegEx" : "Paulson.,? \\Q1964\\E",
      "shortCiteRegEx" : "Paulson.",
      "year" : 1964
    }, {
      "title" : "On two-stage selection procedures and related probability-inequalities",
      "author" : [ "Y. Rinott" ],
      "venue" : "Communications in Statistics-Theory and methods,",
      "citeRegEx" : "Rinott.,? \\Q1978\\E",
      "shortCiteRegEx" : "Rinott.",
      "year" : 1978
    }, {
      "title" : "Learning to optimize via information-directed sampling",
      "author" : [ "D. Russo", "B. Van Roy" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Russo and Roy.,? \\Q2014\\E",
      "shortCiteRegEx" : "Russo and Roy.",
      "year" : 2014
    }, {
      "title" : "How much does your data exploration overfit? Controlling bias via information usage",
      "author" : [ "D. Russo", "J. Zou" ],
      "venue" : "arXiv preprint arXiv:1511.05219,",
      "citeRegEx" : "Russo and Zou.,? \\Q2015\\E",
      "shortCiteRegEx" : "Russo and Zou.",
      "year" : 2015
    }, {
      "title" : "On the convergence rates of expected improvement methods",
      "author" : [ "I.O. Ryzhov" ],
      "venue" : "Operations Research,",
      "citeRegEx" : "Ryzhov.,? \\Q2016\\E",
      "shortCiteRegEx" : "Ryzhov.",
      "year" : 2016
    }, {
      "title" : "The knowledge gradient algorithm for a general class of online learning problems",
      "author" : [ "I.O. Ryzhov", "W.B. Powell", "P.I. Frazier" ],
      "venue" : "Operations Research,",
      "citeRegEx" : "Ryzhov et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Ryzhov et al\\.",
      "year" : 2012
    }, {
      "title" : "Overview of content experiments: Multi-armed bandit experiments, 2016",
      "author" : [ "S.L. Scott" ],
      "venue" : "URL https: //support.google.com/analytics/answer/2844870?hl=en. [Online; accessed 9-November2016]",
      "citeRegEx" : "Scott.,? \\Q2016\\E",
      "shortCiteRegEx" : "Scott.",
      "year" : 2016
    }, {
      "title" : "Information-theoretic regret bounds for Gaussian process optimization in the bandit setting",
      "author" : [ "N. Srinivas", "A. Krause", "S.M. Kakade", "M. Seeger" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "Srinivas et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Srinivas et al\\.",
      "year" : 2012
    }, {
      "title" : "Automatic ad format selection via contextual bandits",
      "author" : [ "L. Tang", "R. Rosales", "A. Singh", "D. Agarwal" ],
      "venue" : "In Proceedings of the 22nd ACM international conference on Conference on information & knowledge management,",
      "citeRegEx" : "Tang et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2013
    }, {
      "title" : "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples",
      "author" : [ "W.R. Thompson" ],
      "venue" : null,
      "citeRegEx" : "Thompson.,? \\Q1933\\E",
      "shortCiteRegEx" : "Thompson.",
      "year" : 1933
    }, {
      "title" : "Multi-armed bandit models for the optimal design of clinical trials: Benefits and challenges",
      "author" : [ "S.S. Villar", "J. Bowden", "J. Wason" ],
      "venue" : "Statistical Science,",
      "citeRegEx" : "Villar et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Villar et al\\.",
      "year" : 2015
    }, {
      "title" : "Probability with martingales",
      "author" : [ "D. Williams" ],
      "venue" : "Cambridge university press,",
      "citeRegEx" : "Williams.,? \\Q1991\\E",
      "shortCiteRegEx" : "Williams.",
      "year" : 1991
    }, {
      "title" : "As a result, there is no problem with finite k for which the sampling ratios in (14) are optimal. One can show, in fact, that any optimal multi-armed bandit algorithm that attains the lower bound of Lai and Robbins [1985] also satisfies equations (13) and (14). The main innovation in this paper is to show how to build on such bandit algorithms to attain near-optimal rates for the best-arm identification",
      "author" : [ "Glynn", "Juneja", "Jennison" ],
      "venue" : null,
      "citeRegEx" : "Glynn et al\\.,? \\Q1982\\E",
      "shortCiteRegEx" : "Glynn et al\\.",
      "year" : 1982
    }, {
      "title" : "2016] also studies the knowledge gradient policy, which could offer improved performance",
      "author" : [ "problem. Ryzhov" ],
      "venue" : null,
      "citeRegEx" : "Ryzhov,? \\Q2016\\E",
      "shortCiteRegEx" : "Ryzhov",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 54,
      "context" : "1 Multi-armed bandit models of clinical trails date back to Thompson [1933], but bandit algorithms lack statistical power in detecting the best treatment at the end of the trial [Villar et al., 2015].",
      "startOffset" : 178,
      "endOffset" : 199
    }, {
      "referenceID" : 53,
      "context" : "1 Multi-armed bandit models of clinical trails date back to Thompson [1933], but bandit algorithms lack statistical power in detecting the best treatment at the end of the trial [Villar et al.",
      "startOffset" : 60,
      "endOffset" : 76
    }, {
      "referenceID" : 4,
      "context" : "However, we will see that optimal rules from this perspective also allocate fewer patients to very poor treatments, potentially leading to more ethical trials [Berry, 2004].",
      "startOffset" : 159,
      "endOffset" : 172
    }, {
      "referenceID" : 50,
      "context" : "For example, there are various reports of Thompson sampling being used in A/B testing [Scott, 2016] and in clinical trials [Berry, 2004].",
      "startOffset" : 86,
      "endOffset" : 99
    }, {
      "referenceID" : 4,
      "context" : "For example, there are various reports of Thompson sampling being used in A/B testing [Scott, 2016] and in clinical trials [Berry, 2004].",
      "startOffset" : 123,
      "endOffset" : 136
    }, {
      "referenceID" : 49,
      "context" : ", 2015], information-directed sampling [Russo and Van Roy, 2014], the knowledge gradient [Ryzhov et al., 2012], and optimistic Gittins indices [Gutin and Farias, 2016].",
      "startOffset" : 89,
      "endOffset" : 110
    }, {
      "referenceID" : 26,
      "context" : ", 2012], and optimistic Gittins indices [Gutin and Farias, 2016].",
      "startOffset" : 40,
      "endOffset" : 64
    }, {
      "referenceID" : 11,
      "context" : "Recent work has cast this problem in a decision-theoretic framework [Chick and Gans, 2009].",
      "startOffset" : 68,
      "endOffset" : 90
    }, {
      "referenceID" : 10,
      "context" : ", 2010] or a continuous-time problem with only two alternatives [Chick and Frazier, 2012] – and then extend those solutions heuristically to build measurement and stopping rules in more general settings.",
      "startOffset" : 64,
      "endOffset" : 89
    }, {
      "referenceID" : 0,
      "context" : ", 2012, Kaufmann, 2016], Thompson sampling [Agrawal and Goyal, 2012, Korda et al., 2013, Gopalan et al., 2014, Johnson et al., 2015], information-directed sampling [Russo and Van Roy, 2014], the knowledge gradient [Ryzhov et al., 2012], and optimistic Gittins indices [Gutin and Farias, 2016]. These heuristic algorithms can be applied effectively to complicated learning problems beyond the specialized settings in which the Gittins index theorem holds, have been shown to have strong performance in simulation, and have theoretical performance guarantees. In several cases, they are known to attain sharp asymptotic limits on the performance of any adaptive algorithm due to Lai and Robbins [1985]. The pure-exploration problem studied in this paper is not nearly as well understood.",
      "startOffset" : 44,
      "endOffset" : 700
    }, {
      "referenceID" : 47,
      "context" : "work by Ryzhov [2016] studies the long run distribution of measurement effort allocated by the expected-improvement and shows this is related to the optimal computing budget allocation of Chen et al.",
      "startOffset" : 8,
      "endOffset" : 22
    }, {
      "referenceID" : 7,
      "context" : "work by Ryzhov [2016] studies the long run distribution of measurement effort allocated by the expected-improvement and shows this is related to the optimal computing budget allocation of Chen et al. [2000]. This contribution is very similar in spirit to this paper, as it relates the longrun behavior of a simple Bayesian measurement strategy to a notion of an approximately optimal allocation.",
      "startOffset" : 188,
      "endOffset" : 207
    }, {
      "referenceID" : 33,
      "context" : "See Kim and Nelson [2006] and Kim and Nelson [2007] for reviews.",
      "startOffset" : 4,
      "endOffset" : 26
    }, {
      "referenceID" : 33,
      "context" : "See Kim and Nelson [2006] and Kim and Nelson [2007] for reviews.",
      "startOffset" : 4,
      "endOffset" : 52
    }, {
      "referenceID" : 3,
      "context" : "Beginning with Bechhofer [1954], many papers have focused on an indifference zone formulation, where, for user-specified , δ > 0, the goal is to guarantee with probability at least 1 − δ the algorithm returns a design within of optimal.",
      "startOffset" : 15,
      "endOffset" : 32
    }, {
      "referenceID" : 3,
      "context" : "Beginning with Bechhofer [1954], many papers have focused on an indifference zone formulation, where, for user-specified , δ > 0, the goal is to guarantee with probability at least 1 − δ the algorithm returns a design within of optimal. Assuming measurement noise is Gaussian with known variance σ2, one can guarantee this indifference-zone criterion by gathering O ( (σk/ 2) log(k/δ) ) total measurements, divided equally among the k designs, and then returning the design with highest empirical-mean. For the case of unknown variances, Rinott [1978] proposes a two stage procedure, where the first stage is used to estimate the variance of each population, and the number of samples collected from each design in the second stage is scaled by its estimated standard deviation.",
      "startOffset" : 15,
      "endOffset" : 552
    }, {
      "referenceID" : 3,
      "context" : "Beginning with Bechhofer [1954], many papers have focused on an indifference zone formulation, where, for user-specified , δ > 0, the goal is to guarantee with probability at least 1 − δ the algorithm returns a design within of optimal. Assuming measurement noise is Gaussian with known variance σ2, one can guarantee this indifference-zone criterion by gathering O ( (σk/ 2) log(k/δ) ) total measurements, divided equally among the k designs, and then returning the design with highest empirical-mean. For the case of unknown variances, Rinott [1978] proposes a two stage procedure, where the first stage is used to estimate the variance of each population, and the number of samples collected from each design in the second stage is scaled by its estimated standard deviation. In the machine learning literature, Even-Dar et al. [2002] shows that when measurement noise is uniformly bounded, the indifferencezone criterion is satisfied by a sequential elimination strategy that uses only O ( (k/ 2) log(1/δ) ) samples on average.",
      "startOffset" : 15,
      "endOffset" : 838
    }, {
      "referenceID" : 3,
      "context" : "Beginning with Bechhofer [1954], many papers have focused on an indifference zone formulation, where, for user-specified , δ > 0, the goal is to guarantee with probability at least 1 − δ the algorithm returns a design within of optimal. Assuming measurement noise is Gaussian with known variance σ2, one can guarantee this indifference-zone criterion by gathering O ( (σk/ 2) log(k/δ) ) total measurements, divided equally among the k designs, and then returning the design with highest empirical-mean. For the case of unknown variances, Rinott [1978] proposes a two stage procedure, where the first stage is used to estimate the variance of each population, and the number of samples collected from each design in the second stage is scaled by its estimated standard deviation. In the machine learning literature, Even-Dar et al. [2002] shows that when measurement noise is uniformly bounded, the indifferencezone criterion is satisfied by a sequential elimination strategy that uses only O ( (k/ 2) log(1/δ) ) samples on average. Mannor and Tsitsiklis [2004] provide a matching lower bound.",
      "startOffset" : 15,
      "endOffset" : 1061
    }, {
      "referenceID" : 3,
      "context" : "Beginning with Bechhofer [1954], many papers have focused on an indifference zone formulation, where, for user-specified , δ > 0, the goal is to guarantee with probability at least 1 − δ the algorithm returns a design within of optimal. Assuming measurement noise is Gaussian with known variance σ2, one can guarantee this indifference-zone criterion by gathering O ( (σk/ 2) log(k/δ) ) total measurements, divided equally among the k designs, and then returning the design with highest empirical-mean. For the case of unknown variances, Rinott [1978] proposes a two stage procedure, where the first stage is used to estimate the variance of each population, and the number of samples collected from each design in the second stage is scaled by its estimated standard deviation. In the machine learning literature, Even-Dar et al. [2002] shows that when measurement noise is uniformly bounded, the indifferencezone criterion is satisfied by a sequential elimination strategy that uses only O ( (k/ 2) log(1/δ) ) samples on average. Mannor and Tsitsiklis [2004] provide a matching lower bound. Similar to minimax bounds, this shows the upper bound of Even-Dar et al. [2002] is tight, up to a constant factor, for a certain worst case problem instance.",
      "startOffset" : 15,
      "endOffset" : 1173
    }, {
      "referenceID" : 3,
      "context" : "Beginning with Bechhofer [1954], many papers have focused on an indifference zone formulation, where, for user-specified , δ > 0, the goal is to guarantee with probability at least 1 − δ the algorithm returns a design within of optimal. Assuming measurement noise is Gaussian with known variance σ2, one can guarantee this indifference-zone criterion by gathering O ( (σk/ 2) log(k/δ) ) total measurements, divided equally among the k designs, and then returning the design with highest empirical-mean. For the case of unknown variances, Rinott [1978] proposes a two stage procedure, where the first stage is used to estimate the variance of each population, and the number of samples collected from each design in the second stage is scaled by its estimated standard deviation. In the machine learning literature, Even-Dar et al. [2002] shows that when measurement noise is uniformly bounded, the indifferencezone criterion is satisfied by a sequential elimination strategy that uses only O ( (k/ 2) log(1/δ) ) samples on average. Mannor and Tsitsiklis [2004] provide a matching lower bound. Similar to minimax bounds, this shows the upper bound of Even-Dar et al. [2002] is tight, up to a constant factor, for a certain worst case problem instance. Since Paulson [1964], many authors have sought to reduce the number of samples required on easier problem instances by designing algorithms that sequentially eliminate arms once they are determined to be suboptimal with high confidence.",
      "startOffset" : 15,
      "endOffset" : 1272
    }, {
      "referenceID" : 3,
      "context" : "Beginning with Bechhofer [1954], many papers have focused on an indifference zone formulation, where, for user-specified , δ > 0, the goal is to guarantee with probability at least 1 − δ the algorithm returns a design within of optimal. Assuming measurement noise is Gaussian with known variance σ2, one can guarantee this indifference-zone criterion by gathering O ( (σk/ 2) log(k/δ) ) total measurements, divided equally among the k designs, and then returning the design with highest empirical-mean. For the case of unknown variances, Rinott [1978] proposes a two stage procedure, where the first stage is used to estimate the variance of each population, and the number of samples collected from each design in the second stage is scaled by its estimated standard deviation. In the machine learning literature, Even-Dar et al. [2002] shows that when measurement noise is uniformly bounded, the indifferencezone criterion is satisfied by a sequential elimination strategy that uses only O ( (k/ 2) log(1/δ) ) samples on average. Mannor and Tsitsiklis [2004] provide a matching lower bound. Similar to minimax bounds, this shows the upper bound of Even-Dar et al. [2002] is tight, up to a constant factor, for a certain worst case problem instance. Since Paulson [1964], many authors have sought to reduce the number of samples required on easier problem instances by designing algorithms that sequentially eliminate arms once they are determined to be suboptimal with high confidence. See the recent work of Frazier [2014] and the references therein.",
      "startOffset" : 15,
      "endOffset" : 1526
    }, {
      "referenceID" : 3,
      "context" : "Beginning with Bechhofer [1954], many papers have focused on an indifference zone formulation, where, for user-specified , δ > 0, the goal is to guarantee with probability at least 1 − δ the algorithm returns a design within of optimal. Assuming measurement noise is Gaussian with known variance σ2, one can guarantee this indifference-zone criterion by gathering O ( (σk/ 2) log(k/δ) ) total measurements, divided equally among the k designs, and then returning the design with highest empirical-mean. For the case of unknown variances, Rinott [1978] proposes a two stage procedure, where the first stage is used to estimate the variance of each population, and the number of samples collected from each design in the second stage is scaled by its estimated standard deviation. In the machine learning literature, Even-Dar et al. [2002] shows that when measurement noise is uniformly bounded, the indifferencezone criterion is satisfied by a sequential elimination strategy that uses only O ( (k/ 2) log(1/δ) ) samples on average. Mannor and Tsitsiklis [2004] provide a matching lower bound. Similar to minimax bounds, this shows the upper bound of Even-Dar et al. [2002] is tight, up to a constant factor, for a certain worst case problem instance. Since Paulson [1964], many authors have sought to reduce the number of samples required on easier problem instances by designing algorithms that sequentially eliminate arms once they are determined to be suboptimal with high confidence. See the recent work of Frazier [2014] and the references therein. However, in a sense described below, Jennison et al. [1982] show formally that there are problems with Gaussian observations where any sequential-elimination algorithm will require substantially more samples than optimal adaptive allocation rules.",
      "startOffset" : 15,
      "endOffset" : 1614
    }, {
      "referenceID" : 20,
      "context" : ", 1982], simulation optimization [Glynn and Juneja, 2004], and, concurrently with this paper, in the machine learning literature [Garivier and Kaufmann, 2016].",
      "startOffset" : 33,
      "endOffset" : 57
    }, {
      "referenceID" : 17,
      "context" : ", 1982], simulation optimization [Glynn and Juneja, 2004], and, concurrently with this paper, in the machine learning literature [Garivier and Kaufmann, 2016].",
      "startOffset" : 129,
      "endOffset" : 158
    }, {
      "referenceID" : 11,
      "context" : "We described attainable rates of performance on a worst-case problem instance characterized by Even-Dar et al. [2002] and Mannor and Tsitsiklis [2004].",
      "startOffset" : 95,
      "endOffset" : 118
    }, {
      "referenceID" : 11,
      "context" : "We described attainable rates of performance on a worst-case problem instance characterized by Even-Dar et al. [2002] and Mannor and Tsitsiklis [2004]. A great deal of work has sought “problem dependent” bounds, which reveal that the best-arm can be identified more rapidly when the true problem instance is easier.",
      "startOffset" : 95,
      "endOffset" : 151
    }, {
      "referenceID" : 5,
      "context" : "Glynn and Juneja [2004] build on the optimal-computing-budget allocation (OCBA) of Chen et al. [2000] to provide a rigorous large-deviations derivation of the optimal fixed allocation.",
      "startOffset" : 83,
      "endOffset" : 102
    }, {
      "referenceID" : 5,
      "context" : "Glynn and Juneja [2004] build on the optimal-computing-budget allocation (OCBA) of Chen et al. [2000] to provide a rigorous large-deviations derivation of the optimal fixed allocation. In particular, assuming the design with the highest empirical mean is returned, there is a fixed allocation under which the probability of incorrect selection decays exponentially, and the exponent is optimal under all fixed-allocation rules. The setting studied by this paper is often called the “fixed-budget” setting in the recent multi-armed bandit literature. Unfortunately, it may be difficult to implement the allocation in Glynn and Juneja [2004] without additional prior knowledge.",
      "startOffset" : 83,
      "endOffset" : 640
    }, {
      "referenceID" : 5,
      "context" : "Glynn and Juneja [2004] build on the optimal-computing-budget allocation (OCBA) of Chen et al. [2000] to provide a rigorous large-deviations derivation of the optimal fixed allocation. In particular, assuming the design with the highest empirical mean is returned, there is a fixed allocation under which the probability of incorrect selection decays exponentially, and the exponent is optimal under all fixed-allocation rules. The setting studied by this paper is often called the “fixed-budget” setting in the recent multi-armed bandit literature. Unfortunately, it may be difficult to implement the allocation in Glynn and Juneja [2004] without additional prior knowledge. Later work by Glynn and Juneja [2015] provides a substantial discussion of this issue.",
      "startOffset" : 83,
      "endOffset" : 714
    }, {
      "referenceID" : 5,
      "context" : "Glynn and Juneja [2004] build on the optimal-computing-budget allocation (OCBA) of Chen et al. [2000] to provide a rigorous large-deviations derivation of the optimal fixed allocation. In particular, assuming the design with the highest empirical mean is returned, there is a fixed allocation under which the probability of incorrect selection decays exponentially, and the exponent is optimal under all fixed-allocation rules. The setting studied by this paper is often called the “fixed-budget” setting in the recent multi-armed bandit literature. Unfortunately, it may be difficult to implement the allocation in Glynn and Juneja [2004] without additional prior knowledge. Later work by Glynn and Juneja [2015] provides a substantial discussion of this issue. This paper was highly influenced by a classic paper by Chernoff [1959] on the sequential design of experiments for binary hypothesis testing.",
      "startOffset" : 83,
      "endOffset" : 834
    }, {
      "referenceID" : 5,
      "context" : "Glynn and Juneja [2004] build on the optimal-computing-budget allocation (OCBA) of Chen et al. [2000] to provide a rigorous large-deviations derivation of the optimal fixed allocation. In particular, assuming the design with the highest empirical mean is returned, there is a fixed allocation under which the probability of incorrect selection decays exponentially, and the exponent is optimal under all fixed-allocation rules. The setting studied by this paper is often called the “fixed-budget” setting in the recent multi-armed bandit literature. Unfortunately, it may be difficult to implement the allocation in Glynn and Juneja [2004] without additional prior knowledge. Later work by Glynn and Juneja [2015] provides a substantial discussion of this issue. This paper was highly influenced by a classic paper by Chernoff [1959] on the sequential design of experiments for binary hypothesis testing. Chernoff’s asymptotic derivations give great insight best-arm identification, which can be formulated as a multiple-hypothesis testing problem with sequentially chosen experiments, but surprisingly this connection does not seem to be discussed in the literature. Chernoff looks at a different scaling than Glynn and Juneja [2004]. Rather than take the budget of available measurements to infinity, he allows the algorithm to stop and declare the hypothesis true or false at any time, but takes the cost of gathering measurements to zero while the cost of an incorrect terminal decision stays fixed.",
      "startOffset" : 83,
      "endOffset" : 1235
    }, {
      "referenceID" : 1,
      "context" : "Chernoff makes restrictive technical assumptions, some of which have been removed in subsequent work [Albert, 1961, Kiefer and Sacks, 1963, Keener, 1984, Nitinawarat et al., 2013, Naghshvar et al., 2013]. Jennison et al. [1982] study an indifference zone formulation of the problem of identifying the best-design.",
      "startOffset" : 102,
      "endOffset" : 228
    }, {
      "referenceID" : 1,
      "context" : "Chernoff makes restrictive technical assumptions, some of which have been removed in subsequent work [Albert, 1961, Kiefer and Sacks, 1963, Keener, 1984, Nitinawarat et al., 2013, Naghshvar et al., 2013]. Jennison et al. [1982] study an indifference zone formulation of the problem of identifying the best-design. Like Chernoff [1959], they allow the algorithm to stop and return an estimate of the best-arm at any time, but rather than penalize incorrect decisions, they require that the probability correct selection (PCS) exceeds 1 − δ > 0 for every problem instance.",
      "startOffset" : 102,
      "endOffset" : 335
    }, {
      "referenceID" : 1,
      "context" : "Chernoff makes restrictive technical assumptions, some of which have been removed in subsequent work [Albert, 1961, Kiefer and Sacks, 1963, Keener, 1984, Nitinawarat et al., 2013, Naghshvar et al., 2013]. Jennison et al. [1982] study an indifference zone formulation of the problem of identifying the best-design. Like Chernoff [1959], they allow the algorithm to stop and return an estimate of the best-arm at any time, but rather than penalize incorrect decisions, they require that the probability correct selection (PCS) exceeds 1 − δ > 0 for every problem instance. Intuitively, the expected number of samples required by an algorithm satisfying this PCS constraint must tend to infinity as δ → 0. In the case of Gaussian measurement noise, Jennison et al. [1982] characterize the optimal asymptotic scaling of expected number of samples in this limit.",
      "startOffset" : 102,
      "endOffset" : 769
    }, {
      "referenceID" : 1,
      "context" : "Chernoff makes restrictive technical assumptions, some of which have been removed in subsequent work [Albert, 1961, Kiefer and Sacks, 1963, Keener, 1984, Nitinawarat et al., 2013, Naghshvar et al., 2013]. Jennison et al. [1982] study an indifference zone formulation of the problem of identifying the best-design. Like Chernoff [1959], they allow the algorithm to stop and return an estimate of the best-arm at any time, but rather than penalize incorrect decisions, they require that the probability correct selection (PCS) exceeds 1 − δ > 0 for every problem instance. Intuitively, the expected number of samples required by an algorithm satisfying this PCS constraint must tend to infinity as δ → 0. In the case of Gaussian measurement noise, Jennison et al. [1982] characterize the optimal asymptotic scaling of expected number of samples in this limit. The recent multi-armed bandit literature refers to this formulation as the “fixed-confidence” setting. A large body of work in the recent machine learning literature has sought to characterize various notions of the complexity of best-arm identification [Even-Dar et al., 2002, Mannor and Tsitsiklis, 2004, Audibert and Bubeck, 2010, Gabillon et al., 2012, Karnin et al., 2013, Jamieson and Nowak, 2014]. However, upper and lower bounds match only up to constant or logarithmic factors, and only for particular hard problem instances. Substantial progress was presented by Kaufmann and Kalyanakrishnan [2013] and Kaufmann et al.",
      "startOffset" : 102,
      "endOffset" : 1467
    }, {
      "referenceID" : 1,
      "context" : "Chernoff makes restrictive technical assumptions, some of which have been removed in subsequent work [Albert, 1961, Kiefer and Sacks, 1963, Keener, 1984, Nitinawarat et al., 2013, Naghshvar et al., 2013]. Jennison et al. [1982] study an indifference zone formulation of the problem of identifying the best-design. Like Chernoff [1959], they allow the algorithm to stop and return an estimate of the best-arm at any time, but rather than penalize incorrect decisions, they require that the probability correct selection (PCS) exceeds 1 − δ > 0 for every problem instance. Intuitively, the expected number of samples required by an algorithm satisfying this PCS constraint must tend to infinity as δ → 0. In the case of Gaussian measurement noise, Jennison et al. [1982] characterize the optimal asymptotic scaling of expected number of samples in this limit. The recent multi-armed bandit literature refers to this formulation as the “fixed-confidence” setting. A large body of work in the recent machine learning literature has sought to characterize various notions of the complexity of best-arm identification [Even-Dar et al., 2002, Mannor and Tsitsiklis, 2004, Audibert and Bubeck, 2010, Gabillon et al., 2012, Karnin et al., 2013, Jamieson and Nowak, 2014]. However, upper and lower bounds match only up to constant or logarithmic factors, and only for particular hard problem instances. Substantial progress was presented by Kaufmann and Kalyanakrishnan [2013] and Kaufmann et al. [2014], who seek to exactly characterize the asymptotic complexity of identifying the best arm in both the fixed-budget and fixed-confidence settings.",
      "startOffset" : 102,
      "endOffset" : 1494
    }, {
      "referenceID" : 1,
      "context" : "Chernoff makes restrictive technical assumptions, some of which have been removed in subsequent work [Albert, 1961, Kiefer and Sacks, 1963, Keener, 1984, Nitinawarat et al., 2013, Naghshvar et al., 2013]. Jennison et al. [1982] study an indifference zone formulation of the problem of identifying the best-design. Like Chernoff [1959], they allow the algorithm to stop and return an estimate of the best-arm at any time, but rather than penalize incorrect decisions, they require that the probability correct selection (PCS) exceeds 1 − δ > 0 for every problem instance. Intuitively, the expected number of samples required by an algorithm satisfying this PCS constraint must tend to infinity as δ → 0. In the case of Gaussian measurement noise, Jennison et al. [1982] characterize the optimal asymptotic scaling of expected number of samples in this limit. The recent multi-armed bandit literature refers to this formulation as the “fixed-confidence” setting. A large body of work in the recent machine learning literature has sought to characterize various notions of the complexity of best-arm identification [Even-Dar et al., 2002, Mannor and Tsitsiklis, 2004, Audibert and Bubeck, 2010, Gabillon et al., 2012, Karnin et al., 2013, Jamieson and Nowak, 2014]. However, upper and lower bounds match only up to constant or logarithmic factors, and only for particular hard problem instances. Substantial progress was presented by Kaufmann and Kalyanakrishnan [2013] and Kaufmann et al. [2014], who seek to exactly characterize the asymptotic complexity of identifying the best arm in both the fixed-budget and fixed-confidence settings. Still, the upper and lower bounds presented there do not match. A short abstract of the current paper appeared in the 2016 Conference on Learning Theory. In the same conference, independent work by Garivier and Kaufmann [2016] provided matching upper and lower bounds on the complexity of identifying the best arm in the “fixed-confidence” setting.",
      "startOffset" : 102,
      "endOffset" : 1865
    }, {
      "referenceID" : 1,
      "context" : "Chernoff makes restrictive technical assumptions, some of which have been removed in subsequent work [Albert, 1961, Kiefer and Sacks, 1963, Keener, 1984, Nitinawarat et al., 2013, Naghshvar et al., 2013]. Jennison et al. [1982] study an indifference zone formulation of the problem of identifying the best-design. Like Chernoff [1959], they allow the algorithm to stop and return an estimate of the best-arm at any time, but rather than penalize incorrect decisions, they require that the probability correct selection (PCS) exceeds 1 − δ > 0 for every problem instance. Intuitively, the expected number of samples required by an algorithm satisfying this PCS constraint must tend to infinity as δ → 0. In the case of Gaussian measurement noise, Jennison et al. [1982] characterize the optimal asymptotic scaling of expected number of samples in this limit. The recent multi-armed bandit literature refers to this formulation as the “fixed-confidence” setting. A large body of work in the recent machine learning literature has sought to characterize various notions of the complexity of best-arm identification [Even-Dar et al., 2002, Mannor and Tsitsiklis, 2004, Audibert and Bubeck, 2010, Gabillon et al., 2012, Karnin et al., 2013, Jamieson and Nowak, 2014]. However, upper and lower bounds match only up to constant or logarithmic factors, and only for particular hard problem instances. Substantial progress was presented by Kaufmann and Kalyanakrishnan [2013] and Kaufmann et al. [2014], who seek to exactly characterize the asymptotic complexity of identifying the best arm in both the fixed-budget and fixed-confidence settings. Still, the upper and lower bounds presented there do not match. A short abstract of the current paper appeared in the 2016 Conference on Learning Theory. In the same conference, independent work by Garivier and Kaufmann [2016] provided matching upper and lower bounds on the complexity of identifying the best arm in the “fixed-confidence” setting. Like the present paper, but unlike Jennison et al. [1982], these results apply whenever observation distributions are in the exponential family and do not require an indifference zone.",
      "startOffset" : 102,
      "endOffset" : 2045
    }, {
      "referenceID" : 1,
      "context" : "Chernoff makes restrictive technical assumptions, some of which have been removed in subsequent work [Albert, 1961, Kiefer and Sacks, 1963, Keener, 1984, Nitinawarat et al., 2013, Naghshvar et al., 2013]. Jennison et al. [1982] study an indifference zone formulation of the problem of identifying the best-design. Like Chernoff [1959], they allow the algorithm to stop and return an estimate of the best-arm at any time, but rather than penalize incorrect decisions, they require that the probability correct selection (PCS) exceeds 1 − δ > 0 for every problem instance. Intuitively, the expected number of samples required by an algorithm satisfying this PCS constraint must tend to infinity as δ → 0. In the case of Gaussian measurement noise, Jennison et al. [1982] characterize the optimal asymptotic scaling of expected number of samples in this limit. The recent multi-armed bandit literature refers to this formulation as the “fixed-confidence” setting. A large body of work in the recent machine learning literature has sought to characterize various notions of the complexity of best-arm identification [Even-Dar et al., 2002, Mannor and Tsitsiklis, 2004, Audibert and Bubeck, 2010, Gabillon et al., 2012, Karnin et al., 2013, Jamieson and Nowak, 2014]. However, upper and lower bounds match only up to constant or logarithmic factors, and only for particular hard problem instances. Substantial progress was presented by Kaufmann and Kalyanakrishnan [2013] and Kaufmann et al. [2014], who seek to exactly characterize the asymptotic complexity of identifying the best arm in both the fixed-budget and fixed-confidence settings. Still, the upper and lower bounds presented there do not match. A short abstract of the current paper appeared in the 2016 Conference on Learning Theory. In the same conference, independent work by Garivier and Kaufmann [2016] provided matching upper and lower bounds on the complexity of identifying the best arm in the “fixed-confidence” setting. Like the present paper, but unlike Jennison et al. [1982], these results apply whenever observation distributions are in the exponential family and do not require an indifference zone. The current paper looks at a different measure. We study a frequentist setting in which the true quality of each design is fixed, and characterize the rate of posterior convergence attainable for each problem instance. We also describe, as a function of the problem instance, the long-run fraction of measurement effort allocated to each design by any algorithm attaining this rate of convergence. These asymptotic limits turn out to be closely related to some of the aforementioned results. In particular, the optimal exponent given in Subsection 6.4 mirrors the complexity measure of Chernoff [1959]. This exponent is then simplified into a form that mirrors one derived by Glynn and Juneja [2004], and, for Gaussian distributions, one derived by Jennison et al.",
      "startOffset" : 102,
      "endOffset" : 2774
    }, {
      "referenceID" : 1,
      "context" : "Chernoff makes restrictive technical assumptions, some of which have been removed in subsequent work [Albert, 1961, Kiefer and Sacks, 1963, Keener, 1984, Nitinawarat et al., 2013, Naghshvar et al., 2013]. Jennison et al. [1982] study an indifference zone formulation of the problem of identifying the best-design. Like Chernoff [1959], they allow the algorithm to stop and return an estimate of the best-arm at any time, but rather than penalize incorrect decisions, they require that the probability correct selection (PCS) exceeds 1 − δ > 0 for every problem instance. Intuitively, the expected number of samples required by an algorithm satisfying this PCS constraint must tend to infinity as δ → 0. In the case of Gaussian measurement noise, Jennison et al. [1982] characterize the optimal asymptotic scaling of expected number of samples in this limit. The recent multi-armed bandit literature refers to this formulation as the “fixed-confidence” setting. A large body of work in the recent machine learning literature has sought to characterize various notions of the complexity of best-arm identification [Even-Dar et al., 2002, Mannor and Tsitsiklis, 2004, Audibert and Bubeck, 2010, Gabillon et al., 2012, Karnin et al., 2013, Jamieson and Nowak, 2014]. However, upper and lower bounds match only up to constant or logarithmic factors, and only for particular hard problem instances. Substantial progress was presented by Kaufmann and Kalyanakrishnan [2013] and Kaufmann et al. [2014], who seek to exactly characterize the asymptotic complexity of identifying the best arm in both the fixed-budget and fixed-confidence settings. Still, the upper and lower bounds presented there do not match. A short abstract of the current paper appeared in the 2016 Conference on Learning Theory. In the same conference, independent work by Garivier and Kaufmann [2016] provided matching upper and lower bounds on the complexity of identifying the best arm in the “fixed-confidence” setting. Like the present paper, but unlike Jennison et al. [1982], these results apply whenever observation distributions are in the exponential family and do not require an indifference zone. The current paper looks at a different measure. We study a frequentist setting in which the true quality of each design is fixed, and characterize the rate of posterior convergence attainable for each problem instance. We also describe, as a function of the problem instance, the long-run fraction of measurement effort allocated to each design by any algorithm attaining this rate of convergence. These asymptotic limits turn out to be closely related to some of the aforementioned results. In particular, the optimal exponent given in Subsection 6.4 mirrors the complexity measure of Chernoff [1959]. This exponent is then simplified into a form that mirrors one derived by Glynn and Juneja [2004], and, for Gaussian distributions, one derived by Jennison et al.",
      "startOffset" : 102,
      "endOffset" : 2872
    }, {
      "referenceID" : 1,
      "context" : "Chernoff makes restrictive technical assumptions, some of which have been removed in subsequent work [Albert, 1961, Kiefer and Sacks, 1963, Keener, 1984, Nitinawarat et al., 2013, Naghshvar et al., 2013]. Jennison et al. [1982] study an indifference zone formulation of the problem of identifying the best-design. Like Chernoff [1959], they allow the algorithm to stop and return an estimate of the best-arm at any time, but rather than penalize incorrect decisions, they require that the probability correct selection (PCS) exceeds 1 − δ > 0 for every problem instance. Intuitively, the expected number of samples required by an algorithm satisfying this PCS constraint must tend to infinity as δ → 0. In the case of Gaussian measurement noise, Jennison et al. [1982] characterize the optimal asymptotic scaling of expected number of samples in this limit. The recent multi-armed bandit literature refers to this formulation as the “fixed-confidence” setting. A large body of work in the recent machine learning literature has sought to characterize various notions of the complexity of best-arm identification [Even-Dar et al., 2002, Mannor and Tsitsiklis, 2004, Audibert and Bubeck, 2010, Gabillon et al., 2012, Karnin et al., 2013, Jamieson and Nowak, 2014]. However, upper and lower bounds match only up to constant or logarithmic factors, and only for particular hard problem instances. Substantial progress was presented by Kaufmann and Kalyanakrishnan [2013] and Kaufmann et al. [2014], who seek to exactly characterize the asymptotic complexity of identifying the best arm in both the fixed-budget and fixed-confidence settings. Still, the upper and lower bounds presented there do not match. A short abstract of the current paper appeared in the 2016 Conference on Learning Theory. In the same conference, independent work by Garivier and Kaufmann [2016] provided matching upper and lower bounds on the complexity of identifying the best arm in the “fixed-confidence” setting. Like the present paper, but unlike Jennison et al. [1982], these results apply whenever observation distributions are in the exponential family and do not require an indifference zone. The current paper looks at a different measure. We study a frequentist setting in which the true quality of each design is fixed, and characterize the rate of posterior convergence attainable for each problem instance. We also describe, as a function of the problem instance, the long-run fraction of measurement effort allocated to each design by any algorithm attaining this rate of convergence. These asymptotic limits turn out to be closely related to some of the aforementioned results. In particular, the optimal exponent given in Subsection 6.4 mirrors the complexity measure of Chernoff [1959]. This exponent is then simplified into a form that mirrors one derived by Glynn and Juneja [2004], and, for Gaussian distributions, one derived by Jennison et al. [1982]. While the",
      "startOffset" : 102,
      "endOffset" : 2944
    }, {
      "referenceID" : 8,
      "context" : "The allocation rules proposed by Chernoff [1959], Jennison et al.",
      "startOffset" : 33,
      "endOffset" : 49
    }, {
      "referenceID" : 8,
      "context" : "The allocation rules proposed by Chernoff [1959], Jennison et al. [1982] and Glynn and Juneja [2004] are essentially developed as a means of proving certain rates are attainable asymptotically, and as Chernoff [1975] writes, “sidestep the issue of how to experiment in the early stages.",
      "startOffset" : 33,
      "endOffset" : 73
    }, {
      "referenceID" : 8,
      "context" : "The allocation rules proposed by Chernoff [1959], Jennison et al. [1982] and Glynn and Juneja [2004] are essentially developed as a means of proving certain rates are attainable asymptotically, and as Chernoff [1975] writes, “sidestep the issue of how to experiment in the early stages.",
      "startOffset" : 33,
      "endOffset" : 101
    }, {
      "referenceID" : 8,
      "context" : "The allocation rules proposed by Chernoff [1959], Jennison et al. [1982] and Glynn and Juneja [2004] are essentially developed as a means of proving certain rates are attainable asymptotically, and as Chernoff [1975] writes, “sidestep the issue of how to experiment in the early stages.",
      "startOffset" : 33,
      "endOffset" : 217
    }, {
      "referenceID" : 5,
      "context" : "The work of Bubeck et al. [2009] shows formally that algorithms satisfying regret bounds of order log(n) are necessarily far from optimal for the problem of identifying the best arm.",
      "startOffset" : 12,
      "endOffset" : 33
    }, {
      "referenceID" : 2,
      "context" : "Audibert and Bubeck [2010]).",
      "startOffset" : 0,
      "endOffset" : 27
    }, {
      "referenceID" : 8,
      "context" : "As highlighted in the literature review, the max-min problem (7) closely mirrors the main sample complexity term in Chernoff’s classic paper on the sequential design of experiments (Chernoff [1959]).",
      "startOffset" : 116,
      "endOffset" : 198
    }, {
      "referenceID" : 20,
      "context" : "The results in this proposition are closely related to those in Glynn and Juneja [2004], in which large deviations rate functions take the place of the functions Ci.",
      "startOffset" : 64,
      "endOffset" : 88
    }, {
      "referenceID" : 33,
      "context" : "In the design of sequential experiments, this idea dates back to Kiefer and Sacks [1963], who builds on the work of Chernoff [1959].",
      "startOffset" : 65,
      "endOffset" : 89
    }, {
      "referenceID" : 8,
      "context" : "In the design of sequential experiments, this idea dates back to Kiefer and Sacks [1963], who builds on the work of Chernoff [1959]. For the problem of best arm identification, it dates back at least to Jennison et al.",
      "startOffset" : 116,
      "endOffset" : 132
    }, {
      "referenceID" : 8,
      "context" : "In the design of sequential experiments, this idea dates back to Kiefer and Sacks [1963], who builds on the work of Chernoff [1959]. For the problem of best arm identification, it dates back at least to Jennison et al. [1982]. These two-stage rules can be shown to attain the optimal large deviations rates described in the previous section.",
      "startOffset" : 116,
      "endOffset" : 226
    }, {
      "referenceID" : 8,
      "context" : "In the design of sequential experiments, this idea dates back to Kiefer and Sacks [1963], who builds on the work of Chernoff [1959]. For the problem of best arm identification, it dates back at least to Jennison et al. [1982]. These two-stage rules can be shown to attain the optimal large deviations rates described in the previous section. But they also have substantial practical limitations, which were discussed explicitly in early papers. Jennison et al. [1982] writes their proposed procedures “typically.",
      "startOffset" : 116,
      "endOffset" : 468
    }, {
      "referenceID" : 8,
      "context" : "In the design of sequential experiments, this idea dates back to Kiefer and Sacks [1963], who builds on the work of Chernoff [1959]. For the problem of best arm identification, it dates back at least to Jennison et al. [1982]. These two-stage rules can be shown to attain the optimal large deviations rates described in the previous section. But they also have substantial practical limitations, which were discussed explicitly in early papers. Jennison et al. [1982] writes their proposed procedures “typically...do not have good small sample size properties. A better procedure would have several stages and a more sophisticated sampling rule.” In a 1975 review of the sequential design of experiments, Chernoff [1975] notes that asymptotic approaches to the optimal sequential design of experiments had been fairly successful in circumventing the need to compute Bayesian optimal designs via dynamic programming, but “the approach is very coarse for moderate sample size problems.",
      "startOffset" : 116,
      "endOffset" : 721
    }, {
      "referenceID" : 8,
      "context" : "In the design of sequential experiments, this idea dates back to Kiefer and Sacks [1963], who builds on the work of Chernoff [1959]. For the problem of best arm identification, it dates back at least to Jennison et al. [1982]. These two-stage rules can be shown to attain the optimal large deviations rates described in the previous section. But they also have substantial practical limitations, which were discussed explicitly in early papers. Jennison et al. [1982] writes their proposed procedures “typically...do not have good small sample size properties. A better procedure would have several stages and a more sophisticated sampling rule.” In a 1975 review of the sequential design of experiments, Chernoff [1975] notes that asymptotic approaches to the optimal sequential design of experiments had been fairly successful in circumventing the need to compute Bayesian optimal designs via dynamic programming, but “the approach is very coarse for moderate sample size problems.” He writes that two-stage procedures of Kiefer and Sacks [1963], “sidestep the issue of how to experiment in the early stages,” while constructing the optimal allocations based on point estimates “treats estimates of θ based on a few observations with as much respect as that based on many observations.",
      "startOffset" : 116,
      "endOffset" : 1048
    }, {
      "referenceID" : 8,
      "context" : "This closely mirrors optimal results in Chernoff [1959], Jennison et al.",
      "startOffset" : 40,
      "endOffset" : 56
    }, {
      "referenceID" : 8,
      "context" : "This closely mirrors optimal results in Chernoff [1959], Jennison et al. [1982] and Kaufmann [2016].",
      "startOffset" : 40,
      "endOffset" : 80
    }, {
      "referenceID" : 8,
      "context" : "This closely mirrors optimal results in Chernoff [1959], Jennison et al. [1982] and Kaufmann [2016]. Does this rule also yield a frequentist probability of incorrect selection that is O(δ) as δ → 0? More generally, an open problem is to show that when combined with an appropriate stopping rule, top-two sampling schemes nearly minimize the expected number of samples E[τδ] as in Jennison et al.",
      "startOffset" : 40,
      "endOffset" : 100
    }, {
      "referenceID" : 8,
      "context" : "This closely mirrors optimal results in Chernoff [1959], Jennison et al. [1982] and Kaufmann [2016]. Does this rule also yield a frequentist probability of incorrect selection that is O(δ) as δ → 0? More generally, an open problem is to show that when combined with an appropriate stopping rule, top-two sampling schemes nearly minimize the expected number of samples E[τδ] as in Jennison et al. [1982] or Kaufmann [2016].",
      "startOffset" : 40,
      "endOffset" : 403
    }, {
      "referenceID" : 8,
      "context" : "This closely mirrors optimal results in Chernoff [1959], Jennison et al. [1982] and Kaufmann [2016]. Does this rule also yield a frequentist probability of incorrect selection that is O(δ) as δ → 0? More generally, an open problem is to show that when combined with an appropriate stopping rule, top-two sampling schemes nearly minimize the expected number of samples E[τδ] as in Jennison et al. [1982] or Kaufmann [2016].",
      "startOffset" : 40,
      "endOffset" : 422
    } ],
    "year" : 2016,
    "abstractText" : "This paper considers the optimal adaptive allocation of measurement effort for identifying the best among a finite set of options or designs. An experimenter sequentially chooses designs to measure and observes noisy signals of their quality with the goal of confidently identifying the best design after a small number of measurements. This paper proposes three simple and intuitive Bayesian algorithms for adaptively allocating measurement effort, and formalizes a sense in which these seemingly naive rules are the best possible. One proposal is top-two probability sampling, which computes the two designs with the highest posterior probability of being optimal, and then randomizes to select among these two. One is a variant of top-two sampling which considers not only the probability a design is optimal, but the expected amount by which its quality exceeds that of other designs. The final algorithm is a modified version of Thompson sampling that is tailored for identifying the best design. We prove that these simple algorithms satisfy a sharp optimality property. In a frequentist setting where the true quality of the designs is fixed, one hopes the posterior definitively identifies the optimal design, in the sense that that the posterior probability assigned to the event that some other design is optimal converges to zero as measurements are collected. We show that under the proposed algorithms this convergence occurs at an exponential rate, and the corresponding exponent is the best possible among all allocation rules.",
    "creator" : "LaTeX with hyperref package"
  }
}