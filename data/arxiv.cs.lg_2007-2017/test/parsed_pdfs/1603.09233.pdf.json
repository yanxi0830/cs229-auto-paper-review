{
  "name" : "1603.09233.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Optimal Recommendation to Users that React: Online Learning for a Class of POMDPs",
    "authors" : [ "Rahul Meshram", "Aditya Gopalan" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n60 3.\n09 23\n3v 1\n[ cs\n.L G\n] 3\n0 M\nar 2\n01 6\nI. INTRODUCTION\nAutomated online recommendation (AOR) systems for different types of content aim to adapt to user’s preferences and issue targeted recommendations for content for which the user is estimated to have a higher preference. In the generation of these recommendations, user behavior is typically modeled as a fixed, stochastic response governed by the preference, or taste, of the user for each specific item that has been recommended for consumption. However, in most AOR systems, the dynamic aspects of the response of the user to recommended content are not modeled or investigated. For example, consider an AOR for music. It is reasonable to assume that for some items, there will be short-term user fatigue for a song that has been just been recommended and played out. In this case, the user’s preference for the item drops sharply immediately after consumption and rises with time subsequently. Hence, for such items it is ideal to allow for an interval of time before recommending the item again. It is also possible that the opposite is true—the user’s appetite is whetted with each successive recommendation of a particular item. Capturing such response dynamics could involve assigning a notion of state to the user’s preference for each item at the time of the choosing the recommendations. This state should in turn\nRahul Meshram and D. Manjunath are with the Electrical Engineering Department of IIT Bombay in Mumbai INDIA. Aditya Gopalan is with the Electrical Communication Engineering Department of Indian Institute of Science in Bangalore INDIA. The work of Rahul Meshram and D. Manjunath was carried out in the Bharti Centre for Communications at IIT Bombay. D. Manjunath is also supported by grants from CEFIPRA and DST.\ndepend on the history of the recommendations or play-outs for the item.\nThere are several technical challenges in capturing or estimating the time dependent user preference to an item. Firstly, even for a recommended item, the user’s taste or preference for an item is never directly observed; only a binary response in the form of like/dislike, or play/skip, depending on the preference at that time is available. Thus the actual preference is a latent quantity which needs to be inferred and tracked continuously. This motivates the use of a hidden Markov model for the state of a user with respect to an item—the state captures the instantaneous preference and the response depends in a stochastic manner on the state. The AOR observes the response but not the state. The second challenge is that the act of recommending content to the user itself changes the user’s state of mind (e.g., fatigued, stimulated) which in turn influences the user’s responses to future content. This means that the model should allow for action dependent transitions between the states. A third challenge is adapting to the heterogeneity among users—two users may have not only different propensities towards a content item but also different rates at which they react dynamically to recommendations. This in turn necessitates learning the associated state transition models under uncertainty of not knowing what state induced a response.\nThis paper frames the problem of optimal recommendation under user adaptation and uncertainty as learning a stylized average-cost partially observable Markov decision process (POMDP). For a given user, an independent POMDP is associated with each item. The state of the POMDP expresses either a high (state 1) interest or a low (state 0) interest of the user for the item. In each step, the AORS recommends one item and the user response for the item is determined by the state. This binary response is also available to the AORS. The states of each of the POMDPs changes accordingly as the the item is recommended or not recommended. Thus the AORS can be seen to be a restless hidden Markov multi-armed bandit. In this paper we develop this model and describe a Thompson sampling mechanism to learn the parameters of the model using the response for each recommendation. Specifically, our contributions in this paper are as follows.\n1) Formulate a POMDP-based model for each item in the database of a recommendation system by incorporating user adaptation, hidden state and uncertainty in model. The AOR itself is modeled as an average cost restless hidden Markov multi-armed bandit. 2) Analyze the structure of the single-armed POMDP and show that the optimal policy is of single-threshold type\n2 in the belief. A consequence of the single threshold is that the optimal policy has a cyclic form with a recommendation step followed by k no-recommendation steps. The optimal k is derived.\n3) Devise a natural online learning algorithm based on Thompson sampling (TS) for optimizing reward in the POMDP with no knowledge of the model parameters. 4) Derive what is, to our knowledge, the first known regret bounds for TS for online learning in a class of POMDPs."
    }, {
      "heading" : "A. Related Work",
      "text" : "Multi-armed bandit models for recommendation systems and for online advertising have been modeled as contextual bandits, e.g., [1]–[3] and the user interests are assumed to be independent of the recommendation history, i.e., they have static models of reward. There are several models for restless multi-armed bandits that use state transitions and state-based rewards with applications in dynamic spectrum access, e.g., [4]–[6]. Such models assume (1) perfect observation of the state when the arm is sampled, and (2) state transitions being independent of/unrelated to actions. There is some work in modeling changing rewards in multi-armed bandits, e.g., [7] but it is again in the fully observable state case, thus circumventing the critical problem of state uncertainty arising in user adaptation. Other approaches towards handling user reactions to recommendations have considered algorithms that use a finite sequence of past user responses as a basis for deciding the current recommendation, e.g., [8]; but these are primarily numerical studies. A more general framework for a restless multi-armed bandit with unobservable states and action-dependent transitions was considered in [9], [10]. In [10] it was shown that the such a system is approximately Whittle-indexable. The restless bandit that we propose in this paper is a special case of that from [10] for which we obtain much stronger results and also a Thompson sampling-based algorithm to learn the parameters of the arms.\nThe rest of the paper is organized as follows. In the next section we describe the model and set up notation. In Section III we analyze the structural properties of the averagecost POMDP corresponding to a single arm. In Section IV a Thompson sampling based algorithm is described to learn the parameters of the POMDP based on the observed reward. In Section V we analyse the regret as compared to the optimal policy. We conclude with some illustrative numerical results and a discussion on extension to the multi-armed bandit case."
    }, {
      "heading" : "II. MODEL DESCRIPTION AND PRELIMINARIES",
      "text" : "The AOR system is modeled as a restless multi-armed bandit with arm i representing the state of the user with respect to item i. Each arm is modeled as an independent partially observable Markov decision processes, with two states and two actions. We first describe the model for a single generic arm. S = {0, 1} is the set of states with 0 corresponding to a low interest and 1 corresponding to high interest. A = {0, 1} is the set of actions with 1 for sampling the arm and 0 for not sampling the arm. r : S ×A → R is the reward function with\n0 1(1− q)\nq\n1\nReward: λ Reward: λ\nState transitions and rewards when At = 0.\n0 11\n1\nReward: 1 w.p. ρ Reward: 1 w.p. 1\nState transitions and rewards when At = 1.\nFig. 1. Single-arm POMDP. The state transitions and rewards for when the item is recommended and when it is not recommended.\nR = {0, 1, λ} being the set of rewards. Time progresses in discrete steps indexed by t = 1, 2, 3, . . . A step is an instant at which a recommendation can potentially be made by the AORS. Xt denotes the state of the arm at the start of time step t, At is the action played at time t and and Rt is the reward obtained at time t.\nThe reward structure for the POMDP is as follows. A unit reward is obtained with probability 1 if At = Xt = 1. This corresponds to recommending the item when the user’s interest is high. A unit reward is obtained with probability ρ if At = 1 and Xt = 0, i.e., if an item is recommended and the interest is low. A reward λ is obtained independent of Xt for At = 0, i.e., when the item is not recommended.\nRemark 1. A restless multi-armed bandit is typically analyzed by first analyzing the single arm case with a reward of λ for At = 0. λ is called the subsidy for not sampling. Such an analysis is used to determine if the Whittle index based policy can be used to optimally choose the arm at each time step. This analysis is also used to calculate the Whittle-index for an arm.\nIf At = 0, then 0 → 1 transitions occur with probability q and 1 → 0 transitions with probability 0. This corresponds to an increasing interest as the time progresses since the last recommendation. If At = 1, then 0 → 0 and 1 → 0 transitions happen with probability 1 corresponding to a decreased interest after a recommendation. Fig. 1 illustrates the preceding discussion in the form of a state transition diagram.\nRecall that the actual state (0 or 1) is never observed. Let πt = Pr (Xt = 0) , denote the belief about the state of the arm at the beginning time t. Let Ht denote the history of actions and rewards up to time t. Let φt : Ht → {0, 1} be the strategy that determines the action at time t. For a strategy φ and an initial belief π at time t = 0 (i.e., Pr (X0 = 0) = π), the\n3 expected finite horizon reward function is\nV φT (π) = E φ\n[\nT−1 ∑\nt=0\nr(Xt, At)\n∣ ∣ ∣ ∣ π0 = π\n]\n= Eφ\n[\nT−1 ∑\nt=0\n[πtρ+ (1− πt)1]\n∣ ∣ ∣ ∣ π0 = π\n]\n.\nThe long term average reward is defined as V φ(π) = limT→∞ 1 T V φT (π).\nIn the next section we assume that (q, ρ), are known and determine the policy φ that maximizes V φ(π)."
    }, {
      "heading" : "III. SINGLE ARM: OPTIMAL POLICY",
      "text" : "We solve the average reward problem described in the previous section by the vanishing discount approach [11], [12]—by first considering a discounted reward system and then taking limits as the discount vanishes. The infinite horizon discounted reward under policy φ and discount β, 0 < β < 1, is\nV φβ (π) := E φ\n[\n∞ ∑\nt=1\nβt−1 (\naφt (πtρ+ (1 − πt))+\n(1− aφt )λ )\n∣ ∣ ∣ ∣ π0 = π ] . (1)\nFrom [10], we can show that the following dynamic program solves (1).\nVβ(π) = max {λ+ βVβ((1− q)π), 1− π(1− ρ) + βVβ(1)} (2)\nFurther, we can state the following about Vβ(π).\nLemma 1. 1) Equation (2) has a unique solution Vβ(π). Further, Vβ(π) is continuous and bounded.\n2) Vβ(π) is convex non-increasing in π and increasing in β. 3) ∣ ∣Vβ(π1) − Vβ(π2) ∣\n∣ < (1 − ρ) for all π ∈ [0, 1] and β ∈ [0, 1). 4) The optimal policy is of threshold type with a single threshold for β ∈ [0, 1) and λL ≤ λ ≤ λH .\nThe first two above follow directly from [10] and the last two are derived in [13].\nDefine V β := Vβ(π) − Vβ(1) for π ∈ [0, 1]. From (2), we get\nV β + (1− β)Vβ(1) = max { λ+ βV β((1 − q)π),\n1− π(1− ρ)} (3)\nFrom Lemma 1, V β(π) is convex monotone in π and by definition V β(1) = 0. Further, from the lemma we know that there is a constant C < ∞ such that ∣\n∣Vβ(π) − Vβ(1) ∣\n∣ < C. This implies that V β(π) is bounded and Lipschitz-continuous. Finally, (1 − β)Vβ(π) is also bounded. Hence we can apply the Arzela-Ascoli theorem [14], to find a subsequence (V βk(π), (1 − β)Vβk(π)) that converges uniformly to (V (π), g) as βk → 1. Thus, as βk → 1, along an appropriate subsequence, (3) reduces to\nV (π) + g = max {λ+ V ((1− q)π), 1 − π(1 − ρ)} , (4)\nfor all π ∈ [0, 1]. (4) is the dynamic programming equation whose solution gives us the optimal value function that maximizes the average reward.\nSince V (π) inherits the structural properties of Vβ(π), we have, from Lemma 1, that\nLemma 2. 1) V (π) is monotone non-increasing and convex in π.\n2) The optimal policy is of threshold type with a single threshold for λL ≤ λ ≤ λH .\nThis in turn leads us to the following theorem which is a direct analog of Theorem 6.17 in [11].\nTheorem 1. If there exists a bounded function V (π) for π ∈ [0, 1] and a constant g that satisfies (4), then there exists a stationary policy φ∗ such that\ng = max φ lim T→∞\n1 T V φT (π) (5)\nfor all π ∈ [0, 1], and moreover, φ∗ is the policy for which the RHS of (4) is maximized."
    }, {
      "heading" : "A. An Equivalent Form for the Optimal Policy",
      "text" : "For the single-armed bandit, the threshold policy of Lemma 2 can be interpreted as follows. Let πT be the threshold such that the optimal policy is At = 1 if πt ≤ πT and At = 0, if πt > πT . We know that if At = 1, then πt+1 = 1, i.e., if the item is recommended then the state becomes 0. When the item is not recommended, the belief about state 0 decreases by a factor of (1 − q). Since there is a single threshold and πt monotonically decreases every time the item is not recommended, the optimal policy will be to wait for k steps before recommending again, where k is the first time that πt has crossed πT . This value k is a function of q and ρ and will be denoted by kpt(q, ρ).\nWe first consider infinite horizon discounted reward problem. In this case, solving (1) is equivalent to solving following optimization problem.\nkβ,opt(q, ρ) = argmax k≥1 Ṽβ(k), (6)\nwhere Ṽβ(k) is the value function obtained by not recommending for k steps between successive recommendations. We can write\nṼβ(k) := { λ+ λβ + λβ2 + · · ·+ λβk−1\n+βk [ (1− q)kρ+ 1− (1− q)k ]}\n+βk+1 { λ+ λβ + λβ2 + · · ·+ λβk−1\n+βk [ (1− q)kρ+ 1− (1− q)k ]}\n+β2(k+1) { λ+ λβ + λβ2 + · · ·+ λβk−1\n+βk [ (1− q)kρ+ 1− (1− q)k ]} + · · ·\nLet Ck denote the reward from the first one cycle of k steps with no recommendations for (k − 1) steps and a\n4 recommendation in the k-th step. We can write\nCk := λ+ λβ + λβ 2 + · · ·+ λβk−1\n+βk [ (1 − q)kρ+ 1− (1− q)k ]\n= λ (1− βk)\n(1 − β) + βk\n[ (1− q)kρ+ 1− (1− q)k ]\nThe first k−1 terms above correspond to the reward from not sampling and the kth term denotes the reward from sampling. Thus, Ṽβ(k) can be rewritten as follows.\nṼβ(k) = Ck [ 1 + βk+1 + β2(k+1) + · · · ]\n= 1\n1− βk+1\n[\nλ (1− βk)\n(1− β) + βk\n[ (1− q)kρ+ 1− (1− q)k ]\n]\nThe preceding discussion gives us the following result on the value function and the optimal policy for the average reward criterion POMDP.\nTheorem 2. 1) The value function for policy k is\nṼ (k) = lim β→1 (1 − β)Ṽβ(k)\n= 1\nk + 1\n[ λk + [ (1− q)kρ+ 1− (1− q)k ]] .\n2) The optimum policy kopt(q, ρ) satisfies\nkopt(q, ρ) = argmax k≥1 Ṽ (k)\n= argmax k≥1\n1\nk + 1\n[ λk + [ (1− q)kρ+ 1− (1− q)k ]] (7)\nThus, for the single armed bandit, given q, ρ and λ, we obtain the optimal policy as the number of steps to wait before recommending the item again. In the next section we describe the Thompson sampling algorithm to learn the parameters based on the reward that is observed. Subsequently, we analyze the regret from the learning process. We remind the reader that the state is never observed in the system and the learning is based only on rewards."
    }, {
      "heading" : "IV. THOMPSON SAMPLING LEARNING ALGORITHM",
      "text" : "We have seen that the optimal policy for the (singlearm) POMDP described by q and ρ is of threshold type (Section III-A), and corresponds to waiting for kopt(q, ρ) steps in between successive recommendations. However, when the parameters q, ρ that describe the Markov chain transition probabilities are unknown a priori1, they must be learnt or inferred from the available feedback in order to attain maximum cumulative reward. This section describes an online algorithm that learns to play the optimal policy using experience, i.e., observations from previously played actions, while at the same time keeping the net reward as high as possible (the exploreexploit problem).\nThe learning algorithm (Algorithm 1) is a version of the popular Thompson sampling strategy [15], developed originally for stochastic multi-armed bandit problems [16], and subsequently extended to learning in Markov Decision Processes\n1as in an AOR system where user behavior is unknown at start\n(MDPs) [17]–[19] and POMDPs. It works in epochs, where an epoch is defined to be the interval of time from an instant at which the POMDP is sampled (action 1 is played) up until the next instant at which it is sampled again. At the beginning, the algorithm initializes a prior or belief distribution2 on the space of all candidate parameters/models, which in our case is any subset X of the unit square [0, 1]× [0, 1] containing all possible POMDPs parameterized by (q, ρ). At the start of each epoch ℓ ≥ 1, a model (qℓ, ρℓ) is randomly and independently sampled according to the current prior over X (this random draw is crucial in inducing exploration over the model space). Then, the optimal policy for this sampled model is computed, which by the previous results corresponds to sampling the chain after an interval of kopt(qℓ, ρℓ) time instants3. This policy is now applied for one cycle, i.e., the algorithm waits for the specified interval of time instants, samples the chain at the next time instant, and obtains an observation for the sampled instant (a Bernoulli-distributed reward). The observed reward is used to update the prior over models via Bayes’ rule, the epoch ends, and the next epoch starts with the updated prior. Notation. In Algorithm 1, B(X ) denotes the Borel σ-algebra of X ⊂ Rd. Pr ( R = r ∣ ∣ (q, ρ), k )\ndenotes the likelihood, under the POMDP model specified by parameters (q, ρ), of observing a reward of r ∈ {0, 1} upon sampling the Markov chain after having not sampled the chain for exactly k previous time instants. Specifically, we have\nAlgorithm 1: Thompson sampling algorithm for learning the optimal policy\nInput: Parameter space X ⊆ [0, 1]2, Policy space K ⊂ {0, 1, 2, . . .}, Observation space R = {0, 1}, Prior probability distribution Z1 over (X ,B(X )) for epoch ℓ = 1, 2, . . . do\nSample (qℓ, ρℓ) ∈ X according to the probability distribution Zℓ Compute the optimal policy kℓ = kopt(qℓ, ρℓ) for sampled parameters Apply the policy kℓ once: wait for the next (kℓ − 1) time steps and sample the Markov chain at the kℓ-th time instant Observe reward on sampling, denote it by Rℓ ∈ {0, 1} Update the current prior over (q, ρ) to\nZℓ+1(B) :=\n∫\nB Pr\n( R = Rℓ ∣ ∣ (q, ρ), kℓ )\nZℓ(q, ρ)dq dρ ∫\nX Pr\n( R = Rℓ ∣ ∣ (q, ρ), kℓ ) Zℓ(q, ρ)dq dρ\nfor any Borel set B ∈ B(X ). end for\nPr ( R = r ∣ ∣ (q, ρ), k ) =\n{\nf(q, ρ, k) if r = 1\n1− f(q, ρ, k) if r = 0,\n2Note that the prior used in Thompson sampling is merely a parameter of the algorithm (e.g., the uniform measure over a compact domain), carefully designed to induce random exploration, and is not related to any Bayesian modeling assumptions on the true model as such.\n3This could be carried out using either standard planning methods such as value/policy iteration or exhaustive search over threshold-type policies.\n5 where f(q, ρ, k) is simply the probability of observing a reward of 1 after having waited for k time steps since the last sample, when the parameters are (q, ρ). It follows that\nf(q, ρ, k) = (1− q)kρ+ [ 1− (1− q)k ] ."
    }, {
      "heading" : "V. MAIN RESULT – REGRET BOUND",
      "text" : "In this section, we show an analytical performance guarantee for Algorithm 1.\nTo this end, we consider a widely employed measure of performance from online learning theory, namely regret [20], [21]. The regret of a strategy, for the POMDP described by (q∗, ρ∗), is the difference between the cumulative reward which the optimal policy4 kopt(q∗, ρ∗) earns when run from a fixed initial state for a fixed time horizon T , and that which the strategy earns with the same initial state and time horizon. Formally, the regret for a strategy A is the random variable\nRA(q∗,ρ∗)(T ) :=\nT−1 ∑\nt=0\nr ( Xt, A kopt(q ∗,ρ∗) t ) − T−1 ∑\nt=0\nr ( Xt, A A t ) ,\nwhere Akopt(q ∗,ρ∗)\nt (resp. A A t ) represents the action taken by\nkopt(q ∗, ρ∗) (resp. A) at time instant t, and it is assumed that the algorithms A and kopt(q∗, ρ∗) are run on independent POMDP instances. The goal is typically to bound the regret of a sequential decision making algorithm as a function of the structure of the POMDP (number of states/actions in the underlying MDP) and show that it grows only sub-linearly with time T (i.e., the per-round regret vanishes), either in expectation or with high probability.\nTowards bounding the regret of the Thompson sampling POMDP algorithm (Algorithm 1), it is convenient to consider a modified version of regret that essentially counts the number of epochs during the operation of the algorithm in which the policy used is not kopt(q∗, ρ∗), or in other words the length of the epoch consisting of no-sampling instants is not kopt(q∗, ρ∗). This corresponds to the quantity\nR̃(q∗,ρ∗)(L) :=\nL ∑\nℓ=1\n1{kℓ 6=kopt(q∗,ρ∗)},\ndefined for the first L epochs that the algorithm executes. Note that under the reasonable assumption that an upper bound kmax on kopt(q∗, ρ∗) is available a priori, and if the Thompson sampling algorithm samples at all times parameters (q, ρ) for which kopt(q, ρ) ≤ kmax, then the length of each epoch is bounded between 1 and kmax; thus the standard regret R(T ) is bounded in terms of the modified regret R̃(L) by a constant factor kmax (note that the maximum possible reward is 1). In order to focus on the order-wise scaling of the regret with time or number of epochs, we henceforth concentrate on bounding the (modified) regret R̃(T ), with high probability.\nWe will need the following set of mild assumptions on the structure of the parameter space and the initial prior under which a regret bound holds.\n4We overload notation, when the context is clear, to represent the optimal policy using the optimal waiting time kopt(q∗, ρ∗).\nAssumption 1. (a) The parameter space X ⊆ [η, 1 − η] for some η ∈ (\n0, 12 ) , (b) |X | < ∞, (c) The true model (q∗, ρ∗) ∈ X , (d) The prior distribution Z1 over X puts positive mass on the true model, (e) There is a unique (average-reward) optimal policy kopt(q∗, ρ∗) ≤ kmax for the true model with a known upper bound kmax ∈ Z.\nTheorem 3 (Main Result – Thompson sampling regret). Let Assumption 1 hold, and let ǫ, δ ∈ (0, 1). There exists L0 ≡ L0(ǫ) such that the following bound holds, for the cumulative regret of Algorithm 1 with initial prior Z1, with probability at least 1− δ for all L ≥ L0:\nR̃(q∗,ρ∗)(L) ≤ B + C(logL),\nwhere B ≡ B(ǫ, δ, (q∗, ρ∗),X ) is a problem-dependent constant independent of the number of epochs L, and C ≡ C(δ, (q∗, ρ∗),X ) is the solution to an optimization problem (P1).\nNote. The optimization problem is described in detail in Appendix A for the sake of clarity. Discussion. Theorem 3 establishes that the regret of Algorithm 1 scales only logarithmically (thus, sub-linearly) with time (or epochs), with high probability, when starting with a ‘grain-of-truth’ prior that ascribes positive probability to the true model. The algorithm is thus able to achieve a suitable balance between exploring across different sampling policies and exploiting its improving knowledge about the true model (q∗, ρ∗) to keep the regret controlled, in a nontrivial fashion. Moreover, this is achieved in a POMDP model in which the state of the Markov chain is never available at any time instant, but instead only a stochastic reward correlated with the current state is observed that conveys implicit information about the true parameter (q∗, ρ∗). The logarithmic growth of the regret with time is controlled by the quantity C, which depends on Kullback-Leibler (KL) divergences of the distribution of the observations under different models and policies, thus encoding the information structure of the observations.\nThe theorem is proven by following closely the method developed to show [22, Theorem 1 and Proposition 2] and [18, Theorem 1 and 5], namely the strategy of bounding the posterior mass (with high probability) both from below (in a neighborhood of the true model) and from above (outside the neighborhood, for parameters corresponding to suboptimal policies) spelt out in detail in [22, Appendix A]. We describe the derivation of Theorem 3 in Appendix A.\nThe following accompanying result provides more insight into the order-wise scaling of regret. In the following, D(p||q) := p log (\np q\n) + (1 − p) log (\n1−p 1−q\n)\ndenotes the KL divergence between Bernoulli distributions of parameter p and q, 0 < p, q < 1.\nTheorem 4. Consider L to be large enough so that\nmax (q,ρ)∈X ,k≤kmax\nD(f(q∗, ρ∗, k)||f(q, ρ, k)) ≤ 1 + ǫ\n1− ǫ logL.\nThen, there exists ∆2 > 0 such that\nC(logL) ≤\n(\n1\n∆2\n)\n2(1 + ǫ)\n1− ǫ logL.\n6 Discussion. Theorem 4 highlights a key property of the regret induced by the information structure of the POMDP problem – that the regret asymptotically does not scale with the total number kmax of candidate optimal policies, which could be large by itself. This can be contrasted with running a simple multi-armed bandit algorithm such as UCB [23] with the ‘arms’ being different waiting-duration policies with the duration ranging from 0, 1, . . . , kmax (a total of kmax + 1 arms), and the reward being the reward from applying a single cycle of any such policy while disregarding the POMDP structure entirely. It follows from standard stochastic bandit regret bounds that such an algorithm would achieve regret that scales with the total number of arms, i.e., O(kmax logT ). The advantage of using Thompson sampling with a prior on POMDP structures (q, ρ) is that every application of any waiting-time policy provides a non-trivial amount of information (in the sense of the prior-posterior update) about the true POMDP (q∗, ρ∗), and hence about all other policies (in the multi-armed bandit view this is akin to any arm providing reward information about all arms following every pull). The proof of the result is motivated by [22, Proposition 2], and is detailed in the appendix."
    }, {
      "heading" : "VI. NUMERICAL RESULTS AND DISCUSSION",
      "text" : "We present some numerical results to show the rate of convergence of Algorithm 1 to the optimal policy of the POMDP model, for various configurations of the true model and initial prior. We fix λ = 0.3 in all of the simulations and consider four combinations of the true models (q∗, ρ∗); (1) (0.05,0.25), (2) (0.05,0.15), (3) (0.05,0.35), and (4) (0.15,0.35). For each of these four values of the true parameters, we present two performance measures as a function of the time step—the regret and also probability mass on the true values. These values are average from 300 runs of the simulation. The first set of plots (shown in Fig. 2) are obtained by discretising the parameter space coarsely into a (5 × 5) grid at (0.05, 0.15, 0.25, 0.35, 0.45) and starting with the uniform a distribution on the 25 points. The second set of plots (shown in Fig. 3) is obtained by using a finer 10 × 10 grid at (0.05, 0.10, . . . , 0.50).\nWe observe that true model and the initial prior (i.e., supported on the coarse/fine grid) both affect the convergence rate of regret and probability distribution of true model. The effect of the prior on the finer grid is to increase the overall regret and slow down the convergence to the true model. This is presumably due to (a) the fact that imposing a prior over a fairly coarse grid is equivalent to providing a large amount of side information about the true model (i.e., that it must be one of a small set of models), and on a related note, (b) the presence of confounding or competing models that are closer to it than in the coarse grid prior, which must be eliminated to achieve low regret (this is analogous to the phenomenon of smaller ‘gap’ in multi-armed bandits leading to higher regret)."
    }, {
      "heading" : "A. Discussion and Directions – Multi-armed bandit case",
      "text" : "We formulated the problem of optimizing recommendations for a single user, whose taste in a certain item changes\nwith recommendations, as online learning in a two-state, twoparameter POMDP. Using this approach, we developed and analyzed the performance of a natural Thompson-sampling algorithm for learning the optimal policy for a user-item pair. A logical next step in this investigation is to treat the multi-armed bandit version of the problem with multiple independently evolving POMDPs, each representing different users/items, and a resource constraint on which users/items can be activated at any instant, e.g., decide which of several items is to be shown to a user at a certain time, given that the user remembers how far ago she consumed a certain item and may respond accordingly to item recommendations.\nThe Thompson sampling based algorithm proposed in this work could be extended to cover the multi-armed bandit case by jointly sampling parameters for all POMDPs, computing the optimal refresh rate for each of them, and scheduling the recommendations accordingly while at the same time updating its current prior to incorporate observations. This opens up new avenues for analysis of performance for such algorithms, and we plan to pursue it as part of future work."
    }, {
      "heading" : "A. Proof of Theorem 3",
      "text" : "We sketch how the proof of the result can be adapted from that of [22, Theorem 1]; due to space constraints the reader is referred to [18], [22] for precise estimates and details. We first define the decision regions based on KL-divergence for each policy k. Let Sk := {(q, ρ) ∈ X : kopt(q, ρ) = k} be the collection of all models (q, ρ) ∈ X for which the optimal policy is k. Denote k∗ := kopt(q∗, ρ∗), let ǫ > 0, and define the following sub-decision regions ∀k 6= k∗:\nS ′ k := S ′ k(ǫ) = {(q, ρ) ∈ Sk : D (f(q ∗ , ρ ∗ , k ∗)||f(q, ρ, k∗)) ≤ ǫ}\nS ′′ k := Sk \\ S ′ k = {(q, ρ) ∈ Sk : D (f(q ∗ , ρ ∗ , k ∗)||f(q, ρ, k∗)) > ǫ} .\nLet Nk(l) = ∑l\ni=1 1{(qi,ρi)∈Sk} be the number of times up to and including epoch l for which the policy employed by Algorithm 1 is k. Also, Nk(l) = ∑l\ni=1 1{(qi,ρi)∈S′k} +\n∑l i=1 1{(qi,ρi)∈S′′k } . Define N ′ k(l) := ∑l i=1 1{(qi,ρi)∈S′k} and\nN ′′ k (l) := ∑l\ni=1 1{(qi,ρi)∈S′′k } . Next, it can be shown that\nthe posterior on S ′′\nk decays exponentially with t, leading to a negligible, i.e., O(1), regret from S ′′\nk . To obtain posterior distribution on S ′\nk to be small, we need ∑kmax\nk Nk(l)D (f(q ∗, ρ∗, k)||f(q, ρ, k)) ≈ logL. This mean that suboptimal models are sampled as long as their posterior probability mass is greater than 1\nL . If the posterior probability\nof a model parameter is less than 1 L , then the number of times that parameter sampled up to epoch L is O(1) and this is negligible compared to regret. It is thus enough to bound the maximum amount of time that the posterior probability of any S ′\nk, k 6= k ∗, can stay above 1/L, when non-trivial regret\nis incurred. We now define N ′ (l) := ( N ′ k(l) ) , at l ≥ 0, N ′\n(0) = (0, · · · , 0). The policy k is eliminated when all its model losses exceed logL. Let τ1 be the first time when some policy k1 is eliminated, k1 6= k∗. The play count of policy k1 fixed at N ′\nk1 (τ1) for remaining horizon up to L. Next τ2 ≥ τ1\nwhen policy k2 /∈ {k∗, k1} is eliminated and play count of k2 fixed at N ′\nk2 (τ2). This process goes on until all subop-\ntimal policies eliminated. N ′ (τi) = ( N ′ k(τi) ) {k=1,···kmax} is play count vector of all policies at time τi. Let Yi := N ′ (τi) = ( N ′ k(τi) )\n{k=1,···kmax} . Since the play count of\npolicy ki fixed at N ′\nki (τi) for remaining horizon, we have\nconstraints Yi(kj) = Yj(kj), for i ≥ j. That means plays of policy kj do not occur after time τj . Let D(f(q, ρ)) := (D (f(q∗, ρ∗, k)||f(q, ρ, k))){k=1,···kmax} is a vector of the marginal Kullback-Leibler divergences for all policies. As the policy ki eliminated at time τi, this translates into the following problem: min(q,ρ)∈S′\nki\n〈Yi, D(f(q, ρ))〉 ≥ 1+ǫ 1−ǫ logL,\nwhere 〈x, y〉 denotes the standard inner product in Euclidean space. We summarize the discussion on the elimination of suboptimal policies in the following constrained optimization\n8 problem that depends on the marginal KL divergences.\nC(logL) :=\nmax\nkmax−1 ∑\ni=1\nYi(ki)\ns.t. Yi ∈ R kmax + , i = 1, · · · , kmax − 1\nYi(kmax) = 0, k = 1, · · · , kmax − 1 Yi ≥ Yj , i ≥ j, j = 1 · · · , kmax − 1 Yi(j) = Yj(j), i ≥ j, j = 1, · · · , kmax − 1 σ : {1, · · · , kmax − 1} → {1, · · · , kmax} − {k ∗} injective\nmin (q,ρ)∈S ′\nσ(i)\n〈Yi, D(f(q, ρ))〉 = 1 + ǫ\n1− ǫ logL,\ni = 1, · · · , kmax − 1. (P1)"
    }, {
      "heading" : "B. Preliminary Results Towards Proving Theorem 4",
      "text" : "We collect here some useful assertions towards showing the result. We first note that the variation distance provides a lower bound on KL-divergence and it is given as\nD(f(q∗, ρ∗, k)||f(q, ρ, k)) ≥ 1\n2 ln 2 d 2(f(q∗, ρ∗, k), f(q, ρ, k))\n≥ 1\nln 2 dk(q, ρ) (8)\nHere, d(f(q∗, ρ∗, k), f(q, ρ, k)) is variation distance between f(q∗, ρ∗, k) and f(q, ρ, k) and this is described as follows.\nd(f(q∗, ρ∗, k), f(q, ρ, k)) = 2 ∣ ∣f(q∗, ρ∗, k)− f(q, ρ, k) ∣ ∣\nWe can rewrite ∣ ∣f(q∗, ρ∗, k)− f(q, ρ, k) ∣ ∣ 2 as follows.\n∣ ∣f(q∗, ρ∗, k)− f(q, ρ, k) ∣ ∣ 2 =\n[\nqk(ρ− 1)− q∗ k (ρ∗ − 1)\n]2\n,\nwhere q = 1 − q, and q∗ = 1 − q∗. Define dk(q, ρ) := [ qk(ρ− 1)− q∗ k (ρ∗ − 1) ]2 , and d(q, ρ) :=\n[d1(q, ρ), · · · , dkmax(q, ρ)] . We need the following series of lemmas to prove Theorem 4.\nLemma 3. For every ǫ > 0, there exists δ > 0 such that if (q, ρ) and (q∗, ρ∗) sufficiently away and D(f(q∗, ρ∗, k∗)||f(q, ρ, k∗)) ≤ ǫ then D(f(q∗, ρ∗, k)||f(q, , ρ, k)) ≥ δ.\nProof: Since (q, ρ) is sufficiently away from (q∗, ρ∗), the difference |qk − q∗ k | will be positive for all policies k = 1, 2, · · · , kmax. We set\nδ1 := min k\n[\nqk(ρ− 1)− q∗ k (ρ∗ − 1)\n]2\n> 0.\nThen, using inequality in (8), we obtain\nD(f(q∗, ρ∗, k)||f(q, ρ, k)) > 1\nln 2 δ1 > δ,\nwhere δ = 1ln 2δ1. This completes the proof.\nLemma 4. One can find ǫ > 0 such that it can not happen that there exists (q, ρ) /∈ Nǫ1(q ∗, ρ∗) and k, k ′ , k 6= k ′\nfor which dk(q, ρ) ≤ ǫ and dk′ (q, ρ) ≤ ǫ.\nProof: Suppose dk(q, ρ) = dk′ (q, ρ) = 0, then we will have\nqk\nq∗ k\n= 1− ρ∗\n1− ρ =\nqk ′\nq∗ k ′\n(9)\nThis implies that qk−k ′ = q∗ k−k\n′\nNow observe that when k 6= k ′\nand q is not in neighborhood of q∗, so equality (9) is not true. This means that our assumption dk(q, ρ) = dk′ (q, ρ) = 0 is not true. Further, it implies that only one of the following claim is true.\n1) if dk(q, ρ) = 0, then dk′ (q, ρ) > 0 for k 6= k ′ 2) if dk′ (q, ρ) = 0 then dk(q, ρ) > 0 for k 6= k ′ .\nIn other word, we can find ǫ > 0 for which either dk(q, ρ) ≤ ǫ, dk′ (q, ρ) > ǫ is true, or dk′ (q, ρ) ≤ ǫ dk(q, ρ) > ǫ is true.\nLemma 5. Consider any parameter (q, ρ) 6= (q∗, ρ∗) and (q, ρ) /∈ Nǫ1(q ∗, ρ∗), where Nǫ1(q ∗, ρ∗) is ǫ1 neighborhood of (q∗, ρ∗). Then there exists an integer κ ∈ {1, 2, 3, · · · , kmax− 1} and ∆ > 0 such that for all (q, ρ) /∈ Nǫ1(q\n∗, ρ∗) : ∣ ∣{k : dk(q, ρ) ≥ ∆} ∣ ∣ ≥ κ. (10)\nAlso, for sufficiently small ∆ > 0, we have κ = kmax − 1.\nProof: From Lemma 4, notice that in d(q, ρ), there can be at most one element which can be zero or arbitrary close zero, say, dl(q, ρ) ≤ ǫ and remaining entries, dk(q, ρ) > ǫ, k 6= l. When (q, ρ) /∈ Nǫ1(q\n∗, ρ∗), implies |q − q∗| ≥ ǫ1 and |ρ− ρ∗| ≥ ǫ1. Thus we obtain\ndk(q, ρ) = [ q k(ρ− 1)− q∗ k (ρ∗ − 1) ]2\n≥ min k\n[\n(q∗ + ǫ1) k((ρ∗ + ǫ1)− 1)− q∗ k (ρ∗ − 1)\n]2\n∆ := min1≤k≤kmax\n[\n(q∗ + ǫ1) k((ρ∗ + ǫ1)− 1) − q∗ k (ρ∗ − 1)\n]2\n,\nthis ∆ > 0. Now, combining Lemma 4, and dk(q, ρ) ≥ ∆, there exists κ ∈ {1, 2, 3, · · · , kmax−1}, such that for (q, ρ) /∈ Nǫ1(q\n∗, ρ∗), we have ∣\n∣{k : dk(q, ρ) ≥ ∆} ∣ ∣ ≥ κ. (11)\nFurther, for sufficiently small ǫ > 0, and (q, ρ) /∈ Nǫ1(q ∗, ρ∗), we have kmax − 1 nonzero entries in vector d(q, ρ). In this case, fix ∆ = ǫ, we obtain ∣ ∣{k : dk(q, ρ) > ∆} ∣\n∣ = kmax − 1. Thus, κ = kmax − 1."
    }, {
      "heading" : "C. Proof of Theorem 4",
      "text" : "From lemma 5, we know that for sufficiently small ∆ > 0, we have κ = kmax − 1 and ∣ ∣{k : dk(q, ρ) ≥ ∆} ∣\n∣ = kmax − 1. Thus, we have ∆2 = 1ln 2∆ > 0 and κ = kmax − 1 such that ∣ ∣{k ∈ K : k 6= k∗, D(f(q∗, ρ∗, k)||f(q, ρ, k)) ≥ ∆2} ∣ ∣ = kmax − 1\nFrom Lemmas 3, 5 and eqn. (8), we note that all assumptions in [22, Proposition 2] are satisfied. Note that the upper bound on C(logL) is given in [22, Proposition 2] and it is as follows.\nC(logL) ≤\n(\nkmax − κ\n∆2\n)\n2(1 + ǫ)\n1− ǫ logL\nHere, we substitute κ = kmax − 1, and required upper bound on C(logL) follows."
    } ],
    "references" : [ {
      "title" : "The epoch-greedy algorithm for contextual multi-armed bandits",
      "author" : [ "J. Langford", "T. Zhang" ],
      "venue" : "Advances in Neural Information Processing Systems. NIPS, Dec. 2007, pp. 1–8.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Leveraging side observations in stochastic bandits",
      "author" : [ "S. Caron", "B. Kveton", "M. Lelarge", "S. Bhagat" ],
      "venue" : "Arxiv, 2012.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "A contextual-bandit approach to personalized news article recommendation",
      "author" : [ "L. Li", "W. Chu", "J. Langford", "R.E. Schapire" ],
      "venue" : "Conference on World Wide Web. ACM, April 2010, pp. 661–670.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Indexability of restless bandit problems and optimality of Whittle index for dynamic multichannel access",
      "author" : [ "K. Liu", "Q. Zhao" ],
      "venue" : "IEEE Transactions Information Theory, vol. 56, no. 11, pp. 5557–5567, November 2010.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Exploiting channel memory for joint estimation and scheduling in downlink networks",
      "author" : [ "W. Ouyang", "S. Murugesan", "A. Eyrilmaz", "N. Shroff" ],
      "venue" : "Proceedings of IEEE INFOCOM, 2011.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Network utility maximization over partially observable Markovian channels",
      "author" : [ "C. Li", "M.J. Neely" ],
      "venue" : "Performance Evaluation, vol. 70, no. 7–8, pp. 528–548, July 2013.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Optimality of myopic policy for a class of monotone affine restless multi-armed bandits",
      "author" : [ "P. Mansourifard", "T. Javidi", "B. Krishnamachari" ],
      "venue" : "Proceedings of IEEE CDC, 2012.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Context-aware music recommendation based on latent topic sequential patterns",
      "author" : [ "N. Hariri", "B. Mobasher", "R. Burke" ],
      "venue" : "Proceedings of the Sixth ACM Conference on Recommender ystems (RecSys ’12), 2012, pp. 131–138.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "A restless bandit with no observable states for recommendation systems and communication link scheduling",
      "author" : [ "R. Meshram", "D. Manjunath", "A. Gopalan" ],
      "venue" : "Proceedings of IEEE CDC, 2015.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "On the Whittle index for restless multi-armed hidden markov bandits",
      "author" : [ "R. Meshram", "D. Manjunath", "A. Gopalan" ],
      "venue" : "Submitted for Publication. Also available on Arxiv:1603.04739, 2016.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Applied Probability Models with Optimization Applications",
      "author" : [ "S.M. Ross" ],
      "venue" : "Dover Publications,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1993
    }, {
      "title" : "Whittle index policy for crawling ephemeral content",
      "author" : [ "K. Avrachenkov", "V.S. Borkar" ],
      "venue" : "Tech. Rep. Research Report No. 8702, INRIA, 2015.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Adaptive playlists from hidden markov bandits",
      "author" : [ "R. Meshram", "D. Manjunath", "A. Gopalan" ],
      "venue" : "Submitted for Publication. Also available arxiv, 2016.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Principles of mathematical analysis, McGraw-Hill Book Co., New York, third edition, 1976, International Series in Pure and Applied Mathematics",
      "author" : [ "Walter Rudin" ],
      "venue" : null,
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1976
    }, {
      "title" : "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples",
      "author" : [ "William R. Thompson" ],
      "venue" : "Biometrika, vol. 24, no. 3–4, pp. 285–294, 1933.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 1933
    }, {
      "title" : "Model-based reinforcement learning and the eluder dimension",
      "author" : [ "Ian Osband", "Benjamin V. Roy" ],
      "venue" : "Advances in Neural Information Processing Systems 27, 2014, pp. 1466–1474.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Thompson sampling for learning parameterized markov decision processes",
      "author" : [ "Aditya Gopalan", "Shie Mannor" ],
      "venue" : "Conf. on Learning Theory, COLT 2015, Paris, France, July 3-6, 2015, 2015, pp. 861–898.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Improved Algorithms for Linear Stochastic Bandits",
      "author" : [ "Yasin Abbasi-Yadkori", "David Pal", "Csaba Szepesvari" ],
      "venue" : "Proc. NIPS, 2011, pp. 2312–2320.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Prediction, Learning, and Games",
      "author" : [ "N. Cesa-Bianchi", "G. Lugosi" ],
      "venue" : null,
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2006
    }, {
      "title" : "Near-optimal Regret Bounds for Reinforcement Learning",
      "author" : [ "Thomas Jaksch", "Ronald Ortner", "Peter Auer" ],
      "venue" : "JMLR, vol. 11, pp. 1563–1600, 2010.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Thompson Sampling for Complex Online Problems",
      "author" : [ "Aditya Gopalan", "Shie Mannor", "Yishay Mansour" ],
      "venue" : "Proc. International Conf. on Machine Learning, 2014.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : ", [1]–[3] and the user interests are assumed to be independent of the recommendation history, i.",
      "startOffset" : 2,
      "endOffset" : 5
    }, {
      "referenceID" : 2,
      "context" : ", [1]–[3] and the user interests are assumed to be independent of the recommendation history, i.",
      "startOffset" : 6,
      "endOffset" : 9
    }, {
      "referenceID" : 3,
      "context" : ", [4]–[6].",
      "startOffset" : 2,
      "endOffset" : 5
    }, {
      "referenceID" : 5,
      "context" : ", [4]–[6].",
      "startOffset" : 6,
      "endOffset" : 9
    }, {
      "referenceID" : 6,
      "context" : ", [7] but it is again in the fully observable state case, thus circumventing the critical problem of state uncertainty arising in user adaptation.",
      "startOffset" : 2,
      "endOffset" : 5
    }, {
      "referenceID" : 7,
      "context" : ", [8]; but these are primarily numerical studies.",
      "startOffset" : 2,
      "endOffset" : 5
    }, {
      "referenceID" : 8,
      "context" : "A more general framework for a restless multi-armed bandit with unobservable states and action-dependent transitions was considered in [9], [10].",
      "startOffset" : 135,
      "endOffset" : 138
    }, {
      "referenceID" : 9,
      "context" : "A more general framework for a restless multi-armed bandit with unobservable states and action-dependent transitions was considered in [9], [10].",
      "startOffset" : 140,
      "endOffset" : 144
    }, {
      "referenceID" : 9,
      "context" : "In [10] it was shown that the such a system is approximately Whittle-indexable.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 9,
      "context" : "The restless bandit that we propose in this paper is a special case of that from [10] for which we obtain much stronger results and also a Thompson sampling-based algorithm to learn the parameters of the arms.",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 10,
      "context" : "SINGLE ARM: OPTIMAL POLICY We solve the average reward problem described in the previous section by the vanishing discount approach [11], [12]—by first considering a discounted reward system and then taking limits as the discount vanishes.",
      "startOffset" : 132,
      "endOffset" : 136
    }, {
      "referenceID" : 11,
      "context" : "SINGLE ARM: OPTIMAL POLICY We solve the average reward problem described in the previous section by the vanishing discount approach [11], [12]—by first considering a discounted reward system and then taking limits as the discount vanishes.",
      "startOffset" : 138,
      "endOffset" : 142
    }, {
      "referenceID" : 9,
      "context" : "From [10], we can show that the following dynamic program solves (1).",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 0,
      "context" : "∣ < (1 − ρ) for all π ∈ [0, 1] and β ∈ [0, 1).",
      "startOffset" : 24,
      "endOffset" : 30
    }, {
      "referenceID" : 9,
      "context" : "The first two above follow directly from [10] and the last two are derived in [13].",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 12,
      "context" : "The first two above follow directly from [10] and the last two are derived in [13].",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 0,
      "context" : "Define V β := Vβ(π) − Vβ(1) for π ∈ [0, 1].",
      "startOffset" : 36,
      "endOffset" : 42
    }, {
      "referenceID" : 13,
      "context" : "Hence we can apply the Arzela-Ascoli theorem [14], to find a subsequence (V βk(π), (1 − β)Vβk(π)) that converges uniformly to (V (π), g) as βk → 1.",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 0,
      "context" : "Thus, as βk → 1, along an appropriate subsequence, (3) reduces to V (π) + g = max {λ+ V ((1− q)π), 1 − π(1 − ρ)} , (4) for all π ∈ [0, 1].",
      "startOffset" : 131,
      "endOffset" : 137
    }, {
      "referenceID" : 10,
      "context" : "17 in [11].",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 0,
      "context" : "If there exists a bounded function V (π) for π ∈ [0, 1] and a constant g that satisfies (4), then there exists a stationary policy φ such that",
      "startOffset" : 49,
      "endOffset" : 55
    }, {
      "referenceID" : 0,
      "context" : "for all π ∈ [0, 1], and moreover, φ is the policy for which the RHS of (4) is maximized.",
      "startOffset" : 12,
      "endOffset" : 18
    }, {
      "referenceID" : 14,
      "context" : "The learning algorithm (Algorithm 1) is a version of the popular Thompson sampling strategy [15], developed originally for stochastic multi-armed bandit problems [16], and subsequently extended to learning in Markov Decision Processes 1as in an AOR system where user behavior is unknown at start (MDPs) [17]–[19] and POMDPs.",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 15,
      "context" : "The learning algorithm (Algorithm 1) is a version of the popular Thompson sampling strategy [15], developed originally for stochastic multi-armed bandit problems [16], and subsequently extended to learning in Markov Decision Processes 1as in an AOR system where user behavior is unknown at start (MDPs) [17]–[19] and POMDPs.",
      "startOffset" : 303,
      "endOffset" : 307
    }, {
      "referenceID" : 17,
      "context" : "The learning algorithm (Algorithm 1) is a version of the popular Thompson sampling strategy [15], developed originally for stochastic multi-armed bandit problems [16], and subsequently extended to learning in Markov Decision Processes 1as in an AOR system where user behavior is unknown at start (MDPs) [17]–[19] and POMDPs.",
      "startOffset" : 308,
      "endOffset" : 312
    }, {
      "referenceID" : 0,
      "context" : "At the beginning, the algorithm initializes a prior or belief distribution2 on the space of all candidate parameters/models, which in our case is any subset X of the unit square [0, 1]× [0, 1] containing all possible POMDPs parameterized by (q, ρ).",
      "startOffset" : 178,
      "endOffset" : 184
    }, {
      "referenceID" : 0,
      "context" : "At the beginning, the algorithm initializes a prior or belief distribution2 on the space of all candidate parameters/models, which in our case is any subset X of the unit square [0, 1]× [0, 1] containing all possible POMDPs parameterized by (q, ρ).",
      "startOffset" : 186,
      "endOffset" : 192
    }, {
      "referenceID" : 0,
      "context" : "Algorithm 1: Thompson sampling algorithm for learning the optimal policy Input: Parameter space X ⊆ [0, 1], Policy space K ⊂ {0, 1, 2, .",
      "startOffset" : 100,
      "endOffset" : 106
    }, {
      "referenceID" : 18,
      "context" : "To this end, we consider a widely employed measure of performance from online learning theory, namely regret [20], [21].",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 19,
      "context" : "To this end, we consider a widely employed measure of performance from online learning theory, namely regret [20], [21].",
      "startOffset" : 115,
      "endOffset" : 119
    } ],
    "year" : 2016,
    "abstractText" : "We describe and study a model for an Automated Online Recommendation System (AORS) in which a user’s preferences can be time-dependent and can also depend on the history of past recommendations and play-outs. The three key features of the model that makes it more realistic compared to existing models for recommendation systems are (1) user preference is inherently latent, (2) current recommendations can affect future preferences, and (3) it allows for the development of learning algorithms with provable performance guarantees. The problem is cast as an average-cost restless multi-armed bandit for a given user, with an independent partially observable Markov decision process (POMDP) for each item of content. We analyze the POMDP for a single arm, describe its structural properties, and characterize its optimal policy. We then develop a Thompson sampling-based online reinforcement learning algorithm to learn the parameters of the model and optimize utility from the binary responses of the users to continuous recommendations. We then analyze the performance of the learning algorithm and characterize the regret. Illustrative numerical results and directions for extension to the restless hidden Markov multiarmed bandit problem are also presented.",
    "creator" : "LaTeX with hyperref package"
  }
}