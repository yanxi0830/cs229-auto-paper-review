{
  "name" : "1007.2049.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "joelv@cse.unsw.edu.au", "keesiong.ng@gmail.com", "marcus.hutter@anu.edu.au", "davidstarsilver@googlemail.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n00 7.\n20 49\nv1 [\ncs .L\nG ]\n1 3\nJu l 2\n01 0\nContents"
    }, {
      "heading" : "1 Introduction 1",
      "text" : ""
    }, {
      "heading" : "2 The Agent Setting 1",
      "text" : ""
    }, {
      "heading" : "3 Bayesian Agents 2",
      "text" : ""
    }, {
      "heading" : "4 Monte Carlo Expectimax Approximation 3",
      "text" : ""
    }, {
      "heading" : "5 Action-Conditional CTW 4",
      "text" : ""
    }, {
      "heading" : "6 Theoretical Results 6",
      "text" : ""
    }, {
      "heading" : "7 Experimental Results 6",
      "text" : ""
    }, {
      "heading" : "8 Related Work 7",
      "text" : ""
    }, {
      "heading" : "9 Limitations 8",
      "text" : ""
    }, {
      "heading" : "10 Conclusion 8",
      "text" : ""
    }, {
      "heading" : "11 Acknowledgements 8",
      "text" : "Keywords: Reinforcement Learning (RL); Context Tree Weighting (CTW); Monte Carlo Tree Search (MCTS); Upper Confidence bounds applied to Trees (UCT); Partially Observable Markov Decision Process (POMDP); Prediction Suffix Trees (PST)."
    }, {
      "heading" : "1 Introduction",
      "text" : "Consider an agent that exists within some unknown environment. The agent interacts with the environment in cycles. At each cycle, the agent executes an action and receives in turn an observation and a reward. The general reinforcement learning problem is to construct an agent that, over time, collects as much reward as possible from an initially unknown environment.\nThe AIXI agent [Hut05] is a formal, mathematical solution to the general reinforcement learning problem. It can be decomposed into two main components: planning and prediction. Planning amounts to performing an expectimax operation to determine each action. Prediction uses Bayesian model averaging, over the largest possible model class expressible on a Turing Machine, to predict future observations and rewards based on past experience. AIXI is shown in [Hut05] to be optimal in the sense that it will rapidly learn an accurate model of the unknown environment and exploit it to maximise its expected future reward.\nAs AIXI is only asymptotically computable, it is by no means an algorithmic solution to the general reinforcement learning problem. Rather it is best understood as a Bayesian optimality notion for decision making in general unknown environments. This paper demonstrates, for the first time, how a practical agent can be built from the AIXI theory. Our solution directly approximates the planning and prediction components of AIXI. In particular, we use a generalisation of UCT [KS06] to approximate the expectimax operation, and an agent-specific extension of CTW [WST95], a Bayesian model averaging algorithm for prediction suffix trees, for prediction and learning. Perhaps surprisingly, this kind of direct approximation is possible, practical and theoretically appealing. Importantly, the essential characteristic of AIXI, its generality, can be largely preserved."
    }, {
      "heading" : "2 The Agent Setting",
      "text" : "This section introduces the notation and terminology we will use to describe strings of agent experience, the true underlying environment and the agent’s model of the environment.\nThe (finite) action, observation, and reward spaces are denoted by A,O, and R respectively. An observationreward pair or is called a percept. We use X to denote the percept space O × R.\nDefinition 1 A history h is an element of (A × X)∗ ∪ (A × X)∗ ×A.\nNotation: A string x1x2 . . . xn of length n is denoted by x1:n. The empty string is denoted by ǫ. The concatenation of two\nstrings s and r is denoted by sr. The prefix x1: j of x1:n, j ≤ n, is denoted by x≤ j or x< j+1. The notation generalises for blocks of symbols: e.g. ax1:n denotes a1x1a2x2 . . .anxn and ax< j denotes a1x1a2x2 . . . a j−1x j−1.\nThe following definition states that the environment takes the form of a probability distribution over possible percept sequences conditioned on actions taken by the agent.\nDefinition 2 An environment ρ is a sequence of conditional probability functions {ρ0, ρ1, ρ2, . . . }, where ρn : An → Density (Xn), that satisfies\n∀a1:n∀x<n : ρn−1(x<n | a<n) = ∑\nxn∈X\nρn(x1:n | a1:n). (1)\nIn the base case, we have ρ0(ǫ | ǫ) = 1.\nEquation 1, called the chronological condition in [Hut05], captures the natural constraint that action an has no effect on observations made before it. For convenience, we drop the index n in ρn from here onwards.\nGiven an environment ρ,\nρ(xn | ax<nan) := ρ(x1:n | a1:n) ρ(x<n | a<n)\n(2)\nis the ρ-probability of observing xn in cycle n given history h = ax<nan, provided ρ(x<n | a<n) > 0. It now follows that\nρ(x1:n | a1:n) = ρ(x1 | a1)ρ(x2 | ax1a2) · · ·ρ(xn | ax<nan). (3)\nDefinition 2 is used to describe both the true (but unknown) underlying environment and the agent’s subjective model of the environment. The latter is called the agent’s environment model and is typically learnt from data. Definition 2 is extremely general. It captures a wide variety of environments, including standard reinforcement learning setups such as MDPs and POMDPs.\nThe agent’s goal is to accumulate as much reward as it can during its lifetime. More precisely, the agent seeks a policy that will allow it to maximise its expected future reward up to a fixed, finite, but arbitrarily large horizon m ∈ N. Formally, a policy is a function that maps a history to an action. The expected future value of an agent acting under a particular policy is defined as follows.\nDefinition 3 Given history ax1:t, the m-horizon expected future reward of an agent acting under policy π : (A × X)∗ → A with respect to an environment ρ is\nvmρ (π, ax1:t) := Ext+1:t+m∼ρ\n       t+m ∑\ni=t+1\nRi(ax≤t+m)\n       , (4)\nwhere for t + 1 ≤ k ≤ t + m, ak := π(ax<k), and Rk(aor≤t+m) := rk. The quantity vmρ (π, ax1:tat+1) is defined similarly, except that at+1 is now no longer defined by π.\nThe optimal policy π∗ is the policy that maximises the expected future reward. The maximal achievable expected future reward of an agent with history h in environment ρ looking m steps ahead is Vmρ (h) := v m ρ (π\n∗, h). It is easy to see that if h ≡ ax1:t ∈ (A× X)t, then\nVmρ (h) = maxat+1\n∑\nxt+1\nρ(xt+1 | hat+1) · · ·max at+m\n∑\nxt+m\nρ(xt+m | haxt+1:t+m−1at+m)\n       t+m ∑\ni=t+1\nri\n       . (5)\nWe will refer to Equation 5 as the expectimax operation. The m-horizon optimal action a∗t+1 at time t + 1 is related to the expectimax operation by\na∗t+1 = arg maxat+1 Vmρ (ax1:tat+1) (6)\nEqs 4 and 5 can be modified to handle discounted reward, however we focus on the finite-horizon case since it both aligns with AIXI and allows for a simplified presentation."
    }, {
      "heading" : "3 Bayesian Agents",
      "text" : "In the general reinforcement learning setting, the environment ρ is unknown to the agent. One way to learn an environment model is to take a Bayesian approach. Instead of committing to any single environment model, the agent uses a mixture of environment models. This requires committing to a class of possible environments (the model class), assigning an initial weight to each possible environment (the prior), and subsequently updating the weight for each model using Bayes rule (computing the posterior) whenever more experience is obtained.\nThe above procedure is similar to Bayesian methods for predicting sequences of (singly typed) observations. The key difference in the agent setup is that each prediction is now also dependent on previous agent actions. We incorporate this by using the action-conditional definitions and identities of Section 2.\nDefinition 4 Given a model class M := {ρ1, ρ2, . . . } and a prior weight wρ0 > 0 for each ρ ∈ M such that ∑\nρ∈M w ρ\n0 = 1, the mixture environment model is ξ(x1:n | a1:n) := ∑\nρ∈M\nwρ0ρ(x1:n | a1:n).\nThe next result follows immediately.\nProposition 1 A mixture environment model is an environment model.\nProposition 1 allows us to use a mixture environment model whenever we can use an environment model. Its importance will become clear shortly.\nTo make predictions using a mixture environment model ξ, we use\nξ(xn | ax<nan) = ξ(x1:n | a1:n) ξ(x<n | a<n) , (7)\nwhich follows from Proposition 1 and Eq. 2. The RHS of Eq. 7 can be written out as a convex combination of model predictions to give\nξ(xn | ax<nan) = ∑\nρ∈M\nwρn−1ρ(xn | ax<nan), (8)\nwhere the posterior weight wρn−1 for ρ is given by\nwρn−1 := wρ0ρ(x<n | a<n) ∑\nµ∈M\nwµ0µ(x<n | a<n) = Pr(ρ | ax<n). (9)\nBayesian agents enjoy a number of strong theoretical performance guarantees; these are explored in Section 6. In practice, the main difficulty in using a mixture environment model is computational. A rich model class is required if the mixture environment model is to possess general prediction capabilities, however naively using (8) for online prediction requires at least O(|M|) time to process each new piece of experience. One of our main contributions, introduced in Section 5, is a large, efficiently computable mixture environment model that runs in time O(log(log |M|)). Before looking at that, we will examine in the next section a Monte Carlo Tree Search algorithm for approximating the expectimax operation."
    }, {
      "heading" : "4 Monte Carlo Expectimax Approximation",
      "text" : "Full-width computation of the expectimax operation (5) takes O(|A×X|m) time, which is unacceptable for all but tiny values of m. This section introduces ρUCT, a generalisation of the popular UCT algorithm [KS06] that can be used to approximate a finite horizon expectimax operation given an environment model ρ. The key idea of Monte Carlo search is to sample observations from the environment, rather than exhaustively considering all possible observations. This allows for effective planning in environments with large observation spaces. Note that since an environment model subsumes both MDPs and POMDPs, ρUCT effectively extends the UCT algorithm to a wider class of problem domains.\nThe UCT algorithm has proven effective in solving large discounted or finite horizon MDPs. It assumes a generative model of the MDP that when given a state-action pair (s, a) produces a subsequent state-reward pair (s′, r) distributed according to Pr(s′, r | s, a). By successively sampling trajectories through the state space, the UCT algorithm incrementally constructs a search tree, with each node containing an estimate of the value of each state. Given enough time, these estimates converge to the true values.\nThe ρUCT algorithm can be realised by replacing the notion of state in UCT by an agent history h (which is always a sufficient statistic) and using an environment model ρ(or | h)\nto predict the next percept. The main subtlety with this extension is that the history used to determine the conditional probabilities must be updated during the search to reflect the extra information an agent will have at a hypothetical future point in time.\nWe will use Ψ to represent all the nodes in the search tree, Ψ(h) to represent the node corresponding to a particular history h, V̂mρ (h) to represent the sample-based estimate of the expected future reward, and T (h) to denote the number of times a node Ψ(h) has been sampled. Nodes corresponding to histories that end or do not end with an action are called chance and decision nodes respectively.\nAlgorithm 1 describes the top-level algorithm, which the agent calls at the beginning of each cycle. It is initialised with the agent’s total experience h (up to time t) and the planning horizon m. It repeatedly invokes the Sample routine until out of time. Importantly, ρUCT is an anytime algorithm; an approximate best action, whose quality improves with time, is always available. This is retrieved by BestAction, which computes a∗t = arg maxat V̂mρ (ax<tat).\nAlgorithm 1 ρUCT(h,m) Require: A history h Require: A search horizon m ∈ N\n1: Initialise(Ψ) 2: repeat 3: Sample(Ψ, h,m) 4: until out of time 5: return BestAction(Ψ, h)\nAlgorithm 2 describes the recursive routine used to sample a single future trajectory. It uses the SelectAction routine to choose moves at interior nodes, and invokes the Rollout routine at unexplored leaf nodes. The Rollout routine picks actions uniformly at random until the (remaining) horizon is reached, returning the accumulated reward. After a complete trajectory of length m is simulated, the value estimates are updated for each node traversed. Notice that the recursive calls on Lines 6 and 11 append the most recent percept or action to the history argument.\nAlgorithm 3 describes the UCB [Aue02] policy used to select actions at decision nodes. The α and β constants denote the smallest and largest elements of R respectively. The parameter C varies the selectivity of the search; larger values grow bushier trees. UCB automatically focuses attention on the best looking action in such a way that the sample estimate V̂ρ(h) converges to Vρ(h), whilst still exploring alternate actions sufficiently often to guarantee that the best action will be found.\nThe ramifications of the ρUCT extension are particularly significant to Bayesian agents described in Section 3. Proposition 1 allows ρUCT to be instantiated with a mixture environment model, which directly incorporates\nAlgorithm 2 Sample(Ψ, h,m) Require: A search tree Ψ Require: A history h Require: A remaining search horizon m ∈ N\n1: if m = 0 then 2: return 0 3: else if Ψ(h) is a chance node then 4: Generate (o, r) from ρ(or | h) 5: Create node Ψ(hor) if T (hor) = 0 6: reward ← r + Sample(Ψ, hor,m − 1) 7: else if T (h) = 0 then 8: reward ← Rollout(h,m) 9: else\n10: a ← SelectAction(Ψ, h,m) 11: reward ← Sample(Ψ, ha,m) 12: end if 13: V̂(h) ← 1T (h)+1 [reward + T (h)V̂(h)] 14: T (h) ← T (h) + 1 15: return reward\nmodel uncertainty into the planning process. This gives (in principle, provided that the model class contains the true environment and ignoring issues of limited computation) the well known Bayes-optimal solution to the exploration/exploitation dilemma; namely, if a reduction in model uncertainty would lead to higher expected future reward, ρUCT would recommend an information gathering action.\nAlgorithm 3 SelectAction(Ψ, h,m) Require: A search tree Ψ Require: A history h Require: A remaining search horizon m ∈ N Require: An exploration/exploitation constant C > 0\n1: U = {a ∈ A : T (ha) = 0} 2: if U , {} then 3: Pick a ∈ U uniformly at random 4: Create node Ψ(ha) 5: return a 6: else 7: return arg max\na∈A\n{\n1 m(β−α) V̂(ha) + C\n√\nlog(T (h)) T (ha)\n}\n8: end if"
    }, {
      "heading" : "5 Action-Conditional CTW",
      "text" : "We now introduce a large mixture environment model for use with ρUCT. Context Tree Weighting (CTW) [WST95] is an efficient and theoretically well-studied binary sequence prediction algorithm that works well in practice. It is an online Bayesian model averaging algorithm that com-\nputes, at each time point t, the probability\nPr(y1:t) = ∑\nM\nPr(M) Pr(y1:t | M), (10)\nwhere y1:t is the binary sequence seen so far, M is a prediction suffix tree [RST96], Pr(M) is the prior probability of M, and the summation is over all prediction suffix trees of bounded depth D. A naive computation of (10) takes time O(22 D ); using CTW, this computation requires only O(D) time. In this section, we outline how CTW can be extended to compute probabilities of the form\nPr(x1:t | a1:t) = ∑\nM\nPr(M) Pr(x1:t | M, a1:t), (11)\nwhere x1:t is a percept sequence, a1:t is an action sequence, and M is a prediction suffix tree as in (10). This extension allows CTW to be used as a mixture environment model (Definition 4) in the ρUCT algorithm, where we combine (11) and (2) to predict the next percept given a history.\nKrichevsky-Trofimov Estimator. We start with a brief review of the KT estimator for Bernoulli distributions. Given a binary string y1:t with a zeroes and b ones, the KT estimate of the probability of the next symbol is given by\nPrkt(Yt+1 = 1 | y1:t) := b + 1/2\na + b + 1 . (12)\nThe KT estimator can be obtained via a Bayesian analysis by putting an uninformative (Jeffreys Beta(1/2,1/2)) prior Pr(θ) ∝ θ−1/2(1 − θ)−1/2 on the parameter θ ∈ [0, 1] of the Bernoulli distribution. The probability of a string y1:t is given by\nPrkt(y1:t) = Prkt(y1 | ǫ)Prkt(y2 | y1) · · ·Prkt(yt | y<t)\n= ∫ θb(1 − θ)a Pr(θ) dθ.\nPrediction Suffix Trees. We next describe prediction suffix trees. We consider a binary tree where all the left edges are labelled 1 and all the right edges are labelled 0. The depth of a binary tree M is denoted by d(M). Each node in M can be identified by a string in {0, 1}∗ as usual: ǫ represents the root node of M; and if n ∈ {0, 1}∗ is a node in M, then n1 and n0 represent respectively the left and right children of node n. The set of M’s leaf nodes is denoted by L(M) ⊂ {0, 1}∗. Given a binary string y1:t where t ≥ d(M), we define M(y1:t) := ytyt−1 . . . yt′ , where t′ ≤ t is the (unique) positive integer such that ytyt−1 . . . yt′ ∈ L(M).\nDefinition 5 A prediction suffix tree (PST) is a pair (M,Θ), where M is a binary tree and associated with each l ∈ L(M) is a distribution over {0, 1} parameterised by θl ∈ Θ. We call M the model of the PST and Θ the parameter of the PST.\nA PST (M,Θ) maps each binary string y1:t, t ≥ d(M), to θM(y1:t); the intended meaning is that θM(y1:t) is the probability that the next bit following y1:t is 1. For example, the PST in Figure 1 maps the string 1110 to θM(1110) = θ01 = 0.3, which means the next bit after 1110 is 1 with probability 0.3.\nAction-Conditional PST. In the agent setting, we reduce the problem of predicting history sequences with general non-binary alphabets to that of predicting the bit representations of those sequences. Further, we only ever condition on actions; this is achieved by appending bit representations of actions to the input sequence without updating the PST parameters.\nAssume |X| = 2lX for some lX > 0. Denote by ~x = x[1, lX] = x[1]x[2] . . . x[lX] the bit representation of x ∈ X. Denote by ~x1:t = ~x1 ~x2 . . . ~xt the bit representation of a sequence x1:t. Action symbols are treated similarly.\nTo do action-conditional sequence prediction using a PST with a given model M but unknown parameter, we start with θl := Prkt(1 | ǫ) = 1/2 at each l ∈ L(M). We set aside an initial portion of the binary history sequence to initialise the variable h and then repeat the following steps as long as needed:\n1. set h := h~a , where a is the current selected action;\n2. for i := 1 to lX do\n(a) predict the next bit using the distribution θM(h);\n(b) observe the next bit x[i], update θM(h) using (12) according to the value of x[i], and then set h := hx[i].\nLet M be the model of a prediction suffix tree, a1:t an action sequence, x1:t a percept sequence, and h := ~ax1:t . For each node n in M, define hM,n by\nhM,n := hi1 hi2 · · · hik (13)\nwhere 1 ≤ i1 < i2 < · · · < ik ≤ t and, for each i, i ∈ {i1, i2, . . . ik} iff hi is a percept bit and n is a prefix of M(h1:i−1). We have the following expression for the probability of x1:t given M and a1:t:\nPr(x1:t | M, a1:t) = t ∏\ni=1\nlX ∏\nj=1\nPr(xi[ j] | M, ~ax<iai xi[1, j − 1])\n= ∏\nn∈L(M)\nPrkt(hM,n). (14)\nContext Tree Weighting. The above deals with actionconditional prediction using a single PST. We now show how we can efficiently perform action-conditional prediction using a Bayesian mixture of PSTs. There are two main computational tricks: the use of a data structure to represent all PSTs of a certain maximum depth and the use of probabilities of sequences in place of conditional probabilities.\nDefinition 6 A context tree of depth D is a perfect binary tree of depth D such that attached to each node (both internal and leaf) is a probability on {0, 1}∗.\nThe weighted probability Pnw of each node n in the context tree T after seeing h := ~ax1:t is defined as follows:\nPnw :=\n      \nPrkt(hT,n) if n is a leaf node; 1 2 Prkt(hT,n) + 1 2 P n0 w × P n1 w otherwise.\nThe following is a straightforward extension of a result due to [WST95].\nLemma 1 Let T be the depth-D context tree after seeing h := ~ax1:t . For each node n in T at depth d, we have\nPnw = ∑\nM∈CD−d\n2−ΓD−d (M) ∏\nl∈L(M)\nPrkt(hT,nl), (15)\nwhere Cd is the set of all models of PSTs with depth ≤ d, and Γd(M) is the code-length for M given by the number of nodes in M minus the number of leaf nodes in M of depth d.\nA corollary of Lemma 1 is that at the root node ǫ of the context tree T after seeing h := ~ax1:t , we have\nPǫw(x1:t|a1:t) = ∑\nM∈CD\n2−ΓD(M) ∏\nl∈L(M)\nPrkt(hT,l) (16)\n= ∑\nM∈CD\n2−ΓD(M) ∏\nl∈L(M)\nPrkt(hM,l) (17)\n= ∑\nM∈CD\n2−ΓD(M) Pr(x1:t | M, a1:t), (18)\nwhere the last step follows from (14). Notice that the prior 2−ΓD(·) penalises PSTs with large tree structures. The conditional probability of xt given ax<tat can be obtained from (2). We can also efficiently sample the individual bits of xt one by one.\nComputational Complexity. The Action-Conditional CTW algorithm grows the context tree dynamically. Using a context tree with depth D, there are at most O(tD log(|O||R|)) nodes in the context tree after t cycles. In practice, this is a lot less than 2D, the number of nodes in a fully grown context tree. The time complexity of Action-Conditional CTW is also impressive, requiring O(D log(|O||R|)) time to process each new piece of agent experience and O(mD log(|O||R|)) to sample a single trajectory when combined with ρUCT. Importantly, this is independent of t, which means that the computational overhead does not increase as the agent gathers more experience."
    }, {
      "heading" : "6 Theoretical Results",
      "text" : "Putting the ρUCT and Action-Conditional CTW algorithms together yields our approximate AIXI agent. We now investigate some of its properties.\nModel Class Approximation. By instantiating (5) with the mixture environment model (18), one can show that the optimal action for an agent at time t, having experienced ax<t, is given by\narg max at\n∑\nxt\n· · ·max at+m\n∑\nxt+m\n       t+m ∑\ni=t\nri\n       ∑\nM∈CD\n2−ΓD(ρ) Pr(x1:t+m | M, a1:t+m).\nCompare this to the action chosen by the AIXI agent\narg max at\n∑\nxt\n. . .max at+m\n∑\nxt+m\n       t+m ∑\ni=t\nri\n       ∑\nρ∈M\n2−K(ρ)ρ(x1:t+m | a1:t+m),\nwhere class M consists of all computable environments ρ and K(ρ) denotes the Kolmogorov complexity of ρ. Both use a prior that favours simplicity. The main difference is in the subexpression describing the mixture over the model class. AIXI uses a mixture over all enumerable chronological semimeasures, which is completely general but incomputable. Our approximation uses a mixture of all prediction suffix trees of a certain maximum depth, which is still a rather general class, but one that is efficiently computable.\nConsistency of ρUCT. [KS06] shows that the UCT algorithm is consistent in finite horizon MDPs and derive finite sample bounds on the estimation error due to sampling. By interpreting histories as Markov states, the general reinforcement learning problem reduces to a finite horizon MDP and the results of [KS06] are now directly applicable. Restating the main consistency result in our notation, we have\n∀ǫ∀h lim T (h)→∞\nPr ( |Vmρ (h) − V̂ m ρ (h)| ≤ ǫ ) = 1. (19)\nFurthermore, the probability that a suboptimal action (with respect to Vmρ (·)) is chosen by ρUCT goes to zero in the limit.\nConvergence to True Environment. The next result, adapted from [Hut05], shows that if there is a good model of the (unknown) environment in CD, then Action-Conditional CTW will predict well.\nTheorem 1 Let µ be the true environment, and Υ ≡ Pǫw the mixture environment model formed from (18). The µexpected squared difference of µ and Υ is bounded as fol-\nlows. For all n ∈ N, for all a1:n,\nn ∑\nk=1\n∑\nx<k\nµ(x<k | a<k) ∑\nxk\n( µ(xk | ax<kak) − Υ(xk | ax<kak) )2\n≤ min M∈CD\n{ ΓD(M) ln 2 + KL(µ(· | a1:n) ‖ Pr(· | M, a1:n)) } ,\n(20)\nwhere KL(· ‖ ·) is the KL divergence of two distributions.\nIf the RHS of (20) is finite over all n, then the sum on the LHS can only be finite if Υ converges sufficiently fast to µ. If KL grows sublinear in n, then Υ still converges to µ (in a weaker Cesaro sense), which is for instance the case for all k-order Markov and all stationary processes µ.\nOverall Result. Theorem 1 above in conjunction with [Hut05, Thm.5.36] imply Vm\nΥ (h) converges to Vmµ (h) as long\nas there exists a model in the model class that approximates the unknown environment µ well. This, and the consistency (19) of the ρUCT algorithm, imply that V̂m\nΥ (h) converges to\nVmµ (h). More detail can be found in [VNHS09]."
    }, {
      "heading" : "7 Experimental Results",
      "text" : "This section evaluates our approximate AIXI agent on a variety of test domains. The Cheese Maze, 4x4 Grid and Extended Tiger domains are taken from the POMDP literature. The TicTacToe domain comprises a repeated series of games against an opponent who moves randomly. The Biased RockPaperScissor domain is described in [FMWR07], which involves the agent repeatedly playing RockPaperScissor against an exploitable opponent. Two more challenging domains are included: Kuhn Poker [HSHB05], where the agent plays second against a Nash optimal player and a partially observable version of Pacman described in [VNHS09]. With the exception of Pacman, each domain has a known optimal solution. Although our domains are modest, requiring the agent to learn the environment from scratch significantly increases the difficulty of each of these problems.\nTable 1 outlines the parameters used in each experiment. The sizes of the action and observation spaces are given,\nalong with the number of bits used to encode each space. The context depth parameter D specifies the maximal number of recent bits used by the Action-Conditional CTW prediction scheme. The search horizon is given by the parameter m. Larger D and m increase the capabilities of our agent, at the expense of linearly increasing computation time; our values represent an appropriate compromise between these two competing dimensions for each problem domain.\nFigure 2 shows how the performance of the agent scales with experience, measured in terms of number of interaction cycles. Experience was gathered by a decaying ǫgreedy policy, which chose randomly or used ρUCT. The results are normalised with respect to the optimal average reward per time step, except in Pacman, where we normalised to an estimate. Each data point was obtained by starting the agent with an amount of experience given by the x-axis and running it greedily for 2000 cycles. The amount of search used for each problem domain, measured by the number of ρUCT simulations per cycle, is given in Table 2. (The average search time per cycle is also given.) The agent converges to optimality on all the test domains with known optimal values, and exhibits good scaling properties on our challenging Pacman variant. Visual inspection1 of Pacman shows that the agent, whilst not playing perfectly, has already learnt a number of important concepts.\nTable 2 summarises the resources required for approximately optimal performance on our test domains. Timing statistics were collected on an Intel dual 2.53Ghz Xeon. Domains that included a planning component such as Tiger required more search. Convergence was somewhat slower in TicTacToe; the main difficulty for the agent was learning not to lose the game immediately by playing an illegal move. Most impressive was that the agent learnt to play an approximate best response strategy for Kuhn Poker, without knowing the rules of the game or the opponent’s strategy.\n1http://www.youtube.com/watch?v=RhQTWidQQ8U"
    }, {
      "heading" : "8 Related Work",
      "text" : "The BLHT algorithm [SH99] is closely related to our work. It uses symbol level PSTs for learning and an (unspecified) dynamic programming based algorithm for control. BLHT uses the most probable model for prediction, whereas we use a mixture model, which admits a much stronger convergence result. A further distinction is our usage of an Ockham prior instead of a uniform prior over PST models.\nThe Active-LZ [FMWR07] algorithm combines a Lempel-Ziv based prediction scheme with dynamic programming for control to produce an agent that is provably asymptotically optimal if the environment is n-Markov. We implemented the Active-LZ test domain, Biased RPS, and compared against their published results. Our agent was able to achieve optimal levels of performance within 106 cycles; in contrast, Active-LZ was still suboptimal after 108 cycles. U-Tree [McC96] is an online agent algorithm that attempts to discover a compact state representation from a raw stream of experience. Each state is represented as the leaf of a suffix tree that maps history sequences to states. As more experience is gathered, the state representation is refined according to a heuristic built around the KolmogorovSmirnov test. This heuristic tries to limit the growth of the suffix tree to places that would allow for better prediction of future reward. Value Iteration is used at each time step to update the value function for the learned state representation, which is then used by the agent for action selection.\nIt is instructive to compare and contrast our AIXI approximation with the Active-LZ and U-Tree algorithms. The small state space induced by U-Tree has the benefit of limiting the number of parameters that need to be estimated from data. This has the potential to dramatically speed up the model-learning process. In contrast, both Active-LZ and our approach require a number of parameters proportional to the number of distinct contexts. This is one of the reasons why Active-LZ exhibits slow convergence in practice. This problem is much less pronounced in our approach for two reasons. First, the Ockham prior in CTW ensures that future predictions are dominated by PST structures that have seen enough data to be trustworthy. Secondly, value function estimation is decoupled from the process of context estimation. Thus it is reasonable to expect ρUCT to make good local decisions provided ActionConditional CTW can predict well. The downside however\nis that our approach requires search for action selection. Although ρUCT is an anytime algorithm, in practice more computation is required per cycle compared to approaches like Active-LZ and U-Tree that act greedily with respect to an estimated global value function.\nThe U-Tree algorithm is well motivated, but unlike Active-LZ and our approach, it lacks theoretical performance guarantees. It is possible for U-Tree to prematurely converge to a locally optimal state representation from which the heuristic splitting criterion can never recover. Furthermore, the splitting heuristic contains a number of configuration options that can dramatically influence its performance [McC96]. This parameter sensitivity somewhat limits the algorithm’s applicability to the general reinforcement learning problem.\nOur work is also related to Bayesian Reinforcement Learning. In model-based Bayesian RL [PV08, Str00], a distribution over (PO)MDP parameters is maintained. In contrast, we maintain an exact Bayesian mixture of PSTs. The ρUCT algorithm shares similarities with Bayesian Sparse Sampling [WLBS05]; the key differences are estimating the leaf node values with a rollout function and guiding the search with the UCB policy.\nA more comprehensive discussion of related work can be found in [VNHS09]."
    }, {
      "heading" : "9 Limitations",
      "text" : "The main limitation of our current AIXI approximation is the restricted model class. Our agent will perform poorly if the underlying environment cannot be predicted well by a PST of bounded depth. Prohibitive amounts of experience will be required if a large PST model is needed for accurate prediction. For example, it would be unrealistic to think that our current AIXI approximation could cope with real-world image or audio data.\nThe identification of efficient and general model classes that better approximate the AIXI ideal is an important area for future work. Some preliminary ideas are explored in [VNHS09]."
    }, {
      "heading" : "10 Conclusion",
      "text" : "We have introduced the first computationally tractable approximation to the AIXI agent and shown that it provides a promising approach to the general reinforcement learning problem. Investigating multi-alphabet CTW for prediction, parallelisation of ρUCT, further expansion of the model class (ideally, beyond variable-order Markov models) or more sophisticated rollout policies for ρUCT are exciting areas for future investigation."
    }, {
      "heading" : "11 Acknowledgements",
      "text" : "This work received support from the Australian Research Council under grant DP0988049. NICTA is funded by the Australian Government as represented by the Department of Broadband, Communications and the Digital Economy and the Australian Research Council through the ICT Centre of Excellence program."
    } ],
    "references" : [ {
      "title" : "JMLR",
      "author" : [ "Peter Auer. Using confidence bounds for exploitation-exploration trade-offs" ],
      "venue" : "3:397–422,",
      "citeRegEx" : "Aue02",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Universal Reinforcement Learning",
      "author" : [ "V. Farias", "C. Moallemi", "T. Weissman", "B. Van Roy" ],
      "venue" : "CoRR, abs/0707.3087",
      "citeRegEx" : "FMWR07",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "In AAAI’05",
      "author" : [ "Bret Hoehn", "Finnegan Southey", "Robert C. Holte", "Valeriy Bulitko. Effective short-term opponent exploitation in simplified poker" ],
      "venue" : "pages 783–788,",
      "citeRegEx" : "HSHB05",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Universal Artificial Intelligence: Sequential Decisions Based on Algorithmic Probability",
      "author" : [ "Marcus Hutter" ],
      "venue" : "Springer,",
      "citeRegEx" : "Hut05",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "In ECML",
      "author" : [ "Levente Kocsis", "Csaba Szepesvári. Bandit based Monte-Carlo planning" ],
      "venue" : "pages 282–293,",
      "citeRegEx" : "KS06",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "PhD thesis",
      "author" : [ "Andrew Kachites McCallum. Reinforcement Learning with Selective Perception", "Hidden State" ],
      "venue" : "University of Rochester,",
      "citeRegEx" : "McC96",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Model-based Bayesian Reinforcement Learning in Partially Observable Domains",
      "author" : [ "Pascal Poupart", "Nikos Vlassis" ],
      "venue" : "ISAIM,",
      "citeRegEx" : "PV08",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "The power of amnesia: Learning probabilistic automata with variable memory length",
      "author" : [ "D. Ron", "Y. Singer", "N. Tishby" ],
      "venue" : "Machine Learning, 25(2):117–150",
      "citeRegEx" : "RST96",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "In NIPS",
      "author" : [ "Nobuo Suematsu", "Akira Hayashi. A reinforcement learning algorithm in partially observable environments using short-term memory" ],
      "venue" : "pages 1059– 1065,",
      "citeRegEx" : "SH99",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "A Bayesian framework for reinforcement learning",
      "author" : [ "M. Strens" ],
      "venue" : "ICML, pages 943–950",
      "citeRegEx" : "Str00",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "CoRR",
      "author" : [ "Joel Veness", "Kee Siong Ng", "Marcus Hutter", "David Silver. A Monte Carlo AIXI Approximation" ],
      "venue" : "abs/0909.0801,",
      "citeRegEx" : "VNHS09",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Bayesian sparse sampling for on-line reward optimization",
      "author" : [ "T. Wang", "D.J. Lizotte", "M.H. Bowling", "D. Schuurmans" ],
      "venue" : "ICML, pages 956–963",
      "citeRegEx" : "WLBS05",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "The Context Tree Weighting Method: Basic Properties",
      "author" : [ "Frans M.J. Willems", "Yuri M. Shtarkov", "Tjalling J. Tjalkens" ],
      "venue" : "IEEE Transactions on Information Theory, 41:653–664,",
      "citeRegEx" : "WST95",
      "shortCiteRegEx" : null,
      "year" : 1995
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "The AIXI agent [Hut05] is a formal, mathematical solution to the general reinforcement learning problem.",
      "startOffset" : 15,
      "endOffset" : 22
    }, {
      "referenceID" : 3,
      "context" : "AIXI is shown in [Hut05] to be optimal in the sense that it will rapidly learn an accurate model of the unknown environment and exploit it to maximise its expected future reward.",
      "startOffset" : 17,
      "endOffset" : 24
    }, {
      "referenceID" : 4,
      "context" : "In particular, we use a generalisation of UCT [KS06] to approximate the expectimax operation, and an agent-specific extension of CTW [WST95], a Bayesian model averaging algorithm for prediction suffix trees, for prediction and learning.",
      "startOffset" : 46,
      "endOffset" : 52
    }, {
      "referenceID" : 12,
      "context" : "In particular, we use a generalisation of UCT [KS06] to approximate the expectimax operation, and an agent-specific extension of CTW [WST95], a Bayesian model averaging algorithm for prediction suffix trees, for prediction and learning.",
      "startOffset" : 133,
      "endOffset" : 140
    }, {
      "referenceID" : 3,
      "context" : "Equation 1, called the chronological condition in [Hut05], captures the natural constraint that action an has no effect on observations made before it.",
      "startOffset" : 50,
      "endOffset" : 57
    }, {
      "referenceID" : 4,
      "context" : "This section introduces ρUCT, a generalisation of the popular UCT algorithm [KS06] that can be used to approximate a finite horizon expectimax operation given an environment model ρ.",
      "startOffset" : 76,
      "endOffset" : 82
    }, {
      "referenceID" : 0,
      "context" : "Algorithm 3 describes the UCB [Aue02] policy used to select actions at decision nodes.",
      "startOffset" : 30,
      "endOffset" : 37
    }, {
      "referenceID" : 12,
      "context" : "Context Tree Weighting (CTW) [WST95] is an efficient and theoretically well-studied binary sequence prediction algorithm that works well in practice.",
      "startOffset" : 29,
      "endOffset" : 36
    }, {
      "referenceID" : 7,
      "context" : "where y1:t is the binary sequence seen so far, M is a prediction suffix tree [RST96], Pr(M) is the prior probability of M, and the summation is over all prediction suffix trees of bounded depth D.",
      "startOffset" : 77,
      "endOffset" : 84
    }, {
      "referenceID" : 12,
      "context" : "The following is a straightforward extension of a result due to [WST95].",
      "startOffset" : 64,
      "endOffset" : 71
    }, {
      "referenceID" : 4,
      "context" : "[KS06] shows that the UCT algorithm is consistent in finite horizon MDPs and derive finite sample bounds on the estimation error due to sampling.",
      "startOffset" : 0,
      "endOffset" : 6
    }, {
      "referenceID" : 4,
      "context" : "By interpreting histories as Markov states, the general reinforcement learning problem reduces to a finite horizon MDP and the results of [KS06] are now directly applicable.",
      "startOffset" : 138,
      "endOffset" : 144
    }, {
      "referenceID" : 3,
      "context" : "The next result, adapted from [Hut05], shows that if there is a good model of the (unknown) environment in CD, then Action-Conditional CTW will predict well.",
      "startOffset" : 30,
      "endOffset" : 37
    }, {
      "referenceID" : 10,
      "context" : "More detail can be found in [VNHS09].",
      "startOffset" : 28,
      "endOffset" : 36
    }, {
      "referenceID" : 1,
      "context" : "The Biased RockPaperScissor domain is described in [FMWR07], which involves the agent repeatedly playing RockPaperScissor against an exploitable opponent.",
      "startOffset" : 51,
      "endOffset" : 59
    }, {
      "referenceID" : 2,
      "context" : "Two more challenging domains are included: Kuhn Poker [HSHB05], where the agent plays second against a Nash optimal player and a partially observable version of Pacman described in [VNHS09].",
      "startOffset" : 54,
      "endOffset" : 62
    }, {
      "referenceID" : 10,
      "context" : "Two more challenging domains are included: Kuhn Poker [HSHB05], where the agent plays second against a Nash optimal player and a partially observable version of Pacman described in [VNHS09].",
      "startOffset" : 181,
      "endOffset" : 189
    }, {
      "referenceID" : 8,
      "context" : "The BLHT algorithm [SH99] is closely related to our work.",
      "startOffset" : 19,
      "endOffset" : 25
    }, {
      "referenceID" : 1,
      "context" : "The Active-LZ [FMWR07] algorithm combines a Lempel-Ziv based prediction scheme with dynamic programming for control to produce an agent that is provably asymptotically optimal if the environment is n-Markov.",
      "startOffset" : 14,
      "endOffset" : 22
    }, {
      "referenceID" : 5,
      "context" : "U-Tree [McC96] is an online agent algorithm that attempts to discover a compact state representation from a raw stream of experience.",
      "startOffset" : 7,
      "endOffset" : 14
    }, {
      "referenceID" : 5,
      "context" : "Furthermore, the splitting heuristic contains a number of configuration options that can dramatically influence its performance [McC96].",
      "startOffset" : 128,
      "endOffset" : 135
    }, {
      "referenceID" : 11,
      "context" : "The ρUCT algorithm shares similarities with Bayesian Sparse Sampling [WLBS05]; the key differences are estimating the leaf node values with a rollout function and guiding the search with the UCB policy.",
      "startOffset" : 69,
      "endOffset" : 77
    }, {
      "referenceID" : 10,
      "context" : "A more comprehensive discussion of related work can be found in [VNHS09].",
      "startOffset" : 64,
      "endOffset" : 72
    }, {
      "referenceID" : 10,
      "context" : "Some preliminary ideas are explored in [VNHS09].",
      "startOffset" : 39,
      "endOffset" : 47
    } ],
    "year" : 2010,
    "abstractText" : "This paper introduces a principled approach for the design of a scalable general reinforcement learning agent. This approach is based on a direct approximation of AIXI, a Bayesian optimality notion for general reinforcement learning agents. Previously, it has been unclear whether the theory of AIXI could motivate the design of practical algorithms. We answer this hitherto open question in the affirmative, by providing the first computationally feasible approximation to the AIXI agent. To develop our approximation, we introduce a Monte Carlo Tree Search algorithm along with an agent-specific extension of the Context Tree Weighting algorithm. Empirically, we present a set of encouraging results on a number of stochastic, unknown, and partially observable domains.",
    "creator" : "LaTeX with hyperref package"
  }
}