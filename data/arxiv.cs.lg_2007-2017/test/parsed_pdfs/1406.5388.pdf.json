{
  "name" : "1406.5388.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Learning computationally efficient dictionaries and their implementation as fast transforms",
    "authors" : [ "Luc Le Magoarou", "Rémi Gribonval" ],
    "emails" : [ "luc.le-magoarou@inria.fr", "remi.gribonval@inria.fr" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Sparse representations using dictionaries are a popular way of providing concise descriptions of high-dimensional vectors. The goal of dictionary learning is to find an appropriate dictionary D allowing the sparse approximation of a training collection, gathered in a data matrix X, as:\nX ≈ DΓ, (1) where Γ has sparse columns. Historically, the only way to come up with a dictionary was to analyse mathematically the data and derive a ”simple” formula to construct the dictionary. Dictionaries designed this way are called analytic dictionaries [1] (e.g.: associated to Fourier, wavelets and Hadamard transforms). Due to the relative simplicity of analytic dictionaries, they are often associated with a fast algorithm such as the Fast Fourier Transform (FFT) [2] or the Discrete Wavelet Transform (DWT) [3]. On the other hand, the development of modern computers allowed the surfacing of automatic methods that learn a dictionary directly from the data. Such learned dictionaries are usually well adapted to the data at hand, but due to their lack of structure, they do not lead to fast algorithms and are costly to store. A survey on dictionaries, analytic or learned, can be found in [1].\nCan one design dictionaries as well adapted to the data as learned dictionaries, while as fast to manipulate and as cheap to store as analytic ones? Such an objective can seem unrealistic, but in [4], and more recently in [5], the authors introduced new dictionary structures that seek to bridge the gap between the two categories. The model we introduce actually generalizes these approaches. We build on the simple observation that the fast transforms associated with analytic dictionaries can be seen as consecutive multiplications of the input vector by sparse matrices, indicating that such dictionaries can be expressed as a product of sparse matrices1:\nD = M∏ j=1 Sj . (2)\n1The product being taken from left to right: ∏N\ni=1 Ai = A1 · · ·AN\nar X\niv :1\n40 6.\n53 88\nv3 [\ncs .L\nG ]\n2 6\nFe b\nThis factorizable structure is precisely what enables fast algorithms to multiply a vector by the dictionary or its adjoint. For example each step of the butterfly radix-2 FFT can be seen as the multiplication by a sparse matrix having only two non-zero entries per row an per column, which leads to the well-known complexity savings. Another example can be found in Figure 4 where we show the Hadamard dictionary along with its factorized form.\nOur objective is thus to learn a factorizable dictionary (i.e. taking the form or eq.(2)), making it intrinsically fast to manipulate and cheap to store. We will express this as an highly non-convex optimization problem, and rely on recent advances in optimization such as the PALM algorithm proposed in [6] to address it. In Section 2 we formulate the problem and link our work with relevant others, in Section 3 we present a general algorithm to solve it, and finally in Section 4 we present experimental results showing the interest of the proposed method. In particular, we demonstrate its ability to factor the Hadamard matrix in a way that enables its multiplication by an arbitrary vector as efficiently as with the fast Hadamard transform."
    }, {
      "heading" : "2 Problem formulation and related works",
      "text" : "Notation. Throughout this paper, matrices are denoted by bold upper-case letters: A. Vectors are denoted by bold lower-case letters: a. The ith column of a matrix A is denoted by: ai. Sets are denoted by calligraphical symbols: A. The standard vectorization operator is denoted by vec(·) and the Kronecker product by ⊗. The usual `0-norm is denoted by ‖·‖00 (it counts the number of non-zero elements), ‖·‖F denotes the Frobenius norm, and ‖·‖2 the operator norm.\nObjective. As stated in the introduction, our goal is to learn dictionaries that are intrinsically fast to manipulate and cheap to store. In order to meet these requirements, we impose that our dictionary be the product of several sparse matrices. Mathematically speaking, let X ∈ Rd×n be our data matrix, each of its n columns xi being a training vector, D ∈ Rd×a be our dictionary with a atoms and Γ ∈ Ra×n be the corresponding sparse representation matrix such that X ≈ DΓ. In order to meet the requirements and be intrinsically fast, D must take the form of eq.(2), where the Sjs are sparse matrices in Raj×aj+1 with a1 = d and aM+1 = a. Then, denoting SM+1 = Γ for ease of notation, our goal is to find the sparse factors Sjs such that:\nX ≈ M+1∏ j=1 Sj . (3)\nUnder this form, our problem amounts to a factorisation of the data matrix intoM+1 sparse factors, thus it can be cast as a general optimization problem:\nMinimize S1,...,SM+1\nd ( X, M+1∏ j=1 Sj ) + M+1∑ j=1 gj(Sj), (4)\nwhere d(·, ·) is some distance measure and the gjs are sparsity-seeking penalties or constraints. This general optimization problem has been studied recently by various authors in several domains.\nRelated works. For dictionary learning, as mentioned in the introduction, two main works have begun to explore this way. In [4], the authors propose to learn a dictionary which atoms are sparse linear combinations of atoms of a so-called base dictionary. The base dictionary should be associated with a fast algorithm (it takes the form of eq.(2)), so that the whole learned dictionary can be efficiently stored and manipulated. It can be seen as having theM−1 leftmost factors fixed in eq.(3) (let us call it Dbase), the M th factor being the sparse representation of the dictionary over the base dictionary (D = DbaseSM ), and theM+1th being the sparse representation of the training data over the learned dictionary. The major drawback with this formulation is that the learned dictionary is highly biased toward the base dictionary, so that we do not have full adaptability. In [5], the authors propose to learn a dictionary in which each atom is the composition of several circular convolutions with sparse kernels, so that the dictionary is fast to manipulate. Their model can be seen as eq.(3), with the gjs corresponding to the M leftmost factors imposing sparse circulant matrices. This formulation is limited in nature to the case where the dictionary is well approximated by a product of sparse circulant matrices.\nIn statistics and data analysis, some researchers have been interested in statistical models in which the covariance matrix of the data takes the form of eq.(3), so that estimating this covariance matrix amounts to the problem of eq.(4). Recent representative works in this direction are [7] and [8].\nEven more recently, similar models were proposed in machine learning. In [9], the authors introduce the sparse multi-factor NMF, that can be seen as modelling the data as in eq.(3), with all Sjs being non-negative matrices. In [10] and [11], the authors assume that the data come from a deep neural network, assuming that consecutive layers are sparsely connected and neglecting the non-linearities, they provide some strategies to recover the structure of the network. This model can be seen as modelling the data like in eq.(3) with the M leftmost factors representing a layer of the network each (the non-linear part being omitted), and the M + 1th factor being the input of the network.\nNote that another concern, somewhat related to that of having a computationally efficient dictionary, is that of being able to rapidly compute the sparse code Γ corresponding to the training data X given the dictionary D. Models addressing this problematic have been proposed in [12] and [13]."
    }, {
      "heading" : "3 Optimization framework",
      "text" : "In this section we explicit the considered optimization problem, and describe an algorithm that is guaranteed to converge to a stationary point of the objective function."
    }, {
      "heading" : "3.1 Objective function",
      "text" : "To learn a dictionary that is well adapted to the data while being fast to manipulate and cheap to store, we will minimize an objective function of the form of eq.(4). We will take as distance measure the squared Frobenius norm of the difference d(X, ∏M+1 j=1 Sj) := 1 2‖X− ∏M+1 j=1 Sj‖2F and as sparsityseeking penalties some indicator functions of sets of sparse matrices: gj := δEj , with δT (S) = 0 if S ∈ T and δT (S) =∞ otherwise. The keen reader might have noticed that this basic formulation of the objective is invariant under relative scalings of the factors if the constraint sets are scale invariant themselves, and we address this issue below.\nChoice of the constraints. The choice of the constraint sets is crucial, because they entirely determine the storage and multiplication cost of the learned dictionary. Indeed, storing/multiplying the dictionary in the factorized form will cost O( ∑M j=1 ‖vec(Sj)‖ 0 0), whereas classical dictionary learning methods would typically provide dense dictionaries for which storing/multiplying would cost O(da). This simple statement allows to introduce the Relative Complexity (RC) of the learned dictionary:\nRC :=\n∑M j=1 ‖vec(Sj)‖ 0 0\nda . (5)\nThis quantity is clearly positive and should be smaller than 1 in order to make complexity savings. In practice, we will usually choose Eis that are subsets of ”`0 balls”, namely they will take the form: Ej = Nj ∩ {A ∈ Raj×aj+1 : ‖vec(A)‖00 ≤ pj}, where Nj is an arbitrary set imposing additional constraints. These constraints will give us: RC ≤ ∑M j=1 pj/da.\nCoping with the scaling ambiguity. In order to avoid scaling ambiguities, it is common [5, 9] to normalize the factors and introduce a multiplicative scalar λ in the data fidelity term. Doing so, the actual problem that we consider is the following:\nMinimize λ,S1,...,SM+1 Ψ(S1, . . . ,SM+1, λ) := 1 2 ∥∥∥X− λM+1∏ j=1 Sj ∥∥∥2 F + M+1∑ j=1 δEj (Sj), (6)\nwith a new canonical form for the Ejs namely Ej = Nj ∩ {A ∈ Raj×aj+1 : ‖vec(A)‖00 ≤ pj , ‖A‖F = 1}, so that the factors are normalized."
    }, {
      "heading" : "3.2 Algorithm overview",
      "text" : "The formulation of the problem in eq.(6) is unfortunately highly non-convex, and the sparsity enforcing part is non-smooth. Stemming on recent advances in non-convex optimization, we propose next an algorithm with convergence guarantees to a stationary point of the problem. In [6], the authors consider cost functions depending on N blocks of variables of the form:\nΨ(x1, . . . ,xN ) := H(x1, . . . ,xN ) + N∑ j=1 fj(xj), (7)\nwhere the function H is smooth, and the fjs are proper and lower semi-continuous (the exact assumptions are given below). It is to be stressed that no convexity of any kind is assumed. Here, we assume for simplicity that the fjs are indicator functions of constraint sets Tj . To handle this objective function, the authors propose an algorithm called Proximal Alternating Linearized Minimization (PALM)[6], that updates alternatively each block of variable by a proximal (or projected in our case) gradient step. The structure of the PALM algorithm is given in Algorithm 1, where PTj (·) is the projection operator onto the set Tj and cij defines the step size and depends on the Lipschitz constant of the gradient of H (we give its expression in the next subsection). The following conditions are sufficient (not necessary) to ensure that each bounded sequence generated by PALM converges to a stationary point of its objective:\n(i) The fjs are proper and lower semi-continuous. (ii) H is smooth.\n(iii) Ψ is semi-algebraic. (iv) ∇xjH is globally Lipschitz for all j, with Lipschitz moduli Lj(x1. . .xj−1,xj+1. . .xN ). (v) ∀i, cij > Lj(x i+1 1 . . .x i+1 j−1,x i j+1. . .x i N ) (the inequality need not be strict for convex fj).\nAlgorithm 1 PALM (summary) for i ∈ {1 · · ·Niter} do\nfor j ∈ {1 · · ·N} do Set xi+1j = PTj ( xij − 1cij∇xjH ( xi+11 . . .x i j . . .x i N )) end for\nend for"
    }, {
      "heading" : "3.3 Algorithm details",
      "text" : "Let us now instantiate PALM for our purpose, namely to handle the objective of eq.(6). It is quite straightforward to see that there is a match between eq.(6) and eq.(7) taking N = M + 2, xj = Sj for j ∈ {1 . . .M + 1}, xM+2 = λ, H is the data fidelity term, fj(·) = δEj (.) for j ∈ {1 . . .M + 1} and fM+2(·) = δEM+2(·) = δR(·) = 0 (there is no constraint on λ). With this particular instance of the problem, conditions (i), (ii) and (iii) are trivially fulfilled provided the Ejs are semi-algebraic sets, which is indeed the case for all the sets considered in this work.\nProjection operator. In the case where the Ejs are defined like in Section 3.1 with no additional constraints, namely Ej = {A ∈ Raj×aj+1 : ‖vec(A)‖00 ≤ pj , ‖A‖F = 1} for j ∈ {1 . . .M + 1}, then the projection operator PEj (·) simply keeps the pj greatest entries (in absolute value) of its argument, sets all the other entries to zero, and then normalize its argument so that it has unit norm (see proof in appendix). Regarding EM+2 = R, the projection operator is the identity mapping.\nGradient and Lipschitz moduli. Let us now analyse more precisely the iterations of PALM specialized to our problem. For that we fix the iteration i and the factor j. We also need to introduce new notations. First we will call Si := Sij the factor that we are updating, L := ∏j−1 k=1 S i+1 k\nwhat is on the left of the factor we are updating and R := ∏M+1 k=j+1 S i k what is on the right\n(with the convention ∏ k∈∅ Sk = Id). Moreover, and to simplify the notation when we update\nλ, let us introduce X̂ = ∏M+1 k=1 S i+1 k . With these new notations we have when updating the jth factor: H(Si+11 . . .S i j . . .S i M+1, λ i) = 12‖X − λ iLSiR‖2F . Or equivalently when updating λ: H(Si+11 . . .S i+1 M+1, λ i) = 12‖X− λ iX̂‖2F .\nThe gradient of this smooth part of the objective with respect to the jth factor reads:\n∇SijH(S i+1 1 . . .S i j . . .S i M+1, λ i) = λiLT (λiLSiR−X)RT ,\nwhich allows us to verify condition (iv) with Lj(L,R, λi) = (λi)2 ‖R‖22 . ‖L‖ 2 2 (see proof in appendix). Fixing a step size cij so as to verify the condition (v), The update of S i j can be rewritten:\nSi+1j = PEj ( Sij − 1\ncij λiLT (λiLSijR−X)RT\n) .\nNow looking at λ we have:\n∇λiH(Si+11 . . .S i+1 M+1, λ i) = λiTr(X̂T X̂)− Tr(XT X̂).\nSince fM+2(λ) = 0 is a convex penalty, it is enough to check (v) with a non-strict inequality [6], this leads to the update rule:\nλi+1 = Tr(XT X̂)\nTr(X̂T X̂)\nAn explicit version of the algorithm is given in Algorithm 2. Note that for simplicity we introduce a new notation for the total number of factors Q := M + 1.\nAlgorithm 2 PALM for learning efficient dictionaries (palm4LED) Input: The data matrix X, the desired number of factors Q, the constraint sets Ej , j ∈ {1 . . . Q}\nand a stopping criterion (e.g., here, a number of iterations Niter). 1: for i = 0 to Niter − 1 do 2: for j = 1 to Q do 3: L← ∏j−1 k=1 S i+1 k\n4: R← ∏Q k=j+1 S i k 5: Set cij > (λ i)2 ‖R‖22 . ‖L‖ 2 2\n6: Si+1j ← PEj ( Sij − 1cij λL T (λLSijR−X)RT ) 7: end for 8: X̂← ∏Q k=1 S i+1 k 9: λi+1 ← Tr(X T X̂)\nTr(X̂T X̂) 10: end for Output: The estimated factorization: λNiter ,{SNiterk } Q k=1 = palm4LED(X, Q, {Ej} Q j=1)"
    }, {
      "heading" : "3.4 Practical strategy",
      "text" : "Algorithm 2 presented above factorizes a data matrix into sparse factors and converges to a stationary point of the problem stated in eq.(6). However, while we are primarily interested in stationary points where the data fidelity term of the cost function is small, there is unfortunately no guarantee that the algorithm converges to such a stationary point. This is illustrated by a very simple experiment where Algorithm 2 is applied to a data matrix X = D with a known factorization in M factors: D = ∏M j=1 Sj , such as the Hadamard dictionary. The naive approach consists in taking directly Q = M in Algorithm 2, and setting the constraints so as to reflect the actual sparsity of the true\nfactors. This simple strategy performs quite poorly in practice, and the attained local minimum is very often not satisfactory (the data fidelity part of the objective function is big).\nWe noticed experimentally that taking fewer factors (Q small) and allowing more non-zero entries per factor led to better results in general. This observation suggested to adopt a hierarchical strategy. Indeed, when X = ∏M+1 j=1 Sj is the product ofM+1 sparse factors, it is also the product X = T1T2\nof 2 factors T1 = S1 and T2 = ∏M+1 j=2 Sj , so that T1 is sparser than T2. Our strategy is then to factorize the data matrix X in 2 factors, one being sparse (corresponding to T1), and the other less sparse (corresponding to T2). The process can be repeated on the less sparse factor, and so on until we attain the desired number Q of factors. This strategy turns out to be surprisingly effective and the attained local minima are very good, as illustrated in the next section.\nThe proposed hierarchical strategy is summarized in Algorithm 3, where we need to specify at each step the constraint sets related to the two factors. For that let us introduce some notation: Ek will be the constraint set for the left factor and Ẽk the one for the right factor at the kth factorization. The global optimization step (line 5) is done by initializing palm4LED with the current values of {Sj}kj=1 and R. It is here to keep an attach to the data matrix X. Roughly we can say that line 3 of the algorithm is here to yield complexity savings, whereas line 5 is here to keep low the data fidelity term of the cost function. Note: the hierarchical strategy can also be applied the other way around (starting from the right), just by transposing the input. We only present here the version that starts from the left because the induced notations are simpler.\nAlgorithm 3 Hierarchical factorization Input: The data matrix X, the desired number of factors Q and the constraint sets Ek, k ∈ {1 . . . Q− 1} and Ẽk, k ∈ {1 . . . Q− 1}.\n1: R← X 2: for k = 1 to Q− 1 do 3: Factorize the residual R into 2 factors: λ′,{T1,T2} = palm4LED(R, 2, {Ek, Ẽk}) 4: Sk ← λ′T1 and R← T2 5: Global optimization: λ, { {Sj}kj=1,R } = palm4LED(X, k + 1, { {Ej}kj=1, Ẽk } ) 6: end for 7: SQ ← R\nOutput: The estimated factorization λ,{Sk}Qk=1."
    }, {
      "heading" : "4 Experiments",
      "text" : "In all experiments, we consider square dictionaries and square factors."
    }, {
      "heading" : "4.1 Learning a fast implementation of the Hadamard transform",
      "text" : "We begin by a dictionary factorization experiment. Consider a data matrix X = D with a known factorization in M factors, D = ∏M j=1 Sj : in Section 3.4, we evoked the failure of Algorithm 2 for this factorization problem. In contrast, Figure 2 illustrates the result of the proposed hierarchical strategy (Algorithm 3) with D the Hadamard dictionary in dimension n = 32. The obtained factorization is exact and as good as the reference one shown on Figure 4 in terms of complexity savings. The running time is less than a second. Factorization of the Hadamard matrix in dimension up to n = 1024 showed identical performance, with running time O(n2) up to ten minutes."
    }, {
      "heading" : "4.2 Learning computationally efficient dictionaries",
      "text" : "We now test Algorithm 3 in a more realistic framework on a dictionary learning problem with synthetic data, and compare it to classical learned dictionaries and analytic dictionaries, in terms of approximation quality and relative complexity.\nData. To build the data matrix X, we generated 500 training samples by selecting uniformly at random 5 atoms in a dictionary D0 ∈ R32×32 with i.i.d. Gaussian coefficients to build each sample. Gathering the coefficients in the matrix Γ0 ∈ R32×500 we get our training data matrix X = D0Γ0 ∈ R32×500. Two reference dictionaries D0 are considered:\n• Factorizable dictionary (FACT): D0 is the product of M = 5 sparse matrices: D0 = ∏5 i=1 S 0 i ,\neach having a random number p0i of i.i.d. Gaussian non-zero entries with 64 ≤ p0i ≤ 128, and being full rank so that the dictionary spans the signal space.\n• Random dictionary (RAND): D0 has i.i.d. Gaussian entries.\nBaselines. We compare Algorithm 3 with the following methods. All methods involve a coefficient update step which is performed using Orthogonal Matching Pursuit (OMP) [14]:\n• K-SVD [15], one of the most used algorithm that provides a learned dictionary. We use the implementation described in [16], running 300 iterations (which proved empirically sufficient to ensure convergence). Note that we also tested the online dictionary learning (ODL) method of [17]. Its performance being almost identical to that of K-SVD in our setting, we decided to consider these two methods as one (K-SVD/ODL) in the interpretation of the results. • Sparse K-SVD [4], a method that seeks to bridge the gap between learned dictionaries and analytic dictionaries. The implementation of [4] is used, with Dbase the Discrete Cosine Transform (DCT) matrix and 100 iterations, ensuring convergence in practice. The estimated dictionary D has columns 4-sparse in Dbase. • A fixed analytic dictionary with a known fast implementation (either the DCT, the Haar wavelets (HAAR) or the Hadamard matrix (HAD)).\nSettings of our algorithm. We tested several configurations for Algorithm 3, and we present here only the best one (PROPOSED). It amounts to Algorithm 3 starting from the right, with two modifications. First, we performed the first factorization (k = 1) by K-SVD/ODL to compute SM+1 = Γ. Second, we noticed that it was beneficial to update the coefficient matrix Γ with OMP after each global optimization step of Algorithm 3 (line 5). We tested various numbers of factors Q ∈ {3 . . . 6}. The considered constraint sets were: E1 = {A ∈ R32×500, ‖an‖00 ≤ 5} and for k ∈ {2 . . . Q − 1}, Ek = {A ∈ R32×32, ‖vec(A)‖00 ≤ p, ‖A‖F = 1} and Ẽk = {A ∈ R32×32, ‖vec(A)‖00 ≤ P 2k−2\n, ‖A‖F = 1}. We show the results for sparsity constraints given by p ∈ {2, 3, 4} and P ∈ 512 × {1, 1.2, 1.4, 1.6}. Algorithm 3 was implemented in Matlab, and executed on Intel Core i7-3667U CPU. It typically took between 8 and 9 seconds to converge for each drawn X. The stopping criterion for palm4LED combined a maximum number of iterations Ntier = 500 and a bound = 10−6 on the variation of the approximation error between consecutive iterations.\nPerformance measures. The ideal dictionary should approximate well the data at hand, while being at the same time fast to manipulate and cheap to store. The computational efficiency of the dictionary is measured through the Relative complexity (RC) quantity introduced in Section 3.1. The quality of approximation is expressed using the Root-Mean-Square Error (RMSE)[4, 15]: RMSE := 1√\ndn ‖X−DΓ‖F .\nDiscussion of the results. The experiment has been repeated 100 times with each data generation method. For a given configuration of the algorithm the relative complexity is constant over all trials, and the results shown on Figure 3 display the average RMSE.\nWith a factorizable dictionary (left), as expected, the methods that use a fast dictionary (HAD, DCT and HAAR) perform quite poorly in approximation (vertical axis), but very good in relative complexity (horizontal axis) taking advantage of their intrinsic structure. On the other hand, K-SVD exhibits good approximation performance, while the lack of structure of the obtained dictionary does not lead to any complexity savings (RC = 1). In between these two extremes, Sparse K-SVD, thanks to its layer of adaptivity, performs better than the analytic dictionaries in approximation at the expense of a slightly higher relative complexity. The proposed method (PROPOSED) has the ability to achieve a flexible tradeoff between complexity and adaptation to the data. More specifically, we can identify several behaviors for the proposed method. With Q = 3 factors and P = 1.4× 512 or P = 1.6×512, the proposed method performs almost as good as K-SVD in terms of approximation, with reduced relative complexities between 0.7 and 0.9. On the other hand, with p = 2 andQ = 5 or Q = 6, the proposed method provides dictionaries almost as compact as analytic dictionaries (RC ≈ 0.3), while being better adapted to the data (RMSE up to twice smaller). The other configurations of p, P and Q all lie between these two behaviors in terms of performance.\nWith a random dictionary (right), the methods exhibit qualitatively the same comparative behavior as with a factorizable dictionary. Notably, the proposed method can learn a dictionary as computationally efficient as the one provided by Sparse K-SVD but with half the approximation error."
    }, {
      "heading" : "5 Conclusion",
      "text" : "We proposed a dictionary learning framework that provides a flexible tradeoff between computational efficiency and adaptation to the training data. Stemming on recent advances in non-convex optimization, we derived an algorithm with convergence guarantees to learn efficient dictionaries and demonstrated experimentally its ability to provide complexity/accuracy tradeoffs that state of the art dictionary learning methods could not achieve. Besides the obvious need to further test the approach on real data and with redundant dictionaries, and to better understand the role of its parameters in the control of the desired tradeoff, a particular challenge will be to leverage the gained complexity to speed up the learning process itself, in order to efficiently learn efficient dictionaries."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work was supported in part by the European Research Council, PLEASE project (ERC-StG2011-277906). The authors wish to thank François Malgouyres and Olivier Chabiron for the fruitful discussions that helped in producing that work."
    }, {
      "heading" : "A Factorizations examples",
      "text" : "In this appendix we show that the matrices of usual transforms associated with fast algorithm can be factorized into sparse factors.\nA.1 The Discrete Fourier Transform\nWe are going to look at the DFT matrix in dimension 8 and show that it can be factorized into sparse matrices. Note that a similar factorization can be done in any power of two dimension. Let us take\nC ∈ C8×8 to be the DFT matrix:\nC =  W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 1 W 2 W 3 W 4 W 5 W 6 W 7 W 0 W 2 W 4 W 6 W 0 W 2 W 4 W 6 W 0 W 3 W 6 W 1 W 4 W 7 W 2 W 5 W 0 W 4 W 0 W 4 W 0 W 4 W 0 W 4 W 0 W 5 W 2 W 7 W 4 W 1 W 6 W 3 W 0 W 6 W 4 W 2 W 0 W 6 W 4 W 2\nW 0 W 7 W 6 W 5 W 4 W 3 W 2 W 1\n , (8)\nwith W = exp( 2πi8 ). Applying permutations of rows and columns (bit-reversed order), we obtain:\nPrCPc =  W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 0 W 4 W 4 W 4 W 4 W 0 W 0 W 4 W 4 W 2 W 2 W 6 W 6 W 0 W 0 W 4 W 4 W 6 W 6 W 2 W 2 W 0 W 4 W 2 W 6 W 1 W 5 W 3 W 7 W 0 W 4 W 2 W 6 W 5 W 1 W 7 W 3 W 0 W 4 W 6 W 2 W 3 W 7 W 1 W 5\nW 0 W 4 W 6 W 2 W 7 W 3 W 5 W 1\n . (9)\nThis matrix can be factorized as follows:\nPrCPc =  W 0 W 0 0 0 0 0 0 0 W 0 W 4 0 0 0 0 0 0 0 0 W 0 W 0 0 0 0 0 0 0 W 0 W 4 0 0 0 0 0 0 0 0 W 0 W 0 0 0 0 0 0 0 W 0 W 4 0 0 0 0 0 0 0 0 W 0 W 0\n0 0 0 0 0 0 W 0 W 4\n ×  W 0 W 0 W 0 W 0 0 0 0 0 0 0 0 0 W 0 W 0 W 0 W 0 W 0 W 0 W 4 W 4 0 0 0 0 0 0 0 0 W 2 W 2 W 6 W 6 W 0 W 4 W 2 W 6 0 0 0 0 0 0 0 0 W 1 W 5 W 3 W 7 W 0 W 4 W 6 W 2 0 0 0 0\n0 0 0 0 W 3 W 7 W 5 W 1\n .\n(10)\nAt this point the left factor can be further factorized:\nPrCPc =  W 0 W 0 0 0 0 0 0 0 W 0 W 4 0 0 0 0 0 0 0 0 W 0 W 0 0 0 0 0 0 0 W 0 W 4 0 0 0 0 0 0 0 0 W 0 W 0 0 0 0 0 0 0 W 0 W 4 0 0 0 0 0 0 0 0 W 0 W 0\n0 0 0 0 0 0 W 0 W 4\n\n×  W 0 W 0 0 0 0 0 0 0 0 0 W 0 W 0 0 0 0 0 W 0 W 4 0 0 0 0 0 0 0 0 W 2 W 6 0 0 0 0 0 0 0 0 W 0 W 0 0 0 0 0 0 0 0 0 W 0 W 0 0 0 0 0 W 0 W 4 0 0\n0 0 0 0 0 0 W 2 W 6\n\n×  W 0 W 0 0 0 0 0 0 0 0 0 W 0 W 0 0 0 0 0 0 0 0 0 W 0 W 0 0 0 0 0 0 0 0 0 W 0 W 0 W 0 W 4 0 0 0 0 0 0 0 0 W 2 W 6 0 0 0 0 0 0 0 0 W 1 W 5 0 0\n0 0 0 0 0 0 W 3 W 7\n\n, (11)\nThis factorization actually corresponds to the butterfly radix-2 FFT.\nA.2 The Hadamard transform\nA.3 The wavelet transform\nWe are interested in the factorization’s structure of the Discrete Wavelet Transform (DWT) matrix. More precisely, we wish to express the synthesis of a signal x ∈ R2M from its wavelet coefficients γ ∈ R2M as a sequence of simple linear transformations, i.e. a product of γ by multiple sparse matrices.\nThe discrete signal x can be seen as the coordinates aM in a basis {φj,M}2 M\nj=1 of the projection of the underlying continuous signal onto the approximation space VM . Multi Resolution Analysis (MRA) consists in defining a hierarchy of subspaces : VM ⊃ VM−1 ⊃ · · · ⊃ V0 and their direct complement Vk = Vk−1 ⊕Wk−1 such that Vk−1 and Wk−1 are of dimension 2k−1. By induction we have: VM = V0 ⊕W0 ⊕ · · · ⊕WM−1. The DWT is then a change of basis from the canonical basis to a basis which is the union of bases from each subspace V0,W0, · · · ,WM−1.\nWe define γ = (aT0 |bT0 |bT1 | · · · |bTM−1)T , where ak ∈ R2 k , bk ∈ R2 k\nas the DWT of a signal x = aM ∈ R2 M that can be obtained by M iterations of the following filterbank:\nThe inverse transform is obtained by M iterations of this other filterbank:\nDownsampling, upsampling and filtering being linear transformations, the synthesis and analysis elementary blocks can be seen as matrix products. Let us focus on the synthesis case. We define Mk ∈ R2\nk×2k such that ak+1 = Mk+1.(aTk |bTk )T . The matrix Mk accounts for upsampling followed by filtering with two different filters, so it takes the following form:\nMk = ( Gk Hk ) . (\nUk 0 0 Uk\n) = ( Gk↓2 Hk↓2 ) , (12)\nit is the concatenation of two columnwise downsampled Toeplitz (or circulant) matrices.\nIntroducing Idk ∈ R(2 M−2k)×(2M−2k) the identity in dimension 2M − 2k, and the matrix Sk ∈ R2M×2M taking the form: Sk = ( Mk 0 0 Idk ) , (13)\nthe inverse DWT of γ can be expressed:\nx = M∏ k=1 Skγ (14)"
    } ],
    "references" : [ {
      "title" : "Dictionaries for Sparse Representation Modeling",
      "author" : [ "Ron Rubinstein", "A.M. Bruckstein", "Michael Elad" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2010
    }, {
      "title" : "An algorithm for the machine calculation of complex Fourier series",
      "author" : [ "James Cooley", "John Tukey" ],
      "venue" : "Mathematics of Computation,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1965
    }, {
      "title" : "A theory for multiresolution signal decomposition : the wavelet representation",
      "author" : [ "Stéphane Mallat" ],
      "venue" : "IEEE Transaction on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1989
    }, {
      "title" : "Double sparsity: learning sparse dictionaries for sparse signal approximation",
      "author" : [ "Ron Rubinstein", "Michael Zibulevsky", "Michael Elad" ],
      "venue" : "IEEE Transactions on Signal Processing,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2010
    }, {
      "title" : "Toward fast transform learning",
      "author" : [ "Olivier Chabiron", "Francois Malgouyres", "Jean-Yves Tourneret", "Nicolas Dobigeon" ],
      "venue" : "Technical report,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2013
    }, {
      "title" : "Proximal alternating linearized minimization for nonconvex and nonsmooth problems",
      "author" : [ "Jérôme Bolte", "Shoham Sabach", "Marc Teboulle" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2013
    }, {
      "title" : "Treelets - an adaptive multi-scale basis for sparse unordered data",
      "author" : [ "Ann B. Lee", "Boaz Nadler", "Larry Wasserman" ],
      "venue" : "The Annals of Applied Statistics,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2008
    }, {
      "title" : "The sparse matrix transform for covariance estimation and analysis of high dimensional signals",
      "author" : [ "Guangzhi Cao", "L.R. Bachega", "C.A. Bouman" ],
      "venue" : "Image Processing, IEEE Transactions on,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2011
    }, {
      "title" : "On algorithms for sparse multi-factor NMF",
      "author" : [ "Siwei Lyu", "Xin Wang" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2013
    }, {
      "title" : "Sparse matrix factorization",
      "author" : [ "Behnam Neyshabur", "Rina Panigrahy" ],
      "venue" : "CoRR, abs/1311.3315,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2013
    }, {
      "title" : "Provable bounds for learning some deep representations",
      "author" : [ "Sanjeev Arora", "Aditya Bhaskara", "Rong Ge", "Tengyu Ma" ],
      "venue" : "CoRR, abs/1310.6343,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2013
    }, {
      "title" : "Learning fast approximations of sparse coding",
      "author" : [ "Karol Gregor", "Yann LeCun" ],
      "venue" : "In Proceedings of the 27th Annual International Conference on Machine Learning,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2010
    }, {
      "title" : "Learning efficient sparse and low rank models",
      "author" : [ "Pablo Sprechmann", "Alexander M. Bronstein", "Guillermo Sapiro" ],
      "venue" : "CoRR, abs/1212.3631,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2012
    }, {
      "title" : "Matching pursuits with time-frequency dictionaries",
      "author" : [ "S.G. Mallat", "Zhifeng Zhang" ],
      "venue" : "IEEE Transactions on Signal Processing,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1993
    }, {
      "title" : "svd: An algorithm for designing overcomplete dictionaries for sparse representation",
      "author" : [ "M. Aharon", "M. Elad", "A. Bruckstein. K" ],
      "venue" : "Signal Processing, IEEE Transactions on,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2006
    }, {
      "title" : "Efficient Implementation of the K- SVD Algorithm using Batch Orthogonal Matching Pursuit",
      "author" : [ "Ron Rubinstein", "Michael Zibulevsky", "Michael Elad" ],
      "venue" : "Technical report,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Dictionaries designed this way are called analytic dictionaries [1] (e.",
      "startOffset" : 64,
      "endOffset" : 67
    }, {
      "referenceID" : 1,
      "context" : "Due to the relative simplicity of analytic dictionaries, they are often associated with a fast algorithm such as the Fast Fourier Transform (FFT) [2] or the Discrete Wavelet Transform (DWT) [3].",
      "startOffset" : 146,
      "endOffset" : 149
    }, {
      "referenceID" : 2,
      "context" : "Due to the relative simplicity of analytic dictionaries, they are often associated with a fast algorithm such as the Fast Fourier Transform (FFT) [2] or the Discrete Wavelet Transform (DWT) [3].",
      "startOffset" : 190,
      "endOffset" : 193
    }, {
      "referenceID" : 0,
      "context" : "A survey on dictionaries, analytic or learned, can be found in [1].",
      "startOffset" : 63,
      "endOffset" : 66
    }, {
      "referenceID" : 3,
      "context" : "Can one design dictionaries as well adapted to the data as learned dictionaries, while as fast to manipulate and as cheap to store as analytic ones? Such an objective can seem unrealistic, but in [4], and more recently in [5], the authors introduced new dictionary structures that seek to bridge the gap between the two categories.",
      "startOffset" : 196,
      "endOffset" : 199
    }, {
      "referenceID" : 4,
      "context" : "Can one design dictionaries as well adapted to the data as learned dictionaries, while as fast to manipulate and as cheap to store as analytic ones? Such an objective can seem unrealistic, but in [4], and more recently in [5], the authors introduced new dictionary structures that seek to bridge the gap between the two categories.",
      "startOffset" : 222,
      "endOffset" : 225
    }, {
      "referenceID" : 5,
      "context" : "We will express this as an highly non-convex optimization problem, and rely on recent advances in optimization such as the PALM algorithm proposed in [6] to address it.",
      "startOffset" : 150,
      "endOffset" : 153
    }, {
      "referenceID" : 3,
      "context" : "In [4], the authors propose to learn a dictionary which atoms are sparse linear combinations of atoms of a so-called base dictionary.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 4,
      "context" : "In [5], the authors propose to learn a dictionary in which each atom is the composition of several circular convolutions with sparse kernels, so that the dictionary is fast to manipulate.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 6,
      "context" : "Recent representative works in this direction are [7] and [8].",
      "startOffset" : 50,
      "endOffset" : 53
    }, {
      "referenceID" : 7,
      "context" : "Recent representative works in this direction are [7] and [8].",
      "startOffset" : 58,
      "endOffset" : 61
    }, {
      "referenceID" : 8,
      "context" : "In [9], the authors introduce the sparse multi-factor NMF, that can be seen as modelling the data as in eq.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 9,
      "context" : "In [10] and [11], the authors assume that the data come from a deep neural network, assuming that consecutive layers are sparsely connected and neglecting the non-linearities, they provide some strategies to recover the structure of the network.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 10,
      "context" : "In [10] and [11], the authors assume that the data come from a deep neural network, assuming that consecutive layers are sparsely connected and neglecting the non-linearities, they provide some strategies to recover the structure of the network.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 11,
      "context" : "Models addressing this problematic have been proposed in [12] and [13].",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 12,
      "context" : "Models addressing this problematic have been proposed in [12] and [13].",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 4,
      "context" : "In order to avoid scaling ambiguities, it is common [5, 9] to normalize the factors and introduce a multiplicative scalar λ in the data fidelity term.",
      "startOffset" : 52,
      "endOffset" : 58
    }, {
      "referenceID" : 8,
      "context" : "In order to avoid scaling ambiguities, it is common [5, 9] to normalize the factors and introduce a multiplicative scalar λ in the data fidelity term.",
      "startOffset" : 52,
      "endOffset" : 58
    }, {
      "referenceID" : 5,
      "context" : "In [6], the authors consider cost functions depending on N blocks of variables of the form:",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 5,
      "context" : "To handle this objective function, the authors propose an algorithm called Proximal Alternating Linearized Minimization (PALM)[6], that updates alternatively each block of variable by a proximal (or projected in our case) gradient step.",
      "startOffset" : 126,
      "endOffset" : 129
    }, {
      "referenceID" : 5,
      "context" : "Since fM+2(λ) = 0 is a convex penalty, it is enough to check (v) with a non-strict inequality [6], this leads to the update rule:",
      "startOffset" : 94,
      "endOffset" : 97
    }, {
      "referenceID" : 13,
      "context" : "All methods involve a coefficient update step which is performed using Orthogonal Matching Pursuit (OMP) [14]:",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 14,
      "context" : "• K-SVD [15], one of the most used algorithm that provides a learned dictionary.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 15,
      "context" : "We use the implementation described in [16], running 300 iterations (which proved empirically sufficient to ensure convergence).",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 3,
      "context" : "• Sparse K-SVD [4], a method that seeks to bridge the gap between learned dictionaries and analytic dictionaries.",
      "startOffset" : 15,
      "endOffset" : 18
    }, {
      "referenceID" : 3,
      "context" : "The implementation of [4] is used, with Dbase the Discrete Cosine Transform (DCT) matrix and 100 iterations, ensuring convergence in practice.",
      "startOffset" : 22,
      "endOffset" : 25
    }, {
      "referenceID" : 3,
      "context" : "The quality of approximation is expressed using the Root-Mean-Square Error (RMSE)[4, 15]: RMSE := 1 √ dn ‖X−DΓ‖F .",
      "startOffset" : 81,
      "endOffset" : 88
    }, {
      "referenceID" : 14,
      "context" : "The quality of approximation is expressed using the Root-Mean-Square Error (RMSE)[4, 15]: RMSE := 1 √ dn ‖X−DΓ‖F .",
      "startOffset" : 81,
      "endOffset" : 88
    } ],
    "year" : 2015,
    "abstractText" : "Dictionary learning is a branch of signal processing and machine learning that aims at finding a frame (called dictionary) in which some training data admits a sparse representation. The sparser the representation, the better the dictionary. The resulting dictionary is in general a dense matrix, and its manipulation can be computationally costly both at the learning stage and later in the usage of this dictionary, for tasks such as sparse coding. Dictionary learning is thus limited to relatively small-scale problems. In this paper, inspired by usual fast transforms, we consider a general dictionary structure that allows cheaper manipulation, and propose an algorithm to learn such dictionaries –and their fast implementation– over training data. The approach is demonstrated experimentally with the factorization of the Hadamard matrix and with synthetic dictionary learning experiments.",
    "creator" : "LaTeX with hyperref package"
  }
}