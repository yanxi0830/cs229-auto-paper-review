{
  "name" : "1606.07315.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Nearly-optimal Robust Matrix Completion",
    "authors" : [ "Yeshwanth Cherapanamjeri", "Kartik Gupta", "Prateek Jain" ],
    "emails" : [ "t-yecher@microsoft.com", "t-kagu@microsoft.com", "prajain@microsoft.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n60 6.\n07 31\n5v 1\n[ cs\n.L G\n] 2\n3 Ju\nn 20"
    }, {
      "heading" : "1 Introduction",
      "text" : "In this paper, we study the Robust Matrix Completion (RMC) problem where the goal is to recover an underlying low-rank matrix by observing a small number of sparsely corrupted entries from the matrix. Formally, RMC: Find rank-r matrix L∗ ∈ Rm×n using Ω and PΩ(L∗) + S∗, (1) where Ω ⊆ [m]× [n] is the set of observed entries (throughout the paper we assume that m ≤ n), S∗ denotes the sparse corruptions of the observed entries, i.e., Supp(S∗) ∈ Ω. Sampling operator PΩ : Rm×n → Rm×n is defined as:\n(PΩ(A))ij = Aij , if (i, j) ∈ Ω, (PΩ(A))ij = 0, otherwise. (2)\nRMC is an important problem with several applications such as recommendation systems with outliers. Similarly, the problem is also heavily used to model PCA under gross outliers as well as erasures [JRVS11]. Finally, as we show later, an efficient solution to RMC enables faster solution for the robust PCA (RPCA) problem as well. The goal in RPCA is to find a low-rank matrix L∗ and sparse matrix S̃∗ by observing their sum, i.e., M = L∗ + S̃∗. State-of-the-art results for RPCA shows exact recovery of a rank-r, µ-incoherent L∗ (see Assumption 1, Section 3) if at most ρ = 1\nµ2r fraction of the entries in each row/column of S̃∗ are\ncorrupted [HKZ11, NUNS+14].\nHowever, the existing state-of-the-art results for RMC with optimal ρ = 1 µ2r fraction of corrupted entries, either require at least a constant fraction of the entries of L∗ to be observed [CJSC11, CLMW11] or require restrictive assumptions like support of corruptions S̃∗ being uniformly random [Li13]. [KLT14] also considers RMC problem but studies the noisy setting and do not provide exact recovery bounds. Moreover, most of the\nexisting methods for RMC use convex relaxation for both low-rank and sparse components, and in general exhibit large time complexity (O(m2n)).\nIn this work, we attempt to answer the following open question (assuming m ≤ n): Can RMC be solved exactly by using |Ω| = O(rn log n) observations out of which O( 1\nµ2r ) fraction of the\nobserved entries in each row/column are corrupted.\nNote that both |Ω| (for uniformly random Ω) and ρ values mentioned in the question above denote the information theoretic limits. Hence, the goal is to solve RMC for nearly-optimal number of samples and nearly-optimal fraction of corruptions.\nUnder standard assumptions on L∗, S∗, Ω and for n = O(m), we answer the above question in affirmative albeit with |Ω| which is O(r) (ignoring log factors) larger than the optimal sample complexity (see Theorem 1). In particular, we propose a simple projected gradient (PGD) style method for RMC that alternately cleans up corrupted entries by hard-thresholding; our method’s computational complexity is also nearly optimal (O(|Ω|r+ (m+n)r2 + r3)). Note that our method applies non-convex opeators like low-rank projection and hard-thresholding. Hence, standard convex analysis techniques cannot be used for our algorithm.\nSeveral recent results [JN15, NUNS+14, JTK14, HW14, Blu11] show that under certain assumptions, projection onto non-convex sets indeed lead to provable algorithms with fast convergence to the global optima. However, as explained in Section 3, RMC presents unique set of challenges as we have to perform error analysis with the errors arising due to missing entries as well as sparse corruptions, both of which interact among themselves as well. In fact, our careful error analysis also enables us to improve results for the matrix completion as well as the RPCA problem.\nMatrix Completion (MC): The goal of MC is to find rank-r L∗ using PΩ(L∗). State-of-the-art result for MC uses nuclear norm minimization and requires |Ω| ≥ µ2nr2 log2 n under standard µ-incoherence assumption (see Section 3), but the method requires O(m2n) time in general. The best sample complexity result for a non-convex iterative method (with at most logarithmic dependence on the condition number of L∗) achieve exact recovery when |Ω| ≥ µ6nr5 log2 n and needs O(|Ω|r) computational steps. In contrast, assuming n = O(m), our method achieves nearly the same sample complexity of trace-norm but with nearly linear time algorithm (O(|Ω|r)). See Table 1 for a detailed comparison of our result with the existing methods.\nRPCA: Several recent results show that RPCA can be solved if ρ = O( 1 µ2r )-fraction of entries in each row and column of L∗ are corrupted [NUNS+14, HKZ11] where L∗ is assumed to be µ-incoherent. Moreover, St-NcRPCA algorithm [NUNS+14] can solve the problem in time O(mnr2). Corollary 2 shows that by sampling Ω uniformly at random, we can solve the problem in time O(nr3) only. That is, we can recover L∗ without even observing the entire input matrix. Moreover, if the goal is to recover the sparse corruption as well, then we can obtain a two-pass (over the input matrix) algorithm that solves the RPCA problem exactly. St-NcRPCA algorithm requires r2 log(1/ǫ) passes over the data. Our method has significantly smaller space complexity as well.\nOur empirical results on synthetic data demonstrates effectiveness of our method. We also apply our method to the foreground background separation problem; our method is an order of magnitude faster than the stateof-the-art method (St-NcRPCA) while achieving similar accuracy.\nIn summary, this paper’s main contributions are: (a) RMC: We propose a nearly linear time method that solves RMC with |Ω| = O(nr2 log2 n log2 ‖M‖2/ǫ) random entries and with optimal fraction of corruptions (ρ = 1\nµ2r ).\n(b) Matrix Completion: Our result improves upon the existing linear time algorithm’s sample complexity by an O(r3) factor, and time complexity by O(r4) factor, although with an extra O(log ‖L∗‖/ǫ) factor in both time and sample complexity. (c) RPCA: We present a nearly linear time (O(nr3)) algorithm for RPCA under optimal fraction of cor-\nruptions, improving upon O(mnr2) time complexity of the existing methods.\nNotations: We assume that M = L∗ + S̃∗ and PΩ(M) = PΩ(L∗) + S∗, i.e., S∗ = PΩ(S̃∗). ‖v‖p denotes ℓp norm of a vector v; ‖v‖ denotes ℓ2 norm of v. ‖A‖2, ‖A‖F , ‖A‖∗ denotes the operator, Frobenius, and nuclear norm of A, respectively; by default ‖A‖ = ‖A‖2. Operator PΩ is given by (2), operators Pk(A) and HT ζ(A) are defined in Section 2. σi(A) denotes i-th singular value of A and σ∗i denotes the i-th singular value of L∗.\nPaper Organization: We present our main algorithm in Section 2 and our main results in Section 3. We also present the proof of one of our main results in Section 3. Section 4 presents our empirical result. Due to lack of space, we present most of the proofs and useful lemmas in Appendix."
    }, {
      "heading" : "2 Algorithm",
      "text" : "In this section we present our algorithm for solving the RMC (Robust Matrix Completion) problem: given Ω and PΩ(M) where M = L ∗ + S̃∗ ∈ Rm×n, rank(L∗) ≤ r, ‖S̃∗‖0 ≤ s and S∗ = PΩ(S̃∗), the goal is to recover L∗. To this end, we focus on solving the following non-convex optimization problem:\n(L∗, S∗) = argmin L,S ‖PΩ(M)− PΩ(L)− S‖2F , s.t., rank(L) ≤ r,PΩ(S) = S, ‖S‖0 ≤ s. (3)\nFor the above problem, we propose a simple iterative algorithm that combines projected gradient descent (for L) with alternating projections (for S). In particular, we maintain iterates L(t) (with rank k ≤ r) and sparse S(t). L(t+1) is computed using gradient descent step for objective (3) and then projecting back onto the set of rank k matrices. That is,\nL(t+1) = Pk ( L(t) + 1\np PΩ(M − L(t) − S(t))\n) , (4)\nwhere Pk(A) denotes projection of A onto the set of rank-k matrices and can be computed efficiently using SVD of A, p = |Ω| mn\n. S(t+1) is computed by projecting the residual PΩ(M−L(t+1)) onto set of sparse matrices using a hard-thresholding operator, i.e.,\nS(t+1) = HT ζ(M − L(t+1)), (5)\nwhere HT ζ : Rm×n → Rm×n is the hard thresholding operator defined as: (HT ζ(A))ij = Aij if |Aij | ≥ ζ and 0 otherwise. Intuitively, a better estimate of the sparse corruptions for each iteration will reduce the noise of the projected gradient descent step and a better estimate of the low rank matrix will enable better estimation of the sparse corruptions. Hence, under correct set of assumptions, the algorithm should recover L∗, S̃∗ exactly.\nUnfortunately, just the above two simple iterations cannot handle problems where L∗ has poor condition number, as the intermediate errors can be significantly larger than the smallest singular values of L∗, making recovery of the corresponding singular vectors challenging. To alleviate this issue, we propose an algorithm that proceeds in stages. In the q-th stage, we project L(t) onto set of rank-kq matrices. Rank kq is monotonic w.r.t. q. Under standard assumptions, we show that we can increase kq in a manner such that after each stage\n∥∥L(t) − L∗ ∥∥ ∞ decreased by at least a constant factor. Hence, the number of stages is only logarithmic\nin the condition number of L∗.\nSee Algorithm 1 for a psuedo-code of the algorithm. Our algorithm has an “outer loop” (see Line 6) which sets rank kq of iterates L\n(t) appropriately (see Line 7). We then update L(t) and S(t) in the “inner loop” using (4), (5). We set threshold for the hard-thresholding operator using singular values of current gradient descent update (see Line 12). Note that, we divide Ω uniformly into Q ·T sets, where Q is an upper bound on the number of outer iterations and T is the number of inner iterations. This division ensures independence\nAlgorithm 1 L̂ = PG-RMC (Ω,PΩ(M), ǫ, r, µ, η) 1: Input: Observed entries Ω, Matrix PΩ(M) ∈ Rm×n, convergence criterion ǫ, target rank r, incoherence\nparameter µ, thresholding parameter η\n2: T ← 10 log 10µ 2rn2‖PΩ(M)‖2\n|Ω|ǫ , Q ← T /* Number of inner and outer iterations */ 3: Partition Ω into Q · T subsets {Ωq,t : q ∈ [Q], t ∈ [T ]} uniformly at random 4: L(0) = 0,M (0) = mn|Ω|PΩ(M), ζ ← ησ1(M (0)) /* Initialization */ 5: k0 ← 0, q ← 0 6: while σkq+1(M (0)) > ǫ2ηn do\n7: q ← q + 1, kq ← ∣∣∣∣{i : σi(M (0)) ≥ σkq−1+1(M (0)) 2 } ∣∣∣∣ /* Setting rank of the q-th stage */\n8: for Iteration t = 0 to t = T do 9: S(t) = HT ζ(PΩq,t(M − L(t))) /* Projection onto set of sparse matrices */ 10: M (t) = L(t) − mn|Ωq,t|PΩq,t(L (t) + S(t) −M) /* Gradient descent update */ 11: L(t+1) = Pkq (M (t)) /* Projected gradient descent update */\n12: Set threshold ζ ← η ( σkq+1(M (t)) + ( 1 2 )t−2 σkq (M (t)) ) 13: end for 14: S(0) = S(T ), L(0) = L(T ), M (0) = M (T ) /* Initialization for the next outer iteration */ 15: end while 16: Return: L(T )\nacross iterates that is critical to application of standard concentration bounds; such division is a standard technique in the matrix completion related literature [JN15, HW14, Rec11]. Also, η is a tunable parameter which should be less than one and is smaller for “easier” problems.\nNote that updating S(t) requires O(|Ω| · r+(m+n) · r) computational steps. Computation of L(t+1) requires computing SVD for projection Pr, which can be computed in time O(|Ω| ·r+(m+n) ·r2+ r3) time (ignoring log factors); see [JMD10] for more details. Hence, the computational complexity of each step of the algorithm is linear in |Ω|·r (assuming |Ω| ≥ r·(m+n)). As we show in the next section, the algorithm exhibits geometric convergence rate under standard assumptions and hence the overall complexity is still nearly linear in |Ω| (assuming r is just a constant).\nRank based Stagewise algorithm: We also provide a rank-based stagewise algorithm where the outer loop increments kq by one at each stage, i.e., the rank is q in the q-th stage. Our analysis extends for this algorithm as well, however, its time and sample complexity trades off a factor of O(log(σ1/ǫ)) from the complexity of PG-RMC with a factor of r (rank of L∗). We provide the detailed algorithm in Appendix 5.2 due to lack of space (see Algorithm 2)."
    }, {
      "heading" : "3 Analysis",
      "text" : "We now present our analysis for both of our algorithms PG-RMC (Algorithm 1) and R-RMC (Algorithm 2). In general the problem of Robust PCA with Missing Entries (3) is harder than the standard Matrix Completion problem and hence is NP-hard [HMRW14]. Hence, we need to impose certain (by now standard) assumptions on L∗, S̃∗, and Ω to ensure tractability of the problem: Assumption 1. Rank and incoherence of L∗: L∗ ∈ Rm×n is a rank-r incoherent matrix, i.e.,∥∥e⊤i U∗ ∥∥ 2 ≤ µ √ r m , ∥∥e⊤j V ∗ ∥∥ 2 ≤ µ √ r n , ∀i ∈ [m], ∀j ∈ [n], where L∗ = U∗Σ∗(V ∗)⊤ is the SVD of L∗. Assumption 2. Sampling (Ω): In each iteration, Ωq,t is obtained by sampling each entry with probability p = |Ωq,t| mn .\nAssumption 3. Sparsity of S̃∗, S∗: We assume that at most ρ ≤ c µ2r fraction of the elements in each row and column of S̃∗ are non-zero for a small enough constant c. Moreover, we assume that Ω is independent of S̃∗. Hence, S∗ = PΩ(S̃∗) also has at most p · ρ fraction of the entries in expectation. Assumptions 1, 2 are standard assumptions in the provable matrix completion literature [CR09, Rec11, JN15], while Assumptions 1, 3 are standard assumptions in the robust PCA (low-rank+sparse matrix recovery) literature [CSPW11, CLMW11, HKZ11]. Hence, our setting is a generalization of both the standard and popular problems and as we show later in the section, our result can be used to meaningfully improve the state-of-the-art for both these problems.\nWe first present our main result for Algorithm 1 under the assumptions given above. Theorem 1. Let Assumptions 1, 2 and 3 on L∗, S̃∗ and Ω hold respectively. Let m ≤ n, n = O(m), and let the number of samples |Ω| satisfy:\nE[|Ω|] ≥ Cαµ4r2n log2 (n) log2 ( µ2rσ1\nǫ\n) ,\nwhere C is a global constant. Then, with probability at least 1−n− log α2 , Algorithm 1 with η = 4µ2r m , at most O(log(‖M‖2/ǫ))) outer iterations and O(log(µ 2r‖M‖2\nǫ )) inner iterations, outputs a matrix L̂ such that:\n∥∥∥L̂− L∗ ∥∥∥ F ≤ ǫ.\nNote that our number of samples increase with the desired accuracy ǫ. However, using argument similar to that of [JN15], we should be able to replace ǫ by σ∗min which should modify the ǫ term to be log\n2 κ where κ = σ1(L ∗)/σr(L∗). We leave ironing out the details for future work.\nNote that the numbe of samples matches information theoretic bound upto O(r logn log2 σ∗1/ǫ) factor. Also, the number of allowed corruptions in S̃∗ also matches the known lower bounds (up to a constant factor) and cannot be improved upon information theoretically.\nWe now present our result for the rank based stagewise algorithm (Algorithm 2). Theorem 2. Under Assumptions 1, 2 and 3 on L∗, S̃∗ and Ω respectively and Ω satisfying:\nE[|Ω|] ≥ Cαµ4r3n log2 (n) log ( µ2rσ1\nǫ\n) ,\nfor a large enough constant C, then Algorithm 2 with η set to 4µ 2r\nm outputs a matrix L̂ such that: ∥∥∥L̂− L∗ ∥∥∥ F ≤\nǫ, w.p. ≥ 1− n− log α2 . Notice that the sample complexity of Algorithm 2 has an additional multiplicative factor of O(r) when compared to that of Algorithm 1, but shaves off a factor of O(log(κ)). Similarly, computational complexity of Algorithm 2 also trades off a O(log κ) factor for O(r) factor from the computational complexity of Algorithm 1.\nResult for Matrix Completion: Note that for S̃∗ = 0, the RMC problem with Assumptions 1,2 is exactly the same as the standard matrix completion problem and hence, we get the following result as a corollary of Theorem 1: Corollary 1 (Matrix Completion). Suppose we observe Ω and PΩ(L ∗) where Assumptions 1,2 hold for L∗ and Ω. Also, let E[|Ω|] ≥ Cα2µ4r2 log2 n log2 σ1/ǫ and m ≤ n. Then, w.p. ≥ 1 − n− log α 2 , Algorithm 1 outputs L̂ s.t. ‖L̂− L∗‖2 ≤ ǫ. Table 1 compares our sample and time complexity bounds for low-rank MC. Note that our sample complexity is nearly the same as that of nuclear-norm methods while the running time of our algorithm is significantly\nbetter than the existing results that have at most logarithmic dependence on the condition number of L∗.\nResult for Robust PCA: Consider the standard Robust PCA problem (RPCA), where the goal is to recover L∗ from M = L∗ + S̃∗. For RPCA as well, we can randomly sample |Ω| entries from M , where Ω satisfies the assumption required by Theorem 1. This leads us to the following corollary: Corollary 2 (Robust PCA). Suppose we observe M = L∗ + S̃∗, where Assumptions 1, 3 hold for L∗ and S̃∗. Generate Ω ∈ [m] × [n] by sampling each entry uniformly at random with probability p, s.t., E[|Ω|] ≥ Cα2µ4r2 log2 n log2 σ1/ǫ. Let m ≤ n. Then, w.p. ≥ 1 − n− log α 2 , Algorithm 1 outputs L̂ s.t. ‖L̂− L∗‖2 ≤ ǫ. Hence, using Theorem 1, we will still be able to recover L∗ but using only the sampled entries. Moreover, the running time of the algorithm is only O(µ2nr3 log2 n log2(σ1/ǫ)), i.e., we are able to solve RPCA problem in time linear in n. To the best of our knowledge, the existing state-of-the-art methods for RPCA require at least O(n2r) time to perform the same task [NUNS+14, GWL16]. Similarly, we don’t need to load the entire data matrix in memory, but we can just sample the matrix and work with the obtained sparse matrix with at most linear number of entries. Hence, our method significantly reduces both time and space complexity, and as demonstrated empirically in Section 4 can help scale our algorithm to very large data sets without losing accuracy."
    }, {
      "heading" : "3.1 Proof of Theorem 1",
      "text" : "We now present our proof for Theorem 1; the proof of Theorem 2 follows similarly. The proofs of all but one of the lemmas used are deferred to the appendix to improve readability. Recall that we assume that M = L∗+S̃∗ and define S∗ = PΩ(S̃∗). Similarly, we define S̃(t) = HT ζ(M−L(t)). Critically, S(t) = PΩ(S̃(t)) (see Line 9 of Algorithm 1), i.e., S̃(t) is the set of iterates that we “could” obtain if entire M was observed. Note that we cannot compute S̃(t), it is introduced only to simplify our analysis.\nWe first re-write the projected gradient descent step for L(t+1) as described in (4):\nL(t+1) = Pkq ( L∗ + (S̃∗ − S̃(t))︸ ︷︷ ︸\nE1\n+ ( I − PΩq,t\np\n) ( E2︷ ︸︸ ︷ (L(t) − L∗)+(S̃(t) − S̃∗))\n︸ ︷︷ ︸ E3\n) (6)\nThat is, L(t+1) is obtained by rank-kq SVD of a perturbed version of L ∗: L∗ + E1 + E3. As we perform entrywise thresholding to reduce ‖S̃∗ − S̃(t)‖∞, we need to bound ‖L(t+1) − L∗‖∞. To this end, we use techniques from [JN15], [NUNS+14] that explicitly model singular vectors of L(t+1) and argue about the\ninfinity norm error using a Taylor series expansion. However, in our case, such an error analysis requires analyzing the following key quantities (H = E1 + E3):\n∀1 ≤ j, s.t., j even : Aj := max q∈[n]\n‖e⊤q ( H⊤H ) j 2 V ∗‖2, Bj := max\nq∈[m] ‖e⊤q\n( HH⊤ ) j 2 U∗‖2,\n∀1 ≤ j, s.t., j odd : Cj := max q∈[n]\n‖e⊤q H⊤ ( HH⊤ )⌊ j2 ⌋ U∗‖2, Dj := max q∈[m] ‖e⊤q H ( H⊤H )⌊ j2 ⌋ V ∗‖2. (7)\nNote that E1 = 0 in the case of standard RPCA which was analyzed in [NUNS +14], while E3 = 0 in the case of standard MC which was considered in [JN15]. In contrast, in our case both E1 and E3 are non-zero. Moreover, E3 is dependent on random variable Ω. Hence, for j ≥ 2, we will get cross terms between E3 and E1 that will also have dependent random variables which precludes application of standard Bernsteinstyle tail bounds. To this end, we use a technique similar to that of [EKYY13, JN15] to provide a careful combinatorial-style argument to bound the above given quantity. That is, we can provide the following key lemma: Lemma 1. Let L∗, Ω, and S̃∗ satisfy Assumptions 1, 2 and 3 respectively. Let L∗ = U∗Σ∗(V ∗)⊤ be the singular value decomposition of L∗. Furthermore, suppose that in the tth iteration of the qth stage, S̃(t) defined as HTζ(M − L(t)) satisfies Supp(S̃(t)) ⊆ Supp(S̃∗), then we have:\nmax{Aa, Ba, Ca, Da} ≤ µ √ r\nm\n( ρn ‖E1‖∞ + c √ n\np (‖E1‖∞ + ‖E2‖∞) log n\n)a ,\n∀c > 0 w.p ≥ 1− n−2 log c4+4, where E1, E2 and E3 are defined in (6), Aa, Ba, Ca, Da are defined in (7). Remark: We would like to note that even for the standard MC setting, i.e., when E1 = 0, we obtain better bound than that of [JN15] as we can bound maxi ‖eTi (E3)qU‖2 directly rather than the weaker√ rmaxi ‖eTi (E3)quj‖ bound that [JN15] uses. In the following lemma, we characterize how the progress in the estimation of L∗ by L(t) depends on the quantities in 7. Lemma 2. Let L(t) = Pk(L ∗ +H), where H is any perturbation matrix that satisfies the following:\n1. ‖H‖2 ≤ σ∗k 4 2. ∀i ∈ [n], a ∈ ⌈ logn2 ⌉ with υ ≤ σ∗k 4\n∥∥∥e⊤i ( H⊤H ) a 2 V ∗ ∥∥∥ 2 , ∥∥∥e⊤i ( HH⊤ ) a 2 U∗ ∥∥∥ 2 ≤ (υ)aµ √ r m when a is even∥∥∥e⊤i H⊤ ( HH⊤ )⌊ a2 ⌋ U∗ ∥∥∥ 2 , ∥∥∥e⊤i H ( H⊤H )⌊ a2 ⌋ V ∗ ∥∥∥ 2 ≤ (υ)aµ √ r m when a is odd\nwhere σ∗k is the k th singular value of L∗. Also, let L∗ satisfy Assumption 1. Then, the following holds:\n∥∥∥L(t+1) − L∗ ∥∥∥ ∞ ≤ µ 2r m ( σ∗k+1 + 20 ‖H‖2 + 8υ )\nwhere µ and r are the rank and incoherence of the matrix L∗ respectively.\nIn the next lemma, we show that with the threshold chosen in the algorithm, we show an improvement in the estimation of S̃∗ by S̃(t+1). Lemma 3. In the tth iterate of the qth stage, assume the following holds:\n1. ∥∥L∗ − L(t) ∥∥ ∞ ≤ 2µ2r m ( σ∗k+1 + ( 1 2 )z σ∗k )\n2. 78 ( σ∗k+1 + ( 1 2 )z σ∗k ) ≤ ( λk+1 + ( 1 2 )z λk ) ≤ 98 ( σ∗k+1 + ( 1 2 )z σ∗k )\nwhere σ∗k and σ ∗ k+1 are the k and (k+1) th singular values of L∗, λk and λk+1 are the k and (k+1)th singular values of M (t) and, r and µ are the rank and incoherence of the m×n matrix L∗ respectively. Then we have\n1. Supp ( S̃(t) ) ⊆ Supp ( S̃∗ ) 2. ∥∥∥S̃(t) − S̃∗\n∥∥∥ ∞ ≤ 8µ2r m ( σ∗k+1 + ( 1 2 )z σ∗k )\nIn the following lemma, we show that we make progress simultaneously in the estimation of both S̃∗ and L∗ by S̃(t) and L(t). We make use of Lemmas 2 and 3 to show how progress in the estimation of one affects the other alternatively. We use Lemma 1 along with Lemma 2 to show improved estimation of L∗ by L(t). Lemma 4. Let L∗, Ω, S̃∗ and S̃(t) satisfy Assumptions 1,2,3 respectively. Then, in the tth iteration of the qth stage of Algorithm 1, S̃(t) and L(t) satisfy:\n∥∥∥S̃(t) − S̃∗ ∥∥∥ ∞ ≤ 8µ 2r m (∣∣∣σ∗kq+1 ∣∣∣+ ( 1 2 )t−3 ∣∣∣σ∗kq ∣∣∣ ) ,\nSupp ( S̃(t) ) ⊆ Supp ( S̃∗ ) , and\n∥∥∥L(t) − L∗ ∥∥∥ ∞ ≤ 2µ 2r m (∣∣∣σ∗kq+1 ∣∣∣+ ( 1 2 )t−3 ∣∣∣σ∗kq ∣∣∣ ) .\nwith probability ≥ 1− ((q − 1)T + t− 1)n−(10+logα) where T is the number of iterations in the inner loop.\nProof. We prove the lemma by induction on both q and t."
    }, {
      "heading" : "Base Case: q = 1 and t = 0",
      "text" : "We begin by first proving an upper bound on ‖L∗‖∞. We do this as follows:\n∣∣L∗ij ∣∣ = ∣∣∣∣∣ r∑\nk=1\nσ∗ku ∗ ikv ∗ jk ∣∣∣∣∣ ≤ r∑\nk=1\nσ∗k ∣∣u∗ikv∗jk ∣∣ ≤ σ∗1 r∑\nk=1\n∣∣u∗ikv∗jk ∣∣ ≤ µ 2r√ mn σ∗1\nwhere the last inequality follows from Cauchy-Schwartz and the incoherence of U∗. This directly proves the third claim of the lemma for the base case. We also note that due to the thresholding step and the incoherence assumption on L∗, we have:\n1. ∥∥E(0) ∥∥ ∞ ≤ 8µ2r m (σ∗2 + 2σ ∗ 1) (ζ) ≤ 8µ2r m ( 8σ∗k1 ) , and 2. Supp ( S̃(t) ) = Supp ( S̃∗ ) .\nwhere (ζ) follows from Lemma 5. So the base case of induction is satisfied."
    }, {
      "heading" : "Induction over t",
      "text" : "We first prove the inductive step over t (for a fixed q). By inductive hypothesis we assume that:\na) ∥∥E(t) ∥∥ ∞ ≤ 8µ2r m ( σ∗kq+1 + ( 1 2 )t−3 σ∗kq ) b) Supp ( S̃(t) ) ⊆ Supp ( S̃∗ ) .\nc) ∥∥L∗ − L(t) ∥∥ ∞ ≤ 2µ2r m ( σ∗kq+1 + ( 1 2 )t−3 σ∗kq )\nwith probability 1− ((q − 1)T + t− 1)n−(10+logα). Then by Lemma 2, we have: ∥∥∥L(t+1) − L∗\n∥∥∥ ∞ ≤ µ 2r m ( σ∗kq+1 + 20 ‖H‖2 + 8υ ) (8)\nFrom Lemma 1, we have:\nυ ≤ ρn ∥∥∥E(t) ∥∥∥ ∞ +8βα logn (ζ1) ≤ 1 100 ( σ∗kq+1 + ( 1 2 )t−3 σ∗kq ) +8βα logn (ζ2) ≤ 1 50 ( σ∗kq+1 + ( 1 2 )t−3 σ∗kq ) (9)\nwhere (ζ1) follows from our assumptions on ρ and our inductive hypothesis on ∥∥E(t) ∥∥ ∞ and (ζ2) follows\nfrom our assumption on p and by noticing that ‖D‖∞ ≤ ∥∥E(t) ∥∥ ∞ + ∥∥L∗ − L(t) ∥∥ ∞. Recall that D = L(t) − L∗ + S̃(t) − S̃∗. From Lemma 11:\n‖H‖2 ≤ 1\n100\n( σ∗kq+1 + ( 1\n2\n)t−3 σ∗kq ) (10)\nwith probability ≥ 1− n−(10+logα). From Equations 10, 9 and 8, we have: ∥∥∥L∗ − L(t+1)\n∥∥∥ ∞ ≤ 2µ 2r m ( σ∗kq+1 + ( 1 2 )t−2 σ∗kq )\nwhich by union bound holds with probability ≥ 1 − ((q − 1)T + t)n−(10+logα). Hence, using Lemma 3 and 12 we have:\n1. ∥∥E(t+1) ∥∥ ∞ ≤ 8µ2r m ( σ∗kq+1 + ( 1 2 )t−2 σ∗kq ) 2. Supp ( S̃(t)t+ 1 ) ⊆ Supp ( S̃∗ ) .\nwhich also holds with probability ≥ 1 − ((q − 1)T + t)n−(10+logα). This concludes the proof for induction over t."
    }, {
      "heading" : "Induction Over Stages q",
      "text" : "We now prove the induction over q. Suppose the hypothesis holds for stage q. At the end of stage q, we have:\n1. ∥∥E(T ) ∥∥ ∞ ≤ 8µ2r m ( σ∗kq+1 + ( 1 2 )T σ∗kq ) ≤ 8µ 2rσ∗kq+1 m + ǫ10n , and 2. Supp ( S̃(T ) ) ⊆ Supp ( S̃∗ ) .\nwith probability ≥ 1− (qT − 1)n−(10+logα). From Lemmas 6 and 11 we get: ∣∣∣σkq+1 ( M (T ) ) − σ∗kq+1 ∣∣∣ ≤ ‖H‖2 ≤ 1\n100\n( σ∗kq+1 + mǫ\n10nµ2r\n) (11)\nwith probability 1 − n−(10+logα). We know that ησkq+1 ( M (t) ) ≥ ǫ2n which with 11 implies that ∣∣∣σ∗kq+1 ∣∣∣ >\nmǫ 10nµ2r .\n∥∥∥L(T+1) − L∗ ∥∥∥ ∞ ≤ 2µ 2r m ( σ∗kq+1 + ( 1 2 )T+1 σ∗kq ) ≤ 2µ 2r m ( σ∗kq+1 +\nmǫ\n20nµ2rn\n)\n≤ 2µ 2r\nm\n( σ∗kq+1 + σ∗kq+1 2 ) ≤ 2µ 2r m ( 2σ∗kq+1 ) (ζ4) ≤ 2µ 2r m ( 8σ∗kq+1 )\nwhere (ζ4) follows from Lemma 5. By union bound this holds with probability ≥ 1− qTn−(10+logα). Now, from 12 and 3, we have through a similar series of arguments as above:\n∥∥∥E(t)T + 1 ∥∥∥ ∞ ≤ 8µ 2r m ( 8σ∗kq+1 ) (12)\nwhich holds with probability ≥ 1− qTn−(10+logα).\nIn the following lemma, we show that that σ∗kq+1 is sufficiently small compared to σ ∗ kq−1+1 and σ∗kq is sufficiently large compared to σ∗kq−1+1. The first condition enables us to show that σ ∗ kq+1\ndecreases geometrically which ensures that only a small number of “outer iterations” are required for the algorithm to converge while the second condition ensures that the error measured by ‖E1‖∞ and ‖E3‖∞ is small in comparison to σ∗kq which is required for the application of Lemma 3. Lemma 5. Assume that L∗, Ω and S̃∗ satisfy Assumptions 1,2 and 3 respectively. Furthermore, suppose at the beginning of the qth stage of algorithm 1:\n1. ∥∥L∗ − L(0) ∥∥ ∞ ≤ 2µ2r m ( 2σ∗kq−1+1 ) 2. ∥∥E(0) ∥∥ ∞ ≤ 8µ2r m ( 2σ∗kq−1+1 )\nThen, the following hold:\n1. σ∗kq ≥ 15 32σ ∗ kq−1+1\n2. σ∗kq+1 ≤ 17 32σ ∗ kq−1+1\nwith probability ≥ 1− n−(10+logα)\nWe can now proceed to prove Theorem 1:\nProof of Theorem 1: From Lemma 13 we know that T ≥ log(3µ 2rσ∗1 ǫ\n). Consider the stage q reached at the termination of the algorithm. We know from Lemma 4 that:\n1. ∥∥E(T ) ∥∥ ∞ ≤ 8µ2r m ( σ∗kq+1 + ( 1 2 )T σ∗kq ) ≤ 8µ2r m σ∗kq+1 + ǫ 10n 2. ∥∥L(T ) − L∗\n∥∥ ∞ ≤ 2µ2r m ( σ∗kq+1 + ( 1 2 )T ∣∣∣σ∗kq ∣∣∣ ) ≤ 2µ2r m σ∗kq+1 + ǫ 10n\nCombining this with Lemmas 6 and 11, we get:\n∣∣σkq+1(MT ) ∣∣ ≥ σ∗kq+1 − 1\n100\n( σ∗kq+1 + mǫ\n10nµ2r\n) (13)\nWhen the while loop terminates, ησkq+1 ( M (T ) ) < ǫ2n , which from 13, implies that σ ∗ kq+1\n< mǫ7nµ2r . So we have:\n‖L− L∗‖∞ = ∥∥∥L(T ) − L∗ ∥∥∥ ∞ ≤ 2µ 2r m σ∗kq+1 + ǫ 10n ≤ ǫ 2n .\nWe will now bound the number of iterations required for the PG-RMC to converge.\nFrom claim 2 of Lemma 5, we have σ∗kq+1 ≤ 17 32σ ∗ kq−1+1 ∀q ≥ 1. By recursively applying this inequality, we get σ∗kq+1 ≤ ( 17 32 )q σ∗1 . We know that when the algorithm terminates, σ ∗ kq+1 < ǫ7µ2r . Since, ( 17 32 )q σ∗1 is an\nupper bound for σ∗kq+1, an upper bound for the number of iterations is 5 log ( 7µ2rσ∗1 ǫ ) . Also, note that an upper bound to this quantity is used to partition the samples provided to the algorithm. This happens with probability ≥ 1− T 2n−(10+logα) ≥ 1− n− logα. This concludes the proof."
    }, {
      "heading" : "4 Experiments",
      "text" : "In this section we discuss the performance of Algorithm 1 on synthetic data and its use in foreground background separation. The goal of the section is two-fold: a) to demonstrate practicality and effectiveness of Algorithm 1 for the RMC problem, b) to show that Algorithm 1 indeed solves RPCA problem in significantly smaller time than that required by the existing state-of-the-art algorithm (St-NcRPCA [NUNS+14]). To this end, we use synthetic data as well as video datasets where the goal is to perform foreground-background separation [CLMW11].\nWe implemented our algorithm in MATLAB and the results for the synthetic data set were obtained by averaging over 20 runs. We obtained a matlab implementation of St-NcRPCA [NUNS+14] from the authors of [NUNS+14]. Note that if the sampling probability is p = 1, then our method is similar to St-NcRPCA; the key difference being how rank is selected in each stage.\nParameters. The algorithm has three main parameters: 1) threshold λ, 2) incoherence µ and 3) sampling probability p (E[|Ω|] = p · mn). In the experiments on synthetic data we observed that keeping λ ∼ µ ∥∥M − S(t)\n∥∥ 2 / √ n speeds up the recovery while for background extraction keeping λ ∼ µ ∥∥M − S(t) ∥∥ 2 /n\ngives a better quality output. The value of µ for real world data sets was figured out using cross validation while for the synthetic data the same value was used as used in data generation. The sampling probability for the synthetic data could be kept as low as 2r log2(n)/n while for the real world data set we got good results for p = 0.05. Also, rather than splitting samples, we use entire set of observed entries to perform our updates (see Algorithm 1).\nSynthetic data. We generate M = L∗+ S̃∗ of two sizes, where L∗ = UV ⊤ ∈ R2000×2000 (and R5000×5000) is a random rank-5 (and rank-10 respectively) matrix with incoherence ≈ 1. S̃∗ is generated by considering a uniformly random subset of size\n∥∥∥S̃∗ ∥∥∥ 0 from [m]× [n] where every entry is i.i.d. from the uniform distribution\nin [ r 2 √ mn , r√ mn ]. This is the same setup as used in [CLMW11]. Figure 1 (a) plots recovery error (‖L − L∗‖F ) vs computational time for our PG-RMC method (with different sampling probabilities) as well as the St-NcRPCA algorithm. Note that even for very small values of sampling p, we can achieve same recovery error using significantly small values. For example, our method with p = 0.1 achieve 0.01 error (‖L− L∗‖F ) in ≈ 2.5s while St-NcRPCA method requires ≈ 10s to achieve the same accuracy. Note that we do not compare against the convex relaxation based methods like IALM from [CLMW11], as [NUNS+14] shows that St-NcRPCA is significantly faster than IALM and several other convex relaxation solvers.\nFigure 1 (b) plots time required to achieve different recovery errors (‖L−L∗‖F ) as the sampling probability p increases. As expected, we observe a linear increase in the run-time with p. Interestingly, for very small values of p, we observe an increase in running time. In this regime, ‖PΩ(M)‖2 p becomes very large (as p\ndoesn’t satisfy the sampling requirements). Hence, increase in the number of iterations (T ≈ log ‖PΩ(M)‖2 pǫ\n) dominates the decrease in per iteration time complexity.\nFigure 1 (c), (d) plots computation time required by our method (PG-RMC , Algorithm 1) versus rank and incoherence, respectively. As expected, as these two problem parameters increase, our method requires more time. Note that our run-time dependence on rank seems to be linear, while our existing results require O(r3) time. This hints at the possibility of further improving the computational complexity analysis of our algorithm.\nWe also study phase transition for different values of sampling probability p. Figure 3 (a) in Appendix 5.4 show a phase transition phenomenon where beyond p > .06 the probability of recovery is almost 1 while below it, it is almost 0.\nForeground-background separation. We also applied our technique to the problem of foregroundbackground separation. We use the usual method of stacking up the vectorized video frames to construct a matrix. The background, being static, will form the low rank component while the foreground can be considered to be the noise.\nWe applied our PG-RMC method (with varying p) to several videos. Figure 2 (a), (d) shows one frame each from two videos (a shopping center video, a restaurant video). Figure 2 (b), (d) shows the extracted background from the two videos by using our method (PG-RMC , Algorithm 1) with probability of sampling p = 0.05. Figure 2 (c), (f) compares objective function value for different p values. Clearly, PG-RMC can recover the true background with p as small as 0.05. We also observe an order of magnitude speedup (≈ 5x) over St-NcRPCA [NUNS+14]. We present results on the video Escalator in Appendix 5.4.\nConclusion. In this work, we studied the Robust Matrix Completion problem. For this problem, we provide exact recovery of the low-rank matrix L∗ using nearly optimal number of observations as well as nearly optimal fraction of corruptions in the observed entries. Our RMC result is based on a simple and efficient PGD algorithm that has nearly linear time complexity as well. Our result improves state-of-the-art for the related Matrix Completion as well as Robust PCA problem. For Robust PCA, we provide first nearly linear time algorithm under standard assumptions.\nOur sample complexity depends on ǫ, the desired accuracy in L∗. We believe that the arguments used by [JN15], we should be able to remove the ǫ dependence as well and leave it for future work. Moreover, improving dependence of sample complexity on r (from r2 to r) also represents an important direction. Finally, similar to foreground background separation, we would like to explore more applications of RMC/RPCA."
    }, {
      "heading" : "5 Appendix",
      "text" : "We divide this section into five parts. In the first part we prove some common lemmas. In the second part we give the convergence guarantee for PG-RMC . In the third part we give another algorithm which has a sample complexity of O(µ4r3n log2 n log µ2rσ∗1\nǫ ) and prove its convergence guarantees. In the fourth part we\nprove a generalized form of lemma 1. In the fifth part we present some additional experiments.\nFor the sake of convenience in the following proofs, we will define some notation here.\nWe define p = |Ωk,t| mn and we consider the following equivalent update step for L(t+1) in the analysis:\nL(t+1) := Pk(M (t)) M (t) := L∗ +H H := E(t) + βG E(t) := S̃∗ − S̃(t)\nS̃(t) := HT ζ ( M (t) − L(t) ) G := 1\nβ\n( I − PΩq,tp ) D\nD := L(t) − L∗ + S̃(t) − S̃∗ β := 2 √ n ‖D‖∞√\np\nThe singular values of L∗ are denoted by σ∗1 , . . . , σ ∗ r where |σ∗1 | ≥ . . . ≥ |σ∗r | and we will let λ1, . . . , λn denote the singular values of M (t) where |λ1| ≥ . . . ≥ |λn|."
    }, {
      "heading" : "5.1 Common Lemmas",
      "text" : "We will begin by restating some lemmas from previous work that we will use in our proofs.\nFirst, we restate Weyl’s perturbation lemma from [Bha97], a key tool in our analysis: Lemma 6. Suppose B = A+E ∈ Rm×n matrix. Let λ1, · · · , λk and σ1, · · · , σk be the singular values of B and A respectively such that λ1 ≥ · · · ≥ λk and σ1 ≥ · · · ≥ σk. Then:\n|λi − σi| ≤ ‖E‖2 ∀ i ∈ [k].\nThis lemma establishes a bound on the spectral norm of a sparse matrix. Lemma 7. Let S ∈ Rm×n be a sparse matrix with row and column sparsity ρ. Then,\n‖S‖2 ≤ ρmax{m,n} ‖S‖∞\nProof. For any pair of unit vectors u and v, we have:\nv⊤Su = ∑\n1≤i≤m,1≤j≤n viujSij ≤\n∑\n1≤i≤m,1≤j≤n |Sij |\n( v2i + u 2 j\n2\n)\n≤ 1 2\n  ∑\n1≤i≤m v2i\n∑\n1≤j≤n |Sij |+\n∑\n1≤j≤n u2j\n∑\n1≤i≤m |Sij |\n  ≤ ρmax{m,n} ‖S‖∞\nLemma now follows by using ‖S‖2 = maxu,v,‖u‖2=1,‖v‖2=1 uTSv.\nNow, we define a 0-mean random matrix with small higher moments values. Definition 1 (Definition 7, [JN15]). H is a random matrix of size m × n with each of its entries drawn independently satisfying the following moment conditions:\nE[hij ] = 0, |hij | < 1, E[|hij |k] ≤ 1max{m,n} ,\nfor i, j ∈ [n] and 2 ≤ k ≤ 2 logn.\nWe now restate two useful lemmas from [JN15]: Lemma 8 (Lemma 12, 13 of [JN15]). We have the following two claims:\n• Suppose H satisfies Definition 1. Then, w.p. ≥ 1− 1/n10+logα, we have: ‖H‖2 ≤ 3 √ α.\n• Let A be a m×n matrix with n ≥ m. Suppose Ω ⊆ [m]× [n] is obtained by sampling each element with probability p ∈ [ 1 4n , 0.5 ] . Then, the following matrix H satisfies Defintion 1:\nH :=\n√ p\n2 √ n ‖A‖∞\n( A− 1\np PΩ(A)\n) .\nLemma 9 (Lemma 13, [JN15]). Let A ∈ Rn×n be a symmetric matrix with eigenvalues σ1, · · · , σn where |σ1| ≥ · · · ≥ |σn|. Let B = A + C be a perturbation of A satisfying ‖C‖2 ≤ σk2 and let Pk(B) = UΛU⊤ by the rank-k projection of B. Then, Λ−1 exists and we have:\n1. ∥∥A−AUΛ−1U⊤A ∥∥ 2 ≤ |σk|+ 5 ‖C‖2, 2. ∥∥AUΛ−aU⊤A ∥∥ 2 ≤ 4 ( |σk| 2 )−a+2 ∀a ≥ 2.\nWe now provide a lemma that bounds ‖ · ‖∞ norm of an incoherent matrix with its operator norm. Lemma 10. Let A ∈ Rm×n be a rank r, µ-incoherent matrix. Then for any C ∈ Rn×m, we have:\n‖ACA‖∞ ≤ µ2r√ mn ‖ACA‖2\nProof. Let A = UΣV T . Then, ACA = UUTACAV V T . The lemma now follows by using definition of incoherence with the fact that ‖UTACAU‖2 ≤ ‖ACA‖2.\nWe now present a lemma that shows improvement in the error ‖L − L∗‖∞ by using gradient descent on L(t). Lemma 11. Let L∗, Ω, S̃∗ satisfy Assumptions 1,2,3 respectively. Also, let the following hold for the t-th inner-iteration of any stage q:\n1. ∥∥L∗ − L(t) ∥∥ ∞ ≤ 2µ2r m ( σ∗k+1 + ( 1 2 )z σ∗k ) 2. ∥∥∥S̃∗ − S̃(t)\n∥∥∥ ∞ ≤ 8µ2r m ( σ∗k+1 + ( 1 2 )z σ∗k )\n3. Supp(S̃(t)) ⊆ Supp(S̃∗)\nwhere z ≥ −3 and σ∗k and σ∗k+1 are the k and (k + 1)th singular values of L∗. Also, let E1 = S̃(t) − S̃∗ and E3 = ( I − PΩq,t\np\n)( L(t) − L∗ + S̃(t) − S̃∗ ) be the error terms defined also in (6). Then, the following holds\nw.p ≥ 1− n−(10+logα): ‖E1 + E3‖2 ≤ 1\n100\n( σ∗k+1 + ( 1\n2\n)z σ∗k ) (14)\nProof. Note from Lemma 8,\n1 β E3 = 1 β\n( I − PΩq,t\np\n)( L(t) − L∗ + S̃(t) − S̃∗ ) ,\nwith β = 2 √ n√ p · ‖L(t) − L∗ + S̃(t) − S̃∗‖∞ satisfies definition 1.\nWe now bound the spectral norm of E1 + E3 as follows:\n‖E1 + E3‖2 ≤ ‖E1‖2 + β · ∥∥∥∥ 1\nβ E3 ∥∥∥∥ 2 (ζ1) ≤ ρn ∥∥∥S̃(t) − S̃∗ ∥∥∥ ∞ + 3β √ α,\n(ζ2)\n≤ 1 200 ( σ∗kq+1 + ( 1 2 )z σ∗kq ) + 60µ2r m √ n p √ α (∣∣∣σ∗kq+1 ∣∣∣+ ( 1 2 )z ∣∣∣σ∗kq ∣∣∣ ) ,\n(ζ3)\n≤ 1 100 ( σ∗kq+1 + ( 1 2 )z σ∗kq ) .\nwhere (ζ1) follows from Lemma 7 and 8. (ζ2) follows by our assumptions on ρ, ∥∥L(t) − L∗ ∥∥ ∞, and ∥∥∥S̃(t) − S̃∗ ∥∥∥ ∞ . (ζ3) follows from our assumption on p.\nIn the following lemma, we prove that the value of the threshold computed using σk(M (t)) = σk(L ∗+E1+E3), where E1, E3 are defined in (6), closely tracks the threshold that we would have gotten had we had access to the true eigenvalues of L∗, σ∗k. Lemma 12. Let L∗, Ω, S̃∗ satisfy Assumptions 1,2,3 respectively. Also, let the following hold for the t-th inner-iteration of any stage q:\n1. ∥∥L∗ − L(t) ∥∥ ∞ ≤ 2µ2r m ( σ∗k+1 + ( 1 2 )z σ∗k ) 2. ∥∥∥S̃∗ − S̃(t)\n∥∥∥ ∞ ≤ 8µ2r m ( σ∗k+1 + ( 1 2 )z σ∗k )\n3. Supp(S̃(t)) ⊆ Supp(S̃∗)\nwhere z ≥ −3 and σ∗k and σ∗k+1 are the k and (k + 1)th singular values of L∗. Also, let E1 = S̃(t) − S̃∗ and E3 = ( I − PΩq,t\np\n)( L(t) − L∗ + S̃(t) − S̃∗ ) be the error terms defined also in (6). Then, the following holds\n∀z > −3 w.p ≥ 1− n−(10+logα):\n7\n8\n( σ∗k+1 + ( 1\n2\n)z+1 σ∗k ) ≤ ( λk+1 + ( 1\n2\n)z+1 λk ) ≤ 9\n8\n( σ∗k+1 + ( 1\n2\n)z+1 σ∗k ) , (15)\nwhere λk := σk(M (t)) = σk(L ∗ + E1 + E3) and E1, E3 are defined in (6). Proof. Using Weyl’s inequality (Lemma 6), we have: : |λk − σ∗k| ≤ ‖E1 + E3‖2 and ∣∣λk+1 − σ∗k+1\n∣∣ ≤ ‖E1 + E3‖2 We now proceed to prove the lemma as follows:\n∣∣∣∣∣λk+1 + ( 1 2 )z+1 λk − σ∗k+1 − ( 1 2 )z+1 σ∗k ∣∣∣∣∣ ≤ ∣∣λk+1 − σ∗k+1 ∣∣+ ( 1 2 )z+1 |λk − σ∗k| ,\n≤ ‖E1 + E3‖2\n( 1 + ( 1\n2\n)z+1) (ζ) ≤ 1\n100\n( σ∗k+1 + ( 1\n2\n)z σ∗k )( 1 + ( 1\n2\n)z+1) ,\n≤ 1 8\n( σ∗k+1 + ( 1\n2\n)z+1 σ∗k ) ,\nwhere (ζ) follows from Lemma 11 and the last inequality follows from the assumption that z ≥ −3.\nNext, we show that the projected gradient descent update (6) leads to a better estimate of L∗, i.e., we bound ‖L(t+1) − L∗‖∞. Under the assumptions of the below given Lemma, the proof follows arguments similar to [NUNS+14] with additional challenge arises due to more involved error terms E1, E3.\nOur proof proceeds by first symmetrizing our matrices by rectangular dilation. We first begin by noting some properties of symmetrized matrices used in the proof of the following lemma.\nRemark 1. Let A be a m× n dimensional matrix with singular value decomposition UΣV ⊤. We denote its symmetrized version be As := [ 0 A⊤\nA 0\n] . Then:\n1. The singular value decomposition of As is given by As = UsΣsU ⊤ s where\nUs := 1√ 2 [ V V U −U ] Σs := [ Σ 0 0 −Σ ]\n2. P2k (As) = [\n0 Pk(A⊤) Pk(A) 0\n]\n3. We have A2js =\n[ (A⊤A)j 0\n0 (AA⊤)j\n] A2j+1s = [ 0 (A⊤A)jA⊤\n(AA⊤)jA 0\n]\n4. We have\nUsΣ −j s U ⊤ s =\n[ V Σ−jV ⊤ 0\n0 UΣ−jU⊤\n] when j is even\nUsΣ −j s U ⊤ s =\n[ 0 VΣ−jU⊤\nUΣ−jV ⊤ 0\n] when j is odd\nProof of Lemma 2: L(t+1)s = P2k (L∗s +Hs)\nLet l = m + n. Let λ1, · · · , λl be the eigenvalues of M (t)s = L∗s + Hs with |λ1| ≥ |λ2| · · · ≥ |λl|. Let u1, u2, · · · , ul be the corresponding eigenvectors of M (t)s . Using Lemma 6 along with the assumption on ‖Hs‖2, we have: |λ2k| ≥ 3σ∗k 4 . Let UΛV be the eigen vector decomposition of L(t+1). Let UsΛsU ⊤ s to be the eigen vector decomposition of L (t+1) s . Then, using Remark 1 we have ∀ i ∈ [2k]:\n(L∗s +Hs)ui = λiui, i.e.\n( I − Hs\nλi\n) ui = L ∗ sui.\nAs |λ2k| ≥ 3σ ∗ k 4 and ‖Hs‖2 ≤ 14σ∗k, we can apply the Taylor’s series expansion to get the following expression for ui:\nui = 1\nλi\n I + ∞∑\nj=0\n( Hs λi )j  L∗sui.\nThat is,\nL(t+1)s =\n2k∑\ni=1\nλiuiu ⊤ i =\n2k∑\ni=1\nλ−1i ∑\n0≤s,t<∞\n( Hs λi )s L∗suiu ⊤ i L ∗ s ( Hs λi )t ,\n= ∑\n0≤s,t<∞\n2k∑\ni=1\nλ −(s+t+1) i H s sL ∗ suiu ⊤ i L ∗ sH t s =\n∑\n0≤s,t<∞ HssL ∗ sUsΛ −(s+t+1) s U ⊤ s L ∗ sH t s.\nSubtracting L∗s on both sides and taking operator norm, we get:\n∥∥∥L(t+1)s − L∗s ∥∥∥ ∞ = ∥∥UsΛsU⊤s − L∗s ∥∥ ∞ = ∥∥∥∥∥∥ ∑ 0≤s,t<∞ HssL ∗ sUsΛ −(s+t+1) s U ⊤ s L ∗ sH t s − L∗s ∥∥∥∥∥∥ ∞ ,\n≤ ∥∥L∗sUsΛ−1s U⊤s L∗s − L∗s ∥∥ ∞ + ∑\n1≤s+t<∞\n∥∥∥HssL∗sUsΛ−(s+t+1)s U⊤s L∗sHts ∥∥∥ ∞ . (16)\nWe separately bound the first and the second term of RHS. The first term can be bounded as follows:\n∥∥L∗sUsΛ−1s U⊤s L∗s − L∗s ∥∥ ∞ (ζ1) ≤ ∥∥∥∥L ∗ s [ 0 V Σ−1U⊤\nUΣ−1V ⊤ 0\n] L∗s − L∗s ∥∥∥∥ ∞\n(17)\n≤ ∥∥L∗V Σ−1U⊤L∗ − L∗ ∥∥ ∞ (ζ2) ≤ µ 2r√ mn ∥∥L∗UΛ−1U⊤L∗ − L∗ ∥∥ 2 (ζ3) ≤ µ 2r√ mn (∣∣σ∗k+1 ∣∣+ 5 ‖H‖2 ) , (18)\nwhere (ζ1) follows Remark 1, (ζ2) from Lemma 10 and (ζ3) follows from Claim 1 of Lemma 9.\nWe now bound second term of RHS of (16) which we again split in two parts. We first bound the terms with s+ t > logn:\n∥∥∥HssL∗sUsΛ−(s+t+1)s U⊤s L∗sHts ∥∥∥ ∞ ≤ ∥∥∥HssL∗sUsΛ−(s+t+1)s U⊤s L∗sHts ∥∥∥ 2 (ζ1) ≤ ‖Hs‖s+t2 4 ( 2\nσ∗k\n)−(s+t−1)\n≤ 4 ‖H‖2 ( ‖H‖2 2\nσ∗k\n)−(s+t−1) (ζ2) ≤ 4µ 2r\nm ‖H‖2\n( 1\n2\n)−(s+t−1−logn) , (19)\nwhere (ζ1) follows from the second claim of Lemma 9 and noting that ‖Hs‖2 = ‖H‖2 and (ζ2) follows from assumption on ‖H‖2 and using the fact that s+ t ≥ log n. Summing up over all terms with s+ t > logn, we get from 19 and 18:\n∥∥∥L(t+1)s − L∗s ∥∥∥ ∞ ≤ µ 2r√ mn (∣∣σ∗k+1 ∣∣+ 20 ‖H‖2 ) +\n∑\n0<s+t≤log n\n∥∥∥HssL∗sUsΛ−(s+t+1)s U⊤s L∗sHts ∥∥∥ ∞\n(20)\nNow, for terms corresponding to 1 ≤ s+ t ≤ logn, we have: ∥∥∥HssL∗sUsΛ−(s+t+1)s U⊤s L∗sHts ∥∥∥ ∞ = max q1∈[m+n],q2∈[m+n] ∣∣∣e⊤q1H s sL ∗ sUsΛ −(s+t+1) s U ⊤ s L ∗ sH t seq2 ∣∣∣\n≤ (\nmax q1∈[m+n]\n∥∥e⊤q1H s sU ∗ s ∥∥ 2 )∥∥∥Σ∗s(U∗s )⊤UsΛ−(s+t+1)s U⊤s U∗sΣ∗s ∥∥∥ 2 ( max q2∈[m+n] ∥∥e⊤q2H tU∗s ∥∥ 2 )\n(ζ1) ≤ µ 2r\nm υs+t ∥∥∥L∗sUsΛ−(s+t+1)s U⊤s L∗s ∥∥∥ 2 (ζ2) ≤ 4µ 2r m υs+t ( 2\nσ∗k\n)s+t−1 ≤ 4µ 2r\nm υ\n( 1\n2\n)s+t−1 , (21)\nwhere (ζ1) follows from assumption on H in the lemma statement, (ζ2) follows from Claim 2 of Lemma 9.\nIt now remains to bound the terms, max q1∈[m+n]\n∥∥e⊤q1HssU∗s ∥∥ 2 . Note from Remark 1.1 that U∗s = 1√ 2\n[ V ∗ V ∗ U∗ −U∗ ] .\nNow, we have the following cases for Hss :\nHjs =\n[( H⊤H ) s 2 0\n0 ( HH⊤ ) s 2\n] when s is even Hjs = [ 0 H⊤ ( HH⊤ )⌊ s2 ⌋\nH ( H⊤H )⌊ s2 ⌋ 0\n] when s is odd\nIn these two cases, we have:\nHssU ∗ s = 1√ 2\n[( H⊤H ) s 2 V ∗\n( H⊤H ) s 2 V ∗(\nHH⊤ ) s 2 U∗ − ( HH⊤ ) s 2 U∗\n] HssU ∗ s = 1√ 2 [ H⊤ ( HH⊤ )⌊ s2 ⌋ U∗ −H⊤ ( HH⊤ )⌊ s2 ⌋ U∗ H ( H⊤H )⌊ s2 ⌋ V ∗ H ( H⊤H )⌊ s2 ⌋ V ∗ ]\nThis leads to the following 4 cases for max q1∈[m+n]\n∥∥e⊤q1HssU∗s ∥∥ 2 :\nfor s even max q′∈[n]\n∥∥∥e⊤q′ ( H⊤H ) s 2 V ∗ ∥∥∥ 2\nmax q′∈[m]\n∥∥∥e⊤q′ ( HH⊤ ) s 2 U∗ ∥∥∥ 2\nfor s odd max q′∈[n]\n∥∥∥e⊤q′H⊤ ( HH⊤ )⌊ s2 ⌋ U∗ ∥∥∥ 2 max q′∈[m] ∥∥∥e⊤q′H ( H⊤H )⌊ s2 ⌋ V ∗ ∥∥∥ 2\nwe get the bound on these terms in Lemma 15. Also, note from the Remark 1.2 that ∥∥∥L∗s − L (t+1) s ∥∥∥ ∞\n= ∥∥L∗ − L(t+1)\n∥∥ ∞.\nNow, summing up 21 over all 1 ≤ s+ t ≤ logn and combining with 20 we get the required result. In the next lemma, we show that with the threshold chosen in the algorithm, we show an improvement in the estimation of S̃∗ by S̃(t+1).\nProof of Lemma 3: We first prove the first claim of the lemma. Consider an index pair (i, j) /∈ Supp(S̃∗).\n∣∣∣Mij − L(t)ij ∣∣∣ ≤ 2µ 2r\nm\n( σ∗k+1 + ( 1\n2\n)z σ∗k ) (ζ1) ≤ 16µ 2r\n7m\n( λk+1 + ( 1\n2\n)z λk ) ≤ η ( λk+1 + ( 1\n2\n)z λk )\nwhere (ζ1) follows from the second assumption. Hence, we do not threshold any entry that is not corrupted by S̃∗.\nNow, we prove the second claim of the lemma. Consider an index entry (i, j) ∈ Supp(S̃∗). Here, we consider two cases:\n1. The entry (i, j) ∈ Supp(S̃(t)): Here the entry (i, j) is thresholded. We know that L(t)ij + S̃ (t) ij = L ∗ ij+ S̃ ∗ ij\nfrom which we get ∣∣∣S̃(t)ij − S̃∗ij ∣∣∣ = ∣∣∣L∗ij − L (t) ij ∣∣∣ ≤ ∥∥∥L∗ − L(t) ∥∥∥ ∞\n2. The entry (i, j) /∈ Supp(S̃(t)): Here the entry (i, j) is not thresholded. We know that ∣∣∣L∗ij + S̃∗ij − L (t) ij ∣∣∣ ≤ ζ from which we get\n∣∣∣S̃∗ij ∣∣∣ ≤ ζ + ∣∣∣L∗ij − L (t) ij ∣∣∣ (ζ2)\n≤ 36µ 2r\n8m\n( σ∗k+1 + ( 1\n2\n)z σ∗k ) + 2µ2r\nm\n( σ∗k+1 + ( 1\n2\n)z σ∗k )\n≤ 8µ 2r\nm\n( σ∗k+1 + ( 1\n2\n)z σ∗k )\nwhere (ζ2) follows from the second assumption along with the assumption about η = µ2r m .\nThe above two cases prove the second statement of the lemma. Lemma 13. The number of iteration T in the inner loop of Algorithm 1 and Algorithm 2 satisfy:\nT ≥ 10 log ( 7n2µ2rσ∗1/ǫ )\nw.p ≥ 1− n−(10+logα). Here σ∗1 is the highest singular value of L∗, r is it’s rank and µ is it’s incoherence.\nProof. We have the bound since\n∥∥∥∥ n1n2 |Ω| PΩ ( M − S̃(0) )∥∥∥∥ 2 = ∥∥∥∥L ∗ + ( I − PΩ p )(( S̃(0) − S∗ ) − L∗ ) + ( S∗ − S̃(0) )∥∥∥∥ 2\n≥ σ∗1 − ‖H‖2 (ζ1) ≥ 3 4 σ∗1\nwhere (ζ1) follows from Lemma 11.\nWe will now prove Lemma 1 Proof of Lemma 1: Recall the definitions ofE1 = ( S̃∗ − S̃(t) ) , E2 = ( L(t) − L∗ ) , E3 = ( I − PΩq,t\np\n) (E2 − E1)\nand β = 2 √\nn p ‖E2 − E1‖∞. Recall that H := E1 + E3 From Lemma 8, we have that 1βE3 satisfies Def-\ninition 1. This implies that the matrix 1 β (E1 + E3) satisfies the conditions of Lemma 15. Now, we have ∀1 ≤ a ≤ ⌈logn⌉ and ∀i ∈ [n]:\n∥∥ei(HH⊤)aU∗ ∥∥ 2 = β2a ∥∥∥∥∥ei (( 1 β H )( 1 β H )⊤)a U∗ ∥∥∥∥∥ 2\n(ζ) ≤ β2a ( ρn\nβ ‖E1‖∞ + c logn\n)2a µ √ r\nm ≤ µ\n√ r\nm\n( ρn ‖E1‖∞ + 2c √ n\np (‖E1‖∞ + ‖E2‖∞) logn\n)2a\nwhere (ζ) follows from the application of Lemma 15 along with the incoherence assumption on U∗. The other statements of the lemma can be proved in a similar manner by invocations of the different claims of Lemma 15.\nProof of Lemma 5: We know that:\nλkq ≤ σ∗kq + ‖H‖2 , λkq−1+1 ≥ σ ∗ kq−1+1 − ‖H‖2 , λkq ≥\nλkq−1+1\n2\nCombining the three inequalities, we get:\nσ∗kq ≥ σ∗kq−1+1 − 3 ‖H‖2\n2\nApplying Lemma 11, we get the first claim of the lemma.\nSimilar to the first claim, we have:\nλkq+1 ≥ σ∗kq+1 − ‖H‖2 , λkq−1+1 ≤ σ ∗ kq−1+1 + ‖H‖2 , λkq+1 ≤\nλkq−1+1\n2\nAgain, combining the three inequalities, we get:\nσ∗kq+1 ≤ σ∗kq−1+1 + 3 ‖H‖2\n2\nAnother application of Lemma 11 gives the second claim.\nAlgorithm 2 L̂ = R-RMC(Ω,PΩ(M), ǫ, r, η): Non-convex Robust Matrix Completion 1: Input: Observed entries Ω, Matrix PΩ(M) ∈ Rm×n, convergence criterion ǫ, target rank r, thresholding\nparameter η\n2: T ← 10 log 10µ 2rn2‖PΩ(M)‖2n\n|Ω|ǫ /*Number of inner iterations*/\n3: Partition Ω into rT subsets {Ωq,t : q ∈ [r], t ∈ [T ]} uniformly at random 4: L(0) = 0, M (0) ← mn|Ω|PΩ(M), ζ ← ηmn |Ω| σ1(PΩ(M)) 5: q ← 0 6: while σq+1(M\n(0)) > ǫ2ηm do 7: q ← q + 1 8: for Iteration t = 0 to t = T do 9: S(t) = Hζ(PΩq,t(M − L(t))) /*Projection onto set of sparse matrices*/ 10: M (t) = L(t) − mn|Ωq,t|PΩq,t(L (t) + S(t) −M) /*Gradient Descent Update*/ 11: L(t+1) = Pq(M (t)) /*Projected Gradient Descent step*/\n12: Set threshold ζ ← η ( σq+1(M (t)) + ( 1 2 )t σq(M (t)) ) 13: end for 14: S(0) = S(T ), L(0) = L(T+1),M (0) = M (T ) 15: end while 16: Return: L(T+1)"
    }, {
      "heading" : "5.2 Algorithm R-RMC",
      "text" : "Proof of Theorem 2: From Lemma 13 we know that T ≥ log(3µ 2nrσ∗1 mǫ ).\nConsider the stage q reached at the termination of the algorithm. We know from Lemma 14 that:\n1. ∥∥E(T ) ∥∥ ∞ ≤ 8µ2r m ( σ∗q+1 + ( 1 2 )T σ∗q ) ≤ 8µ2r m σ∗q+1 + ǫ 10n 2. ∥∥L(T ) − L∗ ∥∥ ∞ ≤ 2µ2r m ( σ∗q+1 + ( 1 2 )T σ∗q ) ≤ 2µ2r m σ∗q+1 + ǫ 10n\nCombining this with Lemmas 6 and 11, we get:\nσq+1(M) ≥ σ∗q+1 − 1\n100\n( σ∗q+1 + mǫ\n10nµ2r\n) (22)\nWhen the while loop terminates, ησq+1 ( M (T ) ) < ǫ2n , which from 22, implies that σ ∗ q+1 < mǫ 7nµ2r . So we have:\n‖L− L∗‖∞ = ∥∥∥L(T ) − L∗ ∥∥∥ ∞ ≤ 2µ 2r m |σ∗kq+1|+ ǫ 10n ≤ ǫ 2n .\nAs in the case of the proof of Theorem 1, the following lemma shows that we simultaneously make progress in both the estimation of L∗ and S̃∗ by L(t) and S̃(t) respectively. Similar to Lemma 4, we make use of Lemmas 3 and 2 to show how improvement in estimation of one of the quantities affects the other and the other five terms, ‖H‖2, max q′∈[n] ∥∥∥e⊤q′ ( H⊤H )j V ∗ ∥∥∥ 2 , max q′∈[m] ∥∥∥e⊤q′ ( HH⊤ )j U∗ ∥∥∥ 2 , max q′∈[n] ∥∥∥e⊤q′H⊤ ( HH⊤ )j U∗ ∥∥∥ 2 and\nmax q′∈[m]\n∥∥∥e⊤q′H ( H⊤H )j V ∗ ∥∥∥ 2 are analyzed the same way:\nLemma 14. Let L∗, Ω, S̃∗ and S̃(t) satisfy Assumptions 1,2,3 respectively. Then, in the tth iteration of the qth stage of Algorithm 2, S̃(t) and L(t) satisfy:\n∥∥∥S̃(t) − S̃∗ ∥∥∥ ∞ ≤ 8µ 2r m ( σ∗q+1 + ( 1 2 )t−1 σ∗q ) ,\nSupp ( S̃(t) ) ⊆ Supp ( S̃∗ ) , and\n∥∥∥L(t) − L∗ ∥∥∥ ∞ ≤ 2µ 2r m ( σ∗q+1 + ( 1 2 )t−1 σ∗q ) .\nwith probability ≥ 1− ((q − 1)T + t− 1)n−(10+logα) where T is the number of iterations in the inner loop.\nProof. We prove the lemma by induction on both q and t."
    }, {
      "heading" : "Base Case: q = 1 and t = 0",
      "text" : "We begin by first proving an upper bound on ‖L∗‖∞. We do this as follows:\n∣∣L∗ij ∣∣ = ∣∣∣∣∣ r∑\nk=1\nσ∗ku ∗ ikv ∗ jk ∣∣∣∣∣ ≤ r∑\nk=1\n∣∣σ∗ku∗ikv∗jk ∣∣ ≤ σ∗1 r∑\nk=1\n∣∣u∗ikv∗jk ∣∣ ≤ µ 2r\nm σ∗1\nwhere the last inequality follows from Cauchy-Schwartz and the incoherence of U∗. This directly proves the third claim of the lemma for the base case. We also note that due to the thresholding step and the incoherence assumption on L∗, we have:\n1. ∥∥E(0) ∥∥ ∞ ≤ 8µ2r m (σ∗2 + 2σ ∗ 1) 2. Supp ( S̃(t) ) = Supp ( S̃∗ ) .\nSo the base case of induction is satisfied."
    }, {
      "heading" : "Induction over t",
      "text" : "We first prove the inductive step over t (for a fixed q). By inductive hypothesis we assume that:\na) ∥∥E(t) ∥∥ ∞ ≤ 8µ2r m ( |σ∗q+1|+ ( 1 2 )t−1 |σ∗q | ) b) Supp ( S̃(t) ) ⊆ Supp ( S̃∗ ) .\nc) ∥∥L∗ − L(t) ∥∥ ∞ ≤ 2µ2r m ( |σ∗q+1|+ ( 1 2 )t−1 |σ∗q | )\nwith probability 1− ((q − 1)T + t− 1)n−(10+logα). Then by Lemma 2, we have:\n∥∥∥L(t+1) − L∗ ∥∥∥ ∞ ≤ µ 2r m ( |σ∗kq+1|+ 20 ‖H‖2 + 8υ ) (23)\nFrom Lemma 1, we have:\nυ ≤ ρn ∥∥∥E(t) ∥∥∥ ∞ + 8βα logn (ζ1) ≤ 1 100 ( σ∗q+1 + ( 1 2 )t−1 σ∗q ) + 8βα logn (ζ2) ≤ 1 50 ( σ∗q+1 + ( 1 2 )t−1 σ∗q ) (24)\nwhere (ζ1) follows from our assumptions on ρ and our inductive hypothesis on ∥∥E(t) ∥∥ ∞ and (ζ2) follows\nfrom our assumption on p and by noticing that ‖D‖∞ ≤ ∥∥E(t) ∥∥ ∞ + ∥∥L∗ − L(t) ∥∥ ∞. Recall that D = L(t) − L∗ + S̃(t) − S̃∗.\nFrom Lemma 11:\n‖H‖2 ≤ 1\n100\n( σ∗q+1 + ( 1\n2\n)t−1 σ∗q ) (25)\nwith probability ≥ 1− n−(10+logα). From Equations 25, 24 and 23, we have:\n∥∥∥L∗ − L(t+1) ∥∥∥ ∞ ≤ 2µ 2r m ( σ∗q+1 + ( 1 2 )t σ∗q )\nwhich by union bound holds with probability ≥ 1 − ((q − 1)T + t)n−(10+logα). Hence, using Lemma 3 and 12 we have:\n1. ∥∥E(t+1) ∥∥ ∞ ≤ 8µ2r m ( σ∗q+1 + ( 1 2 )t σ∗q ) 2. Supp ( S̃(t+1) ) ⊆ Supp ( S̃∗ ) .\nwhich also holds with probability ≥ 1 − ((q − 1)T + t)n−(10+logα). This concludes the proof for induction over t."
    }, {
      "heading" : "Induction Over Stages q",
      "text" : "We now prove the induction over q. Suppose the hypothesis holds for stage q. At the end of stage q, we have:\n1. ∥∥E(T ) ∥∥ ∞ ≤ 8µ2r m ( σ∗q+1 + ( 1 2 )T σ∗q ) ≤ 8µ 2rσ∗q+1 m + ǫ10n 2. Supp ( S̃(T ) ) ⊆ Supp ( S̃∗ ) .\nwith probability ≥ 1− (qT − 1)n−(10+logα). From Lemmas 6 and 11 we get:\n∣∣∣σq+1 ( M (T ) ) − σ∗q+1 ∣∣∣ ≤ ‖H‖2 ≤ 1\n100\n( σ∗q+1 + mǫ\n10nµ2r\n) (26)\nwith probability 1−n−(10+logα). We know that ησq+1 ( M (t) ) ≥ ǫ2n which with 26 implies that σ∗q+1 > mǫ10nµ2r .\n∥∥∥L(T+1) − L∗ ∥∥∥ ∞ ≤ 2µ 2r m ( σ∗q+1 + ( 1 2 )T+1 σ∗q ) ≤ 2µ 2r m ( σ∗q+1 + mǫ 20µ2rn )\n≤ 2µ 2r\nm\n( σ∗q+1 + σ∗q+1 2 ) ≤ 2µ 2r m ( 2σ∗q+1 )\nBy union bound this holds with probability ≥ 1− qTn−(10+logα). Now, from 12 and 3, we have through a similar series of arguments as above:\n∥∥∥E(T+1) ∥∥∥ ∞ ≤ 8µ 2r m ( 2σ∗kq+1 ) (27)\nwhich holds with probability ≥ 1− qTn−(10+logα)."
    }, {
      "heading" : "5.3 Proof of a generalized form of Lemma 1",
      "text" : "Lemma 15. Suppose H = H1 + H2 and H ∈ Rm×n where H1 satisfies Definition 1 (Definition 7 from [JN15]) and H2 is a matrix with column and row sparsity ρ. Let U be a matrix with rows denoted as u1, . . . , um and let V be a matrix with rows denoted as v1, . . . , vn. Let eq be the q\nth vector from standard basis. Let τ = max{max\ni∈[m] ‖ui‖ ,max i∈[n] ‖vi‖}. Then, for 0 ≤ a ≤ logn:\nmax q∈[n]\n∥∥∥e⊤q ( H⊤H )a V ∥∥∥ 2 , max q∈[m] ∥∥∥e⊤q ( HH⊤ )a U ∥∥∥ 2 ≤ (ρn ‖H2‖∞ + c logn)2aτ\nmax q∈[n]\n∥∥∥e⊤q H⊤ ( HH⊤ )a U ∥∥∥ 2 , max q∈[m] ∥∥∥e⊤q H ( H⊤H )a V ∥∥∥ 2 ≤ (ρn ‖H2‖∞ + c logn)2a+1τ\nwith probability n−2 log c 4+4.\nProof. Similar to [JN15], we will prove the statement for q = 1 and it can be proved for q ∈ [n] by taking a union bound over all q. For the sake of brevity, we will prove only the inequality:\nmax q∈[n]\n∥∥∥e⊤q ( H⊤H )a V ∥∥∥ 2 ≤ (ρn ‖H2‖∞ + c logn)2aτ\nThe rest of the lemma follows by applying similar arguments to the appropriate quantities.\nLet ω : [2a] → {1, 2} be a function used to index a single term in the expansion of (H⊤H)a. We express the term as follows:\n(H⊤H)a = ∑\nω\na∏\ni=1\nH⊤ω(2i−1)Hω(2i)\nWe will now fix one such term ω and then bound the length of the following random vector:\nvω = e ⊤ 1\na∏\ni=1\n(H⊤ω(2i−1)Hω(2i))V\nLet α be used to denote a tuple (i, j) of integers used to index entries in a matrix. Let T (i) be used to denote the parity function computed on i, i.e, 0 if i is divisible by 2 and 1 otherwise. This function indicates if the matrix in the expansion is transposed or not. We now introduce Bp,q(i,j),(k,l), p ∈ {1, 2}, q ∈ {0, 1} and Ap(i,j), p ∈ {1, 2} which are defined as follows:\nAp(i,j) := δi,1(δp,1 + δp,21{(i,j)∈Supp(H2)})\nBp,q(i,j),(k,l) := (δq,1δj,l + δq,0δi,k)(δp,1 + δp,21{(k,l)∈Supp(H2)})\nwhere δi,j = 1 if i = j and 0 otherwise. We will subsequently write the random vector vω in terms of the individual entries of the matrices. The role of Bp,q(i,j),(k,l) and A p (i,j) is to ensure consistency in the terms used to describe vω. We will use hi,α to refer to (Hi)α.\nWith this notation in hand, we are ready to describe vω.\nvω = ∑\nα1,...,α2a α1(1)=1\nAω(1)α1 B ω(2),T (2) α1α2 . . . Bω(2a),T (2a)α2a−1α2a hω(1),α1 · · ·hω(2a),α2avα2a(2)\nWe now write the squared length of vω as follows:\nXω = ∑\nα1,...,α2a,α ′ 1,...,α ′ 2a\nα1(1)=1,α ′ 1(1)=1\nAω(1)α1 B ω(2),T (2) α1α2 . . . Bω(2a),T (2a)α2a−1α2a hω(1),α1 · · ·hω(2a),α2a\nAω(1)α1 B ω(2),T (2) α′1α ′ 2 . . . B ω(2a),T (2a) α′2a−1α ′ 2a hω(1),α′1 · · ·hω(2a),α′2a〈vα2a(2), vα′2a(2)〉\nWe can see from the above equations that the entries used to represent vω are defined with respect to paths in a bipartite graph. In the following, we introduce notations to represent entire paths rather than just individual edges:\nLet α := (α1, . . . , α2a) and\nζα := A ω(1) α1 Bω(2),T (2)α1α2 . . . B ω(2a),T (2a) α2a−1α2a hω(1),α1 . . . hω(2a),α2a\nNow, we can write:\nXω = ∑\nα,α′\nα1(1)=α ′ 1(1)=1\nζαζα′〈vα2a(2), vα′2a(2)〉\nCalculating the kth moment expansion of Xω for some number k, we obtain:\nE[Xkω] = ∑\nα1,...,α2k\nE[ζα1 . . . ζα2k〈vα12a(2), vα22a(2)〉 . . . 〈vα2k−12a (2), vα2k2a(2)〉] (28)\nWe now show how to bound the above moment effectively. Notice that the moment is defined with respect to a collection of 2k paths. We denote this collection by ∆ := (α1, . . . ,α2k). For each such collection, we define a partition Γ(∆) of the index set {(s, l) : s ∈ [2k], l ∈ [2a]} where (s, l) and (s′, l′) are in the same equivalence class if ω(l) = ω(l′) = 1 and αsl = α s′\nl′ . Additionally, each (s, l) such that ω(l) = 2 is in a separate equivalence class.\nWe bound the expression in (28) by partitioning all possible collections of 2k paths based on the partitions defined by them in the above manner. We then proceed to bound the contribution of any one specific path to (28) following a particular partition Γ, the number of paths satisfying that particular partition and finally, the total number of partitions. Since, H1 is a matrix with 0 mean, any equivalence class containing an index (s, l) such that ω(l) = 1 contains at least two elements.\nWe proceed to bound (28) by taking absolute values:\nE[Xkω] ≤ ∑\nα1,...,α2k\nE[|ζα1 | . . . |ζα2k ||〈vα12a(2), vα22a(2)〉| . . . |〈vα2k−12a (2), vα2k2a(2)〉|] (29)\nWe now fix one particular partition and bound the contribution to (29) of all collections of paths ∆ that correspond to a valid partition Γ.\nWe construct from Γ a directed multigraph G. The equivalence classes of Γ form the vertex set of G, V (G). There are 4 kinds of edges in G where each type is indexed by a tuple (p, q) where p ∈ {1, 2}, q ∈ {0, 1}.\nWe denote the edge sets corresponding to these 4 edge types by E(1,0), E(1,1), E(2,0) and E(2,1) respectively. An edge of type (p, q) exists from equivalence class γ1 to equivalence class γ2 if there exists (s, l) ∈ γ1 and (s′, l′) ∈ γ2 such that l′ = l + 1, s = s′, ω(s′) = p and T (l′) = q. The summation in 29 can be written as follows:\nE[|ζα1 | . . . |ζα2k | ∣∣∣〈vα12a(2), vα22a(2)〉 ∣∣∣ . . . ∣∣∣〈vα2k−12a (2), vα2k2a(2)〉 ∣∣∣]\n≤ τ2k ( 2k∏\ns=1\nA ω(1) αs1\n2a−1∏\nl=1\nB ω(l+1),T (l+1) αs\nl ,αs l+1\n) E [( 2k∏\ns=1\n2a∏\nl=1\n∣∣∣hω(l),αs l ∣∣∣ )]\n(ζ1) ≤ τ2k ( 2k∏\ns=1\nA ω(1) αs1\n2a−1∏\nl=1\nB ω(l+1),T (l+1) αs\nl ,αs l+1\n) ∏\nγ∈V1(G)\n1\nn\n∏\nγ∈V2(G) ‖H2‖∞\n= τ2k ‖H2‖w2∞\nnw1\n( 2k∏\ns=1\nA ω(1) αs1\n2a−1∏\nl=1\nB ω(l+1),T (l+1) αs\nl ,αs l+1\n)\nwhere (ζ1) follows from the moment conditions on H1. V1(G) and V2(G) are the vertices in the graph corresponding to tuples (i, j) such that ω(j) = 1 and ω(j) = 2 respectively and w1 = |V1(G)|, w2 = |V2(G)|. We first consider an equivalence class γ1 such that there exists an index (s, l) ∈ γ1 and l = 1. We form a spanning tree T1 of all the nodes reachable from γ1 with γ1 as root. We then remove the nodes V (T1) from the graph G and repeat this procedure until we obtain a set of l trees T1, . . . , Tl with roots γ1, . . . , γl such that l⋃\ni=1\nV (Gi) = V (G). This happens because every node is reachable from some equivalence class which\ncontains an index of the form (s, 1). Also, each of these trees Ti, ∀ i ∈ [l] is disjoint in their vertex sets. Given this decomposition, we can factorize the above product as follows:\nE[Xkω] ≤ τ2k ‖H2‖w2∞\nnw1\nl∏\nj=1\n∑\nα1,...,αvj\nAω(1)α1\n∏\n{γ,γ′}∈E(1,0)(Tj) B1,0αγαγ′ ∏\n{γ,γ′}∈E(1,1)(Tj) B1,1αγαγ′\n∏\n{γ,γ′}∈E(2,0)(Tj) B2,0αγαγ′\n∏\n{γ,γ′}∈E(2,1)(Tj) B2,1αγαγ′ (30)\nFor a single connected component, we can compute the summation bottom up from the leaves. First, notice that:\n∑ αγ′ B2,1αγαγ′ ≤ ρn ∑ αγ′\nB2,0αγαγ′ ≤ ρn ∑ αγ′ B1,1αγαγ′ = n ∑ αγ′ B1,0αγαγ′ = n\nWhere the first two follow from the sparsity of H2. Every node in the tree Tj with the exception of the root has a single incoming edge. For the root, γj , we have:\n∑ α1 A ω(1) α1 ≤ ρn for ω(1) = 2 ∑ α1 A ω(1) α1 = n for ω(1) = 1\nFrom the above two observations, we have:\n∑ α1,...,αvj Aω(1)α1\n∏\n{γ,γ′}∈E(1,0)(Tj) B1,0αγαγ′\n∏\n{γ,γ′}∈E(1,1)(Tj) B1,1αγαγ′\n∏\n{γ,γ′}∈E(2,0)(Tj) B2,0αγαγ′\n∏\n{γ,γ′}∈E(2,1)(Tj) B2,1αγαγ′ ≤ (ρn) w2,jnw1,j\nwhere wk,j represents the number of vertices in the j th component which contain tuples (i, j) such that ω(j) = k for k ∈ {1, 2}. Plugging the above in (30) gives us\nE[Xkω(Γ)] ≤ τ2k ‖H2‖w2∞\nnw1 (ρn)\n∑ j w2,jn ∑ j w1,j = τ2k ‖H2‖w2∞ (ρn)w2\nLet a1 and a2 be defined as |{i : ω(i) = 1}| and |{i : ω(i) = 2}| respectively (Note that w2 = 2a2k). Summing up over all possible partitions (there are (2a1k) 2a1k of them), we get our final bound on E [ X̂kω ] as τ2k(ρn ‖H2‖∞)2a2k(2a1k)2a1k. Now, we bound the probability that X̂ω is too large. Choosing k = ⌈ logn a1 ⌉ and applying the kth moment Markov inequality, we obtain:\nPr [∣∣∣X̂ω ∣∣∣ > (c logn)2a1τ2(ρn ‖H2‖∞)2a2 ] ≤ E [∣∣∣X̂ω ∣∣∣ k ](\n1 (c logn)2a1τ2(ρn ‖H2‖∞)2a2 )k\n≤ (\n2ka1 c logn\n)2ka1\n≤ n−2 log c4\nTaking a union bound over all the 2a possible ω, over values of a from 1 to logn and over the n values of q, we get the required result."
    }, {
      "heading" : "5.4 Additional Experimental Results",
      "text" : "We detail some additional experiments performed with Algorithm 1 in this section. The experiments were performed on synthetic data and real world data sets.\nSynthetic data. We generate a random matrix M ∈ R2000×2000 in the same way as described in Section 4. In these experiments our aim is to analyze the behavior of the algorithm in extremal cases. We consider two of such cases : 1) sampling probability is very low (Figure 3 (a)), 2) number of corruptions is very large (Figure 3 (b)). In the first case, we see that the we get a reasonably good probability of recovery (∼ 0.8) even with very low sampling probability (0.07). In the second case, we observe that the time taken to recover seems almost independent of the number of corruptions as long as they are below a certain threshold. In our experiments we saw that on increasing the ρ to 0.2 the probability of recovery went to 0. To compute the probability of recovery we ran the experiment 20 times and counted the number of successful runs.\nForeground-background separation. We present results for one more real world data set in this section. We applied our PG-RMC method (with varying p) to the Escalator video. Figure 4 (a) shows one frame\nfrom the video. Figure 4 (b) shows the extracted background from the video by using our method (PGRMC , Algorithm 1) with probability of sampling p = 0.05. Figure 4 (c) compares objective function value for different p values.\n0.05 0.1 0.15 0.2 0.25 0.3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n1e-01 1e-02 1e-03 1e-04 1e-05"
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "In this paper, we consider the problem of Robust Matrix Completion (RMC) where the goal is to recover a low-rank matrix by observing a small number of its entries out of which a few can be arbitrarily corrupted. We propose a simple projected gradient descent method to estimate the low-rank matrix that alternately performs a projected gradient descent step and cleans up a few of the corrupted entries using hard-thresholding. Our algorithm solves RMC using nearly optimal number of observations as well as nearly optimal number of corruptions. Our result also implies significant improvement over the existing time complexity bounds for the low-rank matrix completion problem. Finally, an application of our result to the robust PCA problem (low-rank+sparse matrix separation) leads to nearly linear time (in matrix dimensions) algorithm for the same; existing state-of-the-art methods require quadratic time. Our empirical results corroborate our theoretical results and show that even for moderate sized problems, our method for robust PCA is an an order of magnitude faster than the existing methods.",
    "creator" : "LaTeX with hyperref package"
  }
}