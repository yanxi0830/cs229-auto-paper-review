{
  "name" : "1705.07817.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Sparse hierarchical interaction learning with epigraphical projection",
    "authors" : [ "Mingyuan Jiu", "Nelly Pustelnik", "Stefan Janaqi", "Meriam Chebre", "Philippe Ricoux" ],
    "emails" : [ "iemyjiu@zzu.edu.cn.", "nelly.pustelnik@ens-", "stefan.janaqi@mines-ales.fr." ],
    "sections" : [ {
      "heading" : null,
      "text" : "Index Terms—Regression learning, hierarchical interaction, primal-dual algorithm, epigraphical projection\nF"
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "L EARN interactions between features is of main inter-est in data processing such as for genomics [1], [2]. Graphical Lasso [3] is an efficient strategy to obtain a graph of interactions from the inverse covariance matrix but its limitation is to not being adapted for a specific task such as regression, which is the core of this contribution. To learn the interactions in the context of regression, additive sparse regression model has to be extended. Several work have been dedicated to this subject in a recent literature [1], [4], [5], [6], [7]. In this contribution we improve on these recent works by providing a common framework and an efficient minimization strategy based on epigraphical projection and primal-dual proximal algorithms.\nOne challenge for feature interaction learning is that the interaction number quadratically increases as the feature size, for example, 1000 features will have one million possible interactions, this therefore results into an overfitting problem due to insufficient observation samples in most situations. To overcome this weakness, the regression weights and the quadratic interactions can be assumed to be sparse such that most features would not contribute to the decision. Sparsity-based regularization is known to be mainly suitable when the feature size is larger than the training samples. Various sparsity regularizations have been proposed and extensively studied in the additive model, for instance, involving `0-pseudo-norm [8], `1-norm [9], `∞- norm [10], or structural sparsity norm [11]. See [12] for an\n• M. Jiu is with School of Information Engineering, Zhengzhou University, China. Email: iemyjiu@zzu.edu.cn. • N. Pustelnik is with Univ Lyon, ENS de Lyon, Univ Lyon 1, CNRS, Laboratoire de Physique, F-69342 Lyon, France. Email: nelly.pustelnik@enslyon.fr. • S. Janaqi is with LGI2P-EMA, Parc Scientifique Georges Besse, F-30035 Nimes, France. Email: stefan.janaqi@mines-ales.fr. • M. Chebre and P. Ricoux are with TOTAL SA, Direction Scientifique, 92069 Paris La Défense, France. Email: meriam.chebre@total.com, philippe.ricoux@total.com.\nexhaustive list of sparse-based regularization in learning. The idea to integrate quadratic interactions in the learning problem including discrimination task is not new, for example, discriminant quadratic learning [13], [14] learns the covariance matrix in the quadratic term to improve the discrimination ability. However, the idea to learn covariance matrix combined with sparsity regularization is more recent with the preliminary work by Zhao et al. [4] and the following works [1], [5], [6], [7]. Formally, if we denote the regression training set R = {(y`,x`) ∈ R × RN | ` ∈ {1, . . . , L}}, the criterion involved in sparse quadratic interactions learning takes the form\nminimize v∈RN ,Θ∈RN×N\n1\n2 L∑ `=1 (y` − x>` v − x>` Θx`)2 + Ω(v,Θ) (1)\nwhere v = ( v(i) ) 1≤i≤N models the regression weight vector,\nΘ = ( Θ(i,j) ) 1≤i≤N,1≤j≤N denotes the matrix of interactions, and Ω(v,Θ) is the regularization term including sparse regularization and other constraints such as hierarchy constraints.\nThe hierarchy constraints make the relationship between v and Θ, as shown in Fig. 1. Two types of hierarchy constraints have been defined in “hierNet” [1]: (i) weak hierarchy where the interactions happen (i.e., Θ(i,j) 6= 0) if one of the associating main features in v is non-zero (i.e., v(i) 6= 0 or v(j) 6= 0) and (ii) strong hierarchy when both associating features are non-zero. Both regularizations can be written as\nΩ(v,Θ) = λ\n2 ‖Θ‖1 + λ N∑ i=1 max{|v(i)|, ‖Θ(i,·)‖1}+ ιC(Θ)\n(2) where the notation Θ(i,·) = ( Θ(i,j) ) 1≤j≤N and ιC denotes the indicator function1 of a closed convex set C ⊂ RN×N that can either model an unconstrained problem C = RN×N\n1. For every x ∈ H, ιC(x) = 0 if x ∈ C and +∞ otherwise.\nar X\niv :1\n70 5.\n07 81\n7v 2\n[ cs\n.L G\n] 3\nA ug\n2 01\n7\nor impose symmetric structure for matrix of interactions using C = S = {Θ ∈ RN×N |Θ = Θ>} for weak or strong hierarchy constraints, respectively. In [7], a more general framework is derived taking the form\nΩ(v,Θ) = λ\n2 ‖Θ‖1 + λ N∑ i=1 ‖ ( v(i), z ( Θ(i,·) )) ‖q + ιC(Θ) (3)\nwhere z : RN → R and 1 ≤ q ≤ +∞. The contribution on Θ(i,·) w.r.t. v(i) can be (i) reduced to a `r-norm with r > 0 (i.e., z(·) = ‖ · ‖r) or (ii) considered globally by keeping the information contained in all the interaction coefficients considering z(·) = (·)>. Most of the state-ofthe-art penalization may be interpreted as a specific case of this general setting, for instance:\n• [15] : z(·) = (·)>, q = {2,+∞}, • [1], [5] : z(·) = ‖ · ‖1 and q = +∞, • [7] : z(·) = (·)> and q = {2}.\nNote that [15] focus on weak hierarchy using C = RN×N . [1] is encountered under the name “hierNet”, while [5] is named “FAMILY” and [7] is “Type-A GRESH”. The latent overlapping group lasso formulation of [1] is known as “glinternet” [6].\nFrom the algorithmic point of view several strategies can be encountered. In [15] and [7], the iterations are derived from FISTA [16]. This procedure appears to be very efficient when C = RN×N while for C = S it requires inner iterations based on Dykstra algorithm that slower significantly the convergence [17]. On the other hand, [1] and [5] resort to ADMM to deal with C = S.\nUsing recent development of non-smooth convex optimization, the objective of this work is to provide a unified framework and an efficient algorithmic strategy based on primal-dual proximal algorithms and epigraphical projection to solve (2) when C = S, z(·) = ‖·‖r with r = {1,+∞} and q = +∞, i.e.\nΩ(v,Θ) = λ\n2 ‖Θ‖1+λ N∑ i=1 ‖ ( v(i), ‖Θ(i,·)‖r ) ‖∞+ιC(Θ). (4)\nThe algorithmic procedure does not resort to inner iterations leading to faster algorithmic procedure.2\nFollowing arguments provided in [1], Section 2 derives an equivalent writing of (4) using epigraphical formulation and recall the relationship with [15]. Section 3 presents the primal-dual proximal algorithm iterations and the convergence guarantees. Its specification for solving the minimization problem (1) with regularization function (4) is specified\n2. Matlab codes will be made available at the time of the publication.\nand new epigraphical projections are provided. Section 4 evaluates the performance of the proposed strategy compared to FISTA and ADMM formulations proposed in [1], [15] both on synthetic data and real data. Based on the new algorithmic procedure, fair comparisons between a penalization involving r = 1 or r = +∞ can be provided. Finally Section 5 gives our conclusion."
    }, {
      "heading" : "2 EPIGRAPHICAL FORMULATION",
      "text" : "The minimization of (1) with the regularization term defined as in Eq. (4) is difficulty, since it involves non-smooth convex functions and symmetry constraint. In [15], Jenatton et al. solve the problem for r = ∞ in the weak hierarchy formulation by means of proximal algorithm. However, their algorithm is hardly adaptive to strong hierarchy. In [1], the authors reformulate (4) when r = 1 under an epigraphical formulation, which represents the problem as a set of hard constraints. This reformulation helps in the interpretation and it highlights that a reduction in the shrinkage of certain main effects and an increase in the shrinkage of certain interactions. Its second advantage is to help in the design of the ADMM algorithmic scheme. The merit of this formulation is to split the problem as a set of convex constraints, and it allows us to deal with them by computing their projection onto convex sets. The following proposition gives the epigraphical formulation of our minimization problem.\nProposition 1. The minimization problem\nminimize v,Θ\n1\n2 L∑ `=1 (y` − x>` v − x>` Θx`)2 + Ω(v,Θ) (5)\nwhere Ω(v,Θ) is defined in (4) can be equivalently formulated as\nminimize (v+,v−,Θ)∈O×O×C\n1\n2 L∑ `=1 (y` − x>` (v+ − v−)− x>` Θx`)2\n+ λ\n2 ‖Θ‖1 + λ1>(v+ + v−) + N∑ i=1 ιEr ( v+(i), v−(i),Θ(i,·) ) (6)\nwhere O denotes the positive orthant and Er = {(ω+, ω−,u) ∈ R× R× RN | ‖u‖r ≤ ω+ + ω−}.\nThe proof follows arguments derived in [1] for the specific case r = 1. Let v = v+ − v−, (6) can be equivalently written as\nminimize v,v+,v−,Θ\n1\n2 L∑ `=1 (y` − x>` v − x>` Θx`)2 + λ\n2 ‖Θ‖1 + λ1>(2v+ − v)\nsubj. to  ‖Θ(i,·)‖r ≤ 2v+(i) − v(i) (∀i ∈ {1, . . . , N}) v = v+ − v−\n(v+,v−,Θ) ∈ O ×O × C. (7)\nThe constraints involving (v,v+,v−) can be reformulated as\n(∀i ∈ {1, . . . , N})  v+(i) ≥ 12 (‖Θ (i,·)‖r + v(i)), v+(i) ≥ v(i)\nv+(i) ≥ 0.\nyielding to v+(i) ≥ max{max{0, v(i)}, 12 (‖Θ (i),·‖r + v(i))}. Using |v(i)| = 2 max{0, v(i)} − v(i), we get the expected result:\nminimize v,Θ\n1\n2 L∑ `=1 (y` − x>` v − x>` Θx`)2\n+ λ\n2 ‖Θ‖1 + λ N∑ i=1 max{|v(i)|, ‖Θ(i,·)‖r}+ ιC(Θ). (8)\nThe algorithmic strategy designed in next section focus on the epigraphical formulation (6)."
    }, {
      "heading" : "3 PRIMAL-DUAL PROXIMAL ALGORITHM",
      "text" : "Proximal algorithms are derived from two main frameworks that are forward-backward scheme and Douglas-Rachford iterations (both being deduced from Krasnoselskii-Mann scheme) [18]. FISTA can be presented as an accelerated version of forward-backward iterations [16], [19] while ADMM can be viewed as a Douglas-Rachford in the dual [20]. In the literature dedicated to sparse regression the most encountered algorithm is FISTA when dealing with a sum of two convex functions where one is differentiable with a Lipschitz gradient. When the criterion involves more than two functions, typically additional constraint such as the constraint S defined previously, most of the works derive an ADMM procedure or compute the proximity operator by mean of inner iterations which is known to often lead to an approximate solution even if global convergence can be obtained in specific cases [17]. Another class of algorithmic procedures allowing to minimize a criterion with more that two functions, possibly including a differentiable function with a Lipschitz gradient, is the class of primal-dual proximal approaches [21], [22], [23], [24].\nSeveral primal-dual proximal schemes have been derived but one of the most popular is based on forwardbackward iterations [23], [24] in order to estimate\nŵ ∈ Argmin w∈H f(w) + g(w) + h(Hw), (9)\nwhere H : H → G denotes a bounded linear operator, where H and G being two Hilbert spaces, f : H →] − ∞,+∞], g : H →]−∞,+∞] and h : G →]−∞,+∞] denote convex, l.s.c and proper functions. We additionally assume that f is differentiable with a Lipschitz constant denoted β > 0. Iterations are summarized in Algorithm 1 involving the proximity operator defined as\n(∀w ∈ H) proxg(w) = arg minu∈H ‖u−w‖ 2 2 + g(u)\nand h∗ denotes the Fenchel-Rockafellar conjugate of a function h. The proximity operator of the conjugate can be computed according to Moreau identity that is proxσh∗(w) = w − σproxh/σ(w/σ) for σ > 0.\nAlgorithm 1 involves two parameters τ and σ. In finite dimensions, the convergence of the sequence (w[i+1])i∈N to ŵ is insured when\n1 τ − σ‖H‖2 ≥ β 2 .\nAdditional technical assumptions can be found in [23][Theorem 3.1].\nAlgorithm 1 Primal-dual splitting algorithm\nInitialization: Set τ > 0, σ > 0, (w[0],u[0]) ∈ H× G For i = 0, 1, . . .⌊\nw[i+1] = prox τg\n( w[i] − τ∇f(w[i])− τH∗u[i] ) u[i+1] = prox\nσh∗\n( u[i] + σH(2w[i+1] −w[i]) )\nThis algorithmic scheme can be extended for dealing with more than three functions by setting h(Hx) =∑K k=1 hk(Hkx) involving the computation of proxσh∗k . The interest of this scheme compared to ADMM is twofold: it first makes the possibility to deal with the gradient of the differentiable function and second it avoids to invert∑ kH ∗ kHk.\n3.1 Specificity for minimizing (6)\nWe first provide the iterations of Algorithm 1 specified to the minimization of (6). We set w = (v+,v−,Θ) ∈ RN × RN × RN×N , for weak hierarchy, we can split it as follows: f(w) = 12 ∑L `=1(y` − x>` v − x>` Θx`)2 +λ1>(v+ + v−), g(w) = ιO(v +) + ιO(v\n−) + λ2 ‖Θ‖1, h(w) = ∑N i=1 ιEr ( v+(i), v−(i),Θ(i,·) ) ,\n(10)\nand for strong hierarchy: f(w) = 12 ∑L `=1(y` − x>` v − x>` Θx`)2 +λ1>(v+ + v−), g(w) = ιO(v +) + ιO(v −) + ιS(Θ), h1(w) = λ 2 ‖Θ‖1,\nh2(w) = ∑N i=1 ιEr ( v+(i), v−(i),Θ(i,·) ) .\n(11)\nThe respective iterations are summarized in Algorithm 2 and 3. The projection onto the positive orthant and on S have well known closed form expressions [18] that are:{\nPO(·) = max{0, ·}, PS(Θ) = Θ+Θ>\n2 .\nThe difficulty comes from the computation of PEr whose expressions are provided in next section."
    }, {
      "heading" : "3.2 New epigraphical projection",
      "text" : "We first focus on the derivation of PE∞ .\nProposition 2. Let u = (u(i))1≤i≤N . The projection of (ω+, ω−,u) ∈ R× R× RN on the epigraphic set E∞ reads\n(η+, η−,p) = PE∞(ω +, ω−,u)\nwith  η− = ω−−(N−n̄+1)(ω+−ω−)+ ∑N i=n̄ ν (i) 1+2(N−n̄+1) η+ = η− + ω+ − ω−\np = P[−η+−η−,η++η−](u)\n(12)\nwhere (ν(1), . . . , ν(N)) is an ordered version of (|u(i)|)1≤i≤N in an ascending order and with ν(0) = −∞, and n̄ ∈ {1, . . . , N} such that ν(n̄−1) < η+ + η− ≤ ν(n̄).\nAlgorithm 2 – Weak-PD-`r – Primal-dual splitting algorithm to solve (8) when C = RN×N Parameter settings: Set τ > 0, σ > 0, such that 2(1/τ − σ) ≥ β. Initialization: (v+[0],v−[0],Θ[0], s+[0], s−[0],Λ[0]) ∈ RN × RN × RN×N × RN × RN × RN×N . For k = 0, 1, . . . b` = y` − x>` (v+[k] − v−[k])− x>` Θ[k]x` v+[k+1] = PO ( v+[k] + τ ∑ ` x`b` − τs +[k] ) v−[k+1] = PO ( v−[k] − τ ∑ ` x`b` − τs −[k]) Θ[k+1] = P τλ 2 ‖·‖1 ( Θ[k] + τ (∑ ` x (i) ` x (j) ` b` ) 1≤i≤N,1≤j≤N − τΛ [k] ) For i = 1, . . . , N b (s+(i)[k+1], s−(i)[k+1],Λ(i,·)[k+1])\n= prox σι∗ Er\n( s+(i)[k] + σ(2v+[k+1] − v+[k]), s−(i)[k] + σ(2v−(i)[k+1] − v−(i)[k]),Λ+(i,·)[k] + σ(2Θ(i,·)[k+1] −Θ(i,·)[k]) )\nAlgorithm 3 – Strong-PD-`r – Primal-dual splitting algorithm to solve (8) when C = S Parameter settings: Set τ > 0, σ > 0, such that 2(1/τ − σ) ≥ β. Initialization: (v+[0],v−[0],Θ[0], s+[0], s−[0],Λ[0]1 ,Λ [0] 2 ) ∈ RN × RN × RN×N × RN × RN × RN×N × RN×N . For k = 0, 1, . . . b` = y` − x>` (v+[k] − v−[k])− x>` Θ[k]x` v+[k+1] = PO ( v+[k] + τ ∑ ` x`b` − τs +[k] ) v−[k+1] = PO ( v−[k] − τ ∑ ` x`b` − τs −[k]) Θ[k+1] = PS ( Θ[k] + τ (∑ ` x (i) ` x (j) ` b` ) 1≤i≤N,1≤j≤N − τ(Λ [k] 1 + Λ [k] 2 ) ) Λ +[k+1] 1 = proxσ‖·‖∗1 ( Λ +[k] 1 + σ(2Θ [k+1] −Θ[k]) ) For i = 1, . . . , N b (s+(i)[k+1], s−(i)[k+1],Λ(i,·)[k+1]2 )\n= prox σι∗ Er\n( s+(i)[k] + σ(2v+[k+1] − v+[k]), s−(i)[k] + σ(2v−(i)[k+1] − v−(i)[k]),Λ+(i,·)[k]2 + σ(2Θ(i,·)[k+1] −Θ(i,·)[k]) )\nProof. According to the definition of the epigraphical projection, we solve the following minimization problem:\nPE∞(ω +, ω−,u)\n= arg min η+,η−≥0 (η+−ω+)2+(η−−ω−)2+ min |p(1)|<η++η−\n··· |p(N)|<η++η−\n‖p−u‖2\n(13)\nThe inner minimization yields a simple projection for u to [−η+ − η−, η+ + η−], i.e. p = P[−η+−η−,η++η−](u), thus Eq. (13) becomes:\nPE∞(ω +, ω−,u) = arg min\nη+,η−≥0\n{ (η+ − ω+)2 + (η− − ω−)2\n+ N∑ i=1 (max{|u(i)| − η+ − η−, 0})2 } (14)\nand thus\nPE∞(ω +, ω−,u) = proxφ+ιO (ω +, ω−) (15)\nwhere φ(ω+, ω−) = ∑N i=1(max{|u(i)| − η+ − η−, 0})2. We now focus on the computation of proxφ(ω +, ω−).\nFirst, we sort (|u(i)|)1≤i≤N in ascending order to be (ν(1), . . . , ν(N)), such that |ν(1)| ≤ . . . ≤ |ν(N)|, and also ν(0) = −∞; and second n̄ ∈ [1, N ] can be found such that\nν(n̄−1) ≤ η+ + η− ≤ ν(n̄), then φ(ω+, ω−) = ∑M i=n̄(η\n+ + η− − ν(n̄))2, so the proximity operator of φ has: ν(n̄−1) ≤ η+ + η− ≤ ν(n̄), ω+ − η+ = (N − n̄+ 1)(η+ + η−)− ∑N i=n̄ ν (i),\nω− − η− = ω+ − η+, (16)\nand that leads to{ η− = ω−−(N−n̄+1)(ω+−ω−)+ ∑N i=n̄ ν (i)\n1+2(N−n̄+1) ,\nη+ = η− + ω+ − ω−. (17)\nThe above result is obtained from arguments closed than the ones derived in [25] [Proposition 5] for an epigraphical constraint of the form { (ω,u) ∈ R ×\nRN | max{τ1|u(1)|, . . . , τN |u(N)|} ≤ ω }\nwhere (τi)1≤i≤N denotes positive weights.\nNext proposition is a preliminary result to derive PE1 .\nProposition 3. Let E+1 = {(ω+, ω−,u) ∈ R × R × RN | ‖u‖1 = ω++ω−,u ≥ 0}. The projection of (ω+, ω−,u) ∈ R× R× RN on the epigraphic set E+1 reads\n(η+, η−,p) = PE+1 (ω+, ω−,u)\nwith  η− = ∑ñ i=1 µ (i)−ω++(ñ+1)ω− ñ(1+2/ñ) η+ = η− + ω+ − ω− p = u−max { 0, ∑ñ i=1 µ\n(i)−(η++η−) ñ\n} (18)\nwith\nñ = max{n ∈ {1, . . . , N} | µ(n) − ∑n i=1 µ\n(i) − (η+ + η−) n > 0} (19)\nand where µ = (µ(i))1≤i≤N is an ordered version of u = (u(i))1≤i≤N in an descending order.\nProof. The Lagrangian associated to the function involved in PE+1 is:\nL(η+, η−,p, α, ξ) = 1 2 ‖p− u‖2 + 1 2 (η+ − ω+)2\n+ 1\n2 (η− − ω−)2 + α ( N∑ i=1 p(i) − η+ − η− ) − ξ>p. (20)\nThe KKT conditions are: (∀i ∈ {1, . . . , N}) p(i) − u(i) + α− ξi = 0 η+ − ω+ − α = 0 η− − ω− − α = 0 (∀i ∈ {1, . . . , N}) ξ(i)p(i) = 0∑N i=1 p (i) − η+ − η− = 0.\n(21)\nFrom the fourth condition in Eq. (21), we know that if p(i) > 0 then ξ(i) = 0 and p(i) = u(i) − α. Thus, if we denote π = (π(i))1≤i≤N is an ordered version of (p(i))1≤n≤N in a decreasing order, we can write\nN∑ i=1 p(i) = N∑ i=1 π(i) = ñ∑ i=1 π(i) = ñ∑ i=1 (µ(i) − α) = η+ + η−.\n(22) From similar arguments than in [26][Lemma 2] , ñ is computed as (19) and thus\nα = 1\nñ ( ñ∑ i=1 µ(i) − (η+ + η−) ) . (23)\nFrom the second and third equality of the KKT conditions, we have η+ = η− + ω+ − ω−. Finally, combining (23) with η− = ω− + α, yields to\nη− = 1\nñ(1 + 2/ñ) ( ñ∑ i=1 µ(i) − ω+ + (ñ+ 1)ω− ) . (24)\nNext we get the solution of PE1 from the ones of projection to E+1 according to [26][Lemma 3] and have the following proposition:\nProposition 4. The projection of (u+, u−,w) ∈ R × R × RN on the epigraphic set E1 reads\n(η+, η−,p) = PE1(u +, u−,w)\nwith { (v+, v−, p̂) = PE+1 (u+, u−, abs(w))\np = sign(w)p̂ (25)\nwhere abs(·) and sign(·) denote the componentwise absolute value and the componentwise sign.\nThis result can be directly derived from [26][Lemma 3] and the proof is therefore omitted here.\nIntegrating the projection derived in Proposition 2 into Algorithm 2 or Proposition 3 and Proposition 4 into Algorithm 3 with\nproxσι∗Er (u+, u−,w) = (u+, u−,w)− σPEr (u+ σ , u− σ , w σ ) insure the convergence to a minimizer of (6) and, from Proposition 1, to a minimizer of (5), respectively for weak hierarchy and strong hierarchy."
    }, {
      "heading" : "4 EXPERIMENTS",
      "text" : "In this section, we provide comparison results of both `1 and `∞ proposed approaches with the two closest stateof-the-art procedures, that are “hierNet” [1] and Jenatton’s framework [15]. Comparisons are of two types, comparisons in terms of convergence behavior and in term of performing estimation."
    }, {
      "heading" : "4.1 Simulated data",
      "text" : ""
    }, {
      "heading" : "4.1.1 Dataset",
      "text" : "The dataset is created according to [5]. It is initially composed of N main features and of N(N − 1)/2 interactions (due to symmetry). We denote (v,Θ) ∈ RN × RN×N the ground truth generated according to strong hierarchy. v denotes a sparse vector where the non-zero values are associated to randomly selected indexes. The value for the non-zero v(i) is randomly selected from the set {−5,−4, . . . ,−1, 1, . . . , 5}. Due to strong hierarchy constraint, Θ is only non-zero for those whose main effects are not zero. The values are randomly chosen from the set {−10,−8, . . . ,−2, 2, . . . , 8, 10}.\nThe algorithmic performance is evaluated on two simulated datasets. The first one is composed with N = 30 features, the first 10 main features are non-zeros and the interaction ratio is ρ = 3.45% (Dataset30-345). The second dataset is of size N = 100, the first 30 features are non-zeros and the interaction ratio is ρ = 0.30% (Dataset100-030).\nThe datasets are composed with a training, a validation and a testing set with 100 samples each. x` is randomly generated according to normal distribution N (0, IN ). The observation value for each sample is set by y` = x>` v + x>` Θx` + ε`, where ε` is independent Gaussian noise to make the signal-to-noise ratio approximately equals to 5dB."
    }, {
      "heading" : "4.1.2 Comparison with [15] – r =∞ and weak hierarchy",
      "text" : "In order to provide fair comparison with the work in [15], we first investigate the performance of proposed primaldual proximal algorithm (Algorithm 2) when r = ∞ and in the configuration of weak hierarchy (no symmetry constraint is considered, i.e. C = RN×N ): “Weak-PD-`∞”. The algorithmic procedure designed in [15] is based on “FISTA” and it will be named “Weak-FISTA-`∞”. Both are compared in two respects: the convergence of the objective function (5) and the convergence of the iterates (i.e. ‖w[k] −w[+∞]‖).\nThe comparisons are displayed in Fig. 2 (for Dataset30345) and in Fig. 3 (for Dataset100-030). The convergence behaviour are presented both w.r.t iterations (first row) and time (second row). It is observed that:\ni) “Weak-PD-`∞” and “Weak-FISTA-`∞” converge to the same value as shown in the first column of both Fig. 2 and 3;\nii) From the objective function convergence pointof-view, “Weak-FISTA-`∞” converges faster than ‘Weak-PD-`∞” w.r.t iteration numbers and also time.\niii) From the convergence of the iterates point of view (second column of Fig. 2 and 3), “Weak-PD-`∞” converges much faster than “Weak-FISTA-`∞” to the optimal solution, especially from the comparison in terms of time (bottom figures). It shows that the proposed algorithm enables to be closest to the optimal solution with less time; iv) The conclusions are very close for both datasets (Dataset30-345 and Dataset100-030) leading to the conclusion that the proposed method is robust to dimensionality.\nv) Contrary to“Weak-FISTA-`∞”, the proposed solution has a ‘Strong-PD-`∞” counterpart without inner iterations."
    }, {
      "heading" : "4.1.3 Comparison with [1] – r = 1 and strong hierarchy",
      "text" : "Similar experiments are conducted for r = 1 and strong hierarchy (i.e., C = S). The proposed algorithm (Algorithm 3 with r = 1) is called “Strong-PD-`1” and it is compared with “hierNet” whose iterations are derived from an ADMM scheme, named“Strong-ADMM-`1” in our experiments.\nThe comparisons between these two schemes are displayed in Fig. 4 (for Dataset30-345) and Fig. 5 (for Dataset100-030). Both are compared in three respects: the convergence of the objective function (5), the convergence of the iterates (i.e. ‖w[k] −w[+∞]‖), and the distance to the set S. The evolutions are displayed w.r.t iteration number and time. It is observed that:\ni) “Strong-PD-`1” and “Strong-ADMM-`1” converge to the same solution but the convergence of the ob-\njective to the optimum solution is very sensitive to the trade-off hyper-parameter for“Strong-ADMM`1”.\nii) “Strong-PD-`1” is always faster either in iteration or in time and either in terms of functional or in terms of iterations. The explanation mainly comes from the inner iterations required with “Strong-ADMM`1” when dealing with C = S;\niii) Third column of Fig. 4 and 5 highlights that the constraint violations (i.e. distance to S) with the proposed method is always smaller than with ADMM."
    }, {
      "heading" : "4.1.4 Discussion regarding the choice of r",
      "text" : "From the above algorithmic comparisons we have observed that the proposed method either for `1 or `∞ deliver an accurate solution (as other algorithmic strategies) but faster and with the possibility to include the stronghierarchy constraint without inner iterations. In the following experiments, we will thus focus on comparisons between weak/strong `1 or `∞ using the proposed algorithm (“Weak-PD-`∞” , “Weak-PD-`1” , “Strong-PD-`∞” , “Strong-PD-`1”). We perform 20 simulations and the average performance with the associated variances are presented in Fig. 6 for different values of the regularization parameter λ. Moreover, the performance obtained by cross-validation are presented Table 1.\ni) The good behaviour of the strong hierarchy constraint clearly appear for both datasets. Indeed, we recall that the data have been created with strong hierarchy structure and we can clearly observe that for both datasets the performance with the strong hierarchy constraint are always better either for r = 1 or r = +∞.\nii) In our set of experiments, the regularization with r = 1 always leads to better performance than with r = +∞.\niii) MSE values associated with the Dataset100-030 are larger, probably due to overfitting. This assumption can be validated by the results obtained on the training dataset."
    }, {
      "heading" : "4.2 Application to HIV Data",
      "text" : "In this section, we apply the proposed algorithms to HIV dataset on the susceptibility of the HIV-1 virus to six nucleoside reverse transcriptase inhibitors (NRTIs). This dataset3 is collected by [27] and it is used to model HIV-1 susceptibility to the drugs because HIV-1 virus can become resistant through genome mutations for different subjects. In the dataset, there are 639 subjects and 240 genomic locations. For each observation, the mutation status at each genomic location is recorded and also give six drugs (log) susceptibility responses. In this work, we focus on the prediction model for 3TC drug."
    }, {
      "heading" : "4.2.1 Reduced dataset",
      "text" : "Followed by [5], we first work on a reduced dataset over the bins of ten adjacent loci, rather than all 240 genomic locations, resulting from the fact that nearby genomes have similar effects to the drug susceptibility, and also leading to less sparse data. So for the first set of experiments we have N = 24 features. The value for each bin is set to 1 if one of genomes in that bin occurs mutation.\nThe dataset is randomly split into two half sets for training and test respectively as adopted by [1], [5]. We report the average performance of MSE on the test set over 5 splits under different λ for “Weak-PD-`∞”, “Strong-PD`∞”, “Weak-PD-`1” and “Strong-PD-`1” in Fig. 7 (left).\nIt can be observed that “Strong-PD-`∞” and “Strong-PD`1” are slightly better than their weak counterparts, and the best λ for “Strong-PD-`∞” and “Strong-PD-`1” are 12 and 10 respectively. The interactions for one realization are visualized in the Fig. 8 (left). In order to better visualize the effect of the penalizations, we also plot the `∞-norm and `1-norm over each row of Θ for strong hierarchy, as shown in the middle and right of Fig. 8. Both have the strong effect for 19th feature, which is consistent with the results observed in [5]. We also observe an interesting fact from the visualization of interactions, it can been seen that “Strong-PD-`1” is able to learn a sparser interactions matrix than “Strong-PD-`∞”, however, the maximum strength (i.e `∞-norm) and the sum of strength (`1- norm) in Fig. 8 have similar distributions for both cases. The possible reason is that the extra interactions learned by “Strong-PD-`∞” are subtle, it can be also confirmed both performance are very similar in the Fig. 7 (left)."
    }, {
      "heading" : "4.2.2 Full dataset",
      "text" : "We further work on the full data without binning operation leading to N = 240 features and each feature gives the indicator of mutation. The data splitting is the same as the one previously described. The average performance of MSE on the test set under four situations are shown in Fig. 7 (right). It is clear to demonstrate that strong hierarchy obtains better\n3. It can be downloaded from https://hivdb.stanford.edu/pages/ published analysis/genophenoPNAS2006/\nperformance than weak one. The learned interactions and their feature effects from “Strong-PD-`∞” and “Strong-PD`1” with best λ = 1 are shown in Fig. 9. It is found that both algorithms can detect that 184th genomic location has the most strong effect. From the maximum strength and sum of strength for each gene in the Fig. 9, we can see the different properties for both algorithms. For “StrongPD-`∞”, maximum strength for each gene has a more sparse distribution than sum of strength, which is consistent with its objective that the maximum of absolute values of Θ(i,:) is not larger that v(i), whereas a more sparse distribution over sum of strength is obtained for “Strong-PD-`1”, whose objective is to ensure that the sum of absolute values of Θ(i,:) is not larger that v(i). From Fig. 7(right), it also can be observed that “Strong-PD-`1” is slightly better than “StrongPD-`∞” in that it imposes a stronger penalization for Θ than “Strong-PD-`∞”."
    }, {
      "heading" : "5 CONCLUSION",
      "text" : "In this paper, we propose a primal-dual proximal algorithm based on epigraphical projection to optimize the sparse regression learning problem when hierarchical constraints are made between the main effects and the interactions. Four algorithms are derived (“Weak-PD-`∞” , “Weak-PD`1” , “Strong-PD-`∞” , “Strong-PD-`1”) allowing to deal and compare properly different configurations of the regularization term (4). Compared to other state-of-the-art algorithms, for instance, FISTA or ADMM, the proposed algorithm is computationally more efficient (find a solution belonging to S, convergence in terms of iterates is faster). The experiments on the synthetic and real data validate its effectiveness and efficiency."
    } ],
    "references" : [ {
      "title" : "A lasso for hierarchical interactions",
      "author" : [ "J. Bien", "J. Taylor", "R. Tibshirani" ],
      "venue" : "Annals of Statistics, vol. 41, no. 3, pp. 1111–1141, 2013.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "BRANE Cut: Biologically-related apriori network enhancement with graph cuts for gene regulatory network inference",
      "author" : [ "A. Pirayre", "C. Couprie", "F. Bidard", "L. Duval", "J.-C. Pesquet" ],
      "venue" : "BMC Bioinformatics, vol. 16, no. 1, 2015.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Sparse inverse covariance estimation with the graphical lasso",
      "author" : [ "J. Friedman", "T. Hastie", "R. Tibshirani" ],
      "venue" : "Biostatistics, vol. 9, no. 3, pp. 432–441, 2008.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "The composite absolute penalties family for groupes and hierarchical variable selection",
      "author" : [ "P. Zhao", "G. Rocha", "B. Yu" ],
      "venue" : "The Annals of Statistics, vol. 37, no. 6A, pp. 3468–3497, 2009.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Learning interactions through hierarchical group-lasso regularization",
      "author" : [ "M. Lim", "T. Hastie" ],
      "venue" : "Journal of Computational and Graphical Statistics, vol. 24, pp. 627–654, 2015.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Group regularized estimation under structural hierarchy",
      "author" : [ "Y. She", "H. Jiang" ],
      "venue" : "Journal of the American Statistical Association, 2016.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Use of the zero-norm with linear models and kernel methods",
      "author" : [ "J. Weston", "A. Elisseeff", "B. Scholkopf", "M. Tipping" ],
      "venue" : "Journal of Machine Learning Research, vol. 3, pp. 1439–1461, 2003.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Regression shrinkage and selection via the lasso",
      "author" : [ "R. Tibshirani" ],
      "venue" : "Journal of the Royal Statistical Society, Series B, vol. 58, pp. 267–288, 1994.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "The F∞-norm support vector machine",
      "author" : [ "H. Zou", "M. Yuan" ],
      "venue" : "Statistica Sinica, vol. 18, pp. 379–398, 2008.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Structured sparsity through convex optimization",
      "author" : [ "F. Bach", "R. Jenatton", "J. Mairal", "G. Obozinski" ],
      "venue" : "Statistical Science, vol. 27, no. 4, pp. 450–468, 11 2012.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Optimization with sparsity-inducing penalties",
      "author" : [ "——" ],
      "venue" : "Foundations and Trends in Machine Learning, vol. 4, no. 1, pp. 1–106, 2012.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Quadratic logistic discrimination",
      "author" : [ "J.A. Anderson" ],
      "venue" : "Biometrika, vol. 62, pp. 149–154, 1975.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 1975
    }, {
      "title" : "Generalized linear and quadratic discriminant functions using robust  estimates",
      "author" : [ "H.R. Ronald", "D.B. James", "S.R. John", "V.H. Robert" ],
      "venue" : "Journal of the American Statistical Association, vol. 73, pp. 564–568, 1978.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 1978
    }, {
      "title" : "Proximal methods for hierarchical sparse coding",
      "author" : [ "R. Jenatton", "J. Mairal", "G. Obozinski", "F. Bach" ],
      "venue" : "Journal of Machine Learning Research, vol. 12, pp. 2297–2334, 2011.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "A fast iterative shrinkage-thresholding algorithm for linear inverse problems",
      "author" : [ "A. Beck", "M. Teboulle" ],
      "venue" : "SIAM Journal on Imaging Sciences, vol. 2, no. 1, pp. 183–202, 2009.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Nested iterative algorithms for convex constrained image recovery problem",
      "author" : [ "C. Chaux", "J.-C. Pesquet", "N. Pustelnik" ],
      "venue" : "SIAM Journal on Imaging Sciences, vol. 2, no. 2, pp. 730–762, Jun. 2009.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Convex analysis and monotone operator theory in hilbert spaces",
      "author" : [ "H. Bauschke", "P. Combettes" ],
      "venue" : "Springer, New York, 2011.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "On the convergence of the iterates of the fast iterative shrinkage/thresholding algorithm",
      "author" : [ "A. Chambolle", "C. Dossal" ],
      "venue" : "Journal of Optimization Theory and Applications, vol. 166, no. 3, pp. 968–982, Sep. 2015.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Split Bregman algorithm, Douglas-Rachford splitting and frame shrinkage",
      "author" : [ "S. Setzer" ],
      "venue" : "ch. Scale Space and Variational Methods in Computer Vision. SSVM 2009. Lecture Notes in Computer Science,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2009
    }, {
      "title" : "A first-order primal-dual algorithm  JOURNAL OF  LTEX CLASS FILES, VOL. XX, NO. XX, XXXX  10 for convex problems with applications to imaging",
      "author" : [ "A. Chambolle", "T. Pock" ],
      "venue" : "Journal of Mathematical Imaging and Vision, vol. 40, no. 1, pp. 120–145, 2011.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Playing with duality: an overview of recent primal-dual approaches for solving large-scale optimization problems",
      "author" : [ "N. Komodakis", "J.C. Pesquet" ],
      "venue" : "IEEE Signal Processing Magazine, vol. 32, no. 6, pp. 31–54, Nov 2015.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "A primal-dual splitting method for convex optimization involving Lipschitzian, proximable and linear composite terms",
      "author" : [ "L. Condat" ],
      "venue" : "Journal of Optimization Theory and Applications, vol. 158, no. 2, pp. 460–479, 2013.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "A splitting algorithm for dual monotone inclusions involving cocoercive operators",
      "author" : [ "B.C. Vũ" ],
      "venue" : "Advances in Computational Mathematics, vol. 38, no. 3, pp. 667–681, Apr. 2013.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Epigraphical projection and proximal tools for solving constrained convex optimization problems",
      "author" : [ "G. Chierchia", "N. Pustelnik", "J.-C. Pesquet", "B. Pesquet-Popescu" ],
      "venue" : "Signal, Image and Video Processing, vol. 9, no. 8, pp. 1737–1749, 2015.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Efficient projections onto the l1-ball for learning in high dimensions.",
      "author" : [ "J.C. Duchi", "S.S.-S", "Shai", "Y. Singer", "T. Chandra" ],
      "venue" : "in ICML, ser. ACM International Conference Proceeding Series,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2008
    }, {
      "title" : "Genotypic predictors of human immunodeficiency virus type 1 drug resistance",
      "author" : [ "S.-Y. Rhee", "J. Taylor", "G. Wadhera", "A. B-H", "D.L. Brutlag", "R.W. Shafer" ],
      "venue" : "Proceedings of the National Academy of Sciences, vol. 103, no. 46, pp. 17 355–17 360, 2006.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2006
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "L EARN interactions between features is of main interest in data processing such as for genomics [1], [2].",
      "startOffset" : 97,
      "endOffset" : 100
    }, {
      "referenceID" : 1,
      "context" : "L EARN interactions between features is of main interest in data processing such as for genomics [1], [2].",
      "startOffset" : 102,
      "endOffset" : 105
    }, {
      "referenceID" : 2,
      "context" : "Graphical Lasso [3] is an efficient strategy to obtain a graph of interactions from the inverse covariance matrix but its limitation is to not being adapted for a specific task such as regression, which is the core of this contribution.",
      "startOffset" : 16,
      "endOffset" : 19
    }, {
      "referenceID" : 0,
      "context" : "Several work have been dedicated to this subject in a recent literature [1], [4], [5], [6], [7].",
      "startOffset" : 72,
      "endOffset" : 75
    }, {
      "referenceID" : 3,
      "context" : "Several work have been dedicated to this subject in a recent literature [1], [4], [5], [6], [7].",
      "startOffset" : 77,
      "endOffset" : 80
    }, {
      "referenceID" : 4,
      "context" : "Several work have been dedicated to this subject in a recent literature [1], [4], [5], [6], [7].",
      "startOffset" : 87,
      "endOffset" : 90
    }, {
      "referenceID" : 5,
      "context" : "Several work have been dedicated to this subject in a recent literature [1], [4], [5], [6], [7].",
      "startOffset" : 92,
      "endOffset" : 95
    }, {
      "referenceID" : 6,
      "context" : "Various sparsity regularizations have been proposed and extensively studied in the additive model, for instance, involving `0-pseudo-norm [8], `1-norm [9], `∞norm [10], or structural sparsity norm [11].",
      "startOffset" : 138,
      "endOffset" : 141
    }, {
      "referenceID" : 7,
      "context" : "Various sparsity regularizations have been proposed and extensively studied in the additive model, for instance, involving `0-pseudo-norm [8], `1-norm [9], `∞norm [10], or structural sparsity norm [11].",
      "startOffset" : 151,
      "endOffset" : 154
    }, {
      "referenceID" : 8,
      "context" : "Various sparsity regularizations have been proposed and extensively studied in the additive model, for instance, involving `0-pseudo-norm [8], `1-norm [9], `∞norm [10], or structural sparsity norm [11].",
      "startOffset" : 163,
      "endOffset" : 167
    }, {
      "referenceID" : 9,
      "context" : "Various sparsity regularizations have been proposed and extensively studied in the additive model, for instance, involving `0-pseudo-norm [8], `1-norm [9], `∞norm [10], or structural sparsity norm [11].",
      "startOffset" : 197,
      "endOffset" : 201
    }, {
      "referenceID" : 10,
      "context" : "See [12] for an",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 11,
      "context" : "The idea to integrate quadratic interactions in the learning problem including discrimination task is not new, for example, discriminant quadratic learning [13], [14] learns the covariance matrix in the quadratic term to improve the discrimination ability.",
      "startOffset" : 156,
      "endOffset" : 160
    }, {
      "referenceID" : 12,
      "context" : "The idea to integrate quadratic interactions in the learning problem including discrimination task is not new, for example, discriminant quadratic learning [13], [14] learns the covariance matrix in the quadratic term to improve the discrimination ability.",
      "startOffset" : 162,
      "endOffset" : 166
    }, {
      "referenceID" : 3,
      "context" : "[4] and the following works [1], [5], [6], [7].",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 0,
      "context" : "[4] and the following works [1], [5], [6], [7].",
      "startOffset" : 28,
      "endOffset" : 31
    }, {
      "referenceID" : 4,
      "context" : "[4] and the following works [1], [5], [6], [7].",
      "startOffset" : 38,
      "endOffset" : 41
    }, {
      "referenceID" : 5,
      "context" : "[4] and the following works [1], [5], [6], [7].",
      "startOffset" : 43,
      "endOffset" : 46
    }, {
      "referenceID" : 0,
      "context" : "Two types of hierarchy constraints have been defined in “hierNet” [1]: (i) weak hierarchy where the interactions happen (i.",
      "startOffset" : 66,
      "endOffset" : 69
    }, {
      "referenceID" : 5,
      "context" : "In [7], a more general framework is derived taking the form",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 13,
      "context" : "• [15] : z(·) = (·)>, q = {2,+∞}, • [1], [5] : z(·) = ‖ · ‖1 and q = +∞, • [7] : z(·) = (·)> and q = {2}.",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 0,
      "context" : "• [15] : z(·) = (·)>, q = {2,+∞}, • [1], [5] : z(·) = ‖ · ‖1 and q = +∞, • [7] : z(·) = (·)> and q = {2}.",
      "startOffset" : 36,
      "endOffset" : 39
    }, {
      "referenceID" : 5,
      "context" : "• [15] : z(·) = (·)>, q = {2,+∞}, • [1], [5] : z(·) = ‖ · ‖1 and q = +∞, • [7] : z(·) = (·)> and q = {2}.",
      "startOffset" : 75,
      "endOffset" : 78
    }, {
      "referenceID" : 13,
      "context" : "Note that [15] focus on weak hierarchy using C = RN×N .",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 0,
      "context" : "[1] is encountered under the name “hierNet”, while [5] is named “FAMILY” and [7] is “Type-A GRESH”.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "[1] is encountered under the name “hierNet”, while [5] is named “FAMILY” and [7] is “Type-A GRESH”.",
      "startOffset" : 77,
      "endOffset" : 80
    }, {
      "referenceID" : 0,
      "context" : "The latent overlapping group lasso formulation of [1] is known as “glinternet” [6].",
      "startOffset" : 50,
      "endOffset" : 53
    }, {
      "referenceID" : 4,
      "context" : "The latent overlapping group lasso formulation of [1] is known as “glinternet” [6].",
      "startOffset" : 79,
      "endOffset" : 82
    }, {
      "referenceID" : 13,
      "context" : "In [15] and [7], the iterations are derived from FISTA [16].",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 5,
      "context" : "In [15] and [7], the iterations are derived from FISTA [16].",
      "startOffset" : 12,
      "endOffset" : 15
    }, {
      "referenceID" : 14,
      "context" : "In [15] and [7], the iterations are derived from FISTA [16].",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 15,
      "context" : "This procedure appears to be very efficient when C = RN×N while for C = S it requires inner iterations based on Dykstra algorithm that slower significantly the convergence [17].",
      "startOffset" : 172,
      "endOffset" : 176
    }, {
      "referenceID" : 0,
      "context" : "On the other hand, [1] and [5] resort to ADMM to deal with C = S.",
      "startOffset" : 19,
      "endOffset" : 22
    }, {
      "referenceID" : 0,
      "context" : "2 Following arguments provided in [1], Section 2 derives an equivalent writing of (4) using epigraphical formulation and recall the relationship with [15].",
      "startOffset" : 34,
      "endOffset" : 37
    }, {
      "referenceID" : 13,
      "context" : "2 Following arguments provided in [1], Section 2 derives an equivalent writing of (4) using epigraphical formulation and recall the relationship with [15].",
      "startOffset" : 150,
      "endOffset" : 154
    }, {
      "referenceID" : 0,
      "context" : "Section 4 evaluates the performance of the proposed strategy compared to FISTA and ADMM formulations proposed in [1], [15] both on synthetic data and real data.",
      "startOffset" : 113,
      "endOffset" : 116
    }, {
      "referenceID" : 13,
      "context" : "Section 4 evaluates the performance of the proposed strategy compared to FISTA and ADMM formulations proposed in [1], [15] both on synthetic data and real data.",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 13,
      "context" : "In [15], Jenatton et al.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 0,
      "context" : "In [1], the authors reformulate (4) when r = 1 under an epigraphical formulation, which represents the problem as a set of hard constraints.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 0,
      "context" : "The proof follows arguments derived in [1] for the specific case r = 1.",
      "startOffset" : 39,
      "endOffset" : 42
    }, {
      "referenceID" : 16,
      "context" : "Proximal algorithms are derived from two main frameworks that are forward-backward scheme and Douglas-Rachford iterations (both being deduced from Krasnoselskii-Mann scheme) [18].",
      "startOffset" : 174,
      "endOffset" : 178
    }, {
      "referenceID" : 14,
      "context" : "FISTA can be presented as an accelerated version of forward-backward iterations [16], [19] while ADMM can be viewed as a Douglas-Rachford in the dual [20].",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 17,
      "context" : "FISTA can be presented as an accelerated version of forward-backward iterations [16], [19] while ADMM can be viewed as a Douglas-Rachford in the dual [20].",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 18,
      "context" : "FISTA can be presented as an accelerated version of forward-backward iterations [16], [19] while ADMM can be viewed as a Douglas-Rachford in the dual [20].",
      "startOffset" : 150,
      "endOffset" : 154
    }, {
      "referenceID" : 15,
      "context" : "When the criterion involves more than two functions, typically additional constraint such as the constraint S defined previously, most of the works derive an ADMM procedure or compute the proximity operator by mean of inner iterations which is known to often lead to an approximate solution even if global convergence can be obtained in specific cases [17].",
      "startOffset" : 352,
      "endOffset" : 356
    }, {
      "referenceID" : 19,
      "context" : "Another class of algorithmic procedures allowing to minimize a criterion with more that two functions, possibly including a differentiable function with a Lipschitz gradient, is the class of primal-dual proximal approaches [21], [22], [23], [24].",
      "startOffset" : 223,
      "endOffset" : 227
    }, {
      "referenceID" : 20,
      "context" : "Another class of algorithmic procedures allowing to minimize a criterion with more that two functions, possibly including a differentiable function with a Lipschitz gradient, is the class of primal-dual proximal approaches [21], [22], [23], [24].",
      "startOffset" : 229,
      "endOffset" : 233
    }, {
      "referenceID" : 21,
      "context" : "Another class of algorithmic procedures allowing to minimize a criterion with more that two functions, possibly including a differentiable function with a Lipschitz gradient, is the class of primal-dual proximal approaches [21], [22], [23], [24].",
      "startOffset" : 235,
      "endOffset" : 239
    }, {
      "referenceID" : 22,
      "context" : "Another class of algorithmic procedures allowing to minimize a criterion with more that two functions, possibly including a differentiable function with a Lipschitz gradient, is the class of primal-dual proximal approaches [21], [22], [23], [24].",
      "startOffset" : 241,
      "endOffset" : 245
    }, {
      "referenceID" : 21,
      "context" : "Several primal-dual proximal schemes have been derived but one of the most popular is based on forwardbackward iterations [23], [24] in order to estimate",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 22,
      "context" : "Several primal-dual proximal schemes have been derived but one of the most popular is based on forwardbackward iterations [23], [24] in order to estimate",
      "startOffset" : 128,
      "endOffset" : 132
    }, {
      "referenceID" : 21,
      "context" : "Additional technical assumptions can be found in [23][Theorem 3.",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 16,
      "context" : "The projection onto the positive orthant and on S have well known closed form expressions [18] that are: { PO(·) = max{0, ·}, PS(Θ) = Θ+Θ> 2 .",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 23,
      "context" : "The above result is obtained from arguments closed than the ones derived in [25] [Proposition 5] for an epigraphical constraint of the form { (ω,u) ∈ R × R | max{τ1|u|, .",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 24,
      "context" : "(22) From similar arguments than in [26][Lemma 2] , ñ is computed as (19) and thus",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 24,
      "context" : "Next we get the solution of PE1 from the ones of projection to E 1 according to [26][Lemma 3] and have the following proposition:",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 24,
      "context" : "This result can be directly derived from [26][Lemma 3] and the proof is therefore omitted here.",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 0,
      "context" : "In this section, we provide comparison results of both `1 and `∞ proposed approaches with the two closest stateof-the-art procedures, that are “hierNet” [1] and Jenatton’s framework [15].",
      "startOffset" : 153,
      "endOffset" : 156
    }, {
      "referenceID" : 13,
      "context" : "In this section, we provide comparison results of both `1 and `∞ proposed approaches with the two closest stateof-the-art procedures, that are “hierNet” [1] and Jenatton’s framework [15].",
      "startOffset" : 182,
      "endOffset" : 186
    }, {
      "referenceID" : 13,
      "context" : "2 Comparison with [15] – r =∞ and weak hierarchy",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 13,
      "context" : "In order to provide fair comparison with the work in [15], we first investigate the performance of proposed primaldual proximal algorithm (Algorithm 2) when r = ∞ and in the configuration of weak hierarchy (no symmetry constraint is considered, i.",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 13,
      "context" : "The algorithmic procedure designed in [15] is based on “FISTA” and it will be named “Weak-FISTA-`∞”.",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 0,
      "context" : "3 Comparison with [1] – r = 1 and strong hierarchy Similar experiments are conducted for r = 1 and strong hierarchy (i.",
      "startOffset" : 18,
      "endOffset" : 21
    }, {
      "referenceID" : 25,
      "context" : "This dataset3 is collected by [27] and it is used to model HIV-1 susceptibility to the drugs because HIV-1 virus can become resistant through genome mutations for different subjects.",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 0,
      "context" : "The dataset is randomly split into two half sets for training and test respectively as adopted by [1], [5].",
      "startOffset" : 98,
      "endOffset" : 101
    } ],
    "year" : 2017,
    "abstractText" : "This work focus on regression optimization problem with hierarchical interactions between variables, which is beyond the additive models in the traditional linear regression. We investigate more specifically two different fashions encountered in the literature to deal with this problem: “hierNet” and structural-sparsity regularization, and study their connections. We propose a primal-dual proximal algorithm based on epigraphical projection to optimize a general formulation of this learning problem. The experimental setting first highlights the improvement of the proposed procedure compared to state-of-the-art methods based on fast iterative shrinkage-thresholding algorithm (i.e. FISTA) or alternating direction method of multipliers (i.e. ADMM) and second we provide fair comparisons between the different hierarchical penalizations. The experiments are conducted both on the synthetic and real data, and they clearly show that the proposed primal-dual proximal algorithm based on epigraphical projection is efficient and effective to solve and investigate the question of the hierarchical interaction learning problem.",
    "creator" : "LaTeX with hyperref package"
  }
}