{
  "name" : "1605.08370.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Provable Efficient Online Matrix Completion via Non-convex Stochastic Gradient Descent",
    "authors" : [ "Chi Jin", "Sham M. Kakade", "Praneeth Netrapalli" ],
    "emails" : [ "chijin@cs.berkeley.edu", "sham@cs.washington.edu", "praneeth@microsoft.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n60 5.\n08 37\n0v 1\n[ cs\n.L G\n] 2\n6 M\nIn this paper, we propose the first provable, efficient online algorithm for matrix completion. Our algorithm starts from an initial estimate of the matrix and then performs non-convex stochastic gradient descent (SGD). After every observation, it performs a fast update involving only one row of two tall matrices, giving near linear total runtime. Our algorithm can be naturally used in the offline setting as well, where it gives competitive sample complexity and runtime to state of the art algorithms. Our proofs introduce a general framework to show that SGD updates tend to stay away from saddle surfaces and could be of broader interests for other non-convex problems to prove tight rates.\n1 Introduction\nLow rank matrix completion refers to the problem of recovering a low rank matrix by observing the values of only a tiny fraction of its entries. This problem arises in several applications such as video denoising [14], phase retrieval [3] and most famously in movie recommendation engines [16]. In the context of recommendation engines for instance, the matrix we wish to recover would be user-item rating matrix where each row corresponds to a user and each column corresponds to an item. Each entry of the matrix is the rating given by a user to an item. Low rank assumption on the matrix is inspired by the intuition that rating of an item by a user depends on only a few hidden factors, which are much fewer than the number of users or items. The goal is to estimate the ratings of all items by users given only partial ratings of items by users, which would then be helpful in recommending new items to users.\nThe seminal works of Candès and Recht [4] first identified regularity conditions under which low rank matrix completion can be solved in polynomial time using convex relaxation – low rank matrix\n∗UC Berkeley. Email: chijin@cs.berkeley.edu †University of Washington. Email: sham@cs.washington.edu ‡Microsoft Research New England. Email: praneeth@microsoft.com\ncompletion could be ill-posed and NP-hard in general without such regularity assumptions [10]. Since then, a number of works have studied various algorithms under different settings for matrix completion: weighted and noisy matrix completion, fast convex solvers, fast iterative non-convex solvers, parallel and distributed algorithms and so on.\nMost of this work however deals only with the offline setting where all the observed entries are revealed at once and the recovery procedure does computation using all these observations simultaneously. However in several applications [5, 19], we encounter the online setting where observations are only revealed sequentially and at each step the recovery algorithm is required to maintain an estimate of the low rank matrix based on the observations so far. Consider for instance recommendation engines, where the low rank matrix we are interested in is the user-item rating matrix. While we make an observation only when a user rates an item, at any point of time, we should have an estimate of the user-item rating matrix based on all prior observations so as to be able to continuously recommend items to users. Moreover, this estimate should get better as we observe more ratings.\nAlgorithms for offline matrix completion can be used to solve the online version by rerunning the algorithm after every additional observation. However, performing so much computation for every observation seems wasteful and is also impractical. For instance, using alternating minimization, which is among the fastest known algorithms for the offline problem, would mean that we take several passes of the entire data for every additional observation. This is simply not feasible in most settings. Another natural approach is to group observations into batches and do an update only once for each batch. This however induces a lag between observations and estimates which is undesirable. To the best of our knowledge, there is no known provable, efficient, online algorithm for matrix completion.\nOn the other hand, in order to deal with the online matrix completion scenario in practical applications, several heuristics (with no convergence guarantees) have been proposed in literature [2, 20]. Most of these approaches are based on starting with an estimate of the matrix and doing fast updates of this estimate whenever a new observation is presented. One of the update procedures used in this context is that of stochastic gradient descent (SGD) applied to the following non-convex optimization problem\nmin U,V\n‖M−UV⊤‖2F s.t. U ∈ Rd1×k,V ∈ Rd2×k, (1)\nwhere M is the unknown matrix of size d1 × d2, k is the rank of M and UV⊤ is a low rank factorization of M we wish to obtain. The algorithm starts with some U0 and V0, and given a new observation (M)ij , SGD updates the i\nth-row and the jth-row of the current iterates Ut and Vt respectively by\nU (i) t+1 = U (i) t − 2ηd1d2 ( UtV ⊤ t −M ) ij V (j) t , and, V (j) t+1 = V (j) t − 2ηd1d2 ( UtV ⊤ t −M ) ij U (i) t , (2)\nwhere η is an appropriately chosen stepsize, and U(i) denote the ith row of matrix U. Note that each update modifies only one row of the factor matrices U and V, and the computation only involves one row of U,V and the new observed entry (M)ij and hence are extremely fast. These fast updates make SGD extremely appealing in practice. Moreover, SGD, in the context of matrix completion, is also useful for parallelization and distributed implementation [24].\n1.1 Our Contributions\nIn this work we present the first provable efficient algorithm for online matrix completion by showing that SGD (2) with a good initialization converges to a true factorization of M at a geometric rate. Our main contributions are as follows.\n• We provide the first provable, efficient, online algorithm for matrix completion. Starting with a good initialization, after each observation, the algorithm makes quick updates each taking time O(k3) and requires O(µdkκ4(k+log ‖M‖F\nǫ ) log d) observations to reach ǫ accuracy, where\nµ is the incoherence parameter, d = max(d1, d2), k is the rank and κ is the condition number of M.\n• Moreover, our result features both sample complexity and total runtime linear in d, and is competitive to even the best existing offline results for matrix completion. (either improve over or is incomparable, i.e., better in some parameters and worse in others, to these results). See Table 1 for the comparison.\n• To obtain our results, we introduce a general framework to show SGD updates tend to stay away from saddle surfaces. In order to do so, we consider distances from saddle surfaces, show that they behave like sub-martingales under SGD updates and use martingale convergence techniques to conclude that the iterates stay away from saddle surfaces. While [25] shows that SGD updates stay away from saddle surfaces, the stepsizes they can handle are quite small (scaling as 1/poly(d1, d2)), leading to suboptimal computational complexity. Our framework makes it possible to establish the same statement for much larger step sizes, giving us nearoptimal runtime. We believe these techniques may be applicable in other non-convex settings as well.\n1.2 Related Work\nIn this section we will mention some more related work. Offline matrix completion: There has been a lot of work on designing offline algorithms for matrix completion, we provide the detailed comparison with our algorithm in Table 1. The nuclear norm relaxation algorithm [23] has near-optimal sample complexity for this problem but is computationally expensive. Motivated by the empirical success of non-convex heuristics, a long line of works, [15, 9, 13, 25] and so on, has obtained convergence guarantees for alternating minimization, gradient descent, projected gradient descent etc. Even the best of these are suboptimal in sample complexity by poly(k, κ) factors. Our sample complexity is better than that of [15] and is incomparable to those of [9, 13]. To the best of our knowledge, the only provable online algorithm for this problem is that of Sun and Luo [25]. However the stepsizes they suggest are quite small, leading to suboptimal computational complexity by factors of poly(d1, d2). The runtime of our algorithm is linear in d, which makes poly(d) improvements over it.\nOther models for online matrix completion: Another variant of online matrix completion studied in the literature is where observations are made on a column by column basis e.g., [17, 27]. These models can give improved offline performance in terms of space and could potentially work under relaxed regularity conditions. However, they do not tackle the version where only entries (as opposed to columns) are observed.\nNon-convex optimization: Over the last few years, there has also been a significant amount of work in designing other efficient algorithms for solving non-convex problems. Examples include\neigenvector computation [6, 12], sparse coding [21, 1] etc. For general non-convex optimization, an interesting line of recent work is that of [7], which proves gradient descent with noise can also escape saddle point, but they only provide polynomial rate without explicit dependence. Later [18, 22] show that without noise, the space of points from where gradient descent converges to a saddle point is a measure zero set. However, they do not provide a rate of convergence. Another related piece of work to ours is [11], proves global convergence along with rates of convergence, for the special case of computing matrix squareroot. During the preparation of this draft, the recent work [8] was announced which proves the global convergence of SGD for matrix completion and can also be applied to the online setting. However, their result only deals with the case where M is positive semidefinite (PSD) and their rate is still suboptimal by factors of poly(d1, d2).\n1.3 Outline\nThe rest of the paper is organized as follows. In Section 2 we formally describe the problem and all relevant parameters. In Section 3, we present our algorithms, results and some of the key intuition behind our results. In Section 4 we give proof outline for our main results. We conclude in Section 5. All formal proofs are deferred to the Appendix.\n2 Preliminaries\nIn this section, we introduce our notation, formally define the matrix completion problem and regularity assumptions that make the problem tractable.\n1This result only applies to the case where M is symmetric PSD\n2.1 Notation\nWe use [d] to denote {1, 2, · · · , d}. We use bold capital letters A,B to denote matrices and bold lowercase letters u,v to denote vectors. Aij means the (i, j)\nth entry of matrix A. ‖w‖ denotes the ℓ2-norm of vector w and ‖A‖/‖A‖F/‖A‖∞ denotes the spectral/Frobenius/infinity norm of matrix A. σi(A) denotes the i\nth largest singular value of A and σmin(A) denotes the smallest singular value of A. We also let κ(A) = ‖A‖ /σmin(A) denote the condition number of A (i.e., the ratio of largest to smallest singular value). Finally, for orthonormal bases of a subspace W, we also use PW = WW⊤ to denote the projection to the subspace spanned by W.\n2.2 Problem statement and assumptions\nConsider a general rank k matrix M ∈ Rd1×d2 . Let Ω ⊂ [d1]× [d2] be a subset of coordinates, which are sampled uniformly and independently from [d1]× [d2]. We denote PΩ(M) to be the projection of M on set Ω so that:\n[PΩ(M)]ij = {\nMij , if (i, j) ∈ Ω 0, if (i, j) 6∈ Ω\nLow rank matrix completion is the task of recovering M by only observing PΩ(M). This task is ill-posed and NP-hard in general [10]. In order to make this tractable, we make by now standard assumptions about the structure of M.\nDefinition 2.1. Let W ∈ Rd×k be an orthonormal basis of a subspace of Rd of dimension k. The coherence of W is defined to be\nµ(W) def =\nd k max 1≤i≤d ‖PWei‖2 = d k max 1≤i≤d\n∥∥∥e⊤i W ∥∥∥ 2\nAssumption 2.2 (µ-incoherence[4, 23]). We assume M is µ-incoherent, i.e., max{µ(X), µ(Y)} ≤ µ, where X ∈ Rd1×k,Y ∈ Rd2×k are the left and right singular vectors of M.\n3 Main Results\nIn this section, we present our main result. We will first state result for a special case where M is a symmetric positive semi-definite (PSD) matrix, where the algorithm and analysis are much simpler. We will then discuss the general case.\n3.1 Symmetric PSD Case\nConsider the special case where M is symmetric PSD. We let d def = d1 = d2, and we can parametrize a rank k symmetric PSD matrix by UU⊤ where U ∈ Rd×k. Our algorithm for this case is given in Algorithm 1. The following theorem provides guarantees on the performance of Algorithm 1. The algorithm starts by using an initial set of samples Ωinit to construct a crude approximation to the low rank of factorization of M. It then observes samples from M one at a time and updates its factorization after every observation. Note that each update step modifies two rows of Ut and hence takes time O(k).\nAlgorithm 1 Online Algorithm for PSD Matrix Completion. Input: Initial set of uniformly random samples Ωinit of a symmetric PSD matrix M ∈ Rd×d, learning rate η, iterations T Output: U such that UU⊤ ≈ M U0U ⊤ 0 ← top k SVD of d 2\n|Ωinit|PΩinit(M) for t = 0, · · · , T − 1 do\nObserve Mij where (i, j) ∼ Unif ([d]× [d]) Ut+1 ← Ut − 2ηd2(UtU⊤t −M)ij(eie⊤j + eje⊤i )Ut\nend for\nReturn UT\nTheorem 3.1. Let M ∈ Rd×d be a rank k, symmetric PSD matrix with µ-incoherence. There exist some absolute constants c0 and c such that if |Ωinit| ≥ c0µdk2κ2(M) log d, learning rate η ≤ c µdkκ3(M)‖M‖ log d , then for any fixed T ≥ 1, with probability at least 1 − T d10 , we will have for all t ≤ T that: ‖UtU⊤t −M‖2F ≤ ( 1− 1\n2 η · σmin(M) )t( 1 10 σmin(M) )2 .\nRemarks:\n• The algorithm uses an initial set of observations Ωinit to produce a warm start iterate U0, then enters the online stage, where it performs SGD.\n• The sample complexity of the warm start phase is O(µdk2κ2(M) log d). The initialization consists of a top-k SVD on a sparse matrix, whose runtime is O(µdk3κ2(M) log d).\n• For the online phase (SGD), if we choose η = c µdkκ3(M)‖M‖ log d , the number of observations T\nrequired for the error ‖UTU⊤T −M‖F to be smaller than ǫ is O(µdkκ(M)4 log d log σmin(M) ǫ ).\n• Since each SGD step modifies two rows of Ut, its runtime is O(k) with a total runtime for online phase of O(kT ).\nOur proof approach is to essentially show that the objective function is well-behaved (i.e., is smooth and strongly convex) in a local neighborhood of the warm start region, and then use standard techniques to show that SGD obtains geometric convergence in this setting. The most challenging and novel part of our analysis comprises of showing that the iterate does not leave this local neighborhood while performing SGD updates. Refer Section 4 for more details on the proof outline.\n3.2 General Case\nLet us now consider the general case where M ∈ Rd1×d2 can be factorized as UV⊤ with U ∈ Rd1×k and V ∈ Rd2×k. In this scenario, we denote d = max{d1, d2}. We recall our remarks from the previous section that our analysis of the performance of SGD depends on the smoothness and strong convexity properties of the objective function in a local neighborhood of the iterates. Having U 6= V introduces additional challenges in this approach since for any nonsingular k-by-k matrix C, and U′ def = UC⊤,V′ def = VC−1, we have U′V′⊤ = UV⊤. Suppose for instance C is a very small scalar times the identity i.e., C = δI for some small δ > 0. In this case, U′ will be large while V′\nwill be small. This drastically deteriorates the smoothness and strong convexity properties of the objective function in a neighborhood of (U′,V′).\nAlgorithm 2 Online Algorithm for Matrix Completion (Theoretical) Input: Initial set of uniformly random samples Ωinit of M ∈ Rd1×d2 , learning rate η, iterations T Output: U,V such that UV⊤ ≈ M\nU0V ⊤ 0 ← top k SVD of d1d2|Ωinit|PΩinit(M) for t = 0, · · · , T − 1 do WUDW ⊤ V ← SVD(UtV⊤t )\nŨt ← WUD 1 2 , Ṽt ← WV D 1 2 Observe Mij where (i, j) ∼ Unif ([d]× [d]) Ut+1 ← Ũt − 2ηd1d2(ŨtṼ⊤t −M)ijeie⊤j Ṽt Vt+1 ← Ṽt − 2ηd1d2(ŨtṼ⊤t −M)ijeje⊤i Ũt\nend for Return UT ,VT .\nTo preclude such a scenario, we would ideally like to renormalize after each step by doing Ũt ← WUD 1 2 , Ṽt ← WV D 1 2 , where WUDW ⊤ V is the SVD of matrix UtV ⊤ t . This algorithm is described in Algorithm 2. However, a naive implementation of Algorithm 2, especially the SVD step, would incur O(min{d1, d2}) computation per iteration, resulting in a runtime overhead of O(d) over both the online PSD case (i.e., Algorithm 1) as well as the near linear time offline algorithms (see Table 1). It turns out that we can take advantage of the fact that in each iteration we only update a single row of Ut and a single row of Vt, and do efficient (but more complicated) update steps instead of doing an SVD on d1 × d2 matrix. The resulting algorithm is given in Algorithm 3. The key idea is that in order to implement the updates, it suffices to do an SVD of U⊤t Ut and V⊤t Vt which are k × k matrices. So the runtime of each iteration is at most O(k3). The following lemma shows the equivalence between Algorithms 2 and 3.\nAlgorithm 3 Online Algorithm for Matrix Completion (Practical) Input: Initial set of uniformly random samples Ωinit of M ∈ Rd1×d2 , learning rate η, iterations T Output: U,V such that UV⊤ ≈ M\nU0V ⊤ 0 ← top k SVD of d1d2ΩinitPΩinit(M) for t = 0, · · · , T − 1 do RUDUR ⊤ U ← SVD(U⊤t Ut)\nRV DV R ⊤ V ← SVD(V⊤t Vt) QUDQ ⊤ V ← SVD(D 1 2 UR ⊤ URV (D 1 2 V ) ⊤) Observe Mij where (i, j) ∼ Unif ([d]× [d]) Ut+1 ← Ut − 2ηd1d2(UtV⊤t −M)ijeie⊤j VtRV D − 1 2 V QVQ ⊤ UD 1 2 UR ⊤ U Vt+1 ← Vt − 2ηd1d2(UtV⊤t −M)ijeje⊤i UtRUD − 1 2 U QUQ ⊤ V D 1 2 V R ⊤ V\nend for Return UT ,VT .\nLemma 3.2. Algorithm 2 and Algorithm 3 are equivalent in the sense that: given same observations\nfrom M and other inputs, the outputs of Algorithm 2, U,V and those of Algorithm 3, U′,V′ satisfy UV⊤ = U′V′⊤.\nSince the output of both algorithms is the same, we can analyze Algorithm 2 (which is easier than that of Algorithm 3), while implementing Algorithm 3 in practice. The following theorem is the main result of our paper which presents guarantees on the performance of Algorithm 2.\nTheorem 3.3. Let M ∈ Rd1×d2 be a rank k matrix with µ-incoherence and let d def= max(d1, d2). There exist some absolute constants c0 and c such that if |Ωinit| ≥ c0µdk2κ2(M) log d, learning rate η ≤ c\nµdkκ3(M)‖M‖ log d , then for any fixed T ≥ 1, with probability at least 1− T d10 , we will have for all\nt ≤ T that: ‖UtV⊤t −M‖2F ≤ ( 1− 1\n2 η · σmin(M) )t( 1 10 σmin(M) )2 .\nRemarks:\n• Just as in the case of PSD matrix completion (Theorem 3.1), Algorithm 2 needs a an initial set of observations Ωinit to provide a warm start U0 and V0 after which it performs SGD.\n• The sample complexity and runtime of the warm start phase are the same as in symmetric PSD case. The stepsize η and the number of observations T to achieve ǫ error in online phase (SGD) are also the same as in symmetric PSD case.\n• However, runtime of each update step in online phase is O(k3) with total runtime for online phase O(k3T ).\nThe proof of this theorem again follows a similar line of reasoning as that of Theorem 3.1 by first showing that the local neighborhood of warm start iterate has good smoothness and strong convexity properties and then use them to show geometric convergence of SGD. Proof of the fact that iterates do not move away from this local neighborhood however is significantly more challenging due to renormalization steps in the algorithm. Please see Appendix C for the full proof.\n4 Proof Sketch\nIn this section we will provide the intuition and proof sketch for our main results. For simplicity and highlighting the most essential ideas, we will mostly focus on the symmetric PSD case (Theorem 3.1). For the asymmetric case, though the high-level ideas are still valid, a lot of additional effort is required to address the renormalization step in Algorithm 2. This makes the proof more involved.\nFirst, note that our algorithm for the PSD case consists of an initialization and then stochastic descent steps. The following lemma provides guarantees on the error achieved by the initial iterate U0.\nLemma 4.1. Let M ∈ Rd×d be a rank-k PSD matrix with µ-incoherence. There exists a constant c0 such that if |Ωinit| ≥ c0µdk2κ2(M) log d, then with probability at least 1− 1d10 , the top-k SVD of d2 |init|PΩinit(M) satisfies Then there exists universal constant c0, for any m ≥, we have:\n‖M−U0U⊤0 ‖F ≤ 1\n20 σmin(M) and max j\n∥∥∥e⊤j U0 ∥∥∥ 2 ≤ 10µkκ(M)\nd ‖M‖ (3)\nBy Lemma 4.1, we know the initialization algorithm already gives U0 in the local region given by Eq.(3). Intuitively, stochastic descent steps should keep doing local search within this local region.\nTo establish linear convergence on ‖UtU⊤t −M‖2F and obtain final result, we first establish several important lemmas describing the properties of this local regions. Throughout this section, we always denote SVD(M) = XSX⊤, whereX ∈ Rd×k, and diagnal matrix S ∈ Rk×k. We postpone all the formal proofs in Appendix.\nLemma 4.2. For function f(U) = ‖M−UU⊤‖2F and any U1,U2 ∈ {U| ‖U‖ ≤ Γ}, we have:\n‖∇f(U1)−∇f(U2)‖F ≤ 16max{Γ2, ‖M‖} · ‖U1 −U2‖F\nLemma 4.3. For function f(U) = ‖M−UU⊤‖2F and any U ∈ {U|σmin(X⊤U) ≥ γ}, we have:\n‖∇f(U)‖2F ≥ 4γ2f(U)\nLemma 4.2 tells function f is smooth if spectral norm of U is not very large. On the other hand, σmin(X\n⊤U) not too small requires both σmin(U⊤U) and σmin(X⊤W) are not too small, where W is top-k eigenspace of UU⊤. That is, Lemma 4.3 tells function f has a property similar to strongly convex in standard optimization literature, if U is rank k in a robust sense (σk(U) is not too small), and the angle between the top k eigenspace of UU⊤ and the top k eigenspace M is not large. Lemma 4.4. Within the region D = {U| ∥∥M−UU⊤\n∥∥ F ≤ 110σk(M)}, we have:\n‖U‖ ≤ √ 2 ‖M‖, σmin(X⊤U) ≥ √ σk(M)/2\nLemma 4.4 tells inside region {U| ∥∥M−UU⊤ ∥∥ F\n≤ 110σk(M)}, matrix U always has a good spectral property which gives preconditions for both Lemma 4.2 and 4.3, where f(U) is both smooth and has a property very similar to strongly convex.\nWith above three lemmas, we already been able to see the intuition behind linear convergence in Theorem 3.1. Denote stochastic gradient\nSG(U) = 2d2(UU⊤ −M)ij(eie⊤j + eje⊤i )U (4)\nwhere SG(U) is a random matrix depends on the randomness of sample (i, j) of matrix M. Then, the stochastic update step in Algorithm 1 can be rewritten as:\nUt+1 ← Ut − ηSG(Ut)\nLet f(U) = ‖M−UU⊤‖2F, By easy caculation, we know ESG(U) = ∇f(U), that is SG(U) is unbiased. Combine Lemma 4.4 with Lemma 4.2 and Lemma 4.3, we know within region D specified by Lemma 4.4, we have function f(U) is 32 ‖M‖-smooth, and ‖∇f(U)‖2F ≥ 2σmin(M)f(U).\nLet’s suppose ideally, we always have U0, . . . ,Ut inside region D, this directly gives:\nEf(Ut+1) ≤ Ef(Ut)− ηE〈∇f(Ut), SG(Ut)〉+ 16η2 ‖M‖ · E‖SG(Ut)‖2F = Ef(Ut)− ηE‖∇f(Ut)‖2F + 16η2 ‖M‖ · E‖SG(Ut)‖2F ≤ (1− 2ησmin(M))Ef(Ut) + 16η2 ‖M‖ · E‖SG(Ut)‖2F\nOne interesting aspect of our main result is that we actually show linear convergence under the presence of noise in gradient. This is true because for the second-order (η2) term above, we can roughly see from Eq.(4) that ‖SG(U)‖2F ≤ h(U) · f(U), where h(U) is a factor depends on U and always bounded. That is, SG(U) enjoys self-bounded property — ‖SG(U)‖2F will goes to zero, as objective function f(U) goes to zero. Therefore, by choosing learning rate η appropriately small, we can have the first-order term always dominate the second-order term, which establish the linear convergence.\nNow, the only remaining issue is to prove that “U0, . . . ,Ut always stay inside local region D”. In reality, we can only prove this statement with high probability due to the stochastic nature of the update. This is also the most challenging part in our proof, which makes our analysis different from standard convex analysis, and uniquely required due to non-convex setting.\nOur key theorem is presented as follows:\nTheorem 4.5. Let f(U) = ∥∥UU⊤ −M ∥∥2 F and gi(U) = ∥∥e⊤i U ∥∥2. Suppose initial U0 satisfying:\nf(U0) ≤ ( σmin(M)\n20\n)2 , max\ni gi(U0) ≤\n10µkκ(M)2\nd ‖M‖\nThen, there exist some absolute constant c such that for any learning rate η < c µdkκ3(M)‖M‖ log d , with at least 1− T d10 probability, we will have for all t ≤ T that:\nf(Ut) ≤ (1− 1\n2 ησmin(M))\nt\n( σmin(M)\n10\n)2 , max\ni gi(Ut) ≤\n20µkκ(M)2\nd ‖M‖ (5)\nNote function maxi gi(U) indicates the incoherence of matrix U. Theorem 4.5 guarantees if inital U0 is in the local region which is incoherent and U0U ⊤ 0 is close to M, then with high probability for all steps t ≤ T , Ut, Ut will always stay in a slightly relaxed local region, and f(Ut) has linear convergence.\nIt is not hard to show that all saddle point of f(U) satisfies σk(U) = 0, and all local minima are global minima. Since U0, . . . ,Ut automatically stay in region f(U) ≤ (σmin(M)10 )2 with high probability, we know Ut also stay away from all saddle points. The claim that U0, . . . ,Ut stays incoherent is essential to better control the variance and probability 1 bound of SG(Ut), so that we can have large step size and tight convergence rate.\nThe major challenging in proving Theorem 4.5 is to both prove Ut stays in the local region, and achieve good sample complexity and running time (linear in d) in the same time. This also requires the learning rate η in Algorithm 1 to be relatively large. Let the event Et denote the good event where U0, . . . ,Ut satisfies Eq.(5). Theorem 4.5 is claiming that P (ET ) is large. The essential steps in the proof is contructing two supermartingles related to f(Ut)1Et and gi(Ut)1Et (where 1(·) denote indicator function), and use Bernstein inequalty to show the concentration of supermartingales. The 1Etterm allow us the claim all previousU0, . . . ,Ut have all desired properties inside local region.\nFinally, we see Theorem 3.1 as a immediate corollary of Theorem 4.5.\n5 Conclusion\nIn this paper, we presented the first provable, efficient online algorithm for matrix completion, based on nonconvex SGD. In addition to the online setting, our results are also competitive with state of\nthe art results in the offline setting. We obtain our results by introducing a general framework that helps us show how SGD updates self-regulate to stay away from saddle points. We hope our paper and results help generate interest in online matrix completion, and our techniques and framework prompt tighter analysis for other nonconvex problems.\nReferences\n[1] Sanjeev Arora, Rong Ge, Tengyu Ma, and Ankur Moitra. Simple, efficient, and neural algorithms for sparse coding. arXiv preprint arXiv:1503.00778, 2015.\n[2] Matthew Brand. Fast online svd revisions for lightweight recommender systems. In SDM, pages 37–46. SIAM, 2003.\n[3] Emmanuel J Candes, Yonina C Eldar, Thomas Strohmer, and Vladislav Voroninski. Phase retrieval via matrix completion. SIAM Review, 57(2):225–251, 2015.\n[4] Emmanuel J. Candès and Benjamin Recht. Exact matrix completion via convex optimization. Foundations of Computational Mathematics, 9(6):717–772, December 2009.\n[5] James Davidson, Benjamin Liebald, Junning Liu, Palash Nandy, Taylor Van Vleet, Ullas Gargi, Sujoy Gupta, Yu He, Mike Lambert, Blake Livingston, et al. The youtube video recommendation system. In Proceedings of the fourth ACM conference on Recommender systems, pages 293–296. ACM, 2010.\n[6] Christopher De Sa, Kunle Olukotun, and Christopher Ré. Global convergence of stochastic gradient descent for some non-convex matrix problems. arXiv preprint arXiv:1411.1134, 2014.\n[7] Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points—online stochastic gradient for tensor decomposition. arXiv preprint arXiv:1503.02101, 2015.\n[8] Rong Ge, Jason D. Lee, and Tengyu Ma. Matrix completion has no spurious local minimum. arXiv preprint arXiv:1605.07272, 2016.\n[9] Marcus Hardt. Understanding alternating minimization for matrix completion. In Foundations of Computer Science (FOCS), 2014 IEEE 55th Annual Symposium on, pages 651–660. IEEE, 2014.\n[10] Moritz Hardt, Raghu Meka, Prasad Raghavendra, and Benjamin Weitz. Computational limits for matrix completion. In COLT, pages 703–725, 2014.\n[11] Prateek Jain, Chi Jin, Sham M Kakade, and Praneeth Netrapalli. Computing matrix squareroot via non convex local search. arXiv preprint arXiv:1507.05854, 2015.\n[12] Prateek Jain, Chi Jin, Sham M Kakade, Praneeth Netrapalli, and Aaron Sidford. Matching matrix bernstein with little memory: Near-optimal finite sample guarantees for oja’s algorithm. arXiv preprint arXiv:1602.06929, 2016.\n[13] Prateek Jain and Praneeth Netrapalli. Fast exact matrix completion with finite samples. arXiv preprint arXiv:1411.1087, 2014.\n[14] Hui Ji, Chaoqiang Liu, Zuowei Shen, and Yuhong Xu. Robust video denoising using low rank matrix completion. 2010.\n[15] Raghunandan Hulikal Keshavan. Efficient algorithms for collaborative filtering. PhD thesis, STANFORD UNIVERSITY, 2012.\n[16] Yehuda Koren. The BellKor solution to the Netflix grand prize, 2009.\n[17] Akshay Krishnamurthy and Aarti Singh. Low-rank matrix and tensor completion via adaptive sampling. In Advances in Neural Information Processing Systems, pages 836–844, 2013.\n[18] Jason D Lee, Max Simchowitz, Michael I Jordan, and Benjamin Recht. Gradient descent converges to minimizers. University of California, Berkeley, 1050:16, 2016.\n[19] G. Linden, B. Smith, and J. York. Amazon.com recommendations: item-to-item collaborative filtering. IEEE Internet Computing, 7(1):76–80, Jan 2003.\n[20] Xin Luo, Yunni Xia, and Qingsheng Zhu. Incremental collaborative filtering recommender based on regularized matrix factorization. Knowledge-Based Systems, 27:271–280, 2012.\n[21] Julien Mairal, Francis Bach, Jean Ponce, and Guillermo Sapiro. Online learning for matrix factorization and sparse coding. The Journal of Machine Learning Research, 11:19–60, 2010.\n[22] Ioannis Panageas and Georgios Piliouras. Gradient descent converges to minimizers: The case of non-isolated critical points. arXiv preprint arXiv:1605.00405, 2016.\n[23] Benjamin Recht. A simple approach to matrix completion, 2009.\n[24] Benjamin Recht and Christopher Ré. Parallel stochastic gradient algorithms for large-scale matrix completion. Mathematical Programming Computation, 5(2):201–226, 2013.\n[25] Ruoyu Sun and Zhi-Quan Luo. Guaranteed matrix completion via nonconvex factorization. In Foundations of Computer Science (FOCS), 2015 IEEE 56th Annual Symposium on, pages 270–289. IEEE, 2015.\n[26] Joel A Tropp. User-friendly tail bounds for sums of random matrices. Foundations of computational mathematics, 12(4):389–434, 2012.\n[27] Se-Young Yun, Marc Lelarge, and Alexandre Proutiere. Streaming, memory limited matrix completion with noise. arXiv preprint arXiv:1504.03156, 2015.\nA Proof of Initialization\nIn this section, we will prove Lemma 4.1 and a corresponding lemma for asymmetric case as follows (which will be used to prove Theorem 3.3):\nLemma A.1. Assume M ∈ Rd1×d2 is a rank k matrix with µ-incoherence, and Ω is a subset unformly i.i.d sampled from all coordinate. Let U0V ⊤ 0 be the top-k SVD of d1d2 m\nPΩ(M), where |Ω| = m. Let d = max{d1, d2}. Then there exists universal constant c0, for any m ≥ c0µdk2κ2(M) log d, with probability at least 1− 1\nd10 , we have:\n‖M−U0V⊤0 ‖F ≤ 1\n20 σmin(M),\nmax i\n∥∥∥e⊤i U0V⊤0 ∥∥∥ 2 ≤ 10µk\nd1 ‖M‖ , max j\n∥∥∥e⊤j V0U⊤0 ∥∥∥ 2 ≤ 10µk\nd2 ‖M‖ (6)\nWe will focus mostly on Lemma A.1, and prove Lemma 4.1 as a special case. Most of the argument of this section follows from [15]. We include here for completeness. The remaining of this section can be viewed as proving both the Frobenius norm claim and incoherence claim of Lemma A.1 seperately.\nIn this section, We always denote d = max{d1, d2}. For simplicity, WLOG, we also assume ‖M‖ = 1 in all proof. Also, when it’s clear from the context, we use κ to specifically to represent κ(M). Then σmin(M) = 1 κ . Also in the proof, we always denote SVD(M) = XSY⊤, and SVD(UV⊤) = WUDW⊤V, where S and D are k × k diagonal matrix.\nA.1 Frobenius Norm of Initialization\nTheorem A.2 (Matrix Bernstein [26]). A finite sequence {Xt} of independent, random matrices with dimension d! × d2. Assume that each matrix satisfies:\nEXt = 0, and ‖Xt‖ ≤ R almost surely\nDefine\nσ2 = max{ ∥∥∥∥∥ ∑\nt\nE(XtX ⊤ t ) ∥∥∥∥∥ , ∥∥∥∥∥ ∑\nt\nE(X⊤t Xt) ∥∥∥∥∥}\nThen, for all s ≥ 0,\nPr( ∥∥∥∥∥ ∑\nt\nXt ∥∥∥∥∥ ≥ s) ≤ (d1 + d2) · exp( −s2/2 σ2 +Rs/3 )\nLemma A.3. Let |Ω| = m, then there exists universal constant C, c0, for any m ≥ c0µdk log d, with probability at least 1− 1\nd10 , we have:\n∥∥∥∥M− d1d2 m PΩ(M) ∥∥∥∥ ≤ C √ µdk log d m\nProof. We know ∥∥∥∥M− d1d2 m PΩ(M) ∥∥∥∥ = d1d2 m ∥∥∥∥PΩ(M)− m d1d2 M ∥∥∥∥\nand note: PΩ(M)− m\nd1d2 M =\n∑\nij\nMij(Zij − m\nd1d2 )eie\n⊤ j\nwhere Zij are independence Bernoulli(m/d1d2) random variables. Let matrix\nψij = Mij(Zij − m\nd1d2 )eie\n⊤ j\nBy construction, we have: ∥∥∥∥∥∥ ∑\nij\nψij ∥∥∥∥∥∥ = ∥∥∥∥PΩ(M)− m d1d2 M ∥∥∥∥\nClearly Eψij = 0. Let XSY ⊤ = SVD(M), then by µ-incoherence of M, with probability 1:\n‖M‖∞ ≤ max ij |e⊤i XSY⊤ej| ≤ ‖M‖ µk√ d1d2\nAlso: ∥∥∥∥∥∥ ∑\nij\nE(ψijψ ⊤ ij) ∥∥∥∥∥∥ = ∥∥∥∥∥∥ ∑\nij\nEM2ij(Zij − m\nd1d2 )2eie ⊤ i ∥∥∥∥∥∥ ≤ m d1d2 (1− m d1d2 ) ∥∥∥∥∥∥ ∑\nij\nM2ijeie ⊤ i ∥∥∥∥∥∥\n= m d1d2 (1− m d1d2 )max i\n∑\nj\nM2ij ≤ 2m\nd1d2\nµk d1 ‖M‖2 = 2mµk d21d2 ‖M‖2\n∥∥∥∥∥∥ ∑\nij\nE(ψ⊤ijψij) ∥∥∥∥∥∥ = ∥∥∥∥∥∥ ∑\nij\nEM2ij(Zij − m\nd1d2 )2eje ⊤ j ∥∥∥∥∥∥ ≤ m d1d2 (1− m d1d2 ) ∥∥∥∥∥∥ ∑\nij\nM2ijeje ⊤ j ∥∥∥∥∥∥\n= m d1d2 (1− m d1d2 )max j\n∑\ni\nM2ij ≤ 2m\nd1d2\nµk d2 ‖M‖2 = 2mµk d1d 2 2 ‖M‖2\nThen, by matrix Bernstein (Theorem A.2), we have:\nPr( ∥∥∥∥∥∥ ∑\nij\nψij ∥∥∥∥∥∥ ≥ s) ≤ 2(d1 + d2) · exp( −s2/2 2mµdk d2 1 d2 2 ‖M‖2 + ‖M‖ µk 3 √ d1d2 s )\nThat is, with probability at least 1− 1 d10 , for some universal constant C, we have:\n∥∥∥∥PΩ(M)− m\nd1d2 M\n∥∥∥∥ ≤ C ‖M‖ ·max{ √ mµdk log d\nd21d 2 2\n, µk log d√\nd1d2 }\nFor m ≥ µdk log d, we finishes the proof.\nTheorem A.4. Let U0V ⊤ 0 be the top-k SVD of d1d2 m PΩ(M), where |Ω| = m then there exists universal constant c0, for any m ≥ c0µdk2κ2 log d, with probability at least 1− 1d10 , we have:\n∥∥∥M−U0V⊤0 ∥∥∥ F ≤ 1 20κ\nProof. Since M is a rank k matrix, we know σk+1(M) = 0, thus\nσk+1( d1d2 m PΩ(M)) ≤ σk+1(M) + ∥∥∥∥ d1d2 m PΩ(M)−M ∥∥∥∥ = ∥∥∥∥ d1d2 m PΩ(M)−M ∥∥∥∥\nTherefore:\n∥∥∥M−U0V⊤0 ∥∥∥ ≤ ∥∥∥∥M− d1d2 m PΩ(M) ∥∥∥∥+ ∥∥∥∥ d1d2 m PΩ(M)−U0V⊤0 ∥∥∥∥\n≤ ∥∥∥∥M− d1d2 m PΩ(M) ∥∥∥∥+ σk+1( d1d2 m PΩ(M)) ≤ 2 ∥∥∥∥M− d1d2 m PΩ(M) ∥∥∥∥\nMeanwhile, since rank(M) = k, rank(U0V ⊤ 0 ) = k, we know: rank(M−U0V⊤0 ) ≤ 2k, and therefore:\n∥∥∥M−U0V⊤0 ∥∥∥ F ≤ √ 2k ∥∥∥M−U0V⊤0 ∥∥∥ ≤ 2 √ 2k ∥∥∥∥M− d1d2 m PΩ(M) ∥∥∥∥\nby choosing m ≥ c0µdk2 log d · κ2 for large enough constant c0 and apply Lemma A.3, we finishes the proof.\nA.2 Incoherence of Initialization\nLemma A.5. Let UV⊤ be the top-k SVD of d1d2 m PΩ(M), where |Ω| = m. then there exists universal constant c0, for any m ≥ c0µdkκ2 log d, with probability at least 1− 1d10 , we have:\nmax j\n∥∥∥e⊤j (M⊤ −VU⊤) ∥∥∥ ≤ 2\n√ µk\nd2\nProof. Suppose SVD(M) = XSY⊤. Denote X̃ = XS 1 2 and Ỹ = YS 1 2 . Also let SVD(UV⊤) = WUDW ⊤ V .\nThen, we have:\n∥∥∥e⊤j (M⊤ −VU⊤) ∥∥∥ = ∥∥∥∥e ⊤ j (M ⊤ − d1d2 m PΩ(M)⊤WUW⊤U) ∥∥∥∥\n= ∥∥∥∥e ⊤ j (M ⊤ −M⊤WUW⊤U +M⊤WUW⊤U − d1d2 m PΩ(M)⊤WUW⊤U) ∥∥∥∥ ≤ ∥∥∥e⊤j M⊤(I−WUW⊤U) ∥∥∥+ ∥∥∥∥e ⊤ j (M ⊤ − d1d2 m PΩ(M)⊤)WUW⊤U ∥∥∥∥\nFor the first term, since W⊤ U WU,⊥ = 0, we have:\n∥∥∥e⊤j M⊤(I−WUW⊤U) ∥∥∥ ≤ ∥∥∥e⊤j Y ∥∥∥ ∥∥∥SX⊤WU,⊥W⊤U,⊥ ∥∥∥\n=\n√ µk\nd2\n∥∥∥Y⊤(M⊤ −WVDW⊤U)WU,⊥W⊤U,⊥ ∥∥∥\n≤ √ µk\nd2\n∥∥∥M⊤ −WVDW⊤U ∥∥∥ ≤\n√ µk\nd2 · 1 κ\nThe last step is due to sample m ≥ µdkκ2 log d, and theorem A.4. For the second term, we have:\n∥∥∥∥e ⊤ j ( d1d2 m PΩ(M)⊤ −M⊤)WUW⊤U ∥∥∥∥ = ∥∥∥∥∥∥ Ỹ⊤j ( d1d2 m ∑ i:(i,j)∈Ω x̃iw ⊤ U,i − ∑ i x̃iw ⊤ U,i)W ⊤ U ∥∥∥∥∥∥\n≤ √ µk\nd2 · d1d2 m · ∥∥∥∥∥∥ ∑ i:(i,j)∈Ω x̃iw ⊤ U,i − m d1d2 ∑ i x̃iw ⊤ U,i ∥∥∥∥∥∥ (7)\nWhere x̃i and wU,i are the i-th row of X̃ and WU respectively. Let φij = x̃iw\n⊤ U,i(Zij− md1d2 ), where Zij is Bernoulli( m d1d2 ) random variable, Zij = 1 iff (i, j) ∈ Ω. Clearly, we have Eφ = 0, and with probability 1:\n‖φij‖ ≤ 2 ‖x̃i‖ ‖wU,i‖ ≤ 2 √ µk\nd1 max i\n∥∥∥e⊤i WU ∥∥∥\nAlso, we have variance term: ∥∥∥∥∥ ∑\ni\nEφ⊤ijφij ∥∥∥∥∥ = ∥∥∥∥∥ ∑\ni\nE(Zij − m d1d2 )2 ‖x̃i‖2wU,iw⊤U,i ∥∥∥∥∥\n≤ m d1d2 (1− m d1d2 )max i ‖x̃i‖2 ∥∥∥∥∥ ∑\ni\nwU,iw ⊤ U,i ∥∥∥∥∥\n≤ m d1d2 µk d1\n∥∥∥W⊤UWU ∥∥∥ ≤ µkm\nd21d2∥∥∥∥∥ ∑\ni\nEφijφ ⊤ ij ∥∥∥∥∥ = ∥∥∥∥∥ ∑\ni\nE(Zij − m d1d2 )2 ‖wU,i‖2 x̃ix̃⊤i ∥∥∥∥∥\n≤ m d1d2 max i\n∥∥∥e⊤i WU ∥∥∥ 2\nTherefore, with m ≥ µdkκ2 log d, by matrix Bernstein, we have with probability at least 1− 1 d10\n, we know that for all j ∈ [d2], there exists some absolute constant C ′ so that:\n∥∥∥∥∥∥ ∑ i:(i,j)∈Ω x̃iw ⊤ U,i − m d1d2 ∑ i x̃iw ⊤ U,i ∥∥∥∥∥∥ ≤ C ′ √ m log d d1d2 ( √ µk d1 +max i ∥∥∥e⊤i WU ∥∥∥)\nSubstitue into Eq.(7), this gives:\n∥∥∥∥e ⊤ j ( d1d2 m PΩ(M)⊤ −M⊤)WUW⊤U ∥∥∥∥ ≤ C ′ √ µkd1 log d m ( √ µk d1 +max i ∥∥∥e⊤i WU ∥∥∥)\nOn the other hand, we also have: ∥∥∥e⊤i WU ∥∥∥ ≤ ∥∥∥e⊤i WUS ∥∥∥ ∥∥S−1 ∥∥ = 2κ ∥∥∥e⊤i UV⊤ ∥∥∥ ≤ 2κ( ∥∥∥e⊤i (UV⊤ −M) ∥∥∥+ ∥∥∥e⊤i M ∥∥∥)\n≤2κ( √ µk\nd1 +\n∥∥∥e⊤i (UV⊤ −M) ∥∥∥)\nThis gives overall inequality:\nmax j\n∥∥∥e⊤j (VU⊤ −M⊤) ∥∥∥ ≤\n√ µk\nd2 · 1 κ + C ′′\n√ µkd1 log d\nm κ(\n√ µk\nd1 +max i\n∥∥∥e⊤i (UV⊤ −M) ∥∥∥)\nBy symmetry, we will also have:\nmax i\n∥∥∥e⊤i (UV⊤ −M) ∥∥∥ ≤\n√ µk\nd1 · 1 κ +C ′′\n√ µkd2 log d\nm κ(\n√ µk\nd2 +max j\n∥∥∥e⊤j (VU⊤ −M⊤) ∥∥∥)\nCombine above two equations and choose m ≥ c0µdkκ2 log d for some large enough c0. We have:\nmax j\n∥∥∥e⊤j (M⊤ −VU⊤) ∥∥∥ ≤ 2\n√ µk\nd2\nThis finishes the proof.\nTheorem A.6. Let U0V ⊤ 0 be the top-k SVD of d1d2 m PΩ(M), where |Ω| = m. then there exists universal constant c0, for any m ≥ c0µdkκ2 log d, with probability at least 1− 1d10 , we have:\nmax i\n∥∥∥e⊤i U0V⊤0 ∥∥∥ 2 ≤ 9µk\nd1 and max j\n∥∥∥e⊤j V0U⊤0 ∥∥∥ 2 ≤ 9µk\nd2\nProof. By Theorem A.5, we know for any j ∈ [d2]: ∥∥∥e⊤j (M⊤ −V0U⊤0 ) ∥∥∥ ≤ 2 √ µk\nd2\nTherefore, we have:\n∥∥∥e⊤j V0U⊤0 ∥∥∥ ≤[ ∥∥∥e⊤j M⊤ ∥∥∥+ ∥∥∥e⊤j (M⊤ −V0U⊤0 ) ∥∥∥] ≤ 3\n√ µk\nd2\nBy symmetry, we also know for any i ∈ [d1] ∥∥∥e⊤i U0V⊤0 ∥∥∥ ≤ 3 √ µk\nd1\nWhich finishes the proof.\nFor the special case where M ∈ Rd×d is symmetric and PSD, we can easily extends to have following:\nCorollary A.7. Let U0U ⊤ 0 be the top-k SVD of d2 m PΩ(M), where |Ω| = m. then there exists universal constant c0, for any m ≥ c0µdkκ2 log d, with probability at least 1− 1d10 , we have:\nmax i\n∥∥∥e⊤i U0 ∥∥∥ 2 ≤ 10µkκ\nd\nProof. By Corollary A.6, we have:\nmax i\n∥∥∥e⊤i U0U⊤0 ∥∥∥ 2 ≤ 9µk\nd\nOn the other hand, by Theorem A.4, we have:\nσmin(U ⊤ 0 U0) = σk(U0U ⊤ 0 ) ≥ σk(M)− ∥∥∥M−U0U⊤0 ∥∥∥ ≥ 9\n10κ\nTherefore, for any i ∈ [d] we have:\n∥∥∥e⊤i U0 ∥∥∥ 2 ≤\n∥∥e⊤i U0U⊤0 ∥∥2\nσmin(U⊤0 U0) ≤ 10µkκ d\nWhich finishes the proof.\nFinally, Lemma A.1 can be easily concluded from Theorem A.4 and Theorem A.6, while Lemma 4.1 is also directly proved by Theorem A.4 and Corollary A.7.\nB Proof of Symmetric PSD Case\nIn this section, we prove Theorem 3.1. WLOG, we continue to assume ‖M‖ = 1 in all proof. Also, when it’s clear from the context, we use κ to specifically to represent κ(M). Then σmin(M) = 1 κ . Also in this section, we always denote SVD(M) = XSX⊤, and SVD(UU⊤) = WDW⊤. The most essential part to prove Theorem 3.1 is proving following Theorem: Theorem B.1 (restatement of Theorem 4.5). Let f(U) = ∥∥UU⊤ −M\n∥∥2 F and gi(U) = ∥∥e⊤i U ∥∥2. Suppose after initialization, we have:\nf(U0) ≤ ( 1\n20κ\n)2 , max\ni gi(U0) ≤\n10µkκ2\nd\nThen, there exist some absolute constant c such that for any learning rate η < c µdkκ3 log d , with at least 1− T d10 probability, we will have for all t ≤ T that:\nf(Ut) ≤ (1− η 2κ )t ( 1 10κ )2 , max i gi(Ut) ≤ 20µkκ2 d\nTheorem B.1 says once initialization algorithm provides U0 in good local region, with high probability Ut will always stay in this good region and f(Ut) is linear converging to 0. With this theorem, we can then immediately conclude Theorem 3.1 from Theorem B.1 and Lemma 4.1.\nThe rest of this section all focus on proving Theorem B.1. First, we prepare with a few lemmas about the property of objective function, and the spectral property of U in a local Frobenius ball around optimal. Then, we prove Theorem B.1 by constructing two supermartingales related to f(Ut), gi(Ut) each, and applying concentration argument.\nFor symmetric PSD case, we denote the stochastic gradient as:\nSG(U) = 2d2(UU⊤ −M)ij(eie⊤j + eje⊤i )U\nThe update in Algorithm 1 can be now written as:\nUt+1 ← Ut − ηSG(Ut) (8)\nWe immediately have the property:\nESG(U) = ∇f(U) = 4(UU⊤ −M)U\nB.1 Geometric Properties in Local Region\nFirst, we prove two lemmas w.r.t the smoothness and property similar to strongly convex for objective function:\nLemma B.2. (restatement of Lemma 4.2) Within the region D = {U| ‖U‖ ≤ Γ}, we have function f(U) = ‖M−UU⊤‖2F satisfying for any U1,U2 ∈ D:\n‖∇f(U1)−∇f(U2)‖F ≤ β‖U1 −U2‖F\nwhere smoothness parameter β = 16max{Γ2, ‖M‖}.\nProof. Inside region D, we have:\n‖∇f(U1)−∇f(U2)‖F =‖4(U1U⊤1 −M)U1 − 4(U2U⊤2 −M)U2‖F ≤4‖U1U⊤1 U1 −U2U⊤2 U2‖F + 4‖M(U1 −U2)‖F =4‖U1U⊤1 (U1 −U2) +U1(U1 −U2)⊤U2 + (U1 −U2)U⊤2 U2‖F + 4‖M(U1 −U2)‖F ≤12max{‖U1‖2 , ‖U2‖2}‖U1 −U2‖F + 4 ‖M‖ ‖U1 −U2‖F ≤16max{Γ2, ‖M‖}‖U1 −U2‖F\nLemma B.3. (restatement of Lemma 4.3) Within the region D = {U|σmin(X⊤U) ≥ γ}, then we have function f(U) = ‖M−UU⊤‖2F satisfying:\n‖∇f(U)‖2F ≥ αf(U)\nwhere constant α = 4γ2.\nProof. Inside region D, recall we denote WDW⊤ = SVD(UU⊤), thus we have:\n‖∇f(U)‖2F = 16‖(UU⊤ −M)U‖2F =16[‖PW(UU⊤ −M)U‖2F + ‖PW⊥(UU⊤ −M)U‖2F] ≥16[σmin(D)‖PW(UU⊤ −M)PW‖2F + ‖PW⊥MU‖2F] ≥16[σmin(D)‖UU⊤ − PWMPW‖2F + ‖PW⊥MU‖2F]\nOn the other hand, we have:\n‖PW⊥MU‖2F = ‖PW⊥XΣX⊤U‖2F ≥ σ2min(X⊤U)‖PW⊥XΣ‖2F =σ2min(X ⊤U)tr(PW⊥M2PW⊥) = σ2min(X⊤U)‖PW⊥M‖2F\nand σmin(D) = λmin(U\n⊤U) ≥ λmin(U⊤PXU) = σ2min(X⊤U) Therefore, combine all above, we have:\n‖∇f(U)‖2F ≥ 16σ2min(X⊤U)[‖UU⊤ − PWMPW‖2F + ‖PW⊥M‖2F] ≥4σ2min(X⊤U)[‖UU⊤ − PWMPW‖2F + ‖PW⊥MPW‖2F + ‖PWMPW⊥‖2F + ‖PW⊥MPW⊥‖2F] =4σ2min(X ⊤U)‖UU⊤ −M‖2F\nNext, we show as long as we are in some Frobenious ball around optimum, then we have good spectral property over U which guarantees the preconditions for Lemma B.2 and Lemma B.3. Lemma B.4. (restatement of Lemma 4.4) Within the region D = {U| ∥∥M−UU⊤\n∥∥ F ≤ 110σk(M)},\nwe have: ‖U‖ ≤ √ 2 ‖M‖, σmin(X⊤U) ≥ √ σk(M)/2\nProof. For spectral norm of U, we have:\n‖U‖2 ≤ ‖M‖+ ∥∥∥M−UU⊤ ∥∥∥ ≤ ‖M‖+ ∥∥∥M−UU⊤ ∥∥∥ F ≤ 2 ‖M‖\nFor the minimum singular value of U⊤U, we have:\nσmin(U ⊤U) =σk(UU ⊤) ≥ σk(M)− ∥∥∥M−UU⊤ ∥∥∥\n≥σk(UU⊤) ≥ σk(M)− ∥∥∥M−UU⊤ ∥∥∥ F ≥ 9 10 σk(M)\nOn the other hand, we have:\n9\n10 σk(M) ‖X⊥W‖2 ≤σmin(D) ‖X⊥W‖2 ≤\n∥∥∥X⊤⊥WΣW⊤X⊥ ∥∥∥\n≤ ∥∥∥X⊤⊥UU⊤X⊥ ∥∥∥ F = ∥∥∥PX⊥(M−UU⊤)PX⊥ ∥∥∥ F ≤ ∥∥∥M−UU⊤\n∥∥∥ F ≤ 1 10 σk(M)\nLet the principal angle between X and W to be θ. This gives sin2 θ = ∥∥X⊤⊥W ∥∥2 ≤ 19 . Thus cos2 θ = σ2min(X ⊤W) ≥ 89 . Therefore:\nσ2min(X ⊤U) ≥ σ2min(X⊤W)σmin(U⊤U) ≥ σk(M)/2\nB.2 Proof of Theorem B.1\nNow, we are ready for our key theorem. By Lemma B.2, Lemma B.3, and Lemma B.4, we already know the function has good property locally in the region D = {U| ∥∥M−UU⊤ ∥∥ F\n≤ 110σk(M)} which alludes linear convergence. Then, the work remains and also the most challenging part is to prove that once we initialize inside this region, our algorithm will guarantee U never leave this region with high probability even with relatively large stepsize. The requirement for tight sample complexity and near optimal runtime makes it more challenging, and require us to further control the incoherence of Ut over all iterates in addition to the distance ∥∥M−UU⊤ ∥∥ F .\nFollowing is our formal proof.\nProof of Theorem B.1. Define event Et = {∀τ ≤ t, f(Uτ ) ≤ (1− η2κ)t( 110κ )2,maxi gi(Uτ ) ≤ 20µkκ2 d }. Theorem B.1 is equivalent to prove event ET happens with high probability. The proof achieves this by contructing two supermartingales for f(Ut)1Et and gi(Ut)1Et (where 1(·) denote indicator function), applies concentration argument.\nThe proofs follow the structure of:\n1. The constructions of supermartingales\n2. Their probability 1 bound and variance bound in order to apply Azuma-Bernstein inequality\n3. Final combination of concentration results to conclude the proof\nFirst, let filtration Ft = σ{SG(U0), · · · , SG(Ut−1)} where σ{·} denotes the sigma field. Note by definiton of Et, we have Et ⊂ Ft. Also Et+1 ⊂ Et, and thus 1Et+1 ≤ 1Et . Note Et denotes the event which up to time t, Uτ always stay in a local region which both close to M and incoherent.\nBy Lemma B.4, we immediately know that conditioned on Et, we have ‖Ut‖ ≤ √ 2, σmin(X\n⊤Ut) ≥ 1/ √ 2κ and σmin(U ⊤ t Ut) ≥ 1/2κ. We will use this fact throughout the proof.\nConstruction of supermartingale G: Since gi(U) = e ⊤ i UU ⊤ei is a quadratic function, we know for any change ∆U, we have:\ngi(U+∆U) = gi(U) + 2e ⊤ i (∆U)U ⊤ei + ∥∥∥e⊤i ∆U ∥∥∥ 2\nWe know for any l ∈ [d]:\nE‖e⊤l SG(U)‖21Et ≤ E16d4δil(u⊤i uj −Mij)2 max i\n∥∥∥e⊤i U ∥∥∥ 2 1Et\n=16d2 ∥∥∥e⊤l (UU⊤ −M) ∥∥∥ 2 max\ni\n∥∥∥e⊤i U ∥∥∥ 2 1Et ≤ O(µ2k2κ4)1Et\nTherefore, by update Eq.(8), and ESG(U) = ∇f(U) = 4(UU⊤ −M)U, we know:\nE[gi(Ut+1)1Et |Ft]\n=[gi(Ut)− 2ηe⊤i [ESG(Ut)]U⊤t ei + η2\n2 E\n∥∥∥e⊤i SG(Ut) ∥∥∥ 2 ]1Et\n=[tr(U⊤t eie ⊤ i [I− 8η(UtU⊤t −M)]Ut) +\nη2\n2 E\n∥∥∥e⊤i SG(Ut) ∥∥∥ 2 ]1Et\n=[tr(U⊤t eie ⊤ i Ut(I− 8ηU⊤t Ut)) + 8ηtr(U⊤t eie⊤i MUt) + η2O(µ2k2κ4)]1Et ≤[(1− 8ησmin(U⊤t Ut))gi(Ut) + 8ηtr(U⊤t eie⊤i MUt) + η2O(µ2k2κ4)]1Et ≤[(1− 4η\nκ )gi(Ut) + 16\n√ 10 ηµkκ\nd + η2O(µ2k2κ4)]1Et\n≤[(1− 4η κ )gi(Ut) + 60 ηµkκ d )]1Et\nThe last step is true by choosing constant c in learning rate η to be small enough.\nLet Git = (1− 4ηκ )−t(gi(Ut)1Et−1 − 15 µkκ2 d ). This gives:\nEGi(t+1) ≤ (1− 4η κ )−t(gi(Ut)1Et − 15\nµkκ2\nd ) ≤ Git\nThat is Git is supermartingale.\nProbability 1 bound for G: We also know\nGit − E[Git|Ft−1] =(1− 4η κ )−t\n[ −ηe⊤i [SG(Ut)− ESG(Ut)]U⊤t ei\n+ η2\n2 [‖e⊤i SG(Ut)‖2 − E‖e⊤i SG(Ut)‖2]\n] 1Et−1 (9)\nSince when sample (i, j) entry of matrix M, for any l ∈ [d], we have:\ne⊤l [SG(Ut)]U ⊤ t el · 1Et−1 = O(1)tr(U⊤ele⊤l SG(Ut))1Et−1\n=O(1)d2(UU⊤ −M)ijtr[U⊤ele⊤l (eiu⊤j + eju⊤i )]1Et−1 ≤O(1)d2\n∥∥∥UU⊤ −M ∥∥∥ ∞ max i ∥∥∥e⊤i U ∥∥∥ 2 1Et−1 ≤ O(µ2k2κ4)1Et−1\nand\n‖e⊤l SG(Ut)‖21Et−1 = O(1) ∥∥∥e⊤l SG(Ut) ∥∥∥ 2 1Et−1\n=O(1)d4(UU⊤ −M)2ij ∥∥∥e⊤l (eiu⊤j + eju⊤i ) ∥∥∥ 2 1Et−1 ≤O(1)d4 ∥∥∥UU⊤ −M ∥∥∥ 2\n∞ max i\n∥∥∥e⊤i U ∥∥∥ 2 1Et−1 ≤ O(µ3dk3κ6)1Et−1\nTherefore, by Eq.(9), we have with probability 1:\n|Git − E[Git|Ft−1]| ≤ (1− 4η κ )−tηO(µ2k2κ4)1Et−1 (10)\nVariance bound for G: For any l ∈ [d], we also know\nVar(e⊤l [SG(Ut)]U ⊤ t el · 1Et−1 |Ft−1) ≤ E[〈∇gl(Ut), SG(Ut)〉21Et−1 |Ft−1]\n=O(1) 1\nd2\n∑\nij\nd4(UU⊤ −M)2ijtr[U⊤ele⊤l (eiu⊤j + eju⊤i )]21Et−1\n≤O(1)d2 ∑\nj\n(UU⊤ −M)2ljtr[U⊤elu⊤j ]21Et−1\n≤O(1)d2 ∥∥∥e⊤l (UU⊤ −M) ∥∥∥ 2 max\ni\n∥∥∥e⊤i U ∥∥∥ 4 1Et−1 ≤ O( µ3k3κ6\nd )1Et−1\nand\nVar(‖e⊤l SG(U)‖21Et−1 |Ft−1) ≤ E[∇2gk(SG(Ut), SG(Ut))21Et−1 |Ft−1]\n=O(1) 1\nd2\n∑\nij\nd8(UU⊤ −M)4ij ∥∥∥e⊤k (eiu⊤j + eju⊤i ) ∥∥∥ 4 1Et−1\n≤O(1)d6 ∑\nj\n(UU⊤ −M)4kj ‖uj‖4 1Et−1\n≤O(1)d6 ∥∥∥UU⊤ −M ∥∥∥ 2\n∞\n∥∥∥e⊤k (UU⊤ −M) ∥∥∥ 2 max\ni\n∥∥∥e⊤i U ∥∥∥ 4 1Et−1 ≤ O(µ5dk5κ10)1Et−1\nTherefore, by Eq.(9), we have\nVar(Git|Ft−1) ≤ (1− 4η κ )−2tη2O(\nµ3k3κ6\nd )1Et−1 (11)\nBernstein’s inequality for G: Let σ2 = ∑t\nτ=1Var(Giτ |Fτ−1), and R satisfies, with probability 1 that |Giτ −E[Giτ |Fτ−1]| ≤ R, τ = 1, · · · , t. Then By standard Bernstein concentration inequality, we know:\nP (Git ≥ Gi0 + s) ≤ exp( s2/2\nσ2 +Rs/3 )\nSince Gi0 = gi(U0)− 15µkκ 2 d , let s′ = O(1)(1 − 4η κ )t[ √ σ2 log d+R log d], we know\nP ( gi(Ut)1Et−1 ≥ 15 µkκ2\nd + (1− 4η κ )t(gi(U0)− 15\nµkκ2\nd ) + s′\n) ≤ 1\n2d11\nBy Eq.(10), we know R = (1 − 4η κ )−tηO(µ2k2κ4) satisfies that |Giτ − E[Giτ |Fτ−1]| ≤ R, τ =\n1, · · · , t. Also by Eq. (11), we have:\n(1− 4η κ )t √\nσ2 log d ≤ ηO( √ µ3k3κ6 log d\nd )\n√√√√ t∑\nτ=1\n(1− 4η κ )2t−2τ ≤ √ηO(\n√ µ3k3κ7 log d\nd )\nby η < c µdkκ3 log d and choosing c to be small enough, we have:\ns′ = √ ηO(\n√ µ3k3κ7 log d\nd ) + ηO(µ2k2κ4 log d) ≤ µkκ\n2\nd\nSince initialization gives maxi gi(U0) ≤ 10µkκ 2\nd , therefore:\nP (gi(Ut)1Et−1 ≥ 20 µkκ2 d ) ≤ 1 2d11\nThat is equivalent to:\nP (Et−1 ∩ {gi(Ut) ≥ 20 µkκ2 d }) ≤ 1 2d11 (12)\nConstruction of supermartingale F : On the other hand, we also have\nE‖SG(Ut)‖2F 1Et ≤ E16d4(u⊤i uj −Mij)2 max i\n∥∥∥e⊤i U ∥∥∥ 2 1Et\n≤16d2‖UtU⊤t −M‖2F max i\n∥∥∥e⊤i Ut ∥∥∥ 2 1Et ≤ O(µdkκ2)f(Ut)1Et\nTherefore, by update function Eq.(8),\nE[f(Ut+1)1Et |Ft] ≤[f(Ut)− E〈∇f(Ut), ηSG(Ut)〉+ η2E ‖SG(Ut)‖2F ]1Et =[f(Ut)− η ‖∇f(Ut)‖2F + η2E ‖SG(Ut)‖ 2 F ]1Et ≤[(1 − 2η κ )f(Ut) + η 2O(µdkκ2)f(Ut)]1Et ≤(1− η κ )f(Ut)1Et\nLet Ft = (1− ηκ)−tf(Ut)1Et−1 , we know Ft is also a supermartingale.\nProbability 1 bound for F : With probabilty 1, we also have:\nFt − E[Ft|Ft−1] =(1− η κ )−t[−η〈∇f(Ut), SG(Ut)− ESG(Ut)〉\n+ η2\n2 (∇2f(ζt)(SG(Ut), SG(Ut))− E∇2f(ζt)(SG(Ut), SG(Ut)))]1Et−1 (13)\nwhere ζt depends on SG(Ut). First, recall we denote SVD(M) = XSX⊤, and SVD(UU⊤) = WDW⊤ , and observe that:\n∥∥∥UU⊤ −M ∥∥∥ ∞ 1Et−1 = max ij |tr(e⊤i (UU⊤ −M)ej)|1Et−1\n=max ij\n|tr(e⊤i (PX + PX⊥)(UU⊤ −M)ej)|1Et−1\n≤max ij |tr(e⊤i PX(UU⊤ −M)ej)|1Et−1 +max ij |tr(e⊤i PX⊥UU⊤ej)|1Et−1\n≤max i\n∥∥∥e⊤i X ∥∥∥ ∥∥∥UU⊤ −M ∥∥∥ F 1Et−1 +max i ∥∥∥e⊤i W ∥∥∥ ∥∥∥UU⊤ −M ∥∥∥ F 1Et−1\n≤O( √ µkκ3\nd ) √ f(Ut)\nThen, when sample (i, j) entry of matrix M, we have:\n〈∇f(Ut), SG(Ut)〉1Et−1 ≤ O(1) ‖∇f(Ut)‖F ‖SG(Ut)‖F 1Et−1 ≤O(1)d2 √ f(Ut)(UU ⊤ −M)ij ∥∥∥eiu⊤j + eju⊤i ∥∥∥ 2\nF 1Et−1\n≤O(1)d2 √ f(Ut) ∥∥∥UU⊤ −M ∥∥∥ ∞ max i ∥∥∥e⊤i U ∥∥∥ 1Et−1 ≤ O(µdkκ2.5)f(Ut)1Et−1\nand\n∇2f(ζt)(SG(Ut), SG(Ut))1Et−1 ≤ O(1) ‖SG(Ut)‖2F ≤O(1)d4 ∥∥∥UU⊤ −M ∥∥∥ 2\n∞ max i\n∥∥∥e⊤i U ∥∥∥ 2 1Et−1 ≤ O(µ2d2k2κ5)f(Ut)1Et−1\nTherefore, by decomposition Eq.(13), we have with probability 1:\n|Ft−E[Ft|Ft−1]| ≤ (1− η κ )−tηO(µdkκ2.5)f(Ut−1)1Et−1 ≤ (1− η κ )−t(1− η 2κ )tηO(µdkκ0.5)1Et−1 (14)\nVariance bound for F : We also know\nVar(〈∇f(Ut), SG(Ut)〉1Et−1 |Ft−1) ≤ E[〈∇f(Ut), SG(Ut)〉21Et−1 |Ft−1]\n≤‖∇f(Ut)‖2F E ‖SG(Ut)‖ 2 F 1Et−1 ≤ O(1)d2 max\ni\n∥∥∥e⊤i U ∥∥∥ 2 f2(Ut−1)1Et−1\n≤O(µdkκ2)f2(Ut−1)1Et−1\nand\nVar(∇2f(ζt)(SG(Ut), SG(Ut))1Et−1 |Ft−1) ≤ E[∇2f(ζt)(SG(Ut), SG(Ut))21Et−1 |Ft−1]\n≤O(1)E ‖SG(Ut)‖4F = O(1)Ed8(UU⊤ −M)4ij max i\n∥∥∥e⊤i U ∥∥∥ 4 1Et−1\n≤O(1)d6 ∥∥∥UU⊤ −M ∥∥∥ 2\n∞\n∥∥∥UU⊤ −M ∥∥∥ 2\nF max i\n∥∥∥e⊤i U ∥∥∥ 4 1Et−1\n≤O(µ3d3k3κ7)f2(Ut−1)1Et−1\nTherefore, by decomposition Eq.(13), we have:\nVar(Ft|Ft−1) ≤ (1− η κ )−2tη2O(µdkκ2)f2(Ut−1)1Et−1 ≤ (1− η κ )−2t(1− η 2κ )2tη2O( µdk κ2 )1Et−1 (15)\nBernstein’s inequality for F : Let σ2 = ∑t\nτ=1 Var(Fτ |Fτ−1), and R satisfies, with probability 1 that |Fτ −E[Fτ ||Fτ−1]| ≤ R, τ = 1, · · · , t. Then By standard Bernstein concentration inequality, we know:\nP (Ft ≥ F0 + s) ≤ exp( s2/2\nσ2 +Rs/3 )\nLet s′ = O(1)(1 − η κ )t[ √ σ2 log d+R log d], this gives:\nP (f(Ut)1Et−1 ≥ (1− η\nκ )tf(U0) + s ′) ≤ 1 2d10\nBy Eq.(14), we know R = (1− η κ )−t(1− η2κ)tηO(µdkκ0.5) satisfies that |Fτ −E[Fτ |Fτ−1]| ≤ R, τ = 1, · · · , t. Also by Eq. (15), we have:\n(1− η κ )t √\nσ2 log d ≤ ηO( √ µdk log d\nκ2 )\n√√√√ t∑\nτ=1\n(1− η κ )2t−2τ (1− η 2κ )2τ\n≤(1− η 2κ )tηO(\n√ µdk log d\nκ2 )\n√√√√ t∑\nτ=1\n(1− η κ )2t−2τ (1− η 2κ )2τ−2t ≤ (1− η 2κ )t √ ηO(\n√ µdk log d\nκ )\nby η < c µdkκ3 log d and choosing c to be small enough, we have:\ns′ = (1− η 2κ )t[ √ ηO(\n√ µdk log d\nκ ) + ηO(µdkκ0.5)] ≤ (1− η 2κ )t( 1 20κ )2\nSince F0 = f(U0) ≤ ( 120κ)2, therefore:\nP (f(Ut)1Et−1 ≥ (1− η\n2κ )t(\n1 10κ )2) ≤ 1 2d10\nThat is equivalent to:\nP (Et−1 ∩ {f(Ut) ≥ (1− η\n2κ )t(\n1 10κ )2}) ≤ 1 2d10 (16)\nProbability for event ET : Finally, combining the concentration result for martingaleG (Eq.(12)) and martingale F (Eq.(16)), we conclude:\nP (Et−1 ∩ Ēt) = P [ Et−1 ∩ ( ∪i{gi(Ut) ≥ 20 µkκ2\nd } ∪ {f(Ut) ≥ (1−\nη\n2κ )t(\n1\n10κ )2}\n)]\n≤ d∑\ni=1\nP (Et−1 ∩ {gi(Ut) ≥ 20 µkκ2\nd }) + P (Et−1 ∩ {f(Ut) ≥ (1−\nη\n2κ )t(\n1 10κ )2}) ≤ 1 d10\nSince\nP (ĒT ) =\nT∑\nt=1\nP (Et−1 ∩ Ēt) ≤ T\nd10\nWe finishes the proof.\nC Proof of General Asymmetric Case\nIn this section, we first prove Lemma 3.2, set up the equivalence between Algorithm 2 and Algorithm 3. Then we prove the main theorem for general asymmetric matrix (Theorem 3.3). WLOG, we continue to assume ‖M‖ = 1 in all proof. Also, when it’s clear from the context, we use κ to specifically to represent κ(M). Then σmin(M) = 1 κ . Also in this section, we always use d = max{d1, d2} and denote SVD(M) = XSY⊤, and SVD(UV⊤) = WUDW⊤V .\nProof of Lemma 3.2. Let us always denote the iterates in Algorithm 2 by Ut, Vt, and denote the corresponding iterates in Algorithm 3 by U′t, V ′ t using prime version. We use induction to prove the equivalence. Assume at time t we have UtV ⊤ t = U ′ tV ′ t ⊤. Recall in Algorithm 2, we renormalize Ut,Vt to Ũt, Ṽt, this set up the correspondence:\nŨt = U ′ tR ′ UD ′ U − 1 2Q′UD ′ 1 2 Ṽt = V ′ tR ′ V D ′ V − 1 2Q′V D ′ 1 2\nDenote P′U = R ′ UD ′ U − 1 2Q′UD ′ 1 2 , and P′V = V ′ tR ′ V D ′ V − 1 2Q′V D ′ 1 2 . Clearly P′UP ′ V ⊤ = I. Then we have Ũt = U ′ tP ′ U , Ṽt = P ′ V and thus:\nUt+1V ⊤ t+1\n=(Ũt − 2ηd1d2(ŨtṼ⊤t −M)ijeie⊤j Ṽt)(Ṽt − 2ηd1d2(ŨtṼ⊤t −M)ijeje⊤i Ũt)⊤ =(U′tP ′ U − 2ηd1d2(U′tV′t⊤ −M)ijeie⊤j V′tP′V )(V′tP′V − 2ηd1d2(U′tV′t⊤ −M)ijeje⊤i U′tP′U )⊤ =(U′t − 2ηd1d2(U′tV′t⊤ −M)ijeie⊤j V′tP′V P′U−1) · (V′t − 2ηd1d2(U′tV′t⊤ −M)ijeje⊤i U′tP′UP′V −1)⊤ =U′t+1V ′ t+1 ⊤\nClearly with same initialization algorithm, we have U0V ⊤ 0 = U ′ 0V ′ 0 ⊤, by induction, we finish the proof.\nNow we proceed to prove Theorem 3.3. Since Algorithm 2 and Algorithm 3 are equivalent, we will focus our analysis on Algorithm 2 which is more theoretical appealing. As for the symmetric PSD case, we first present the essential ingradient: Theorem C.1. Let f(U,V) = ∥∥UV⊤ −M\n∥∥2 F , gi(U,V) = ∥∥e⊤i UV⊤ ∥∥2, and hj(U,V) = ∥∥∥e⊤j VU⊤ ∥∥∥ 2 ,\nfor i ∈ [d1] and j ∈ [d2]. Suppose after initialization, we have:\nf(U0,V0) ≤ ( 1\n20κ )2, max i gi(U0,V0) ≤\n10µkκ2\nd1 , max j hj(U0,V0) ≤\n10µkκ2\nd2\nThen, there exist some absolute constant c such that for any learning rate η < c µdkκ3 log d , with at least 1− T d10 probability, we will have for all t ≤ T that:\nf(Ut,Vt) ≤ (1− η\n2κ )t(\n1\n10κ )2, max i gi(Ut,Vt) ≤\n100µkκ2\nd1 , max j hj(Ut,Vt) ≤\n100µkκ2\nd2\nTheorem 3.3 can easily be concluded from Theorem C.1 and Lemma A.1. Theorem C.1 also provides similar guarantees as Theorem B.1 in symmetric case. However, due to the additional invariance between U and V, Theorem C.1 need to keep track of more complicated potential function gi(U,V) and hj(U,V) to control the incoherence, which makes the proof more involved.\nThe rest of this section all focus on proving Theorem C.1. Similar to symmetric PSD case, we also first prepare with a few lemmas about the property of objective function, and the spectral property of U,V in a local Frobenius ball around optimal. Then, we prove Theorem C.1 by constructing three supermartingales related to f(Ut,Vt), gi(Ut,Vt), hj(Ut,Vt) each, and applying concentration argument.\nTo make the notation clear, denote gradient ∇f(U,V) ∈ R(d1+d2)×k:\n∇f(U,V) = ( ∂ ∂U\nf(U,V) ∂ ∂V f(U,V)\n)\nAlso denote the stochastic gradient SG(U,V) by (if we sampled entry (i, j) of matrix M)\nSG(U,V) = 2d1d2(UV ⊤ −M)ij\n( eie ⊤ j V\neje ⊤ i U\n)\nESG(U,V) = ∇f(U,V) = 2 (\n(UV⊤ −M)V (UV⊤ −M)⊤U\n)\nBy update function, we know:\n( Ut+1 Vt+1 ) = ( Ũt Ṽt ) − ηSG(Ũt, Ṽt)\nand ŨtṼ ⊤ t = UtV ⊤ t is the renormalized version of UtV ⊤ t .\nC.1 Geometric Properties in Local Region\nSimilar to symmetric PSD case, we first prove two lemmas w.r.t the smoothness and property similar to strongly convex for objective function:\nLemma C.2. Within the region D = {(U,V)| ‖U‖ ≤ Γ, ‖V‖ ≤ Γ}, we have function f(U,V) = ‖M−UV⊤‖2F satisfying:\n‖∇f(U1,V1)−∇f(U2,V2)‖2F ≤ β2(‖U1 −U2‖2F + ‖V1 −V2‖2F)\nwhere smoothness parameter β = 8max{Γ2, ‖M‖}.\nProof. Inside region D, we have:\n‖∇f(U1,V1)−∇f(U2,V2)‖2F =‖ ∂\n∂U f(U1,V1)−\n∂\n∂U f(U2,V2)‖2F + ‖\n∂\n∂V f(U1,V1)−\n∂\n∂V f(U2,V2)‖2F\n=4(‖(U1V⊤1 −M)V1 − (U2V⊤2 −M)V2‖2F + ‖(U1V⊤1 −M)⊤U1 − (U2V⊤2 −M)⊤U2‖2F) ≤64max{Γ4, ‖M‖2}(‖U1 −U2‖2F + ‖V1 −V2‖2F)\nThe last step is by similar technics as in the proof of Lemma B.2, by expanding\nU1V ⊤ 1 V1 −U2V⊤2 V2 = (U1 −U2)V⊤1 V1 +U2(V1 −V2)⊤V1 +U2V⊤2 (V1 −V2)\nLemma C.3. Within the region D = {(U,V)|σmin(X⊤U) ≥ γ, σmin(Y⊤V) ≥ γ}, then we have function f(U,V) = ‖M−UV⊤‖2F satisfying:\n‖∇f(U,V)‖2F ≥ αf(U,V)\nwhere constant α = 4γ2.\nProof. Let Û, V̂ be the left singular vectors of U,V. Inside region D, we have:\n‖(UV⊤ −M)V‖2F =‖P\nÛ (UV⊤ −M)V‖2F + ‖PÛ⊥(UV ⊤ −M)V‖2F ≥σk(V)2‖PÛ(UV ⊤ −M)P V̂ ‖2F + ‖PÛ⊥MV‖ 2 F ≥σk(V)2‖PÛ(UV ⊤ −M)P V̂ ‖2F + σmin(Y⊤V)2‖PÛ⊥XΣ‖ 2 F =σk(V) 2‖P Û (UV⊤ −M)P V̂ ‖2F + σmin(Y⊤V)2‖PÛ⊥M‖ 2 F\nTherefore, by symmetry, we have:\n‖∇f(U,V)‖2F =4(‖(UV⊤ −M)V‖2F + ‖(UV⊤ −M)⊤U‖2F) ≥4γ2(2‖P\nÛ (UV⊤ −M)P V̂ ‖2F + ‖PÛ⊥M‖ 2 F + ‖MPV̂⊥‖ 2 F)\n≥4γ2‖UV⊤ −M‖2F\nNext, we show as long as we are in some Frobenious ball around optimum, then we have good spectral property over U,V which guarantees the preconditions for Lemma C.2 and Lemma C.3. Lemma C.4. Within the region D = {(U,V)| ∥∥M−UV⊤\n∥∥ F ≤ 110σk(M)}, and for U = WUD 1 2 ,V =\nWV D 1 2 where WUDWV = SVD(UV ⊤), we have:\n‖U‖ ≤ √ 2 ‖M‖, σmin(X⊤U) ≥ √ σk(M)/2 ‖V‖ ≤ √ 2 ‖M‖, σmin(Y⊤V) ≥ √ σk(M)/2\nProof. For spectral norm of U, we have:\n‖U‖2 = ‖D‖ = ∥∥∥UV⊤ ∥∥∥ ≤ ‖M‖+ ∥∥∥M−UV⊤ ∥∥∥ ≤ ‖M‖+ ∥∥∥M−UV⊤ ∥∥∥ F ≤ 2 ‖M‖\nFor the minimum singular value of U⊤U, we have:\nσmin(U ⊤U) =σk(D) = σk(UV ⊤) ≥ σk(M)− ∥∥∥M−UV⊤ ∥∥∥\n≥σk(M)− ∥∥∥M−UU⊤ ∥∥∥ F ≥ 9 10 σk(M)\nBy symmetry, the same holds for V. On the other hand, we have:\n1\n10 σk(M) ≥ ∥∥∥M−UV⊤ ∥∥∥ F ≥ ∥∥∥PX⊥(M−UV⊤) ∥∥∥ F = ∥∥∥PX⊥UV⊤ ∥∥∥ F = ‖PX⊥WUD‖F\n≥‖PX⊥WUD‖ ≥ 9\n10 σk(M) ‖X⊥WU‖\nLet the principal angle between X and WU to be θ. This gives sin 2 θ = ∥∥X⊤⊥WU ∥∥2 ≤ 19 . Thus cos2 θ = σ2min(X ⊤WU ) ≥ 89 . Therefore:\nσ2min(X ⊤U) ≥ σ2min(X⊤WU )σmin(U⊤U) ≥ σk(M)/2\nC.2 Proof of Theorem C.1\nNow, we are ready for our key theorem. By Lemma C.2, Lemma C.3, and Lemma C.4, we already know the function has good property locally in the region D = {(U,V)| ∥∥M−UV⊤ ∥∥ F ≤ 110σk(M)} which alludes linear convergence. Similar to the symmetric PSD case, the work remains is to prove that once we initialize inside this region, our algorithm will guarantee U,V never leave this region with high probability even with relatively large stepsize. Again, we also need to control the incoherence of Ut,Vt over all iterates additionally to achieve tight sample complexity and near optimal runtime.\nFollowing is our formal proof.\nProof of Theorem C.1. For simplicity of notation, we assume d = d1 = d2, and do not distinguish d1 and d2. However, it is easy to check our proof never use the property M is square matrix. The proof easily extends to d1 6= d2 case by replacing d in the proof with suitable d1, d2.\nDefine event Et = {∀τ ≤ t, f(Uτ ,Vτ ) ≤ (1− η2κ )t( 110κ)2,maxi gi(Uτ ,Vτ ) ≤ 100µkκ2 d ,maxj hj(Uτ ,Vτ ) ≤\n100µkκ2\nd }. Theorem C.1 is equivalent to prove event ET happens with high probability. The proof achieves this by contructing two supermartingales for f(Ut,Vt)1Et , gi(Ut,Vt)1Et and hi(Ut,Vt)1Et (where 1(·) denote indicator function), applies concentration argument.\nThe proofs also follow similar structure as symmetric PSD case:\n1. The constructions of supermartingales\n2. Their probability 1 bound and variance bound in order to apply Azuma-Bernstein inequality\n3. Final combination of concentration results to conclude the proof\nThen let filtration Ft = σ{SG(U0,V0), · · · , SG(Ut−1,Vt−1)} where σ{·} denotes the sigma field. Also let event , note Et ⊂ Ft. Also Et+1 ⊂ Et, and thus 1Et+1 ≤ 1Et .\nBy Lemma C.4, we immediately know that conditioned on Et, we have ‖Ut‖ ≤ √ 2, ‖Vt‖ ≤ √ 2,\nσmin(X ⊤Ut) ≥ 1/ √ 2κ, σmin(Y ⊤Vt) ≥ 1/ √ 2κ. We will use this fact throughout the proof.\nFor simplicity, when it’s clear from the context, we denote:\n( ∆U ∆V ) = −ηSG(Ũt, Ṽt) = ( Ut+1 Vt+1 ) − ( Ũt Ṽt )\nConstruction of supermartingale G: First, since potential function gl(U,V) is forth-order polynomial, we can expand:\ngl(Ũt+1, Ṽt+1) = gl(Ut+1,Vt+1) = gl(Ũt +∆U, Ṽt +∆V)\n= e⊤l (Ũt +∆U)(Ṽt +∆V) ⊤(Ṽt +∆V)(Ũt +∆U) ⊤el = gl(Ũt, Ṽt) + 2e ⊤ l ∆UṼ ⊤ t ṼtŨ ⊤ t el + 2e ⊤ l ŨtṼ ⊤ t ∆VŨtel +R2\n= gl(Ũt, Ṽt) +R1\nWhere we denote R1 as the sum of first order terms and higher order terms (all second/third/forth order terms), and R2 as the sum of second order terms and higher order terms.\nWe now give a proposition about properties of R1 and R2 which involves a lot calculation, and postpone its proof in the end of this section.\nProposition C.5. With above notations, we have following inequalities hold true.\nE[R21Et |Ft] ≤ η2O(µ2k2κ4)1Et |R1|1Et ≤ ηO(µ2k2κ5)1Et w.p 1 E[R211Et |Ft] ≤ η2O( µ3k3κ6\nd )1Et\nThen by taking conditional expectation, we have:\nE[gl(Ũt+1, Ṽt+1)1Et |Ft] = E[gl(Ut+1,Vt+1)1Et |Ft] ≤E[gl(Ũt, Ṽt) + 2e⊤l ∆UṼ⊤t ṼtŨ⊤t el + 2e⊤l ŨtṼ⊤t ∆VŨtel +R2]1Et\nThe first order term can be calculated as:\n[−E2e⊤l ∆UṼ⊤t ṼtŨ⊤t el + 2e⊤l ŨtṼ⊤t ∆VŨtel]1Et =[−4e⊤l (ŨtṼ⊤t −M)ṼtṼ⊤t ṼtŨ⊤t el − 4e⊤l ŨtŨ⊤t (ŨtṼ⊤t −M⊤)ṼtŨ⊤t el]1Et =[−4e⊤l ŨtṼ⊤t ṼtṼ⊤t ṼtŨ⊤t el + 4e⊤l MṼtṼ⊤t ṼtŨ⊤t el − 4e⊤l ŨtŨ⊤t (ŨtṼ⊤t −M⊤)ṼtŨ⊤t el]1Et ≤[−4[σmin(Ṽ⊤t Ṽt) ∥∥∥e⊤l ŨtṼ⊤t ∥∥∥ 2 + ∥∥∥e⊤l ŨtṼ⊤t ∥∥∥ ∥∥∥ṼtṼ⊤t ∥∥∥ ∥∥∥e⊤l M ∥∥∥\n+ ∥∥∥e⊤l ŨtŨ⊤t ∥∥∥ ∥∥∥ŨtṼ⊤t −M⊤ ∥∥∥ F ∥∥∥e⊤l ŨtṼ⊤t ∥∥∥]]1Et\n≤[−2 κ gl(Ũt, Ṽt) + 80 µkκ d + 4 10κ gl(Ũt, Ṽt)]1Et ≤[−1 κ gl(Ũt, Ṽt) + 80 µkκ d ]1Et\nIn second last inequality, we use key observation:\n∥∥∥e⊤k ŨtṼ⊤t ∥∥∥ = ∥∥∥e⊤k WUDW⊤V ∥∥∥ = ∥∥∥e⊤k WUDW⊤U ∥∥∥ = ∥∥∥e⊤k ŨtŨ⊤t ∥∥∥\nBy Proposition C.5, we know E[R21Et |Ft] ≤ η2O(µ2k2κ4)1Et . Combine both facts and recall η < c µdkκ3 log d , we have:\nE[gi(Ũt+1, Ṽt+1)1Et |Ft] ≤ [(1− η\nκ )gi(Ũt, Ṽt) +\n80ηµkκ\nd +O(η2µ2k2κ4)]1Et\n≤ [(1− η κ )gi(Ũt, Ṽt) + 90ηµkκ d ]1Et\nThe last inequality is achieved by choosing c small enough.\nLet Git = (1− ηκ)−t(gi(Ũt, Ṽt)1Et−1 − 90 µkκ2 d ). This gives:\nE[Gi(t+1)|Ft] ≤ (1− η κ )−t(gi(Ũt, Ṽt)1Et − 90\nµkκ2\nd ) ≤ Git\nThe right inequality is true since 1Et ≤ 1Et−1 . This implies Git is supermartingale.\nProbability 1 bound for G: We also know:\nGi(t+1) − E[Gi(t+1)|Ft] =(1− η κ )−(t+1)[R1 − ER1]1Et\nBy Proposition C.5, we know with probability 1 that |R1|1Et ≤ ηO(µ2k2κ5)1Et . This gives with probability 1:\n|Git − E[Git|Ft−1]| ≤ (1− η κ )−tηO(µ2k2κ5)1Et−1 (17)\nVariance bound for G: We also know\nVar(Gi(t+1)|Ft) = (1− η κ )−2(t+1)[ER211Et − (ER11Et)2] ≤ E[R211Et |Ft]\nBy Proposition C.5, we know that E[R211Et |Ft] ≤ η2O(µ 3k3κ6 d )1Et . This gives:\nVar(Git|Ft−1) ≤ (1− η κ )−2tη2O(\nµ3k3κ6\nd )1Et−1 (18)\nBernstein’s inequality for G: Let σ2 = ∑t\nτ=1Var(Giτ |Fτ−1), and R satisfies, with probability 1 that |Giτ −E[Giτ |Fτ−1]| ≤ R, τ = 1, · · · , t. Then By standard Bernstein concentration inequality, we know:\nP (Git ≥ Gi0 + s) ≤ exp( s2/2\nσ2 +Rs/3 )\nSince Gi0 = gi(Ũ0, Ṽ0)− 90µkκ 2 d , let s′ = O(1)(1 − η κ )t[ √ σ2 log d+R log d], we know\nP ( gi(Ũt, Ṽt)1Et−1 ≥ 90 µkκ2\nd + (1− η κ )t(gi(Ũ0, Ṽ0)− 90\nµkκ2\nd ) + s′\n) ≤ 1\n3d11\nBy Eq.(17), we know R = (1 − η κ )−tηO(µ2k2κ5) satisfies that |Giτ − E[Giτ |Fτ−1]| ≤ R, τ =\n1, · · · , t. Also by Eq. (18), we have:\n(1− η κ )t √\nσ2 log d ≤ ηO( √ µ3k3κ6 log d\nd )\n√√√√ t∑\nτ=1\n(1− η κ )2t−2τ ≤ √ηO(\n√ µ3k3κ7 log d\nd )\nby η < c µdkκ3 log d and choosing c to be small enough, we have:\ns′ = √ ηO(\n√ µ3k3κ7 log d\nd ) + ηO(µ2k2κ5 log d) ≤ 10µkκ\n2\nd\nSince initialization gives maxi gi(U0,V0) ≤ 10µkκ 2\nd , therefore:\nP (gi(Ũt, Ṽt)1Et−1 ≥ 100 µkκ2 d ) ≤ 1 3d11\nThat is equivalent to:\nP (Et−1 ∩ {gi(Ũt, Ṽt) ≥ 100 µkκ2 d }) ≤ 1 3d11 (19)\nBy symmetry, we can also have corresponding result for hj(Ũt, Ṽt).\nConstruction of supermartingale F: Similarly, we also need to construct a martingale for f(Ũt, Ṽt). Again, we can write f as forth order polynomial:\nf(Ũt+1, Ṽt+1) = f(Ut+1,Vt+1) = f(Ũt +∆U, Ṽt +∆V)\n= tr ( [(Ũt +∆U)(Ṽt +∆V)−M][(Ũt +∆U)(Ṽt +∆V)−M]⊤ )\n= f(Ũt, Ṽt) + 2tr(∆UṼ ⊤ t (ŨtṼ ⊤ t −M)⊤) + 2tr(∆VŨ⊤t (ŨtṼ⊤t −M)) +Q2\n= f(Ũt, Ṽt) +Q1\nWhere we denoteQ1 as the sum of first order terms and higher order terms (all second/third/forth order terms), and Q2 as the sum of second order terms and higher order terms.\nWe also now give a proposition about properties of Q1 and Q2 which involves a lot calculation, and postpone its proof in the end of this section.\nProposition C.6. With above notations, we have following inequalities hold true.\nE[Q21Et |Ft] ≤ η2O(µdkκ2)f(Ũt, Ṽt)1Et |Q1|1Et ≤ ηO(µdkκ3)f(Ũt, Ṽt)1Et w.p 1 E[Q211Et |Ft] ≤ η2O(µdkκ2)f2(Ũt, Ṽt)1Et\nThen by Proposition C.6, we know E[Q21Et |Ft] ≤ η2O(µdkκ2)f(Ũt, Ṽt)1Et . By taking conditional expectation, we have:\nE[f(Ut+1)1Et |Ft] ≤[f(Ũt, Ṽt)− E〈∇f(Ũt, Ṽt), ηSG(Ut)〉+ EQ2]1Et =[f(Ũt, Ṽt)− η ∥∥∥∇f(Ũt, Ṽt) ∥∥∥ 2\nF + EQ2]1Et\n≤[(1− 2η κ )f(Ũt, Ṽt) + η 2O(µdkκ2)f(Ũt, Ṽt)]1Et ≤(1− η κ )f(Ũt, Ṽt)1Et\nLet Ft = (1− ηκ)−tf(Ũt, Ṽt)1Et−1 , we know Ft is also a supermartingale.\nProbability 1 bound: We also know\nFt+1 − E[Ft+1|Ft] = (1− η κ )−(t+1)[Q1 − EQ1]1Et\nBy Proposition C.6, we know with probability 1 that |Q1|1Et ≤ ηO(µdkκ3)f(Ut,Vt)1Et . This gives with probability 1:\n|Ft − EFt| ≤ (1− η κ )−tηO(µdkκ3)f(Ut−1)1Et−1 ≤ (1− η κ )−t(1− η 2κ )tηO(µdkκ)1Et−1 (20)\nVariance bound: We also know\nVar(Ft+1|Ft) = (1− η κ )−2(t+1)[EQ211Et − (EQ11Et)2] ≤ (1− η κ )−2(t+1)E[Q211Et |Ft]\nBy Proposition C.6, we know that E[Q211Et |Ft] ≤ η2O(µdkκ2)f2(Ut,Vt)1Et . This gives:\nVar(Ft|Ft−1) ≤ (1− η κ )−2tη2O(µdkκ2)f2(Ut−1)1Et−1 ≤ (1− η κ )−2t(1− η 2κ )2tη2O( µdk κ2 )1Et−1 (21)\nBernstein’s inequality: Let σ2 = ∑t\nτ=1 Var(Fτ |Fτ−1), and R satisfies, with probability 1 that |Fτ − E[Fτ ||Fτ−1]| ≤ R, τ = 1, · · · , t. Then By standard Bernstein concentration inequality, we know:\nP (Ft ≥ F0 + s) ≤ exp( s2/2\nσ2 +Rs/3 )\nLet s′ = O(1)(1 − η κ )t[ √ σ2 log d+R log d], this gives:\nP (f(Ũt, Ṽt)1Et−1 ≥ (1− η\nκ )tf(U0) + s ′) ≤ 1 3d10\nBy Eq.(20), we know R = (1 − η κ )−t(1 − η2κ)tηO(µdkκ) satisfies that |Fτ − E[Fτ |Fτ−1]| ≤ R, τ = 1, · · · , t. Also by Eq. (21), we have:\n(1− η κ )t √\nσ2 log d ≤ ηO( √ µdk log d\nκ2 )\n√√√√ t∑\nτ=1\n(1− η κ )2t−2τ (1− η 2κ )2τ\n≤(1− η 2κ )tηO(\n√ µdk log d\nκ2 )\n√√√√ t∑\nτ=1\n(1− η κ )2t−2τ (1− η 2κ )2τ−2t ≤ (1− η 2κ )t √ ηO(\n√ µdk log d\nκ )\nby η < c µdkκ3 log d and choosing c to be small enough, we have:\ns′ = (1− η 2κ )t[ √ ηO(\n√ µdkκ log d\nκ ) + ηO(µdkκ)] ≤ (1− η 2κ )t( 1 20κ )2\nSince F0 = f(U0) ≤ ( 120κ)2, therefore:\nP (f(Ũt, Ṽt)1Et−1 ≥ (1− η\n2κ )t(\n1 10κ )2) ≤ 1 3d10\nThat is equivalent to:\nP (Et−1 ∩ {f(Ũt, Ṽt) ≥ (1− η\n2κ )t(\n1 10κ )2}) ≤ 1 3d10 (22)\nProbability for event ET : Finally, combining the concentration result for martingaleG (Eq.(19)) and martingale F (Eq.(22)), we conclude:\nP (Et−1 ∩ Ēt)\n=P [ Et−1 ∩ ( [∪i{gi(Ut,Vt) ≥ 100 µkκ2\nd }]\n∪ [∪j{hj(Ut,Vt) ≥ 100 µkκ2\nd }] ∪ {f(Ut) ≥ (1−\nη\n2κ )t(\n1\n10κ )2}\n)]\n≤2 d∑\ni=1\nP (Et−1 ∩ {gi(Ut,Vt) ≥ 100 µkκ2\nd }) + P (Et−1 ∩ {f(Ut,Vt) ≥ (1−\nη\n2κ )t(\n1\n10κ )2})\n≤ 1 d10\nSince\nP (ĒT ) =\nT∑\nt=1\nP (Et−1 ∩ Ēt) ≤ T\nd10\nWe finishes the proof.\nFinally we give proof for Proposition C.5 and Proposition C.6. The proof mostly consistsof expanding every term and careful calculations.\nProof of Proposition C.5. For simplicity of notation, we hide the term 1Et in all following equations. Reader should always think every term in this proof multiplied by 1Et . Recall that:\nSG(U,V) = 2d2(UV⊤ −M)ij ( eie ⊤ j V\neje ⊤ i U\n)\n( ∆U ∆V ) =− ηSG(Ũt, Ṽt) = ( Ut+1 Vt+1 ) − ( Ũt Ṽt )\nWe first prove first three inequality. Recall that:\ngl(Ũt+1, Ṽt+1) = gl(Ut+1,Vt+1) = gl(Ũt +∆U, Ṽt +∆V)\n= e⊤l (Ũt +∆U)(Ṽt +∆V) ⊤(Ṽt +∆V)(Ũt +∆U) ⊤el = gl(Ũt, Ṽt) + 2e ⊤ l ∆UṼ ⊤ t ṼtŨ ⊤ t el + 2e ⊤ l ŨtṼ ⊤ t ∆VŨtel +R2\n= gl(Ũt, Ṽt) +R1\nBy expanding the polynomial, we can write out the first order term:\nR1 −R2 =2e⊤l ∆UṼ⊤t ṼtŨ⊤t el + 2e⊤l ŨtṼ⊤t ∆VŨtel =− 4ηd2(ŨtṼ⊤t −M)ij ( δile ⊤ j ṼtṼ ⊤ t ṼtŨ ⊤ t el + (ŨtṼ ⊤ t )lj(ŨtŨ ⊤ t )il )\nSecond order term:\nR2 −R3 =e⊤l ∆UṼ ⊤ t Ṽt∆ ⊤ Uel + e ⊤ l Ũt∆ ⊤ V∆VŨ ⊤ t el + 2e ⊤ l ∆UṼ ⊤ t ∆VŨ ⊤ t el + 2e ⊤ l ∆U∆ ⊤ VṼtŨ ⊤ t el =4η2d4(ŨtṼ ⊤ t −M)2ij\n· ( δil ∥∥∥e⊤j ṼtṼ⊤t ∥∥∥ 2 + (ŨtŨ ⊤ t ) 2 li + 2δil(ṼtṼ ⊤ t )jj(ŨtŨ ⊤ t )ii + 2δil(ŨtṼ ⊤ t ) 2 ij )\nThird order term:\nR4 −R3 =2e⊤l ∆UṼ⊤t ∆V∆⊤Uel + 2e⊤l ∆U∆⊤V∆VŨ⊤t el =− 16η3d6(ŨtṼ⊤t −M)3ijδil ( (ṼtṼt)jj(ŨtṼt)ij + (ŨtṼt)ij(ŨtŨt)ii )\nFourth order term:\nR4 = e ⊤ l ∆U∆ ⊤ V∆V∆ ⊤ Uel = 16η 4d8(ŨtṼ ⊤ t −M)4ijδil(ŨtṼt)2ij\nFor the ease of proof, we denote χ = µkκ 2\nd , then we know conditioned on event Et, we have:\nmaxi ∥∥∥e⊤i ŨtṼ⊤t ∥∥∥ 2 ≤ O(χ), and maxj ∥∥∥e⊤j ṼtŨ⊤t ∥∥∥ 2 ≤ O(χ). Some key inequality we need to use in the proof are listed here: ∥∥∥e⊤l ŨtṼ⊤t ∥∥∥ = ∥∥∥e⊤l ŨtŨ⊤t ∥∥∥ and ∥∥∥e⊤l ṼtŨ⊤t ∥∥∥ = ∥∥∥e⊤l ṼtṼ⊤t ∥∥∥ (23)\nand |(ŨtṼt)ij | ≤ ∥∥∥e⊤i ŨtṼ⊤t ∥∥∥ ≤ O(√χ) (24)\nThe same also holds true for:\n|(ŨtŨ⊤t )ii| ≤ O( √ χ) and |(ṼtṼ⊤t )jj | ≤ O( √ χ) (25)\nAnother fact we frequently used is:\n1\n2\n∥∥∥e⊤i UV⊤ ∥∥∥ 2 ≤ ∥∥∥e⊤i U ∥∥∥ 2 ≤ 2κ ∥∥∥e⊤i UV⊤ ∥∥∥ 2\nThis gives: ∥∥∥ŨtṼ⊤t −M ∥∥∥ ∞ ≤ max k ∥∥∥ekŨt ∥∥∥max k ∥∥∥ekṼt ∥∥∥+max k ‖ekX‖max k ‖ekY‖ ‖S‖ ≤ O(χκ)\nand recall we choose η < c µdkκ3 log d , where c is some universal constant, then we have:\nηd2 ∥∥∥ŨtṼ⊤t −M ∥∥∥ ∞ = O(ηd2χκ) ≤ O(1) (26)\nWith equation (23), (24), (25), (26), now we are ready to prove Lemma.\nFor the first inequality E[R21Et |Ft] ≤ η2O(µ2k2κ4)1Et :\nE[R21Et |Ft] ≤ E[|R2 −R3|1Et |Ft] + E[|R3 −R4|1Et |Ft] + E[|R4|1Et |Ft]\nFor each term, we can bound as:\nE[|R2 −R3|1Et |Ft] ≤ η2O(d2) ∑\nij\n(ŨtṼ ⊤ t −M)2ij ( δilO(χ) + (ŨtŨ ⊤ t ) 2 li )\n≤η2O(d2)max l′\n∥∥∥e⊤l′ (ŨtṼ⊤t −M) ∥∥∥ 2 ∑\ni\n( δilO(χ) + (ŨtŨ ⊤ t ) 2 li ) ≤ η2O(d2χ2)\nE[|R3 −R4|1Et |Ft] ≤ η3O(d4) ∑\nij\n|ŨtṼ⊤t −M|3ijδilO(χ)\n≤η3O(d4) ∥∥∥ŨtṼ⊤t −M ∥∥∥ ∞ ∥∥∥e⊤l (ŨtṼ⊤t −M) ∥∥∥ 2 O(χ) ≤ η2O(d2χ2)\nE[|R4|1Et |Ft] ≤ η4O(d6) ∑\nij\n(ŨtṼ ⊤ t −M)4ijδilO(χ)\n≤η4O(d6) ∥∥∥ŨtṼ⊤t −M ∥∥∥ 2\n∞\n∥∥∥e⊤l (ŨtṼ⊤t −M) ∥∥∥ 2 O(χ) ≤ η2O(d2χ2)\nThis gives in sum that\nE[R21Et |Ft] ≤ η2O(d2χ2)1Et = η2O(µdkκ2)f2(Ut)1Et\nFor the second inequality |R1|1Et ≤ ηO(µ2k2κ5)1Et w.p 1:\n|R1|1Et ≤ |R1 −R2|1Et + |R2 −R3|1Et + |R3 −R4|1Et + |R4|1Et\nFor each term, we can bound as:\n|R1 −R2|1Et ≤ηO(d2) ∥∥∥ŨtṼ⊤t −M ∥∥∥ ∞ O(χ) ≤ ηO(d2χ2κ) |R2 −R3|1Et ≤η2O(d4) ∥∥∥ŨtṼ⊤t −M ∥∥∥ 2\n∞ O(χ) ≤ ηO(d2χ2κ)\n|R3 −R4|1Et ≤η3O(d6) ∥∥∥ŨtṼ⊤t −M ∥∥∥ 3\n∞ O(χ) ≤ ηO(d2χ2κ)\n|R4|1Et ≤η4O(d8) ∥∥∥ŨtṼ⊤t −M ∥∥∥ 4\n∞ O(χ) ≤ ηO(d2χ2κ)\nThis gives in sum that, with probability 1:\n|R1|1Et ≤ ηO(d2χ2κ)1Et = ηO(µ2k2κ5)1Et\nFor the third inequality E[R211Et |Ft] ≤ η2O(µ 3k3κ6 d )1Et :\nER211Et ≤ 4 [ E(R1 −R2)21Et + E(R2 −R3)21Et + E(R3 −R4)21Et + ER241Et ]\nFor each term, we can bound as:\nE(R1 −R2)21Et ≤ η2O(d2) ∑\nij\n(ŨtṼ ⊤ t −M)2ij ( δilO(χ 2) + (ŨtṼ ⊤ t ) 2 lj(ŨtŨ ⊤ t ) 2 il )\n≤η2O(d2)max l′\n∥∥∥e⊤l′ (ŨtṼ⊤t −M) ∥∥∥ 2 ∑\ni\n( δilO(χ 2) + (ŨtṼ ⊤ t ) 2 lj(ŨtŨ ⊤ t ) 2 il ) ≤ η2O(d2χ3)\nE(R2 −R3)21Et ≤ η4O(d6) ∑\nij\n(ŨtṼ ⊤ t −M)4ij ( δilO(χ 2) + (ŨtŨ ⊤ t ) 4 li )\n≤η4O(d6) ∥∥∥ŨtṼ⊤t −M ∥∥∥ 2\n∞ max l′\n∥∥∥e⊤l′ (ŨtṼ⊤t −M) ∥∥∥ 2∑\ni\n( δilO(χ 2) + (ŨtŨ ⊤ t ) 4 li ) ≤ η2O(d2χ3)\nE(R3 −R4)21Et ≤ η6O(d10) ∑\nij\n|ŨtṼ⊤t −M|6ijδilO(χ2)\n≤η6O(d10) ∥∥∥ŨtṼ⊤t −M ∥∥∥ 4\n∞\n∥∥∥e⊤l (ŨtṼ⊤t −M) ∥∥∥ 2 O(χ2) ≤ η2O(d2χ3)\nER241Et ≤ η8O(d14) ∑\nij\n(ŨtṼ ⊤ t −M)8ijδilO(χ2)\n≤η8O(d14) ∥∥∥ŨtṼ⊤t −M ∥∥∥ 6\n∞\n∥∥∥e⊤l (ŨtṼ⊤t −M) ∥∥∥ 2 O(χ2) ≤ η2O(d2χ3)\nThis gives in sum that:\nER211Et ≤ η2O(d2χ3)1Et = η2O( µ3k3κ6\nd )1Et\nThis finishes the proof.\nProof of Proposition C.6. Similarly to the proof of Proposition C.5, we hide the term 1Et in all following equations. Reader should always think every term in this proof multiplied by 1Et . Recall that:\nf(Ũt+1, Ṽt+1) = f(Ut+1,Vt+1) = f(Ũt +∆U, Ṽt +∆V)\n= tr ( [(Ũt +∆U)(Ṽt +∆V)−M][(Ũt +∆U)(Ṽt +∆V)−M]⊤ )\n= f(Ũt, Ṽt) + 2tr(∆UṼ ⊤ t (ŨtṼ ⊤ t −M)⊤) + 2tr(∆VŨ⊤t (ŨtṼ⊤t −M)) +Q2\n= f(Ũt, Ṽt) +Q1\nBy expanding the polynomial, we can write out the first order term:\nQ1 −Q2 =2tr(∆UṼ⊤t (ŨtṼ⊤t −M)⊤) + 2tr(∆VŨ⊤t (ŨtṼ⊤t −M))\n=− 4ηd2(UV⊤ −M)ij ( e⊤j ṼtṼ ⊤ t (ŨtṼ ⊤ t −M)⊤ei + e⊤i ŨtŨ⊤t (ŨtṼ⊤t −M)ej )\nThe second order term:\nQ2 −Q3 =tr(∆UṼ ⊤ t Ṽt∆ ⊤ U) + tr(Ũt∆ ⊤ V∆VŨ ⊤ t ) + 2tr(∆UṼ ⊤ t ∆VŨ ⊤ t ) + 2tr(∆U∆ ⊤ V (ŨtṼ ⊤ t −M)⊤) =4η2d4(UV⊤ −M)2ij\n· (∥∥∥e⊤j ṼtṼ⊤t ∥∥∥ 2 + ∥∥∥e⊤i ŨtŨ⊤t ∥∥∥ 2 + (ṼtṼ ⊤ t )jj(ŨtŨ ⊤ t )ii + (ŨtṼ ⊤ t )ij(ŨtṼ ⊤ t −M)ij )\nThe third order term:\nQ3 −Q4 =2tr(∆UṼ⊤t ∆V∆⊤U) + 2tr(∆U∆⊤V∆VŨ⊤t )\n=− 16η3d6(UV⊤ −M)3ij ( (ṼtṼt)jj(ŨtṼt)ij + (ŨtṼt)ij(ŨtŨt)ii )\nThe forth order term:\nQ4 = tr(∆U∆ ⊤ V∆V∆ ⊤ U) = 16η 4d8(UV⊤ −M)4ij(ŨtṼt)2ij\nAgain, in addition to equation (23), (23), (24), (25), we also need following inequality:\n∥∥∥ŨtṼ⊤t −M ∥∥∥ ∞ 1Et = max ij |tr(e⊤i (ŨtṼ⊤t −M)ej)|1Et\n=max ij\n|tr(e⊤i (PX + PX⊥)(ŨtṼ⊤t −M)ej)|1Et\n≤max ij |tr(e⊤i PX(ŨtṼ⊤t −M)ej)|1Et +max ij |tr(e⊤i PX⊥ŨtṼ⊤t ej)|1Et\n≤max i\n∥∥∥e⊤i X ∥∥∥ ∥∥∥ŨtṼ⊤t −M ∥∥∥ F 1Et +max j ∥∥∥e⊤j WV ∥∥∥ ∥∥∥ŨtṼ⊤t −M ∥∥∥ F 1Et\n≤O(κ√χ) √\nf(Ut) (27)\nNow we are ready to prove Lemma.\nFor the first inequality E[Q21Et |Ft] ≤ η2O(µdkκ2)f(Ũt, Ṽt)1Et:\nE[Q21Et |Ft] ≤ E[|Q2 −Q3|1Et |Ft] + E[|Q3 −Q4|1Et |Ft] + E[|Q4|1Et |Ft]\nFor each term, we can bound as:\nE[|Q2 −Q3|1Et |Ft] ≤η2O(d2) ∑\nij\n(ŨtṼ ⊤ t −M)2ijO(χ) = η2O(d2χ)f(Ũt, Ṽt)\nE[|Q3 −Q4|1Et |Ft] ≤η3O(d4) ∑\nij\n|ŨtṼ⊤t −M|3ijO(χ)\n≤η3O(d4χ) ∥∥∥ŨtṼ⊤t −M ∥∥∥ ∞ f(Ũt, Ṽt) ≤ η2O(d2χ)f(Ũt, Ṽt)\nE[|Q4|1Et |Ft] ≤η4O(d6) ∑\nij\n(ŨtṼ ⊤ t −M)4ijO(χ)\n≤η4O(d6χ) ∥∥∥ŨtṼ⊤t −M ∥∥∥ 2\n∞ f(Ũt, Ṽt) ≤ η2O(d2χ)f(Ũt, Ṽt)\nThis gives in sum that\nE[Q21Et |Ft] ≤ η2O(d2χ)f(Ũt, Ṽt)1Et = η2O(µdkκ2)f(Ũt, Ṽt)1Et\nFor the second inequality |Q1|1Et ≤ ηO(µdkκ3)f(Ũt, Ṽt)1Et w.p 1:\n|Q1|1Et ≤ |Q1 −Q2|1Et + |Q2 −Q3|1Et + |Q3 −Q4|1Et + |Q4|1Et\nFor each term, we can bound as:\n|Q1 −Q2|1Et ≤ηO(d2) ∥∥∥ŨtṼ⊤t −M ∥∥∥ ∞ ∥∥∥ŨtṼ⊤t −M ∥∥∥ F O( √ χ) ≤ ηO(d2χκ)f(Ũt, Ṽt) |Q2 −Q3|1Et ≤η2O(d4) ∥∥∥ŨtṼ⊤t −M ∥∥∥ 2\n∞ O(χ) ≤ η2O(d4χ2κ2)f(Ũt, Ṽt) = ηO(d2χκ)f(Ũt, Ṽt)\n|Q3 −Q4|1Et ≤η3O(d6) ∥∥∥ŨtṼ⊤t −M ∥∥∥ 3\n∞ O(χ) ≤ ηO(d2χκ)f(Ũt, Ṽt)\n|Q4|1Et ≤η4O(d8) ∥∥∥ŨtṼ⊤t −M ∥∥∥ 4\n∞ O(χ) ≤ ηO(d2χκ)f(Ũt, Ṽt)\nThis gives in sum that, with probability 1:\n|Q1|1Et ≤ ηO(d2χκ)f(Ũt, Ṽt)1Et = ηO(µdkκ3)f(Ũt, Ṽt)1Et\nFor the third inequality E[Q211Et |Ft] ≤ η2O(µdkκ2)f2(Ũt, Ṽt)1Et :\nEQ211Et ≤ 4 [ E(Q1 −Q2)21Et + E(Q2 −Q3)21Et + E(Q3 −Q4)21Et + EQ241Et ]\nFor each term, we can bound as:\nE(Q1 −Q2)21Et ≤η2O(d2) ∑\nij\n(ŨtṼ ⊤ t −M)2ij ∥∥∥ŨtṼ⊤t −M ∥∥∥ 2\nF O(χ) ≤ η2O(d2χ)f2(Ũt, Ṽt)\nE(Q2 −Q3)21Et ≤η4O(d6) ∑\nij\n(ŨtṼ ⊤ t −M)4ijO(χ2)\n≤η4O(d6χ2) ∥∥∥ŨtṼ⊤t −M ∥∥∥ 2\n∞\n∥∥∥ŨtṼ⊤t −M ∥∥∥ 2\nF ≤ η4O(d6χ3κ2)f2(Ũt, Ṽt)\n≤η2O(d2χ)f2(Ũt, Ṽt) E(Q3 −Q4)21Et ≤η6O(d10) ∑\nij\n|ŨtṼ⊤t −M|6ijO(χ2)\n≤η6O(d10χ2) ∥∥∥ŨtṼ⊤t −M ∥∥∥ 4\n∞\n∥∥∥ŨtṼ⊤t −M ∥∥∥ 2\nF ≤ η2O(d2χ)f2(Ũt, Ṽt)\nEQ241Et ≤η8O(d14) ∑\nij\n(ŨtṼ ⊤ t −M)8ijδilO(χ2)\n≤η8O(d14χ2) ∥∥∥ŨtṼ⊤t −M ∥∥∥ 6\n∞\n∥∥∥ŨtṼ⊤t −M ∥∥∥ 2\nF ≤ η2O(d2χ)f2(Ũt, Ṽt)\nThis gives in sum that:\nEQ211Et ≤ η2O(d2χ)f2(Ũt, Ṽt)1Et = η2O(µdkκ2)f2(Ũt, Ṽt)1Et\nThis finishes the proof."
    } ],
    "references" : [ {
      "title" : "Simple, efficient, and neural algorithms for sparse coding",
      "author" : [ "Sanjeev Arora", "Rong Ge", "Tengyu Ma", "Ankur Moitra" ],
      "venue" : "arXiv preprint arXiv:1503.00778,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2015
    }, {
      "title" : "Fast online svd revisions for lightweight recommender systems",
      "author" : [ "Matthew Brand" ],
      "venue" : "In SDM,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2003
    }, {
      "title" : "Phase retrieval via matrix completion",
      "author" : [ "Emmanuel J Candes", "Yonina C Eldar", "Thomas Strohmer", "Vladislav Voroninski" ],
      "venue" : "SIAM Review,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2015
    }, {
      "title" : "Exact matrix completion via convex optimization",
      "author" : [ "Emmanuel J. Candès", "Benjamin Recht" ],
      "venue" : "Foundations of Computational Mathematics,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2009
    }, {
      "title" : "The youtube video recommendation system",
      "author" : [ "James Davidson", "Benjamin Liebald", "Junning Liu", "Palash Nandy", "Taylor Van Vleet", "Ullas Gargi", "Sujoy Gupta", "Yu He", "Mike Lambert", "Blake Livingston" ],
      "venue" : "In Proceedings of the fourth ACM conference on Recommender systems,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2010
    }, {
      "title" : "Global convergence of stochastic gradient descent for some non-convex matrix problems",
      "author" : [ "Christopher De Sa", "Kunle Olukotun", "Christopher Ré" ],
      "venue" : "arXiv preprint arXiv:1411.1134,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2014
    }, {
      "title" : "Escaping from saddle points—online stochastic gradient for tensor decomposition",
      "author" : [ "Rong Ge", "Furong Huang", "Chi Jin", "Yang Yuan" ],
      "venue" : "arXiv preprint arXiv:1503.02101,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2015
    }, {
      "title" : "Matrix completion has no spurious local minimum",
      "author" : [ "Rong Ge", "Jason D. Lee", "Tengyu Ma" ],
      "venue" : "arXiv preprint arXiv:1605.07272,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2016
    }, {
      "title" : "Understanding alternating minimization for matrix completion",
      "author" : [ "Marcus Hardt" ],
      "venue" : "In Foundations of Computer Science (FOCS),",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2014
    }, {
      "title" : "Computational limits for matrix completion",
      "author" : [ "Moritz Hardt", "Raghu Meka", "Prasad Raghavendra", "Benjamin Weitz" ],
      "venue" : "In COLT,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2014
    }, {
      "title" : "Computing matrix squareroot via non convex local search",
      "author" : [ "Prateek Jain", "Chi Jin", "Sham M Kakade", "Praneeth Netrapalli" ],
      "venue" : "arXiv preprint arXiv:1507.05854,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2015
    }, {
      "title" : "Matching matrix bernstein with little memory: Near-optimal finite sample guarantees for oja’s algorithm",
      "author" : [ "Prateek Jain", "Chi Jin", "Sham M Kakade", "Praneeth Netrapalli", "Aaron Sidford" ],
      "venue" : "arXiv preprint arXiv:1602.06929,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2016
    }, {
      "title" : "Fast exact matrix completion with finite samples",
      "author" : [ "Prateek Jain", "Praneeth Netrapalli" ],
      "venue" : "arXiv preprint arXiv:1411.1087,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2014
    }, {
      "title" : "Robust video denoising using low rank matrix completion",
      "author" : [ "Hui Ji", "Chaoqiang Liu", "Zuowei Shen", "Yuhong Xu" ],
      "venue" : null,
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2010
    }, {
      "title" : "Efficient algorithms for collaborative filtering",
      "author" : [ "Raghunandan Hulikal Keshavan" ],
      "venue" : "PhD thesis, STANFORD UNIVERSITY,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2012
    }, {
      "title" : "The BellKor solution to the Netflix grand prize",
      "author" : [ "Yehuda Koren" ],
      "venue" : null,
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2009
    }, {
      "title" : "Low-rank matrix and tensor completion via adaptive sampling",
      "author" : [ "Akshay Krishnamurthy", "Aarti Singh" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2013
    }, {
      "title" : "Gradient descent converges to minimizers",
      "author" : [ "Jason D Lee", "Max Simchowitz", "Michael I Jordan", "Benjamin Recht" ],
      "venue" : "University of California, Berkeley,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2016
    }, {
      "title" : "Amazon.com recommendations: item-to-item collaborative filtering",
      "author" : [ "G. Linden", "B. Smith", "J. York" ],
      "venue" : "IEEE Internet Computing,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2003
    }, {
      "title" : "Incremental collaborative filtering recommender based on regularized matrix factorization",
      "author" : [ "Xin Luo", "Yunni Xia", "Qingsheng Zhu" ],
      "venue" : "Knowledge-Based Systems,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2012
    }, {
      "title" : "Online learning for matrix factorization and sparse coding",
      "author" : [ "Julien Mairal", "Francis Bach", "Jean Ponce", "Guillermo Sapiro" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2010
    }, {
      "title" : "Gradient descent converges to minimizers: The case of non-isolated critical points",
      "author" : [ "Ioannis Panageas", "Georgios Piliouras" ],
      "venue" : "arXiv preprint arXiv:1605.00405,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2016
    }, {
      "title" : "A simple approach to matrix completion",
      "author" : [ "Benjamin Recht" ],
      "venue" : null,
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2009
    }, {
      "title" : "Parallel stochastic gradient algorithms for large-scale matrix completion",
      "author" : [ "Benjamin Recht", "Christopher Ré" ],
      "venue" : "Mathematical Programming Computation,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2013
    }, {
      "title" : "Guaranteed matrix completion via nonconvex factorization",
      "author" : [ "Ruoyu Sun", "Zhi-Quan Luo" ],
      "venue" : "In Foundations of Computer Science (FOCS),",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2015
    }, {
      "title" : "User-friendly tail bounds for sums of random matrices",
      "author" : [ "Joel A Tropp" ],
      "venue" : "Foundations of computational mathematics,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 13,
      "context" : "This problem arises in several applications such as video denoising [14], phase retrieval [3] and most famously in movie recommendation engines [16].",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 2,
      "context" : "This problem arises in several applications such as video denoising [14], phase retrieval [3] and most famously in movie recommendation engines [16].",
      "startOffset" : 90,
      "endOffset" : 93
    }, {
      "referenceID" : 15,
      "context" : "This problem arises in several applications such as video denoising [14], phase retrieval [3] and most famously in movie recommendation engines [16].",
      "startOffset" : 144,
      "endOffset" : 148
    }, {
      "referenceID" : 3,
      "context" : "The seminal works of Candès and Recht [4] first identified regularity conditions under which low rank matrix completion can be solved in polynomial time using convex relaxation – low rank matrix UC Berkeley.",
      "startOffset" : 38,
      "endOffset" : 41
    }, {
      "referenceID" : 9,
      "context" : "completion could be ill-posed and NP-hard in general without such regularity assumptions [10].",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 4,
      "context" : "However in several applications [5, 19], we encounter the online setting where observations are only revealed sequentially and at each step the recovery algorithm is required to maintain an estimate of the low rank matrix based on the observations so far.",
      "startOffset" : 32,
      "endOffset" : 39
    }, {
      "referenceID" : 18,
      "context" : "However in several applications [5, 19], we encounter the online setting where observations are only revealed sequentially and at each step the recovery algorithm is required to maintain an estimate of the low rank matrix based on the observations so far.",
      "startOffset" : 32,
      "endOffset" : 39
    }, {
      "referenceID" : 1,
      "context" : "On the other hand, in order to deal with the online matrix completion scenario in practical applications, several heuristics (with no convergence guarantees) have been proposed in literature [2, 20].",
      "startOffset" : 191,
      "endOffset" : 198
    }, {
      "referenceID" : 19,
      "context" : "On the other hand, in order to deal with the online matrix completion scenario in practical applications, several heuristics (with no convergence guarantees) have been proposed in literature [2, 20].",
      "startOffset" : 191,
      "endOffset" : 198
    }, {
      "referenceID" : 23,
      "context" : "Moreover, SGD, in the context of matrix completion, is also useful for parallelization and distributed implementation [24].",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 24,
      "context" : "While [25] shows that SGD updates stay away from saddle surfaces, the stepsizes they can handle are quite small (scaling as 1/poly(d1, d2)), leading to suboptimal computational complexity.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 22,
      "context" : "The nuclear norm relaxation algorithm [23] has near-optimal sample complexity for this problem but is computationally expensive.",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 14,
      "context" : "Motivated by the empirical success of non-convex heuristics, a long line of works, [15, 9, 13, 25] and so on, has obtained convergence guarantees for alternating minimization, gradient descent, projected gradient descent etc.",
      "startOffset" : 83,
      "endOffset" : 98
    }, {
      "referenceID" : 8,
      "context" : "Motivated by the empirical success of non-convex heuristics, a long line of works, [15, 9, 13, 25] and so on, has obtained convergence guarantees for alternating minimization, gradient descent, projected gradient descent etc.",
      "startOffset" : 83,
      "endOffset" : 98
    }, {
      "referenceID" : 12,
      "context" : "Motivated by the empirical success of non-convex heuristics, a long line of works, [15, 9, 13, 25] and so on, has obtained convergence guarantees for alternating minimization, gradient descent, projected gradient descent etc.",
      "startOffset" : 83,
      "endOffset" : 98
    }, {
      "referenceID" : 24,
      "context" : "Motivated by the empirical success of non-convex heuristics, a long line of works, [15, 9, 13, 25] and so on, has obtained convergence guarantees for alternating minimization, gradient descent, projected gradient descent etc.",
      "startOffset" : 83,
      "endOffset" : 98
    }, {
      "referenceID" : 14,
      "context" : "Our sample complexity is better than that of [15] and is incomparable to those of [9, 13].",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 8,
      "context" : "Our sample complexity is better than that of [15] and is incomparable to those of [9, 13].",
      "startOffset" : 82,
      "endOffset" : 89
    }, {
      "referenceID" : 12,
      "context" : "Our sample complexity is better than that of [15] and is incomparable to those of [9, 13].",
      "startOffset" : 82,
      "endOffset" : 89
    }, {
      "referenceID" : 24,
      "context" : "To the best of our knowledge, the only provable online algorithm for this problem is that of Sun and Luo [25].",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 16,
      "context" : ", [17, 27].",
      "startOffset" : 2,
      "endOffset" : 10
    }, {
      "referenceID" : 22,
      "context" : "Nuclear Norm [23] Õ(μdk) Õ(d3/ √ ǫ) No Alternating minimization [15] Õ(μdkκ8 log 1 ǫ ) Õ(μdk2κ8 log 1 ǫ ) No",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 14,
      "context" : "Nuclear Norm [23] Õ(μdk) Õ(d3/ √ ǫ) No Alternating minimization [15] Õ(μdkκ8 log 1 ǫ ) Õ(μdk2κ8 log 1 ǫ ) No",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 8,
      "context" : "Alternating minimization [9] Õ ( μdk2κ2 ( k + log 1 ǫ )) Õ ( μdk3κ2 ( k + log 1 ǫ )) No",
      "startOffset" : 25,
      "endOffset" : 28
    }, {
      "referenceID" : 12,
      "context" : "Projected gradient descent[13] Õ(μdk) Õ(μdk log 1 ǫ ) No",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 24,
      "context" : "SGD [25] Õ(μdkκ) poly(μ, d, k, κ) log 1 ǫ Yes SGD [8]1 d · poly(μ, k, κ) poly(μ, d, k, κ, 1 ǫ ) Yes Our result Õ ( μdkκ4 ( k + log 1 ǫ )) Õ ( μdk4κ4 log 1 ǫ ) Yes",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 7,
      "context" : "SGD [25] Õ(μdkκ) poly(μ, d, k, κ) log 1 ǫ Yes SGD [8]1 d · poly(μ, k, κ) poly(μ, d, k, κ, 1 ǫ ) Yes Our result Õ ( μdkκ4 ( k + log 1 ǫ )) Õ ( μdk4κ4 log 1 ǫ ) Yes",
      "startOffset" : 50,
      "endOffset" : 53
    }, {
      "referenceID" : 5,
      "context" : "eigenvector computation [6, 12], sparse coding [21, 1] etc.",
      "startOffset" : 24,
      "endOffset" : 31
    }, {
      "referenceID" : 11,
      "context" : "eigenvector computation [6, 12], sparse coding [21, 1] etc.",
      "startOffset" : 24,
      "endOffset" : 31
    }, {
      "referenceID" : 20,
      "context" : "eigenvector computation [6, 12], sparse coding [21, 1] etc.",
      "startOffset" : 47,
      "endOffset" : 54
    }, {
      "referenceID" : 0,
      "context" : "eigenvector computation [6, 12], sparse coding [21, 1] etc.",
      "startOffset" : 47,
      "endOffset" : 54
    }, {
      "referenceID" : 6,
      "context" : "For general non-convex optimization, an interesting line of recent work is that of [7], which proves gradient descent with noise can also escape saddle point, but they only provide polynomial rate without explicit dependence.",
      "startOffset" : 83,
      "endOffset" : 86
    }, {
      "referenceID" : 17,
      "context" : "Later [18, 22] show that without noise, the space of points from where gradient descent converges to a saddle point is a measure zero set.",
      "startOffset" : 6,
      "endOffset" : 14
    }, {
      "referenceID" : 21,
      "context" : "Later [18, 22] show that without noise, the space of points from where gradient descent converges to a saddle point is a measure zero set.",
      "startOffset" : 6,
      "endOffset" : 14
    }, {
      "referenceID" : 10,
      "context" : "Another related piece of work to ours is [11], proves global convergence along with rates of convergence, for the special case of computing matrix squareroot.",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 7,
      "context" : "During the preparation of this draft, the recent work [8] was announced which proves the global convergence of SGD for matrix completion and can also be applied to the online setting.",
      "startOffset" : 54,
      "endOffset" : 57
    }, {
      "referenceID" : 9,
      "context" : "This task is ill-posed and NP-hard in general [10].",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 3,
      "context" : "2 (μ-incoherence[4, 23]).",
      "startOffset" : 16,
      "endOffset" : 23
    }, {
      "referenceID" : 22,
      "context" : "2 (μ-incoherence[4, 23]).",
      "startOffset" : 16,
      "endOffset" : 23
    }, {
      "referenceID" : 0,
      "context" : "[1] Sanjeev Arora, Rong Ge, Tengyu Ma, and Ankur Moitra.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 1,
      "context" : "[2] Matthew Brand.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 2,
      "context" : "[3] Emmanuel J Candes, Yonina C Eldar, Thomas Strohmer, and Vladislav Voroninski.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "[4] Emmanuel J.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "[5] James Davidson, Benjamin Liebald, Junning Liu, Palash Nandy, Taylor Van Vleet, Ullas Gargi, Sujoy Gupta, Yu He, Mike Lambert, Blake Livingston, et al.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "[6] Christopher De Sa, Kunle Olukotun, and Christopher Ré.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 6,
      "context" : "[7] Rong Ge, Furong Huang, Chi Jin, and Yang Yuan.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 7,
      "context" : "[8] Rong Ge, Jason D.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 8,
      "context" : "[9] Marcus Hardt.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 9,
      "context" : "[10] Moritz Hardt, Raghu Meka, Prasad Raghavendra, and Benjamin Weitz.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "[11] Prateek Jain, Chi Jin, Sham M Kakade, and Praneeth Netrapalli.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "[12] Prateek Jain, Chi Jin, Sham M Kakade, Praneeth Netrapalli, and Aaron Sidford.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "[13] Prateek Jain and Praneeth Netrapalli.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 13,
      "context" : "[14] Hui Ji, Chaoqiang Liu, Zuowei Shen, and Yuhong Xu.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 14,
      "context" : "[15] Raghunandan Hulikal Keshavan.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "[16] Yehuda Koren.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "[17] Akshay Krishnamurthy and Aarti Singh.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "[18] Jason D Lee, Max Simchowitz, Michael I Jordan, and Benjamin Recht.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 18,
      "context" : "[19] G.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 19,
      "context" : "[20] Xin Luo, Yunni Xia, and Qingsheng Zhu.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 20,
      "context" : "[21] Julien Mairal, Francis Bach, Jean Ponce, and Guillermo Sapiro.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 21,
      "context" : "[22] Ioannis Panageas and Georgios Piliouras.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 22,
      "context" : "[23] Benjamin Recht.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 23,
      "context" : "[24] Benjamin Recht and Christopher Ré.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 24,
      "context" : "[25] Ruoyu Sun and Zhi-Quan Luo.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 25,
      "context" : "[26] Joel A Tropp.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 14,
      "context" : "Most of the argument of this section follows from [15].",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 25,
      "context" : "2 (Matrix Bernstein [26]).",
      "startOffset" : 20,
      "endOffset" : 24
    } ],
    "year" : 2016,
    "abstractText" : "Matrix completion, where we wish to recover a low rank matrix by observing a few entries from it, is a widely studied problem in both theory and practice with wide applications. Most of the provable algorithms so far on this problem have been restricted to the offline setting where they provide an estimate of the unknown matrix using all observations simultaneously. However, in many applications, the online version, where we observe one entry at a time and dynamically update our estimate, is more appealing. While existing algorithms are efficient for the offline setting, they could be highly inefficient for the online setting. In this paper, we propose the first provable, efficient online algorithm for matrix completion. Our algorithm starts from an initial estimate of the matrix and then performs non-convex stochastic gradient descent (SGD). After every observation, it performs a fast update involving only one row of two tall matrices, giving near linear total runtime. Our algorithm can be naturally used in the offline setting as well, where it gives competitive sample complexity and runtime to state of the art algorithms. Our proofs introduce a general framework to show that SGD updates tend to stay away from saddle surfaces and could be of broader interests for other non-convex problems to prove tight rates.",
    "creator" : "LaTeX with hyperref package"
  }
}