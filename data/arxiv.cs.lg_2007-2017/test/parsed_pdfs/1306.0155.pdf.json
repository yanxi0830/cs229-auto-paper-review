{
  "name" : "1306.0155.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Dynamic ad allocation: bandits with budgets",
    "authors" : [ "Aleksandrs Slivkins" ],
    "emails" : [ "slivkins@microsoft.com." ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n30 6.\n01 55\nv1 [\ncs .L\nG ]\n1 J\nun 2\nWe define a simple, stylized model where an algorithm picks one ad to display in each round, and each ad has a budget: the maximal amount of money that can be spent on this ad. This model admits a natural variant of UCB1, a well-known algorithm for multi-armed bandits with stochastic rewards. We derive strong provable guarantees for this algorithm."
    }, {
      "heading" : "1 Introduction",
      "text" : "Multi-armed bandits (henceforth, MAB), and more generally online decision problems with partial feedback and exploration-exploitation tradeoff, has been studied since 1930’s in Operations Research, Economics and several branches of Computer Science [26, 13, 16, 11]. Such problems arise in diverse domains, e.g., the design of medical experiments, dynamic pricing, and routing in the internet. In the past decade, a surge of interest in MAB problems has been due to their applications in web search and internet advertising.\nIn the most basic MAB problem [2], an algorithm repeatedly chooses among several possible actions (traditionally called arms), and observes the reward for the chosen arm. The rewards are stochastic: the reward from choosing a given arm is an independent sample from some distribution that depends on the arm but not on the round in which this arm is chosen. These reward distributions are not revealed to the algorithm. The algorithm’s goal is to maximize the total expected reward over the time horizon.\nThis paper is concerned with an application of MAB to Internet advertising. This application considers advertisers that derive value when users click on their ads. A predominant market design for such advertisers is pay-per-click: advertisers pay only when their ads are clicked. Users arrive over time, and an algorithm needs to choose which ads to show to each user. Both the ad market and the advertisers experience significant uncertainty on click probabilities;1 the estimates of CTRs can be refined over time. It is because of this uncertainty on CTRs that MAB are relevant to this application domain.\nA standard, and very stylized, way to model these ad-related issues in the MAB framework is as follows (e.g., see [22]). An algorithm chooses one ad in each round (so ads correspond to arms in MAB), and observes whether this ad is clicked on. For each click on every ad i, algorithm receives a fixed payment\n∗Microsoft Research Silicon Valley, Mountain View, CA 94043, USA. Email: slivkins@microsoft.com. Parts of this work has been done while visiting Microsoft Research New York.\n1Click probabilities are also called click-through rates in the industry, or CTRs for short.\nbi from the corresponding advertiser. Thus, the expected reward from showing ad i is equal to bi times the CTR for this ad. The CTRs are not initially known to the algorithm. The algorithm’s goal is to maximize the total expected reward.\nTo the best of our knowledge, prior work on MAB-based approaches to ad allocation has ignored an important practical issue: advertisers are constrained in how much money they can spend on their ad campaign. In particular, each advertiser typically has a budget: the maximal amount of money she is allowed to spend. This is the issue that we focus on in this paper.\n1.1 Problem formulation: BudgetedAdsMAB\nThere are k advertisers (arms), each with one ad that she wishes to be displayed. Each ad i is characterized by the following three quantities: CTR µi ∈ [0, 1], payment-per-click bi and budget Bi. The payments-perclick and the budgets are revealed to the algorithm, but the CTRs are not.\nIn each round an algorithm picks one ad. This ad is displayed (receives an impression), and the algorithm observes whether this ad is clicked on. The click on a given ad i happens independently (from everything else), with probability µi. If ad i is clicked, the corresponding advertiser is charged bi, and her remaining budget is decreased by this amount. An arm is available (can be chosen) in a given round only if its remaining budget is above bi. There is a time horizon T . The goal of the algorithm is to maximize its expected total reward, where the total reward is the sum of all charges.\nThis is a non-Bayesian (prior-independent) formulation: there are no priors on the CTRs that are available to the algorithm, and we are looking for guarantees that hold for any prior.\nThe expected value of one impression of arm i is wi , bi µi. For ease of exposition, we re-order the arms so that w1 ≥ w2 ≥ . . . ≥ wk. Benchmark. We use the omniscient benchmark, standard benchmark in the literature on MAB and related problems. This is the best algorithm that knows all latent information in the problem instance (in this case, the CTRs). In this problem, the omniscient benchmark is very simple: play arm 1 while it is available, then play arm 2 while it is available, and so on. Call it the greedy benchmark. Performance of an algorithm A is measured as greedy regret (regret with respect to the greedy benchmark), defined as expected reward of the greedy benchmark minus the expected reward of the algorithm. Denote it Regret(A).\nIt is worth noting that, given the optimality of the greedy benchmark, the best fixed arm – another standard benchmark in the literature on MAB – is not informative for our setting."
    }, {
      "heading" : "1.2 Our contributions",
      "text" : "We consider a natural algorithm and prove that it works quite well. While the algorithm is essentially the first thing a researcher familiar with prior work on MAB would suggest, our technical contribution is the analysis of this algorithm, and particularly the “coupling argument” therein. The conceptual contribution is that we provide an assurance that the natural approach works, from a theoretical point of view, and suggest the strengths and limitations of this approach.\nOur algorithm, called BudgetedUCB, is a natural modification of UCB1 [2], a well-known algorithm for MAB with stochastic rewards. UCB1 maintains a numerical score (index) for each arm, and in every round chooses an arm with the largest index. The index of arm i is, essentially, the best available upper confidence bound on the expected reward from this arm. BudgetedUCB chooses, in each round, an arm with the maximal index among all available arms. (So the two algorithms coincide if the budgets are infinite.)\nWe formulate our provable guarantees in terms of the last arm whose budget is exhausted by the greedy benchmark. (Recall that the arms i are ordered in the order of decreasing wi = bi µi.) Denote this last arm iB if it exists; set iB = 0 otherwise. Since iB is a random variable, the regret bound is in expectation over the\nrandomness in iB. For most problem instances iB is highly concentrated: it is typically within ±1 from its expectation.\nTheorem 1.1. Consider BudgetedAdsMAB. For each ǫ > 0 it holds that\nRegret(BudgetedUCB) ≤ ǫT +O(log T ) E\n\n max i∈{iB, iB+1}\nk ∑\nj=i+1\nb2j max(ǫ, wi − wj)\n\n , (1)\nwhere the expectation is over the randomness in iB.\nThe regret bound (1) is driven by the differences ∆(i) = wi − wi+1, more specifically by the random quantity ∆(iB). We derive a “pessimistic” corollary for the case when ∆(iB) may be arbitrarily small, and an “optimistic” corollary for the case of large ∆(iB). 2\nCorollary 1.2. Consider BudgetedAdsMAB. Denote v2 = 1 k ∑k j=1 b 2 j .\n(a) Regret(BudgetedUCB) ≤ O(v √ kT log T ).\n(b) Regret(BudgetedUCB) ≤ O ( k δ v2 log T ) for any δ > 0 such that Pr[∆(iB) ≥ δ] ≥ 1− (v/T )2.\nThe regret bounds in this corollary extend the corresponding “pessimistic” and “optimistic” guarantees for UCB1 from the special case of MAB with stochastic rewards (i.e., no budgets and bj ≡ 1) to the full generality of BudgetedAdsMAB.3 Both guarantees are nearly optimal for this special case, respectively up to O(log T ) factors and up to constant factors [20, 2, 3].\nInterestingly, all above regret bounds do not depend on the budgets."
    }, {
      "heading" : "1.3 Discussion",
      "text" : "One common criticism of the work on non-Bayesian (prior-independent, regret-minimizing) MAB problems is that the algorithmic ideas and proof techniques introduced for the numerous MAB models studied in the literature are too specific to their respective models, and do not easily generalize to more general settings that are common in applications. In view of this criticism, it is useful to identify general ideas and techniques that one can build on when working on the (more) general settings, and provide concrete examples of how one can build on them. The present paper contributes to this direction: we build on the algorithmic idea of “UCB indices”, and a certain proof technique to analyze them (both from [2]). These ideas have been tremendously useful in several other MAB settings with stochastic rewards e.g. [19, 30, 12, 24, 1].\nIt is worth noting that BudgetedUCB does not need to input the budgets: instead, it can be implemented via an oracle that determines whether a given arm is available in a given round. In other words, advertisers do not need to submit their budgets upfront; instead, they only need to notify the algorithm whether they are still willing to participate in a given round. This is useful because an advertiser may be reluctant to commit to a specific budget and/or reveal it early in her ad campaign. Also, she may choose to strategically misreport the budget if asked.\n2To derive Corollary 1.2, we pick ǫ = √ log T\nkT in Equation (1) for part (a), and ǫ = kv2/T 2 for part (b).\n3Without budgets, we have iB = 1 and therefore the assumption in Corollary 1.2(b) reduces to ∆(1) ≥ δ.\n2 Our algorithm: BudgetedUCB\nOur algorithm, called BudgetedUCB, is a natural extension of the well-known algorithm UCB1 [2]. For each arm i and time t, let ci(t) and ni(t) be, respectively, the number of clicks and the number of impressions of this arm up to (but not including) time t. Define the confidence radius of arm i as\nri(t) , C\n√\nlog T\n1 + ni(t) . (2)\nHere C is some constant to be chosen later. Informally, the meaning of ri(t) is that\n|µi(t)− νi(t)| ≤ ri(t) (3)\nholds with high probability, where νi(t) , ci(t)/ni(t) is the (current) average CTR. Define the UCB index of arm i as\nIi(t) , bi(νi(t) + ri(t)).\nNote that the index of arm i is an upper confidence bound (UCB) on the quantity biµi which represents the expected value of one impression of i.\nNow that the index is defined, the algorithm is very simple: among available arms, pick an arm with the maximal index, breaking ties arbitrarily.\nDiscussion. The original algorithm UCB1 in [2] is, essentially, a special case of BudgetedUCBwhen all arms are available and all values are bi = 1. Moreover, the algorithm in [19] for sleeping bandits with stochastic rewards coincides with ours for bi ≡ 1 (but the analysis from [19] does not carry over to our setting, see Section 4 for more discussion).\nMost likely, the log T in the definition of the confidence radius can be replaced by log t, which should lead to improved constant factors in the regret bounds. In particular, the algorithms in [2] and [19] have log t there. We use log T because it makes our analysis easier, and increases the regret by at most a constant factor."
    }, {
      "heading" : "3 Analysis: proof of Theorem 1.1",
      "text" : "The technical contribution of this paper is the analysis of BudgetedUCB. The crux thereof is the “coupling argument” encapsulated in Lemma 3.5. To argue about random clicks, an important conceptual step is to consider two different representations of realized clicks (defined below). Also, we build on the technique from the analysis of UCB1 [2], which is encapsulated in Lemma 3.4.\nNotation. Consider an execution of BudgetedUCB. For each arm i, let ni(t) be the number of impressions of arm i before round t. Let ni = ni(T + 1) be the total number of impressions from arm i. Let ~n = (n1, . . . , nk) be the impressions vector for BudgetedUCB. Similarly, let ~m be the impressions vector for the greedy benchmark. Note that ~n and ~m are random variables. Let ~w = (w1, . . . , wk), where wi = bi µi.\nClick realizations. We will use two ways to represent the realization of the random clicks. Each representation is a 0-1 matrix, denoted Y = (Yi,t) and Y ′ = (Y ′i,t) respectively, where rows i range over ads and columns t range over rounds. The first representation, called per-round realization, is as follows: if arm i is selected in round t then it is clicked if and only if Yi,t = 1. The second realization, called the stack realization, is as follows: the t-th time arm i is selected, it is clicked if and only if Y ′i,t = 1. Note that for each pair (i, t), both Yi,t and Y ′i,t are independent 0-1 random variables with expectation µi.\nWhile each of the two representations suffices to formally represent the random clicks, we find it convenient to use both. In particular, the per-round realization is used in Claim 3.1, and the stack realization is used in Claim 3.3 and in the coupling argument in Lemma 3.5.\nClaim 3.1. E[Reward(BudgetedUCB)] = E[~n · ~w]. Proof. Let Xit ∈ {0, 1} be 1 if and only if arm i is selected in round t. Let {Yi,t} be the per-round realization. Since for each pair (i, t) the random variables Xi,t and Yi,t are mutually independent, it follows that\nE[Xi,t Yi,t] = E[Xi,t]E[Yi,t] = µi E[Xi,t].\nNoting that Reward(BudgetedUCB) = ∑\ni,t biXi,tYi,t, we have\nE[Reward(BudgetedUCB)] = ∑\ni,t bi E[Xi,tYi,t]\n= ∑\ni,t bi µi E[Xi,t]\n= ∑ i bi µi E[ ∑ tXi,t] = ∑\niwi E[ni].\nSimilarly, expected reward of the greedy benchmark is E[~n · ~w]. Corollary 3.2. Regret(BudgetedUCB) = E[(~m− ~n) · ~w].\nWe pick the constant C in Equation (2) so that Equation (3) holds with really high probability, so that the failure event when Equation (3) does not hold can, essentially, be ignored in the analysis.4\nClaim 3.3. With probability at least 1− 1 T , for each arm i and each time t Equation (3) holds. Proof Sketch. Consider the stack realization (Y ′i,t). For each arm i and each time t, apply Chernoff Bounds to the sum ∑t\ns=1 Y ′ i,t (which is the number of clicks in the first t times that arm i is selected). Then take the\nUnion Bound over all i and all t.\nIn the rest of the proof we will assume without further notice that the event Equation (3) holds for each arm i and each time t. Essentially, we will argue deterministically from now on, whereas all “probabilistic” reasoning is contained in Claim 3.1 and Claim 3.3.\nThe following lemma says that each sub-optimal arm is not played too often. This is the crucial part of a UCB-style analysis, and it incorporates the main trick from the original analysis in [2].\nLemma 3.4. Let i∗j be the best (lowest numbered) available arm at the last time when arm j has been selected. Then for each arm j such that j 6= i∗j it holds that\nnj ≤ O(log T ) (\nbj w(i∗j )−w(j)\n)2\n. (4)\nProof. We will use the fact that by Equation (3) for each arm j and each arm t it holds that\nwj ≤ Ij(t) ≤ wj + 2 bj rj(t). Let t be the last round when arm i has been selected, and denote i = i∗j . Since arm i has been selected\nin round t, it must have had the highest index at the time. Therefore\nwi ≤ Ii(t) ≤ Ij(t) ≤ wj + 2 bj rj(t).\nIt follows that wi − wj ≤ 2 bj rj(t) = O(bj) √\nlog T nj , which implies the desired bound (4).\n4While C = 10 suffices for the analysis, prior work on UCB1-style algorithms (e.g. in [23, 25]) suggests that a smaller value such as C = 1 can be used in practice.\nFrom now on assume that BudgetedUCB and the greedy benchmark are run on the same stack realization. Arguments in which two random processes are run on a joint probability distribution (coupled) with the same marginal distributions for each process are known in Probability Theory as coupling arguments.\nWe encapsulate the coupling argument in the following lemma. To state this lemma, recall that iB is the last (highest-numbered) arm exhausted by the greedy benchmark if such arm exists, and 0 otherwise. Let iA be the best (lowest-numbered) arm that is not exhausted by BudgetedUCB.\nLemma 3.5. (~m− ~n) · ~w ≤ ∑k j=max(iA,iB)+1 nj(wiA − wj) where iA ≤ iB + 1.\nProof. We consider three cases. The first case is when no arms are exhausted by the greedy benchmark. Then iB = 0, and the greedy benchmark played arm 1 for T rounds, so m1 = T and mj = 0 for all j ≥ 2. Therefore:\n(~m− ~n) · ~w = (T − n1)w1 − ∑k j=2 nj wj = ∑k j=2 nj(w1 − wj). Moreover, since the greedy benchmark has not exhausted arm 1, BudgetedUCB has not exhausted it either, so iA = 1 and we are done.\nFor the other two cases let us assume that the greedy benchmark exhausts at least one arm (i.e., iB ≥ 1). We claim that for each arm i ≤ iB it holds that ni ≤ mi. Indeed, the greedy benchmark exhausts each arm i ≤ iB, and, since BudgetedUCB and the greedy benchmark use the same stack realization, BudgetedUCB would also exhaust arm i after ni impressions, after which this arm would not be available. Claim proved.\nThe second case is that iB ≥ 1 and nj = mj for each arm j ≤ iB. Then iA = iB + 1. (Indeed, if BudgetedUCB exhausted arm iB+1 then the greedy benchmark would have also exhausted it, contradiction.) Let i = iA and note that ni ≤ mi. It follows that\n(~m− ~n) · ~w = ∑j≥i (mj − nj)wj = (mi − ni)wi − ∑ j≥i+1 nj wj\n= ∑ j≥i+1 nj (wi − wj).\nThe remaining third case is that iB ≥ 1 and nj < mj for some arm j ≤ iB. Then iA is the lowestnumbered such arm; in particular, iA ≤ iB. Let i = iA and ℓ = iB + 1. Note that we do not know whether nℓ ≤ mℓ, and so we have to allow for the possibility that nℓ > mℓ. Then:\n∑\nj≤iB (mj − nj)wj ≤ mwi where m ,\n∑\nj≤iB (mj − nj)wj .\n(~m− ~n) · ~w ≤ mwi − (nℓ −mℓ)wℓ − ∑ j≥ℓ+1 njwj\n= ∑ j≥ℓ+1 nj(wi − wj) + (nℓ −mℓ)(wi − wℓ) = ∑\nj≥ℓ nj(wi − wj).\nThis completes the third case. In all three cases we regroup the terms in the sums using the fact that ∑\ni ni = ∑ i mi = T .\nLet i = max(iA, iB) and let S = {j > i : wiA −wj ≥ ǫ}. Then ∑k\nj=i+1 nj(wiA − wj) ≤ ǫT + ∑ j∈S nj(wiA − wj).\nBy Lemma 3.4, noting that i∗j ≤ iA, we have for each j > i that\nnj ≤ O(b2j log T )\n(w(i∗j )− w(j))2 ≤\nO(b2j log T ) (wiA − wj)2 .\nPutting it all together, we obtain the following:\n(~m− ~n) · ~w ≤ ǫT + ∑\nj∈S\nO(b2j log T )\nwiA − wj . (5)\nFor Theorem 1.1 we use a somewhat weaker corollary of Equation (5) which gets rid of iA.\n(~m− ~n) · ~w ≤ ǫT + max i∈{iB, iB+1}\nk ∑\nj=i+1\nO(b2j log T )\nmax(ǫ, wi − wj) . (6)\nUsing Corollary 3.2 and taking expectations in both sides of Equation (6), we obtain the desired regret bound (1) in Theorem 1.1."
    }, {
      "heading" : "4 Related work",
      "text" : "MAB has been an active area of investigation since 1933 [27], in Operations Research, Economics and several branches of Computer Science: machine learning, theoretical computer science, AI, and algorithmic economics. A survey of prior work on MAB is beyond the scope of this paper; a reader is encouraged to refer to [13, 11] for background on prior-independent MAB, and to [26, 16] for background on Bayesian MAB. Starting from [22], much of the work on MAB has been motivated by internet advertising. Below we only discuss the work directly relevant to this paper.\nThe present paper continues the line of work on prior-independent MAB with stochastic rewards (where the reward of a given arm i is an i.i.d. sample of some time-invariant distribution). The basic formulation for MAB with stochastic rewards is well-understood ([20, 2] and the follow-up work, see [11] for references and discussion).\nOur formulation is a special case of sleeping bandits [19, 24] where in each round, a subset of arms is not available (“asleep”) and the goal is to compete with the best available arm. Available arms for a given round are chosen by an adversary. However, this adversary in [19, 24] is oblivious (it decides its selections for all rounds before round 1), whereas in our problem it is adaptive (it decides its selection for round t only after observing what happened before). This is a significant complication. To the best of our knowledge, the results in [19, 24] do not extend to settings where available arms are chosen by an adaptive adversary.\nSleeping bandits are in turn a special case of contextual bandits, where in each round an oblivious adversary provides a context x which determines which arms are available and, moreover, what are the expected payoffs in this round. The goal is to compete with the best (available) arm for a given context. Contextual bandits have been a subject of much recent work, see [11] for a survey.\nSeveral recent papers consider MAB problems with a single limited resource that is consumed by the arms. In such problems, each round yields a reward and a resource consumption, both of which may (stochastically) depend on the chosen arm. A typical example is “dynamic selling”[10, 4], where a seller has a limited supply of items and offers one item for sale in each round; the arms correspond to the offered prices. Other examples include “dynamic buying” [7] (where a buyer has a limited budget of money and interacts with a new seller in each round), and several versions in which the resource consumption for a given arm is deterministic [17, 18, 28, 29]. To the best of our knowledge, no published prior work has addressed MAB with multiple resources / budgets.\nA very recent, yet unpublished, paper [8], concurrent with respect to this paper, considers a generalization of our setting in which the budgets can be specified for arbitrary subsets of ads. They design new algorithms, based on techniques that are very different from ours. (Their algorithms and their analysis extend to a very general setting of MAB with arbitrary knapsack-style constraints, for which ad allocation is\none of the application domains.) However, the guarantees in [8] for BudgetedAdsMAB are much weaker than ours. Essentially, they obtain regret O( √ kT (1 + √\nT/B)), where B is the smallest budget; this is not a very strong guarantee if B is small. Moreover, their analysis does not imply an “optimistic” corollary similar to Corollary 1.2(b).\nAd allocation. A large amount of work has addressed ad allocation in the internet settings. Most papers in this area do not consider the issue of uncertainty on the CTRs. Some of the prominent themes is online matching (of ads and webpages) and the design of ad auctions (where the key issue is that the advertisers may strategically manipulate their bids if it benefits them). A more detailed discussion of this work is beyond the scope of this paper; see Chapter 28 of [21] for background.\nIn the literature on ad auctions, most relevant to our work are the papers that address the strategic issues jointly with the issue of uncertainty on CTRs and/or advertisers’ values-per-click (if these values change over time). There are two somewhat distinct directions: dynamic auctions, in which the advertisers submit bids over time (see [9] for a survey), and MAB mechanisms [6, 14, 5, 15], where the advertisers submit bids only once, and the mechanism allocates ads over time."
    }, {
      "heading" : "Acknowledgements",
      "text" : "The author would like to thank Ashwin Badanidiyuru, Sebastien Bubeck and Robert Kleinberg for many stimulating conversations about multi-armed bandits."
    } ],
    "references" : [ {
      "title" : "Improved algorithms for linear stochastic bandits",
      "author" : [ "Y. Abbasi-Yadkori", "D. Pál", "C. Szepesvári" ],
      "venue" : "25th Advances in Neural Information Processing Systems (NIPS), pages 2312–2320,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Finite-time analysis of the multiarmed bandit problem",
      "author" : [ "P. Auer", "N. Cesa-Bianchi", "P. Fischer" ],
      "venue" : "Machine Learning, 47(2-3):235–256, 2002. Preliminary version in 15th ICML,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "The nonstochastic multiarmed bandit problem",
      "author" : [ "P. Auer", "N. Cesa-Bianchi", "Y. Freund", "R.E. Schapire" ],
      "venue" : "SIAM J. Comput., 32(1):48–77, 2002. Preliminary version in 36th IEEE FOCS,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Dynamic pricing with limited supply",
      "author" : [ "M. Babaioff", "S. Dughmi", "R. Kleinberg", "A. Slivkins" ],
      "venue" : "13th ACM Conf. on Electronic Commerce (EC),",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Truthful mechanisms with implicit payment computation",
      "author" : [ "M. Babaioff", "R. Kleinberg", "A. Slivkins" ],
      "venue" : "11th ACM Conf. on Electronic Commerce (EC), pages 43–52,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Characterizing truthful multi-armed bandit mechanisms",
      "author" : [ "M. Babaioff", "Y. Sharma", "A. Slivkins" ],
      "venue" : "10th ACM Conf. on Electronic Commerce (EC), pages 79–88,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Learning on a budget: posted price mechanisms for online procurement",
      "author" : [ "A. Badanidiyuru", "R. Kleinberg", "Y. Singer" ],
      "venue" : "13th ACM Conf. on Electronic Commerce (EC), pages 128–145,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Bandits with knapsacks",
      "author" : [ "A. Badanidiyuru", "R. Kleinberg", "A. Slivkins" ],
      "venue" : "A technical report on arxiv.org., May",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Dynamic auctions: A survey",
      "author" : [ "D. Bergemann", "M. Said" ],
      "venue" : "Wiley Encyclopedia of Operations Research and Management Science. John Wiley & Sons,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Dynamic pricing without knowing the demand function: Risk bounds and near-optimal algorithms",
      "author" : [ "O. Besbes", "A. Zeevi" ],
      "venue" : "Operations Research, 57:1407–1420,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems",
      "author" : [ "S. Bubeck", "N. Cesa-Bianchi" ],
      "venue" : "Foundations and Trends in Machine Learning, 5(1):1–122,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Open Loop Optimistic Planning",
      "author" : [ "S. Bubeck", "R. Munos" ],
      "venue" : "23rd Conf. on Learning Theory (COLT), pages 477–489,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Prediction, learning, and games",
      "author" : [ "N. Cesa-Bianchi", "G. Lugosi" ],
      "venue" : "Cambridge Univ. Press,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "The price of truthfulness for pay-per-click auctions",
      "author" : [ "N. Devanur", "S.M. Kakade" ],
      "venue" : "10th ACM Conf. on Electronic Commerce (EC), pages 99–106,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "A Truthful Learning Mechanism for Contextual Multi-Slot Sponsored Search Auctions with Externalities",
      "author" : [ "N. Gatti", "A. Lazaric", "F. Trovo" ],
      "venue" : "13th ACM Conf. on Electronic Commerce (EC),",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Multi-Armed Bandit Allocation Indices",
      "author" : [ "J. Gittins", "K. Glazebrook", "R. Weber" ],
      "venue" : "John Wiley & Sons,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Multi-armed bandits with metric switching costs",
      "author" : [ "S. Guha", "K. Munagala" ],
      "venue" : "Proc. 36th International Colloquium on Automata, Languages, and Programming (ICALP), pages 496–507,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Approximation algorithms for correlated knapsacks and non-martingale bandits",
      "author" : [ "A. Gupta", "R. Krishnaswamy", "M. Molinaro", "R. Ravi" ],
      "venue" : "52nd IEEE Symp. on Foundations of Computer Science (FOCS), pages 827–836,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Regret bounds for sleeping experts and bandits",
      "author" : [ "R. Kleinberg", "A. Niculescu-Mizil", "Y. Sharma" ],
      "venue" : "21st Conf. on Learning Theory (COLT), pages 425–436,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Asymptotically efficient Adaptive Allocation Rules",
      "author" : [ "T.L. Lai", "H. Robbins" ],
      "venue" : "Advances in Applied Mathematics, 6:4–22,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 1985
    }, {
      "title" : "Algorithmic Game Theory",
      "author" : [ "N. Nisan", "T. Roughgarden", "E. Tardos", "V.V. (eds" ],
      "venue" : null,
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2007
    }, {
      "title" : "Bandits for Taxonomies: A Model-based Approach",
      "author" : [ "S. Pandey", "D. Agarwal", "D. Chakrabarti", "V. Josifovski" ],
      "venue" : "SIAM Intl. Conf. on Data Mining (SDM),",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Learning diverse rankings with multi-armed bandits",
      "author" : [ "F. Radlinski", "R. Kleinberg", "T. Joachims" ],
      "venue" : "25th Intl. Conf. on Machine Learning (ICML), pages 784–791,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Contextual Bandits with Similarity Information",
      "author" : [ "A. Slivkins" ],
      "venue" : "24th Conf. on Learning Theory (COLT),",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Learning optimally diverse rankings over large document collections",
      "author" : [ "A. Slivkins", "F. Radlinski", "S. Gollapudi" ],
      "venue" : "J. of Machine Learning Research (JMLR), 14(Feb):399–436, 2013. Preliminary version in 27th ICML,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Generalized Bandit Problems",
      "author" : [ "R.K. Sundaram" ],
      "venue" : "D. Austen-Smith and J. Duggan, editors, Social Choice and Strategic Decisions: Essays in Honor of Jeffrey S. Banks (Studies in Choice and Welfare), pages 131–162. Springer, 2005. First appeared as Working Paper, Stern School of Business,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples",
      "author" : [ "W.R. Thompson" ],
      "venue" : "Biometrika, 25(3-4):285294,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 1933
    }, {
      "title" : "ǫ-first policies for budget-limited multi-armed bandits",
      "author" : [ "L. Tran-Thanh", "A. Chapman", "E.M. de Cote", "A. Rogers", "N.R. Jennings" ],
      "venue" : "In Proc. Twenty-Fourth AAAI Conference on Artificial Intelligence",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2010
    }, {
      "title" : "Knapsack based optimal policies for budgetlimited multi-armed bandits",
      "author" : [ "L. Tran-Thanh", "A. Chapman", "A. Rogers", "N.R. Jennings" ],
      "venue" : "Proc. Twenty-Sixth AAAI Conference on Artificial Intelligence (AAAI-12), pages 1134–1140,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Algorithms for Infinitely Many-Armed Bandits",
      "author" : [ "Y. Wang", "J.-Y. Audibert", "R. Munos" ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS), pages 1729–1736,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 25,
      "context" : "Multi-armed bandits (henceforth, MAB), and more generally online decision problems with partial feedback and exploration-exploitation tradeoff, has been studied since 1930’s in Operations Research, Economics and several branches of Computer Science [26, 13, 16, 11].",
      "startOffset" : 249,
      "endOffset" : 265
    }, {
      "referenceID" : 12,
      "context" : "Multi-armed bandits (henceforth, MAB), and more generally online decision problems with partial feedback and exploration-exploitation tradeoff, has been studied since 1930’s in Operations Research, Economics and several branches of Computer Science [26, 13, 16, 11].",
      "startOffset" : 249,
      "endOffset" : 265
    }, {
      "referenceID" : 15,
      "context" : "Multi-armed bandits (henceforth, MAB), and more generally online decision problems with partial feedback and exploration-exploitation tradeoff, has been studied since 1930’s in Operations Research, Economics and several branches of Computer Science [26, 13, 16, 11].",
      "startOffset" : 249,
      "endOffset" : 265
    }, {
      "referenceID" : 10,
      "context" : "Multi-armed bandits (henceforth, MAB), and more generally online decision problems with partial feedback and exploration-exploitation tradeoff, has been studied since 1930’s in Operations Research, Economics and several branches of Computer Science [26, 13, 16, 11].",
      "startOffset" : 249,
      "endOffset" : 265
    }, {
      "referenceID" : 1,
      "context" : "In the most basic MAB problem [2], an algorithm repeatedly chooses among several possible actions (traditionally called arms), and observes the reward for the chosen arm.",
      "startOffset" : 30,
      "endOffset" : 33
    }, {
      "referenceID" : 21,
      "context" : ", see [22]).",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 0,
      "context" : "Each ad i is characterized by the following three quantities: CTR μi ∈ [0, 1], payment-per-click bi and budget Bi.",
      "startOffset" : 71,
      "endOffset" : 77
    }, {
      "referenceID" : 1,
      "context" : "Our algorithm, called BudgetedUCB, is a natural modification of UCB1 [2], a well-known algorithm for MAB with stochastic rewards.",
      "startOffset" : 69,
      "endOffset" : 72
    }, {
      "referenceID" : 19,
      "context" : "3 Both guarantees are nearly optimal for this special case, respectively up to O(log T ) factors and up to constant factors [20, 2, 3].",
      "startOffset" : 124,
      "endOffset" : 134
    }, {
      "referenceID" : 1,
      "context" : "3 Both guarantees are nearly optimal for this special case, respectively up to O(log T ) factors and up to constant factors [20, 2, 3].",
      "startOffset" : 124,
      "endOffset" : 134
    }, {
      "referenceID" : 2,
      "context" : "3 Both guarantees are nearly optimal for this special case, respectively up to O(log T ) factors and up to constant factors [20, 2, 3].",
      "startOffset" : 124,
      "endOffset" : 134
    }, {
      "referenceID" : 1,
      "context" : "The present paper contributes to this direction: we build on the algorithmic idea of “UCB indices”, and a certain proof technique to analyze them (both from [2]).",
      "startOffset" : 157,
      "endOffset" : 160
    }, {
      "referenceID" : 18,
      "context" : "[19, 30, 12, 24, 1].",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 29,
      "context" : "[19, 30, 12, 24, 1].",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 11,
      "context" : "[19, 30, 12, 24, 1].",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 23,
      "context" : "[19, 30, 12, 24, 1].",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 0,
      "context" : "[19, 30, 12, 24, 1].",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 1,
      "context" : "Our algorithm, called BudgetedUCB, is a natural extension of the well-known algorithm UCB1 [2].",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 1,
      "context" : "The original algorithm UCB1 in [2] is, essentially, a special case of BudgetedUCBwhen all arms are available and all values are bi = 1.",
      "startOffset" : 31,
      "endOffset" : 34
    }, {
      "referenceID" : 18,
      "context" : "Moreover, the algorithm in [19] for sleeping bandits with stochastic rewards coincides with ours for bi ≡ 1 (but the analysis from [19] does not carry over to our setting, see Section 4 for more discussion).",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 18,
      "context" : "Moreover, the algorithm in [19] for sleeping bandits with stochastic rewards coincides with ours for bi ≡ 1 (but the analysis from [19] does not carry over to our setting, see Section 4 for more discussion).",
      "startOffset" : 131,
      "endOffset" : 135
    }, {
      "referenceID" : 1,
      "context" : "In particular, the algorithms in [2] and [19] have log t there.",
      "startOffset" : 33,
      "endOffset" : 36
    }, {
      "referenceID" : 18,
      "context" : "In particular, the algorithms in [2] and [19] have log t there.",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 1,
      "context" : "Also, we build on the technique from the analysis of UCB1 [2], which is encapsulated in Lemma 3.",
      "startOffset" : 58,
      "endOffset" : 61
    }, {
      "referenceID" : 1,
      "context" : "This is the crucial part of a UCB-style analysis, and it incorporates the main trick from the original analysis in [2].",
      "startOffset" : 115,
      "endOffset" : 118
    }, {
      "referenceID" : 22,
      "context" : "in [23, 25]) suggests that a smaller value such as C = 1 can be used in practice.",
      "startOffset" : 3,
      "endOffset" : 11
    }, {
      "referenceID" : 24,
      "context" : "in [23, 25]) suggests that a smaller value such as C = 1 can be used in practice.",
      "startOffset" : 3,
      "endOffset" : 11
    }, {
      "referenceID" : 26,
      "context" : "MAB has been an active area of investigation since 1933 [27], in Operations Research, Economics and several branches of Computer Science: machine learning, theoretical computer science, AI, and algorithmic economics.",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 12,
      "context" : "A survey of prior work on MAB is beyond the scope of this paper; a reader is encouraged to refer to [13, 11] for background on prior-independent MAB, and to [26, 16] for background on Bayesian MAB.",
      "startOffset" : 100,
      "endOffset" : 108
    }, {
      "referenceID" : 10,
      "context" : "A survey of prior work on MAB is beyond the scope of this paper; a reader is encouraged to refer to [13, 11] for background on prior-independent MAB, and to [26, 16] for background on Bayesian MAB.",
      "startOffset" : 100,
      "endOffset" : 108
    }, {
      "referenceID" : 25,
      "context" : "A survey of prior work on MAB is beyond the scope of this paper; a reader is encouraged to refer to [13, 11] for background on prior-independent MAB, and to [26, 16] for background on Bayesian MAB.",
      "startOffset" : 157,
      "endOffset" : 165
    }, {
      "referenceID" : 15,
      "context" : "A survey of prior work on MAB is beyond the scope of this paper; a reader is encouraged to refer to [13, 11] for background on prior-independent MAB, and to [26, 16] for background on Bayesian MAB.",
      "startOffset" : 157,
      "endOffset" : 165
    }, {
      "referenceID" : 21,
      "context" : "Starting from [22], much of the work on MAB has been motivated by internet advertising.",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 19,
      "context" : "The basic formulation for MAB with stochastic rewards is well-understood ([20, 2] and the follow-up work, see [11] for references and discussion).",
      "startOffset" : 74,
      "endOffset" : 81
    }, {
      "referenceID" : 1,
      "context" : "The basic formulation for MAB with stochastic rewards is well-understood ([20, 2] and the follow-up work, see [11] for references and discussion).",
      "startOffset" : 74,
      "endOffset" : 81
    }, {
      "referenceID" : 10,
      "context" : "The basic formulation for MAB with stochastic rewards is well-understood ([20, 2] and the follow-up work, see [11] for references and discussion).",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 18,
      "context" : "Our formulation is a special case of sleeping bandits [19, 24] where in each round, a subset of arms is not available (“asleep”) and the goal is to compete with the best available arm.",
      "startOffset" : 54,
      "endOffset" : 62
    }, {
      "referenceID" : 23,
      "context" : "Our formulation is a special case of sleeping bandits [19, 24] where in each round, a subset of arms is not available (“asleep”) and the goal is to compete with the best available arm.",
      "startOffset" : 54,
      "endOffset" : 62
    }, {
      "referenceID" : 18,
      "context" : "However, this adversary in [19, 24] is oblivious (it decides its selections for all rounds before round 1), whereas in our problem it is adaptive (it decides its selection for round t only after observing what happened before).",
      "startOffset" : 27,
      "endOffset" : 35
    }, {
      "referenceID" : 23,
      "context" : "However, this adversary in [19, 24] is oblivious (it decides its selections for all rounds before round 1), whereas in our problem it is adaptive (it decides its selection for round t only after observing what happened before).",
      "startOffset" : 27,
      "endOffset" : 35
    }, {
      "referenceID" : 18,
      "context" : "To the best of our knowledge, the results in [19, 24] do not extend to settings where available arms are chosen by an adaptive adversary.",
      "startOffset" : 45,
      "endOffset" : 53
    }, {
      "referenceID" : 23,
      "context" : "To the best of our knowledge, the results in [19, 24] do not extend to settings where available arms are chosen by an adaptive adversary.",
      "startOffset" : 45,
      "endOffset" : 53
    }, {
      "referenceID" : 10,
      "context" : "Contextual bandits have been a subject of much recent work, see [11] for a survey.",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 9,
      "context" : "A typical example is “dynamic selling”[10, 4], where a seller has a limited supply of items and offers one item for sale in each round; the arms correspond to the offered prices.",
      "startOffset" : 38,
      "endOffset" : 45
    }, {
      "referenceID" : 3,
      "context" : "A typical example is “dynamic selling”[10, 4], where a seller has a limited supply of items and offers one item for sale in each round; the arms correspond to the offered prices.",
      "startOffset" : 38,
      "endOffset" : 45
    }, {
      "referenceID" : 6,
      "context" : "Other examples include “dynamic buying” [7] (where a buyer has a limited budget of money and interacts with a new seller in each round), and several versions in which the resource consumption for a given arm is deterministic [17, 18, 28, 29].",
      "startOffset" : 40,
      "endOffset" : 43
    }, {
      "referenceID" : 16,
      "context" : "Other examples include “dynamic buying” [7] (where a buyer has a limited budget of money and interacts with a new seller in each round), and several versions in which the resource consumption for a given arm is deterministic [17, 18, 28, 29].",
      "startOffset" : 225,
      "endOffset" : 241
    }, {
      "referenceID" : 17,
      "context" : "Other examples include “dynamic buying” [7] (where a buyer has a limited budget of money and interacts with a new seller in each round), and several versions in which the resource consumption for a given arm is deterministic [17, 18, 28, 29].",
      "startOffset" : 225,
      "endOffset" : 241
    }, {
      "referenceID" : 27,
      "context" : "Other examples include “dynamic buying” [7] (where a buyer has a limited budget of money and interacts with a new seller in each round), and several versions in which the resource consumption for a given arm is deterministic [17, 18, 28, 29].",
      "startOffset" : 225,
      "endOffset" : 241
    }, {
      "referenceID" : 28,
      "context" : "Other examples include “dynamic buying” [7] (where a buyer has a limited budget of money and interacts with a new seller in each round), and several versions in which the resource consumption for a given arm is deterministic [17, 18, 28, 29].",
      "startOffset" : 225,
      "endOffset" : 241
    }, {
      "referenceID" : 7,
      "context" : "A very recent, yet unpublished, paper [8], concurrent with respect to this paper, considers a generalization of our setting in which the budgets can be specified for arbitrary subsets of ads.",
      "startOffset" : 38,
      "endOffset" : 41
    }, {
      "referenceID" : 7,
      "context" : ") However, the guarantees in [8] for BudgetedAdsMAB are much weaker than ours.",
      "startOffset" : 29,
      "endOffset" : 32
    }, {
      "referenceID" : 20,
      "context" : "A more detailed discussion of this work is beyond the scope of this paper; see Chapter 28 of [21] for background.",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 8,
      "context" : "There are two somewhat distinct directions: dynamic auctions, in which the advertisers submit bids over time (see [9] for a survey), and MAB mechanisms [6, 14, 5, 15], where the advertisers submit bids only once, and the mechanism allocates ads over time.",
      "startOffset" : 114,
      "endOffset" : 117
    }, {
      "referenceID" : 5,
      "context" : "There are two somewhat distinct directions: dynamic auctions, in which the advertisers submit bids over time (see [9] for a survey), and MAB mechanisms [6, 14, 5, 15], where the advertisers submit bids only once, and the mechanism allocates ads over time.",
      "startOffset" : 152,
      "endOffset" : 166
    }, {
      "referenceID" : 13,
      "context" : "There are two somewhat distinct directions: dynamic auctions, in which the advertisers submit bids over time (see [9] for a survey), and MAB mechanisms [6, 14, 5, 15], where the advertisers submit bids only once, and the mechanism allocates ads over time.",
      "startOffset" : 152,
      "endOffset" : 166
    }, {
      "referenceID" : 4,
      "context" : "There are two somewhat distinct directions: dynamic auctions, in which the advertisers submit bids over time (see [9] for a survey), and MAB mechanisms [6, 14, 5, 15], where the advertisers submit bids only once, and the mechanism allocates ads over time.",
      "startOffset" : 152,
      "endOffset" : 166
    }, {
      "referenceID" : 14,
      "context" : "There are two somewhat distinct directions: dynamic auctions, in which the advertisers submit bids over time (see [9] for a survey), and MAB mechanisms [6, 14, 5, 15], where the advertisers submit bids only once, and the mechanism allocates ads over time.",
      "startOffset" : 152,
      "endOffset" : 166
    } ],
    "year" : 2013,
    "abstractText" : "We consider an application of multi-armed bandits to internet advertising (specifically, to dynamic ad allocation in the pay-per-click model, with uncertainty on the click probabilities). We focus on an important practical issue that advertisers are constrained in how much money they can spend on their ad campaigns. This issue has not been considered in the prior work on bandit-based approaches for ad allocation, to the best of our knowledge. We define a simple, stylized model where an algorithm picks one ad to display in each round, and each ad has a budget: the maximal amount of money that can be spent on this ad. This model admits a natural variant of UCB1, a well-known algorithm for multi-armed bandits with stochastic rewards. We derive strong provable guarantees for this algorithm.",
    "creator" : "LaTeX with hyperref package"
  }
}