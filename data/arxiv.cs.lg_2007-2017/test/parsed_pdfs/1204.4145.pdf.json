{
  "name" : "1204.4145.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Learning From An Optimization Viewpoint",
    "authors" : [ "Karthik Sridharan", "David McAllester", "Arkadi Nemirovski", "Alexander Razborov" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Learning From An Optimization Viewpoint\nby\nKarthik Sridharan\nSubmitted to :\nToyota Technological Institute at Chicago 6045 S. Kenwood Ave, Chicago, IL, 60637\nFor the degree of Doctor of Philosophy in Computer Science\nThesis Committee :\nNathan Srebro (Thesis Supervisor),\nDavid McAllester,\nArkadi Nemirovski,\nAlexander Razborov\nar X\niv :1\n20 4.\n41 45\nv1 [\ncs .L\nG ]\n1 8\nA pr\n2 01\n2\nIn memory of my dear father, Raghavan Sridharan . . .\ni"
    }, {
      "heading" : "Acknowledgements",
      "text" : "Starting to write this acknowledgement I realized that I owed thanks to many people who\nhave directly or indirectly influenced my research and have helped me complete this dis-\nsertation.\nFirst and foremost, I would like to thank my advisor Nati Srebro for his encouragement,\nguidance and patience throughout my PhD. One too many times I have walked into Nati’s\noffice with vague questions and still unformed ideas and every time I walked out with\ncrisp and formalized ideas and clear thoughts on how to approach problems. Nati was the\none who inspired me to look at learning alternatively as an optimization problem, which\nculminated into the central idea of this thesis. Besides research skills, I also learned (or at\nleast started the process of learning) other skills including scientific writing, presenting my\nideas succinctly and choosing the right problems to work on. Perhaps the most important\nthing I learned from Nati is that asking the right questions and formalizing these questions\nprecisely is as important as, if not more than, finding answers.\nI would like to thank Sham Kakade with whom I worked during the first three years of my\nPhD on many exciting projects and learned many things from. I am greatly indebted to Shai\nShalev-Shwartz; apart from being a wonderful collaborator, Shai has guided me in many\nways and greatly influence my research and thinking. I express my heartfelt gratitude to\nAlexander Rakhlin, Ohad Shamir and Ambuj Tewari. Besides being my close collaborators\nand friends, they gave me timely advice and help, and taught me many things regarding\nresearch and other aspects. I would like to also thank my other research collaborators who\ninfluenced and guided my research: Shai Ben-David, Kamalika Chaudhuri, Andrew Cotter,\nOfer Dekel, Dean Foster, Rina Foygel, Claudio Gentile, Karen Livescue and David Loker.\nii\nI would like to thank my committee members David McAllester, Arkadi Nemirovski and\nAlexander Razborov for all their help in guiding and shaping this thesis work. I enjoyed the\nmany insightful discussions with David; I will especially remember the ones we had fairly\nearly in the morning at TTI-Chicago when it used to be the start of Davids day and most\nlikely the end of mine. I am grateful to Arkadi Nemirovski for his valuable suggestions that\nhelped resolve issues I was struggling with, and also make my thesis better . I am grateful\nto Alexander Razborov for his valuable guidance and advice. Overall I feel privileged and\nhonored to have a thesis committee that guided and moulded my research and the way I\nthink about problems.\nI greatly acknowledge the most wonderful staff at TTI-Chicago who made my stay at\nChicago so memorable and pleasant. I would like to specially thank Carole Flemming,\nGary Hamburg, Liv Leader and Christina Novak for helping me with numerous things\nthroughout my PhD. I also thank Avleen Bijral, Wonseok Chae, Heejin Choi, Andrew Cot-\nter, Patrick Donovan, Taehwan Kim, Jianzhu Ma, Arild Brandrud Nss, Jian Peng, Ankan\nSaha, Allie Shapiro, Hao Tang, Hoang Trinh, Zhiyong Wang, Xing Xu, Payman Yadol-\nlahpour, Jian Yao and Feng Zhao for their friendship and making my time at TTI-Chicago\nfun. Thanks also to the other students and faculty at TTI-Chicago with whom I have had\ninsightful discussions and pleasant conversations.\nLast, but not the least, I would like to express my deepest gratitude to my family and\nrelatives, without whose support I would not have been able to complete my PhD. To Aditi,\nPranav, Lavan, Sreeram and Amma, thanks for all the love and encouragement you have\ngiven me. I thank my parents for their unconditional support and confidence they have\nshown in me. I dedicate this dissertation to my late father, Raghavan Sridharan, whose\ngreatest dream was to see me complete my PhD.\niii\nAbstract\nOptimization has always played a central role in machine learning and advances in the field of optimization and mathematical programming have greatly influenced machine learning models. However the connection between optimization and learning is much deeper : one can phrase statistical and online learning problems directly as corresponding optimization problems. In this dissertation I take this viewpoint and analyze learning problems in both the statistical and online learning frameworks from an optimization perspective. In doing so we develop a deeper understanding of the connections between statistical and online learning and between learning and optimization.\nThe dissertation can roughly be divided into two parts. In the first part we consider the question of learnability and possible learning rates for general statistical and online learning problems without regard to tractability issues. In the second part we restrict ourselves to convex learning problems and address the issue of tractability for both online and statistical learning problems by considering the oracle complexity of these learning problems.\nI. We first consider the question of learnability and possible learning rates for statistical learning problems under the general learning setting. The notion of learnability was first introduced by Valiant (1984) for the problem of binary classification in the realizable case. Vapnik (1995) introduced the general learning setting as a unifying framework for the general problem of statistical learning from empirical data. In this framework the learner is provided with a sample of instances drawn i.i.d. from some distribution unknown to the learner. The goal of the learner is to pick a hypothesis with low expected loss based on the sample received. The question of learnability is well studied and fully characterized for binary classification using the Vapnik Chervonenkis (VC) theory and for real valued supervised learning problems using the theory of uniform convergence with tools like Rademacher complexity, covering numbers and fat-shattering dimension etc. However we show that for the general learning setting the traditional approach of using uniform convergence theory to characterize learnability fails. Specifically we phrase the learning problem as a stochastic optimization problem and construct an example of a convex problem where Stochastic Approximation (SA) approach provides successful learning guarantee but Empirical Risk Minimization (ERM) (or equivalently Sample Average Approximation (SAA) approach) fails to give any meaningful learning guarantee. This example establishes that for general learning problems the concept of uniform convergence fails to capture learnability and ERM/SAA ap-\niv\nproaches can fail to provided successful learning algorithms. To fill this void in the theory of statistical learnability in the general setting we instead turned to the concept of stability of learning algorithms to fully characterize learnability in the general setting. We specifically show that a problem is learnable if and only if there exists a stable approximate minimizer of average loss over the sample. Using this notion of stability, we also provide a universal learning procedure that guarantees success whenever the problem is learnable.\nNext we consider the problem of online learning in the general setting. Online learning is a continual and sequential learning process where instances provided to the learner round by round and can be chosen adversarially (as opposed to stochastically). The goal of the learner for an online learning problem is to minimize regret with respect to the single best hypothesis that can be chosen in hindsight. Most of the work on online learning problems so far have been algorithmic and problem specific. The usual approach has been to build algorithm for the specific problem at hand and prove regret guarantees for this algorithm which in turn implies learnability with associated learning rates. Unlike the statistical learning framework there is a dearth of generic tools that can be used to establish learnability and rates for online learning problems in general. Only recently, Ben-David, Pal and Shalev-Shwartz (2009) showed that the Littlestone dimension (introduced by Littlestone (1988)) is an online analog to the VC dimension and fully characterizes learnability for online binary classification problems. However the question of characterizing online learnability for even the real valued supervised learning problems in the online framework was open. In this dissertation, analyzing the so called value of the online learning game, we provide online analogs to classical tools from statistical learning theory like Rademacher complexity, covering numbers, fat-shattering dimension etc. While these tools can be used to provide upper bounds for more general online learning problems, the results mirror uniform convergence theory for online learning problems. Hence analogous to the statistical learning case, we used these tools to fully characterize learnability and rates for real valued online supervised learning problems. We also provide a generic algorithm for the real valued online supervised learning problem. However unlike the statistical learning case, we don’t yet have a full characterization of learnability for general online learning problems and leave this as open problem for future work.\nII. In the first part of the dissertation we focused on the question of learnability and rates for statistical and online learning problems without paying attention to tractability or efficiency of the learning algorithms that could be used. Even the generic algorithms provided in the first part are in general not at all tractable. In the second part of the dissertation we address the issue of building tractable or efficient learning algorithms by first focusing our attention specifically on convex learning problems. In the second part, for general classes of convex learning problems, we provide appropriate mirror descent updates that are guaranteed to be successful for online and statistical learning of these convex problems. Further, using and extending results from the geometry of Banach spaces, we show that the the mirror descent method (with appropriate prox-function and step-size) is near optimal for online convex learning problems and for most reasonable cases, is also near optimal for statistical convex\nv\nlearning problems. Further noting that when used for statistical convex learning, the mirror descent algorithm is a first-order (gradient based), O(1) memory, single pass algorithm, we conclude that mirror descent method is also near optimal in terms of number of gradient accesses and for many commonly encountered problems, optimal also in terms of computational time. We next consider the problem of (offline) convex optimization and to capture the notion of efficiency of optimization procedures for these problems, we use the notion of oracle complexity of the problem introduced by Nemirovski and Yudin (1978). We show that for a general class of convex optimization problems, oracle complexity of the problem can be lower bounded by the so called fat-shattering dimension of the associated linear class. Thus we establish a strong connection between offline convex optimization problems and statistical convex learning problems. We further show that for a large class of infinite dimensional (or high dimensional) optimization problems, mirror descent is in fact near optimal in terms of oracle efficiency even for these offline convex optimization problems.\nvi\nContents"
    }, {
      "heading" : "1 Introduction 1",
      "text" : "1.1 Learning and Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . 2\n1.2 Overview of the Thesis . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n1.2.1 Part I : Statistical and Online Learning : Learnability and Rates . . 3\n1.2.2 Part II : Convex Problems : Oracle Efficient Learning/Optimization 4\n1.3 Main Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n1.4 Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6"
    }, {
      "heading" : "I Statistical and Online Learning : Learnability and Rates 7",
      "text" : ""
    }, {
      "heading" : "2 Preliminary Setup and Notations 8",
      "text" : "2.1 General Learning Problem Setup . . . . . . . . . . . . . . . . . . . . . . . 8\n2.2 More Definitions and Notations . . . . . . . . . . . . . . . . . . . . . . . . 9"
    }, {
      "heading" : "3 Statistical Learning/Optimization 12",
      "text" : "3.1 The Statistical Learning Problem and Learnability . . . . . . . . . . . . . . 13\n3.2 Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n3.2.1 Learnability and Uniform Convergence . . . . . . . . . . . . . . . 16\n3.2.2 Various Complexity Measures and Uniform Convergence . . . . . . 18\n3.2.3 Learnability and Stability . . . . . . . . . . . . . . . . . . . . . . . 20\nvii\n3.3 Failure of Uniform Convergence and ERM/SAA Approaches . . . . . . . . 21\n3.3.1 Learning without Uniform Convergence : Stochastic Convex Opti-\nmization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n3.3.2 Learnability via Stability : Role of Regularization . . . . . . . . . . 24\n3.3.3 Contradiction to Vapnik? . . . . . . . . . . . . . . . . . . . . . . . 26\n3.4 Stability of Learning Rules . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n3.4.1 Comparison with Existing Literature and Other Notions of Stability 28\n3.5 Characterizing Learnability : Main Results . . . . . . . . . . . . . . . . . . 32\n3.6 Randomization, Convexification, and a Generic Learning Rule . . . . . . . 34\n3.6.1 Stronger Results with Randomized Learning Rules . . . . . . . . . 34\n3.6.2 A Generic Learning Rule . . . . . . . . . . . . . . . . . . . . . . . 36\n3.7 Detailed Results and Proofs . . . . . . . . . . . . . . . . . . . . . . . . . . 37\n3.7.1 Detailed Proof of Main Result (Section 3.5) . . . . . . . . . . . . . 37\n3.7.2 Other Proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\n3.8 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50"
    }, {
      "heading" : "4 Online Learning/Optimization 52",
      "text" : "4.1 The Online Learning Problem . . . . . . . . . . . . . . . . . . . . . . . . 53\n4.2 Online Learnability and the Value of the Game . . . . . . . . . . . . . . . 54\n4.3 Sequential Rademacher Complexity . . . . . . . . . . . . . . . . . . . . . 56\n4.3.1 Structural Results . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\n4.4 Sequential Covering Number and Combinatorial Parameters . . . . . . . . 60\n4.4.1 A Combinatorial Upper Bound . . . . . . . . . . . . . . . . . . . . 62\n4.4.2 Finite Class Lemma and the Chaining Method . . . . . . . . . . . 63\n4.5 Martingale Uniform Convergence . . . . . . . . . . . . . . . . . . . . . . 64\n4.6 Charecterizing Learnability of Supervised Learning Problem . . . . . . . . 66\n4.6.1 Generic Algorithm for Supervised Learning Problem . . . . . . . . 68\nviii\n4.7 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70\n4.7.1 Example: Margin Based Regret . . . . . . . . . . . . . . . . . . . 70\n4.7.2 Example : Neural Networks . . . . . . . . . . . . . . . . . . . . . 70\n4.7.3 Example: Decision Trees . . . . . . . . . . . . . . . . . . . . . . . 71\n4.7.4 Example: Online Transductive Learning . . . . . . . . . . . . . . . 72\n4.7.5 Example: Isotron . . . . . . . . . . . . . . . . . . . . . . . . . . . 73\n4.8 Detailed Proofs and More Results . . . . . . . . . . . . . . . . . . . . . . 74\n4.8.1 Proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74\n4.8.2 Exponentially Weighted Average (EWA) Algorithm on Countable\nExperts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100\n4.9 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103"
    }, {
      "heading" : "II Convex Problems : Oracle Efficient Learning/Optimization 104",
      "text" : ""
    }, {
      "heading" : "5 Convex Learning and Optimization Problem Setup 105",
      "text" : "5.1 Convex Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105\n5.2 Various Convex Learning/Optimization Problems . . . . . . . . . . . . . . 106\n5.3 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108"
    }, {
      "heading" : "6 Mirror Descent Methods 109",
      "text" : "6.1 The Mirror Descent Update . . . . . . . . . . . . . . . . . . . . . . . . . . 109\n6.2 Online Mirror Descent . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110\n6.3 Stochastic Mirror Descent . . . . . . . . . . . . . . . . . . . . . . . . . . 113\n6.4 Mirror Descent for Offline Optimization . . . . . . . . . . . . . . . . . . . 115\n6.5 Detailed Proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116\n6.6 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122"
    }, {
      "heading" : "7 Optimality of Mirror Descent for Online Convex Learning Problem 124",
      "text" : "ix\n7.1 Value of the Linear Game . . . . . . . . . . . . . . . . . . . . . . . . . . . 125\n7.2 Value and Martingale Type . . . . . . . . . . . . . . . . . . . . . . . . . . 126\n7.3 Martingale Type and Uniform Convexity . . . . . . . . . . . . . . . . . . . 128\n7.4 Main Result : Optimality of Online Mirror Descent . . . . . . . . . . . . . 129\n7.4.1 Smooth Loss Case . . . . . . . . . . . . . . . . . . . . . . . . . . 130\n7.4.2 Uniformly Convex Loss Case . . . . . . . . . . . . . . . . . . . . 132\n7.5 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133\n7.5.1 Example : `p non-dual pairs . . . . . . . . . . . . . . . . . . . . . 133\n7.5.2 Example : Non-dual Schatten norm pairs in finite dimensions . . . 134\n7.5.3 Example : Non-dual group norm pairs in finite dimensions . . . . . 134\n7.5.4 Example : Max Norm . . . . . . . . . . . . . . . . . . . . . . . . 134\n7.5.5 Example : Interpolation Norms . . . . . . . . . . . . . . . . . . . 135\n7.6 Detailed Proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136\n7.7 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150"
    }, {
      "heading" : "8 Optimality of Mirror Descent for Statistical Convex Learning Problems 151",
      "text" : "8.1 Lower Bounds for Statistical Learning Rates . . . . . . . . . . . . . . . . . 152\n8.1.1 Lower Bounds for Smooth Losses . . . . . . . . . . . . . . . . . . 154\n8.2 Optimal Rates and Rademacher Type . . . . . . . . . . . . . . . . . . . . . 154\n8.3 Main Result : Optimality of Stochastic Mirror Descent . . . . . . . . . . . 155\n8.3.1 Banach Lattices . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157\n8.3.2 Decoupling Inequalities . . . . . . . . . . . . . . . . . . . . . . . 159\n8.3.3 Optimality of Mirror Descent in Terms of Efficiency . . . . . . . . 161\n8.4 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162\n8.4.1 Example : `p non-dual pairs . . . . . . . . . . . . . . . . . . . . . 162\n8.4.2 Example : Non-dual Schatten norm pairs in finite dimensions . . . 162\n8.4.3 Example : Non-dual group norm pairs in finite dimensions . . . . . 163\nx\n8.4.4 Computational Efficiency Issues . . . . . . . . . . . . . . . . . . . 163\n8.5 Detailed Proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163\n8.6 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174"
    }, {
      "heading" : "9 Optimality of Mirror Descent for Offline Convex Optimization 175",
      "text" : "9.1 Oracle-based Offline Convex Optimization . . . . . . . . . . . . . . . . . . 176\n9.2 Lower Bounding Oracle Complexity: Connections to Statistical Convex\nLearning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 178\n9.3 Main Result : Optimality of Mirror Descent for Offline Convex Optimization180\n9.4 Statistical Learning With Distributed Oracles . . . . . . . . . . . . . . . . 181\n9.5 Detailed Proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183\n9.6 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186"
    }, {
      "heading" : "10 Conclusion and Future Work 187",
      "text" : "10.1 Open Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187\n10.1.1 Online Optimization and Stability . . . . . . . . . . . . . . . . . . 187\n10.1.2 Upper Bounding Oracle Complexity in Terms of Fat-Shattering Di-\nmension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187\n10.2 Further Directions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 188\n10.3 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 188\nBibliography 189"
    }, {
      "heading" : "A Relating Various Complexity Measures : Statistical Learning 197",
      "text" : "A.1 The Refined Dudley Integral: Bounding Rademacher Complexity with L2\nCovering Numbers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 197\nA.2 Bounding L∞ covering number by Fat-shattering Dimension . . . . . . . . 199\nA.3 Relating Fat-shattering Dimension and Rademacher complexity . . . . . . 201\nxi\nList of Figures\n3.1 An example of a “trivial” learning situation. Each line represents some\nh ∈ H, and shows the value of `(h, z) for all z ∈ Z . The hypothesis h̃ dominates any other hypothesis (e.g., `(h̃; z) < `(h; z) uniformly for all\nz), and thus the problem is learnable without uniform convergence or any\nother property ofH . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n7.1 Relationship between the various constants . . . . . . . . . . . . . . . . . 129\n8.1 Relationship between the various constants in statistical setting . . . . . . . 155\n8.2 Relationship between the various constants in both online and statistical\nsettings. Red inequality is not always true but true for\nmost commonly encountered problems. . . . . . . . . . . . . . . . . . . . 161\nxii\nList of Tables\n2.1 Summary of Notations . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\nxiii\nChapter 1\nIntroduction\nThere are two main paradigms under which machine learning problems have been commonly studied; these are the statistical learning framework and the online learning framework. In the statistical learning framework, the data source is said to be iid, that is data (or instances) are assumed to be drawn independently from some fixed but unknown (to the learner) source distribution. The goal of the learner in this setting is to pick a hypothesis that minimizes expected error on future examples drawn from the source distribution. The statistical learning framework is well suited for applications like object recognition, natural language processing and other applications where the data generation process is unchanging with time. For instance when we consider a problem of image classification between cat and dog images, it is not unreasonable to assume the data source does not vary over time and each time we sample repeatedly form the same distribution over input images. Roughly speaking cats and dogs would look the same even 10 years from now and so we can think of the distribution from which we draw the image label pair to be fixed.\nThe online learning framework on the other hand is a learning scenario where the learner is faced with a data source that changes over time or even reacts to the learners choices. The online learning framework is often studied as a multiple round game where the data source could even be adversarial. The instances produced by the adversary at any round could depend on the past hypotheses chosen by the learner. The online learning framework is better suited for more interactive learning tasks like spam mail detection and stock market prediction where decisions of the learner could affect future instances the learner receives. For instance in a spam mail filter application, while the company providing the spam filter service works on filtering out spam mails, the spammers themselves try to outwit the system by adapting to how the spam detection software works. In a sense there is a continuous game played between the company providing spam filtering service and the spammers where each one tries to outwit the other. Such a scenario is naturally captured by the online learning framework.\nIn this dissertation we consider learning in both online and statistical learning frameworks. We view learning problems in both the frameworks from an optimization viewpoint and establish connections between learning and optimization. Specifically, in the first part of the dissertation we consider very general learning problems\n1\nin both the statistical and online learning frameworks and focus on the question of when a given learning problem is learnable and at what rates. Of course at that generality we restrict our focus to only questions of learnability and rates for the learning problems and do not pay attention to tractability/efficiency of possible learning algorithms that can be used for these problems.\nIn the second half of the thesis, we restrict our focus to convex learning problems and beyond just characterizing optimal rates for these learning problems, we also provide simple first order methods based on mirror descent and show that they are near optimal both in terms of rate of convergence and in terms of their efficiency. Specifically we establish that for a large family of general convex learning problems, mirror descent algorithm is universal and near optimal for the online learning framework. We also show that for most reasonable cases, mirror descent is near optimal for these convex learning problems even in the statistical learning framework. Further, the fact that mirror descent is an order O(1) memory, simple first order method makes it near optimal even in terms of efficiency for many of these learning problems. We also establish that for several high dimensional problems, mirror descent could be near optimal (in terms of oracle efficiency) for (offline) convex optimization problems. In the second half of the thesis we establish some interesting connections between convex learning problems and convex optimization. We establish that for most reasonable cases online convex learning and statistical convex learning are equally hard (or easy). The main contention of the sec on part of the thesis is that for a large family of convex learning problems, whether it is online or statistical learning framework, the simple one pass, first order method of mirror descent is optimal both in terms of rates and efficiency.\nWhile the work in this dissertation is of a theoretical in nature, we believe that the results and viewpoints provided will drive the choice of learning algorithms considered for various problems and models chosen for learning. Further the second part of the thesis has more direct practical implications and advocates the use of simple first order online learning methods for convex learning problems even when our end goal is statistical learning."
    }, {
      "heading" : "1.1 Learning and Optimization",
      "text" : "Optimization has always played a central role in shaping machine learning algorithms and models. The typical approach taken to tackle learning problems has been to pick a suitable set of models or hypotheses or predictors, pick appropriate empirical cost over the training sample and finally solve the optimization problem of picking the hypothesis from the set of hypotheses that minimize the empirical cost over training sample. The learning algorithm itself in this case corresponds to the optimization algorithm that solves the minimization problem of picking that hypothesis from the set of hypotheses that minimizes empirical cost over training sample. Theoretical guarantees on learning rates are typically provided by decomposing error term into two terms. The first term, the estimation error that accounts for error we would incur if we picked the hypothesis that minimizes empirical cost over training sample chosen. The second term is the optimization error term that accounts for the sub-optimality of the optimization algorithm in minimizing the empirical cost chosen.\n2\nOverall, we see that optimization plays a critical role in the design and analysis of algorithms for learning problems. However the connection between optimization and learning is much deeper. Learning problems under both statistical and online learning frameworks can be directly framed as optimization problems and analysis of the sub-optimality of algorithms the . In this dissertation I will take this viewpoint that learning problems can be viewed as optimization problems and using this point of view establish connections between learning in statistical and online frameworks and optimization. Specifically in the second half of the dissertation, focusing on convex learning problems, we show that mirror descent method originally introduced for convex optimization problems by Nemirovski and Yudin [1], is always near optimal for online learning problems, near optimal for most reasonable statistical learning problems and even near optimal for several high dimensional offline convex optimization problems. We use concepts from the geometry of Banach spaces to establish these optimality results for mirror descent. We also establish some interesting connections between convex learning and convex optimization."
    }, {
      "heading" : "1.2 Overview of the Thesis",
      "text" : "The dissertation can be split into two parts; the first part focuses on the question of learnability and learning rates for fairly general class of learning problems in both the statistical and online learning frameworks. In the first part, we do not concern ourselves with questions of efficiency or tractability of the learning methods/algorithms for solving these problems. In the second part of this dissertation, we restrict ourselves to convex learning problems and show that for most reasonable problems, the first order method of mirror descent is near optimal for both statistical and online convex learning problems. We also establish connections between convex learning and (offline) convex optimization and establish that for several high dimensional (offline) convex optimization problems, mirror descent is optimal in terms of efficiency for (offline) convex optimization problem. The notion of efficiency we use in this case is the notion of oracle complexity introduced by Nemirovski and Yudin (1978)."
    }, {
      "heading" : "1.2.1 Part I : Statistical and Online Learning : Learnability and Rates",
      "text" : "The first part of this dissertation contains two chapters. The first one, Chapter 3, is dedicated to the question of learnability in the statistical learning framework. In this chapter, through an example that is an instance of a stochastic convex optimization problem, we show that the concept of uniform convergence (and hence tools like Rademacher complexity, fat-shattering dimension) that are commonly used to analyze learning rates in the supervised learning settings fail in the general. Thus we establish that the folklore of uniform convergence is necessary and sufficient for learnability is not true in general. This of course opens up the question : “How can one characterize learnability of general learning problems, in the statistical framework ?”. We provide an answer to this question in this chapter by turning to the concept of stability of learning algorithms. We show that learnability in the general setting can be characterized by existence of learning algorithm that is stable and an approximate empirical minimizer. We further proceed to show that if we are allowed to consider randomized learning rules, then we can provide a “Universal Learning Algorithm” which\n3\nhas non-trivial learning rate whenever the problem is learnable.\nChapter 4, is the second chapter in the first part of the dissertation. It deals with the question of learnability and rates for online learning problems. While the question of learnability is well studied in the statistical framework, the question of learnability in the online setting has relatively less explored. Most of the work on online learning problems so far have been algorithmic and problem specific. The usual approach has been to build algorithm for the specific problem at hand and prove regret guarantees for this algorithm which in turn implies learnability with associated learning rates. Unlike the statistical learning framework there is a dearth of generic tools that can be used to establish learnability and rates for online learning problems in general. Only recently, Ben-David, Pal and Shalev-Shwartz (2009) showed that the Littlestone dimension (introduced by Littlestone (1988)) is an online analog to the VC dimension and fully characterizes learnability for online binary classification problems. In general there have been no generic tools like Rademacher complexity, covering numbers and fat-shattering dimension that are present for analyzing statistical learning problems. In this chapter we explore the the question of learnability and optimal rates for online learning by first formalizing them as value of the online learning game. We then build complexity measures analogous to those in the statistical framework like Rademacher complexity, covering numbers and fat-shattering dimension and show that these tools can be used to bound learning rates for online learning problems. These tools can be seen as tools for studying uniform convergence for general stochastic processes (non iid). We go ahead and show that these tools can even characterize learnability and learning rates of real valued supervised learning problems in the online learning framework. Based on these complexity measures, we also provide a generic algorithm for the supervised learning problem in the online learning framework that has diminishing regret whenever the problem is online learnable."
    }, {
      "heading" : "1.2.2 Part II : Convex Problems : Oracle Efficient Learning/Optimization",
      "text" : "In the first part of this thesis while we even provide generic learning algorithms for fairly general class of problems in both statistical and online learning frameworks, we never concerned ourselves with any form of tractability of these learning algorithms. However, in the second part of this dissertation, focusing on convex learning problems, we aim at providing optimal and efficient learning algorithms for both the statistical and online learning frameworks. In doing so we also explore connections between learning and convex optimization. There are five chapters in this part. The first chapter of this part, Chapter 5, introduces the basic set up of the convex learning and optimization problems we consider in this part, describes the online and statistical learning protocols/frameworks for these convex problems and introduces the oracle model for offline convex optimization. The next chapter, Chapter 6 introduces the mirror descent methods (see [1]) for the statistical and online convex learning problems described in the previous chapter and provides upper bounds on learning rate for them. Upper bounds on rate of optimization of the mirror descent method for offline convex optimization problems are also provided. Chapter ?? deals with online convex learning problems and in the chapter we show that mirror descent algorithm is universal near optimal for online convex learning problems. In Chapter 8 statistical convex learning problems are considered and lower bounds on learning rates for these problems are provided. Using these lower bounds we further show that for most\n4\nreasonable cases, the mirror descent method for statistical learning (stochastic mirror descent method) is near optimal even for statistical learning. In the final chapter of the second part of this dissertation, Chapter 9, (offline) convex optimization problems are considered. As mentioned earlier, we use oracle complexity of the learning problem (ie. minimum number of calls to any local oracle needed by any method to achieve desired accuracy) as a measure of efficiency of the optimization procedure. We show that the oracle complexity of convex optimization problems can be lower bounded by fat-shattering dimension of the associated linear function class (a classic concept form statistical learning theory). Using this lower bound and results from previous chapters we show that for certain classes of convex optimization problems (high dimensional), mirror descent method is near optimal even for offline convex optimization problem. These results are also further used to argue that for certain statistical convex learning problems, mirror descent method is near optimal even when the learner has access to a parallel computation oracles (i.e. oracle query on entire sample is considered as one oracle call in the model). This in turn implies that for these convex learning problems, parallelization does not help."
    }, {
      "heading" : "1.3 Main Contributions",
      "text" : "1. Part I :\n(a) Statistical learning\n• Illustrate limitations of uniform convergence and ERM/SAA • Provide characterization of statistical learning in general through stability • Provide universal randomized learning rule\n(b) Online learning\n• Introduce analogs of various complexity measures of hypothesis class for online learning, provide tools martingale uniform convergence theory\n• Characterize Online learnability for supervised learning problems • Generic algorithm for online supervised learning\n2. Part II :\n(a) Online convex learning problems\n• Characterize learnability and rates for various convex learning problems through notion of martingale type\n• Show universality and near optimality of online mirror descent for online convex learning problems\n(b) Statistical convex learning problems\n• Establish lower bounds for oracle complexity and learning rates for various convex learning problems in statistical framework using Rademacher complexity of linear class\n5\n• Show that for most reasonable statistical convex learning problems, mirror descent algorithm is near optimal both in terms of learning rates and number of oracle (gradient) access\n(c) Offline convex optimization problems\n• Generic lower bound on convex optimization problem by fat-shattering dimension of associated linear class\n• Show that for a large class of large dimensional convex optimization problems, mirror descent is near optimal even for offline convex optimization"
    }, {
      "heading" : "1.4 Bibliographic Notes",
      "text" : "Results in Chapter 3 are from joint work with Shai Shalev-Shwartz, Ohad Shamir and Nati Srebro. Early versions of the results can be found in [2, 3]. See [4] for later version which is closer to the one presented in the chapter. The results in Chapter 4 are from joint work with Alexander Rakhlin and Ambuj Tewari and can be found in [5]. In the second part of the dissertation, few of the result from Chapter 6 can be found in [6]. The results in Chapter 7 is from joint work with Nathan Srebro and Ambuj Tewari. Relating basic concept of martingale type and certain online convex learning problems was first done in [7]. In [6] the result of universality and near optimality of mirror descent method from Chapter 7 is provided. Chapters 8 and 9 are joint work with Nati Srebro.\n6\nPart I\nStatistical and Online Learning : Learnability and Rates\n7\nChapter 2\nPreliminary Setup and Notations\nIn this chapter we provide the basic setup, some preliminary definitions and notations used throughout this dissertation."
    }, {
      "heading" : "2.1 General Learning Problem Setup",
      "text" : "In all the learning problems we consider instances provided to the learner are chosen from the instance set Z . The learner in picks hypotheses from set H̄. The instantaneous loss incurred by learner on instance z ∈ Z for picking hypothesis h ∈ H̄ is given by `(h, z) where ` : H̄ × Z 7→ R is the “loss” or cost function. The exact setup of the learning problem and the goal of the learner depends on whether we consider the statistical learning framework or the online learning framework and will formally be specified later on in the corresponding chapters or sections. However, irrespective of the exact framework, the rough goal of the learner for the learning problems we consider, is to pick hypotheses that are competitive with respect to the best hypothesis from a set of hypotheses H ⊆ H̄. Notice that usually, in most of the learning problems considered in literature, the setting considered is one where learner also picks hypothesis from set H. In contrast we shall consider the so called “Improper learning setting” here and allow learner to pick hypothesis from a set H̄ while the goal is to compete with the best hypothesis from a smaller set H. Whenever H̄ = H we call this setting as the proper learning setting. This framework is sufficiently general to include a large portion of the learning and optimization problems we are aware of, such as:\n• Binary Classification: Let Z = X × {0, 1}, let H̄ be a set of functions h : X 7→ {0, 1}, and let `(h; (x, y)) = 1{h(x)6=y}. Here, loss function is simply the 0 − 1 loss, measuring whether the binary hypothesis h misclassified the example (x, y).\n• Regression: Let Z = X ×Y where X and Y are bounded subsets of Rd and R respectively. Let H̄ be a set of bounded functions h : X 7→ R, and let `(h; (x, y)) = (h(x) − y)2. Here, the loss function is\n8\nsimply the squared loss.\n• Large Margin Classification in a Reproducing Kernel Hilbert Space (RKHS): LetZ = X×{0, 1}, where X is a bounded subset of an RKHS, let H̄ be another bounded subset of the same RKHS, and let `(h; (x, y)) = max{0, 1− y 〈x,h〉}. Here, the loss function is the well known hinge loss function, and our goal is to perform margin-based linear classification in the RKHS.\n• K-Means Clustering in Euclidean Space: Let Z = Rn, let H̄ be all subsets of Rn of size k, and let `(h; z) = minc∈h ‖c− z‖2. Here, each h represents a set of k centroids, and the loss ` measures the Euclidean distance squared between an instance z and its nearest centroid, according to the hypothesis h.\n• Density Estimation: LetZ be a subset of Rn, let H̄ be a set of bounded probability densities onZ , and let `(h; z) = − log(h(z)). Here, loss function ` is simply the negative log-likelihood of an instance z according to the hypothesis density h.\n• Convex Learning Problems: Let Z be an arbitrary measurable set, let H̄ be a closed, convex subset of a vector space, and for each z ∈ Z , let the function `(h; z) be convex w.r.t. its first argument. We shall use the letter S to denote a sample of instances, that is a sample S ∈ ⋃ n∈NZn is a sequence of instances. For instance S = (z1, . . . , zn) is a sample of size n, note that the order and and multiplicity of instances may be important. Given a sample S we shall denote the empirical average loss over this sample S as :\nLS(h) = 1\n|S| |S|∑ i=1 `(h, zi)\nWe shall also use the notation L̂(h) to refer LS(h) whenever the sample S is well understood under the context. Further, given a distribution D on instance space Z , we shall use the notation\nLD(h) = Ez∼D [`(h, z)]\nFurther we shall use the notation L(h) to refer to LD(h) whenever the distribution is understood under the context."
    }, {
      "heading" : "2.2 More Definitions and Notations",
      "text" : "An important object that we will encounter especially while analyzing online learning are trees. Unless specified, all trees considered in this paper are either rooted complete binary trees. While it is useful to have the tree picture in mind when reading the paper, it is also necessary to precisely define trees as mathematical objects. We opt for the following definition.\nDefinition 1 (Trees). Given some set Z , a Z-valued tree z, of depth n is a sequence (z1, . . . ,zn) of n mappings zi : {±1}i−1 7→ Z . The root of the tree z is the constant function z1 ∈ Z .\n9\nA tree of infinite depth is defined exactly as above as an infinite sequence (zn)n∈N.\nArmed with this definition, we can talk about various operations on trees. For a function f : Z 7→ U , f(z) denotes the U-valued tree defined by the mappings (f ◦ z1, . . . , f ◦ zn). Analogously, for f : Z × Z 7→ U , the U-valued tree f(z, z′) is defined as mappings (f(z1, z′1), . . . , f(zn, z′n)). In particular, this defines the usual binary arithmetic operations on real-valued trees. Furthermore, for a class of functions F and a tree z, the projection of F onto z is F(z) = {f(z) : f ∈ F}.\nDefinition 2 (Path). A path of length n is a sequence = ( 1, . . . , n−1) ∈ {±1}n−1.\nWe shall abuse notation by referring to zi( 1, . . . , i−1) by zi( ). Clearly zi only depends on the first i − 1 elements of . We will also refer to = ( 1, . . . , n) ∈ {±1}n as a path in a tree of depth T even though the value of T is inconsequential. Next we define the notion of subtrees.\nDefinition 3 (Subtrees). The left subtree z` of z at the root is defined as n−1 mappings (z`1, . . . ,z`n−1) with z`i ( ) = zi+1({−1} × ) for ∈ {±1}n−1. The right subtree zr is defined analogously by conditioning on the first coordinate of zi+1 to be +1.\nGiven two subtrees z, v of the same depth n − 1 and a constant mapping z1, we can join the two subtrees to obtain a new set of mappings (x1, . . . ,xn) as follows. The root is the constant mapping z1. For i ∈ {2, . . . , n} and ∈ {±1}n, xi( ) = zi−1( ) if 1 = −1 and wi( ) = vi−1( ) if 1 = +1.\nWe will also need to talk about the values given by the tree z over all the paths. Formally, let Img(z) = z ({±1}n) = {xt( ) : t ∈ [n], ∈ {±1}n} be the image of the mappings of z.\nTable 2.1 contains a list of the basic notations used.\n10\n11\nChapter 3\nStatistical Learning/Optimization\nIn this chapter we consider the problem of statistical learning in the general learning problems introduced by [8] where we would like to minimize a population risk functional (stochastic objective)\nL(h) = Ez∼D [`(h; z)] (3.0.1)\nbased on i.i.d. sample z1, . . . , zn drawn fromD over some target hypothesis classH. The distributionD over instance space Z is unknown to the learner. Notice that this is basically a stochastic optimization problem. In this chapter we are mainly concerned with the question of statistical “learnability”. That is, when can Eq. (3.0.1) be minimized to within arbitrary precision based only on a finite sample z1, . . . , zn, as n→∞?\nFor supervised classification and regression problems, it is well known that a problem is learnable if and only if the empirical risks\nLS(h) = 1 n n∑ i=1 `(h, zi) (3.0.2)\nfor all h ∈ H converge uniformly to the population risk ([9, 10]). If uniform convergence holds, then the empirical risk minimizer (ERM) is consistent, i.e. the population risk of the ERM converges to the optimal population risk, and the problem is learnable using the ERM. We therefore have:\n• A necessary and sufficient condition for learnability, namely uniform convergence of the empirical risks. Furthermore, this can be shown to be equivalent to a combinatorial condition: having finite VC-dimension in the case of classification, and having finite fat-shattering dimensions in the case of regression.\n• A complete understanding of how to learn: since learnability is equivalent to learnability by ERM, we can focus our attention solely on empirical risk minimizers.\nThe situation, for supervised classification and regression, can be depicted as follows:\n12\nFinite Dim. Uniform\nConvergence\nLearnable with ERM Learnable\nIn this chapter we start by showing that the situation for general learning problems in the statistical learning framework is actually much more complex. In particular, in Subsection 3.3 we show an example of a learning problem which is learnable (using Stochastic Approximation approch), but is not learnable using empirical risk minimization and uniform convergence fails. We discuss how notion stability (through regularization) plays an important role in learnability of the problem. Having shown that uniform convergence fails to characterize learnability for general learning problems we then approach the question of characterizing learnability in general for statistical learning problems. To do so in Section 3.4 we first introduce definitions of stability of learning rules we consider using for characterizing learnability. In section ??, we show that any problem that is learnable, is always learnable with some learning rule which is an “asymptotically ERM”. Moreover, such an AERM must be stable (under a suitable notion of stability). Namely, we provide following characterization of learnability for general statistical learning problems and this can be considered as the highlight of this chapter :\nExists Stable\nAERM\nLearnable\nwith AERM Learnable\nNote that this characterization holds even for learnable problems with no uniform convergence. In this sense, stability emerges as a strictly more powerful notion than uniform convergence for characterizing learnability. Finally in Section 3.6 we show how we can get stronger results by considering randomized learning algorithms and we go on to provide a generic learning rule that is guaranteed to be successful whenever the problem is learnable.\nSection 3.7 contains the details of the proofs and technical results used in the chapter. Following that we conclude with Bibliographic notes and Discussion."
    }, {
      "heading" : "3.1 The Statistical Learning Problem and Learnability",
      "text" : "In the statistical learning problem, instances are drawn i.i.d. from some fixed distribution D unknown to the learner. Given a sample S = z1, . . . , zn of size n, the goal of the learner then is to pick a hypothesis h ∈ H̄ based only on the sample S that has small expected loss LD(h). This problem of learning can in turn be phrased as a stochastic optimization problem where our goal is the minimization problem\nmin h∈H̄ LD(h)\nThe term LD(h) is often referred to as the risk of choosing hypothesis h. However given a target hypothesis classH our goal is only to do as well as the best hypothesis in this target class and so the sub-optimality (also\n13\nrefered to as ) of any hypothesis h ∈ H̄ is given by\nLD(h)− inf h∈H LD(h) .\nOf course notice that for proper learning case, the problem is exactly that of stochastic optimization.\nTo formally refer to the strategy or algorithm used by the learner to pick hypotheses based on sample provided, we now define the notion of learning algorithm for statistical learning problems. Definition 4. A “Statistical Learning Rule” A : ⋃ n∈NZn 7→ H̄ is a mapping from sequences of instances in Z to the set of Hypothesis H̄.\nWe shall refer to any “Learning Rule” A that only outputs hypothesis in the set H instead of entire H̄ as a “Proper Learning Rule”.\nLearnability deals with the question of when it is even possible (information theoretically) to drive the suboptimality for a given problems to 0 with increase in sample size. Note that since we are given a randomly drawn sample we shall requires all results in expectation over draw of sample. Later on we illustrate how such a result can be converted to a result that hold with high probability over the sample.\nDefinition 5. We say that a problem is “Statistically Learnable” given a target hypothesis class H with rate cons(n) if there exists a Statistical Learning Rule, A, such that :\nsup D\nES∼Dn [ LD(A(S))− inf\nh∈H LD(h)\n] ≤ cons(n)\nFurther as long as cons(n)→ 0 we simply say that the problem is statistically learnable/optimizable.\nWhenever a problem is learnable, we will refer to any learning rule A such that\nsup D\nES∼Dn [ LD(A(S))− inf\nh∈H LD(h)\n] → 0\nas a universally consistent learning rule. This definition of learnability, requiring a uniform rate for all distributions, is the relevant notion for studying learnability of a hypothesis class. It is a direct generalization of agnostic PAC-learnability ([11]) to Vapnik”s General Setting of Learning as studied by [12] and others.\nA closely related notion to learnability is that of the sample complexity of a problem. We first define the sample complexity of a given learning rule and then proceed to define sample complexity of a learning problem.\nDefinition 6. Given a learning rule A, and > 0, the sample complexity of the rule for is defined as\nn( ,A,Z) = inf { n ∈ N ∣∣∣∣ sup D ES∼Dn [ LD(A(S))− inf h∈H LD(h) ] ≤ }\nSample complexity of the learning problem is defined as n( ,Z) = infA n( ,A,Z).\n14\nObviously a problem is learnable if and only if for each > 0, n( ) < ∞ and for any universally consistent learning rule A and any > 0, n( ,A) <∞.\nThe notion of sample complexity talked about above is the worst case one in the sense that we want learnability with uniform rates, irrespective of the distribution chosen. Sometimes one might have prior knowledge or restrictions on distributions that can occur and so one might only need rates to hold uniformly over a specific family of distributions. To capture this notion we define below sample complexity of a learning problem given a specific family of distributions D over instances Z .\nDefinition 7. Given a family D of Borel distributions over instance space Z and an > 0, the sample complexity of any rule A is defined as\nnD( ,A) = inf { n ∈ N ∣∣∣∣ sup D∈D ES∼Dn [ LD(A(S))− inf h∈H LD(h) ] ≤ }\nFurther the sample complexity of the learning problem over this family of distributions D is defined as nD( ,Z) = infA nD( ,A,Z).\nA concept closely related to statistical learning that plays an important role in characterizing learnability is the notion of generalization.\nDefinition 8. A learning rule A generalizes with rate gen(n) under distribution D if for all n ∈ N,\nES∼Dn [|L(A(S))− LS(A(S))|] ≤ gen(n). (3.1.1)\nThe rule A is said to universally generalize with rate gen(n) if it generalizes with rate gen(n) under all distributions D over Z .\nWe note that other authors sometimes define “consistent”, and thus also “learnable” as a combination of our notions of “consistent” and “generalizing”.\nEmpirical Risk Minimization (Sample Average Approximation Approach) and Related Notions :\nPerhaps the most common approach used for learning problems is Empirical Risk Minimization (ERM) as it is referred to in machine learning terminology or Sample Average Approximation (SAA) approach as is widely referred to in the stochastic optimization terminology. The basic idea is for the learning algorithm to return hypothesis in target class H that minimizes average loss over sample. Specifically a learning rule AERM is an ERM (Empirical Risk Minimizer) if it minimizes the average loss, that is :\nLS(AERM(S)) = LS(ĥS) = inf h∈H LS(h). (3.1.2)\n15\nwhere we use LS(ĥS) = infh∈H LS(h) to refer to the minimal empirical loss. But since there might be several hypotheses minimizing the empirical risk, ĥS does not refer to a specific hypotheses and there might be many rules which are all ERM’s. While Empirical Risk Minimization (or equivalently the Sample Average Approximation approach) is a widely used learning rule, we now introduce a closely related concept of an Asymptotic Empirical Risk Minimizer which will play an important role in characterizing learnability in general problems.\nDefinition 9. We say that a rule A is an AERM (Asymptotic Empirical Risk Minimizer) with rate erm(n) under distribution D if:\nES∼Dn [ L̂(A(S))− L̂(ĥS) ] ≤ erm(n) (3.1.3)\nFurther, a learning rule is universally an AERM with rate erm(n), if it is an AERM with rate erm(n) under all distributions D over Z . A\nYet another closely related notion of an approximate ERM is the following notion of alway AERM.\nDefinition 10. A learning rule A is an always AERM with rate erm(n), if for any sample S of size n, it holds that\nL̂(A(S))− L̂(ĥS) ≤ erm(n) (3.1.4)"
    }, {
      "heading" : "3.2 Background",
      "text" : ""
    }, {
      "heading" : "3.2.1 Learnability and Uniform Convergence",
      "text" : "As discussed in the introduction, a central notion for characterizing learnability is uniform convergence. Formally, we say that uniform convergence holds for a learning problem, if the empirical risks of hypotheses in the hypothesis class converges to their population risk uniformly, with a distribution-independent rate:\nsup D\nES∼Dn [\nsup h∈H |L(h)− LS(h)|\n] n→∞−→ 0. (3.2.1)\nIt is straightforward to show that if uniform convergence holds, then a problem can be learned with the ERM learning rule.\nFor binary classification problems (where Z = X × {0, 1}, each hypothesis is a mapping from X to {0, 1}, and `(h; (x, y)) = 1{h(x)6=y}), [13] showed that the finiteness of a simple combinatorial measure known as the VC-dimension implies uniform convergence. Furthermore, it can be shown that binary classification problems with infinite VC-dimension are not learnable in a distribution-independent sense. This establishes the condition of having finite VC-dimension, and thus also uniform convergence, as a necessary and sufficient condition for learnability.\nSuch a characterization can also be extended to regression, such as regression with squared loss, where h is now a real-valued function, and `(h; (x, y)) = (h(x) − y)2. The property of having finite fat-shattering di-\n16\nmension at all finite scales now replaces the property of having finite VC-dimension, but the basic equivalence still holds: a problem is learnable if and only if uniform convergence holds ([10], see also [14], Chapter 19). These results are usually based on clever reductions to binary classification. However, the General Learning Setting that we consider is much more general than classification and regression, and includes setting where a reduction to binary classification is impossible.\nTo justify the necessity of uniform convergence even in the General Learning Setting, Vapnik attempted to show that in this setting, learnability with the ERM learning rule is equivalent to uniform convergence ([15]). Vapnik noted that this result does not hold, due to “trivial” situations. In particular, consider the case where we take an arbitrary learning problem (with hypothesis class H), and add to H a single hypothesis h̃ such that `(h̃, z) < infh∈H `(h, z) for all z ∈ Z (see figure 3.1 below). This learning problem is now trivially learnable, with the ERM learning rule which always picks h̃. Note that no assumptions whatsoever are made on H - in particular, it can be arbitrarily complex, with no uniform convergence or any other particular property. Note also that such a phenomenon is not possible in the binary classification setting, where `(h; (x, y)) = 1{h(x)6=y}, since on any (x, y) we will have hypotheses with `(h; (x, y)) = `(h̃; (x, y)) and thus if H is very complex (has infinite VC dimension) then on every training set there will be many hypotheses with zero empirical error.\nTo exclude such “trivial” cases, Vapnik introduced a stronger notion of consistency, termed as “strict consistency”, which in our notation is defined as\n∀c ∈ R, inf h:L(h)≥c\nLS(h) n→∞−→ inf\nh:L(h)≥c L(h) ,\nwhere the convergence is in probability. The intuition is that we require the empirical risk of the ERM to\n17\nconverge to the lowest possible risk, even after discarding all the “good” hypotheses whose risk is smaller than some threshold. Vapnik then showed that such strict consistency of the ERM is in fact equivalent to (one-sided) uniform convergence, of the form\nsup h∈H\n(L(h)− LS(h)) n→∞−→ 0 (3.2.2)\nin probability. Note that this equivalence holds for every distribution separately, and does not rely on universal consistency of the ERM.\nThese results seem to imply that up to “trivial” situations, a uniform convergence property indeed characterizes learnability, at least using the ERM learning rule. However, as we will see later on, the situation is in fact not that simple."
    }, {
      "heading" : "3.2.2 Various Complexity Measures and Uniform Convergence",
      "text" : "We begin by defining the class of functions F ⊆ RZ we will often refer to the loss class, as :\nF(H,Z) := {z 7→ `(h, z) : h ∈ H} (3.2.3)\nWe shall often drop the arguments H and Z and simply use F to mean F(H,Z). Notice that the function class is indexed by hypotheses in class H. Note that the notion uniform convergence in Equation 3.2.1 can now be rewritten as :\nsup D\nES∼Dn [\nsup f∈F\n( E [f ]− ÊS [f ] )] n→∞−→ 0.\nVarious complexity measures have been introduced in machine learning and empirical process theory literature to bound rates if uniform convergence and thus get upper bounds on learning rates for ERM/SA algorithms. We introduce and discuss about a few of these below. See Appendix ?? for the relationships among these various complexity measures.\nDefinition 11 (Statistical Rademacher Complexity). The empirical Rademacher Complexity of a function class F ⊂ RZ given a sample S = {z1, . . . , z|S|} is defined as :\nR̂S(F) := E sup f∈F 1 |S| |S|∑ i=1 tf(zi)  Further for any n ∈ N, we define the worst-case statistical Rademacher complexity as :\nRiidn (F) = sup z1,...,zn∈Z E [ sup f∈F 1 n n∑ i=1 if(zi) ] (3.2.4)\n18\nIt can be easily shown that for any distribution D,\nES∼Dn [\nsup f∈F\n( E [f ]− ÊS [f ] )] ≤ 2ES∼Dn [ R̂S(F) ] ≤ 2Riidn (F)\nand so the empirical Rademacher complexity and so also the worst case statistical Rademacher complexity provide upper bound on the rate of uniform convergence over any function class F .\nAnother tool from empirical process theory that is often used to upper bound rates of uniform convergence is covering numbers of the function class. The statistical covering number of a function class is defined below.\nDefinition 12 (Statistical Covering Number). A set V ⊂ Rn is an α-cover (with respect to `p-norm) of a function class F ⊆ RZ on a sample S = z1, . . . , zn if,\n∀f ∈ F , ∀ ∈ {±1}n ∃v ∈ V s.t.\n( 1\nn n∑ t=1\n|vt − f(zt)|p )1/p ≤ α\nThe statistical covering number of a function class F on a given sample S = z1, . . . , zn is defined as\nN iidp (α,F , S) = min{|V | : V is an α− cover w.r.t. `p-norm of F on S}.\nFurther defineN iidp (α,F , n) = supS∈Zn N iidp (α,F , S), the maximal `p covering number ofF over samples of size n.\nPollard’s bound [16] and Dudley integral bound [17] can be used to bound rates of uniform convergence in terms of covering numbers. See A.1 in the appendix for a bound on statistical Rademacher complexity in terms of covering numbers through a refined Dudley bound. Yet another combinatorial tool that can be used to bound rates of uniform convergence is the so called fat-shattering dimension defined below.\nDefinition 13 (Fat-Shattering Dimension). A sample S = z1, . . . , zd is said to be α-shattered by a function class F ⊆ RX , if there exists s1, . . . , sd ∈ R such that\n∀ ∈ {±1}d, ∃f ∈ F s.t. ∀t ∈ [d], t(f(zt)− st) ≥ α/2\nThe sequence s1, . . . , sd is called the witness to shattering. The statistical fat-shattering dimension fatiidα (F ,Z) at scale α is the largest d such that F α-shatters some sample S of size d.\nOne can upper bound covering numbers in terms of fat-shattering dimension. See Section A.2 of Appendix A for a bound on covering number in terms of fat shattering dimension of function class.\n19"
    }, {
      "heading" : "3.2.3 Learnability and Stability",
      "text" : "Instead of focusing on the hypothesis class, and ensuring uniform convergence of the empirical risks of hypothesis in this class, an alternative approach is to directly control the variance of the learning rule. Here, it is not the complexity of the hypothesis class which matters, but rather the way that the learning rule explores this hypothesis class. This alternative approach leads to the notion of stability in learning. It is important to note that stability is a property of a learning rule, not of the hypothesis class.\nIn the context of modern learning theory1, the use of stability can be traced back at least to the work of [21], which noted that the sensitivity of a learning algorithm with regard to small changes in the sample controls the variance of the leave-one-out estimate. The authors used this observation to obtain generalization bounds (w.r.t. the leave-one-out estimate) for the k-nearest neighbor algorithm. It is interesting to note that a uniform convergence approach for analyzing this algorithm simply cannot work, because the “hypothesis class” in this case has unbounded complexity. These results were later extended to other “local” learning algorithms (see [22] and references therein). In addition, practical methods have been developed to introduce stability into learning algorithms, in particular the Bagging technique introduced by [23].\nOver the last decade, stability was studied as a generic condition for learnability. [24] showed that an algorithm operating on a hypothesis class with finite VC dimension is also stable (under a certain definition of stability). [25] introduced a strong notion of stability (denoted as uniform stability) and showed that it is a sufficient condition for learnability, satisfied by popular learning algorithms such as regularized linear classifiers and regressors in Hilbert spaces (including several variants of SVM). [26] introduced several weaker variants of stability, and showed how they are sufficient to obtain generalization bounds for algorithms stable in their sense.\nThe works cited above mainly considered stability as a sufficient condition for learnability. A more recent line of work ([27],[28]) studied stability as a necessary condition for learnability. However, the line of argument is specific to settings where uniform convergence holds and is necessary for learning. With this assumption, it is possible to show that the ERM algorithm is stable, and thus stability is also a necessary condition for learning. However, as we will see later on in this chapter, uniform convergence is in fact not necessary for learning in the General Learning Setting, and stability plays there a key role which has nothing to do with uniform convergence.\nFinally, it is important to note that the results cited above make use of many different definitions of stability, which unfortunately are not always comparable. All of them measure stability as the amount of change in the algorithm’s output as a function of small changes to the sample on which the algorithm is run. However, “amount of change to the output” and “small changes to the sample” can be defined in many different ways. “Amount of change to the output” can mean change in risk, change in loss with respect to particular examples, or supremum of change in loss over all examples. “Small changes to the sample” usually mean either deleting one example or replacing it with another one (and even here, one can talk about removing/replacing one instance at random, or in some arbitrary manner). Finally, this measure of change can be measured with\n1In a more general mathematical context, stability has been around for much longer. The necessity of stability for so-called inverse problems to be well posed was first recognized by [18]. The idea of regularization (that is, introducing stability into ill-posed inverse problems) became widely known through the works of [19] and [20]. We return to the notion of regularization later on.\n20\nrespect to any arbitrary sample, in expectation over samples drawn from the underlying distribution; or in high probability over samples. ."
    }, {
      "heading" : "3.3 Failure of Uniform Convergence and ERM/SAA Approaches",
      "text" : "In this section, we study a special case of the General Learning Setting, where there is a real gap between learnability and uniform convergence, in the sense that there are non-trivial problems where no uniform convergence holds (not even in a local sense), but they are still learnable. Moreover, some of these problems are learnable with an ERM (again, without any uniform convergence), and some are not learnable with an ERM, but rather with a different mechanism. We also discuss why this peculiar behavior does not formally contradict Vapnik’s results on the equivalence of strict consistency of the ERM and uniform convergence, as well as the important role that regularization seems to play here, but in a different way than in standard theory."
    }, {
      "heading" : "3.3.1 Learning without Uniform Convergence : Stochastic Convex Optimization",
      "text" : "A stochastic convex optimization problem is a special case of the General Learning Setting discussed above, with the added constraints that the objective function `(h; z) is Lipschitz-continuous and convex in h for every z, and that H is closed, convex and bounded. We will focus here on problems where H is a subset of a Hilbert space. A special case is the familiar linear prediction setting, where z = (x, y) is an instance-label pair, each hypothesis h belongs to a subset H of a Hilbert space, and `(h;x, y) = `(〈h, φ(x)〉 , y) for some feature mapping φ and a loss function ` : R× Y → R, which is convex w.r.t. its first argument.\nThe situation in which the stochastic dependence on h is linear, as in the preceding example, is fairly well understood. When the domainH and the mapping φ are bounded, we have uniform convergence, in the sense that |L(h) − L̂(h)| is uniformly bounded over all h ∈ H (see [29]). This uniform convergence of L̂(h) to L(h) justifies choosing the empirical minimizer ĥS = arg minh L̂(h), and guarantees that the expected value of L(ĥS) converges to the optimal value L∗ = infh L(h).\nEven if the dependence on h is not linear, it is still possible to establish uniform convergence (using covering number arguments) provided thatH is finite dimensional. Unfortunately, when we turn to infinite dimensional hypothesis spaces, uniform convergence might not hold and the problem might not be learnable with empirical minimization. Surprisingly, it turns out that this does not imply that the problem is unlearnable. We will show that using a regularization mechanism, it is possible to devise a learning algorithm for any stochastic convex optimization problem, even when uniform convergence does not hold. This mechanism is fundamentally related to the idea of stability, and will be a good starting point for our more general treatment of stability and learnability in the next section of this chapter.\nWe now turn to discuss our first concrete example. Consider the convex stochastic optimization problem\n21\ngiven by\n`Eq. (3.3.1)(h; (x, α)) = ‖α ∗ (h− x)‖ = √∑\ni\nα2[i](h[i]− x[i])2 , (3.3.1)\nwhere for now we let H to be the d-dimensional unit sphere H = { h ∈ Rd : ‖h‖ ≤ 1 } , we let z = (x, α) with α ∈ [0, 1]d and x ∈ H, and we define u ∗ v to be an element-wise product. We will first consider a sequence of problems, where d = 2n for any sample size n, and establish that we cannot expect a convergence rate which is independent of the dimensionality d. We then formalize this example in infinite dimensions.\nOne can think of the problem in Eq. (3.3.1) as that of finding the “center” of an unknown distribution over x ∈ Rd, where we also have stochastic per-coordinate “confidence” measures α[i]. We will actually focus on the case where some coordinates are missing, namely that α[i] = 0.\nConsider the following distribution over (x, α): x = 0 with probability one, and α is uniform over {0, 1}d. That is, α[i] are i.i.d. uniform Bernoulli. For a random sample (x1, α1), . . . , (xn, αn) if d > 2n then we have that with probability greater than 1−e−1 > 0.63, there exists a coordinate j ∈ 1 . . . d such that all confidence vectors αi in the sample are zero on the coordinate j, that is αi[j] = 0 for all i = 1..n. Let ej ∈ H be the standard basis vector corresponding to this coordinate. Then\nL̂Eq. (3.3.1)(ej) = 1\nn n∑ i=1 ‖αi ∗ (ej − 0)‖ = 1 n n∑ i=1 |αi[j]| = 0,\nwhere L̂Eq. (3.3.1)(·) denotes the empirical risk w.r.t. the function `Eq. (3.3.1)(·). On the other hand, letting LEq. (3.3.1)(·) denote the actual risk w.r.t. `Eq. (3.3.1)(·), we have\nLEq. (3.3.1)(ej) = Ex,α [‖α ∗ (ej − 0)‖] = Ex,α [|α[j]|] = 1/2.\nTherefore, for any n, we can construct a convex Lipschitz-continuous objective in a high enough dimension such that with probability at least 0.63 over the sample, suph ∣∣∣LEq. (3.3.1)(h)− L̂Eq. (3.3.1)(h)∣∣∣ ≥ 1/2. Furthermore, since `(·; ·) is non-negative, we have that ej is an empirical minimizer, but its expected value LEq. (3.3.1)(ej) = 1/2 is far from the optimal expected value minh LEq. (3.3.1)(h) = LEq. (3.3.1)(0) = 0.\nTo formalize the example in a sample-size independent way, take H to be the unit sphere of an infinitedimensional Hilbert space with orthonormal basis e1, e2, . . ., where for v ∈ H, we refer to its coordinates v[j] = 〈v, ej〉 w.r.t this basis. The confidences α are now a mapping of each coordinate to [0, 1]. That is, an infinite sequence of reals in [0, 1]. The element-wise product operation α ∗ v is defined with respect to this basis and the objective function `Eq. (3.3.1)(·) of Eq. (3.3.1) is well defined in this infinite-dimensional space.\nWe again take a distribution over z = (x, α) where x = 0 and α is an infinite i.i.d. sequence of uniform Bernoulli random variables (that is, a Bernoulli process with each αi uniform over {0, 1} and independent of all other αj). Now, for any finite sample there is almost surely a coordinate j with αi[j] = 0 for all i, and so we a.s. have an empirical minimizer L̂Eq. (3.3.1)(ej) = 0 with LEq. (3.3.1)(ej) = 1/2 > 0 = LEq. (3.3.1)(0).\nAs a result, we see that the empirical values L̂Eq. (3.3.1)(h) do not converge uniformly to their expectations, and empirical minimization is not guaranteed to solve the problem. Moreover, it is possible to construct a\n22\nsharper counterexample, in which the unique empirical minimizer ĥS is far from having optimal expected value. To do so, we augment `Eq. (3.3.1)(·) by a small term which ensures its empirical minimizer is unique, and far from the origin. Consider:\n`Eq. (3.3.2)(h; (x, α)) = `Eq. (3.3.1)(h; (x, α)) + ∑ i 2−i(h[i]−1)2 (3.3.2)\nwhere = 0.01. The objective is still convex and (1 + )-Lipschitz. Furthermore, since the additional term is strictly convex, we have that `Eq. (3.3.2)(h; z) is strictly convex w.r.t. h and so the empirical minimizer is unique.\nConsider the same distribution over z: x = 0 while α[i] are i.i.d. uniform zero or one. The empirical minimizer is the minimizer of L̂Eq. (3.3.2)(h) subject to the constraints ‖h‖ ≤ 1. Identifying the solution to this constrained optimization problem is tricky, but fortunately not necessary. It is enough to show that the optimum of the unconstrained optimization problem h∗UC = arg min L̂Eq. (3.3.2)(h) (without constraining h ∈ H) has norm ‖h∗UC‖ ≥ 1. Notice that in the unconstrained problem, whenever αi[j] = 0 for all i = 1..n, only the second term of `Eq. (3.3.2) depends on h[j] and we have h∗UC[j] = 1. Since this happens a.s. for some coordinate j, we can conclude that the solution to the constrained optimization problem lies on the boundary ofH, that is\n∥∥∥ĥS∥∥∥ = 1. But for such a solution we have LEq. (3.3.2)(ĥS) ≥ Eα √∑ i α[i]ĥ2S [i]  ≥ Eα [∑ i α[i]ĥ2S [i] ] = ∑ i ĥ2S [i]Eα [α[i]] = 1 2 ∥∥∥ĥS∥∥∥2 = 1 2 ,\nwhile L∗ ≤ L(0) = .\nIn conclusion, no matter how big the sample size is, the unique empirical minimizer ĥS of the stochastic convex optimization problem in Eq. (3.3.2) is a.s. much worse than the population optimum, L(ĥS) ≥ 12 > ≥ L∗, and certainly does not converge to it.\nStill Learnable Using Stochastic Approximation (SA) Approach :\nSo far we established that the learning problem specified in Equation 3.3.2 fails to satisfy uniform convergence and further ERM/SAA approach fails to provide any non-trivial guarantee. Specifically we saw that with the particular distribution over α as the uniform distribution over {±1}N, we have that L(ĥS) ≥ 12 but however L∗ ≤ = 0.01. However we shall show that this problem is still learnable. Specifically we will see that using the so called Stochastic Approximation (SA) approach, we can get a guarantee over learning rate of order 1/ √ n for the problem.\nThe first thing we already noticed was that the problem specified in Equation 3.3.2 is (1 + )-Lipschitz. Further note that the hypothesis set H is the unit ball in a Hilbert space and that the problem is convex in its first argument. In the second part of this dissertation we will formally define Stochastic Approximation (SA) approach for convex learning problems and in a more general way than usually described in literature. For now we will informally use the term Stochastic Approximation approach to refer to stochastic gradient\n23\ndescent (or online gradient descent followed by averaging). To this end consider the learning algorithm for the problem which given sample S = {(x1, α1), . . . , (xn, αn)} is described below :\nStochastic Gradient Descent for problem Eq. (3.3.2): Initialize h1 = 0 and η = 1/ √ |S| for t = 1 to |S| h′t+1 = ht − η∇`Eq. (3.3.2)(ht; (xt, αt))\nht+1 =  h ′ t+1 if ∥∥h′t+1∥∥ ≤ 1 h′t+1\n‖h′t+1‖ otherwise\nend for Return hS = 1|S| ∑|S| t=1 ht.\nSince the problem is convex and (1 + )-Lipschitz and sinceH is the unit ball in the Hilbert space it follows (for instance from [30] + online to batch conversion) that irrespective of which distribution D over instances we use,\nLD(hS)− inf h∈H\nLD(h) ≤ √ 2(1 + )\nn\nThus we can conclude that the problem is learnable and in fact enjoys a rate of order 1√ n , yet as we already say both uniform convergence and ERM (SAA approach) fails."
    }, {
      "heading" : "3.3.2 Learnability via Stability : Role of Regularization",
      "text" : "At this point, we have seen an example in the stochastic convex optimization framework where uniform convergence does not hold, and the ERM algorithm fails. Yet we saw that the problem was learnable using SA approach. We will now see an alternate explanation for why the problem is learnable that mainly uses stability to explain the success. We will specifically consider an algorithm that minimizes a regularized average loss and show that this algorithm can guarantee a similar learning rate as the stochastic gradient descent approach. We will show that the regularization induces stability to the learning problem and this stability in turn assures learnability.\nGiven a stochastic convex optimization problem with an objective function `(h; z), consider a regularized version of it: instead of minimizing the expected risk Ez [`(h; z)] over h ∈ H, we will try to minimize\nEz [ `(h; z) + λ\n2 ‖h‖2 ] for some λ > 0. Notice that this is simply a stochastic convex optimization problem w.r.t. the objective function `(h; z) + λ2 ‖h‖\n2. We will show that this regularized problem is learnable using the ERM algorithm (namely, by attempting to minimize 1n ∑ i `(h; zi) + λ 2 ‖h‖\n2), by showing that the ERM algorithm is stable. By taking λ→ 0 at an appropriate rate as the sample size increases, we are able to solve the original stochastic problem optimization problem, w.r.t. `(h; z).\n24\nThe key characteristic of the regularized objective function we need is that it is λ-strongly convex. Formally, we say that a real function g(·) over a domainH in a Hilbert space is λ-strongly convex (where λ ≥ 0), if the function g(·)− λ2 ‖ · ‖ 2 is convex. In this case, it is easy to verify that if h minimizes g then\n∀h′, g(h′)− g(h) ≥ λ2 ‖h ′ − h‖2 .\nWhen λ = 0, strong convexity corresponds to standard convexity. In particular, it is immediate from the defintion that `(h; z) + λ2 ‖h‖ 2 is λ-strongly convex w.r.t. h (assuming `(h; z) is convex).\nThe arguments above are formalized in the following two theorems:\nTheorem 1. Consider a stochastic convex optimization problem such that `(h; z) is λ-strongly convex and L-Lipschitz with respect to h ∈ H. Let z1, . . . , zn be an i.i.d. sample and let ĥS be the empirical minimizer. Then, we have that :\nES [ L(ĥS)− inf\nh∈H L(h)\n] ≤ 4L 2\nλn . (3.3.3)\nTheorem 2. Let f : H×Z → R be such thatH is bounded by B and `(h, z) is convex and L-Lipschitz with respect to h. Let z1, . . . , zn be an i.i.d. sample, let λ =\n√ 16L2\nB2 n and let ĥλ be the minimizer of\nĥλ = min h∈H\n( 1 n n∑ i=1 `(h, zi) + λ 2 ‖h‖ 2 ) (3.3.4)\nThen, we have that\nES [ L(ĥλ)− inf\nh∈H L(h)\n] ≤ 4 √ L2B2\nn\n( 1 + 8\nn\n) .\nFrom the above theorem, we see that regularization is essential for convex stochastic optimization. It is important to note that even for the strongly convex optimization problem in Theorem 1, where the ERM algorithm does work, it is not due to uniform convergence. To see this, consider augmenting the objective function `Eq. (3.3.1)(·) from Eq. (3.3.1) with a strongly convex term:\n`Eq. (3.3.5)(h;x, α) = `Eq. (3.3.1)(h;x, α) + λ\n2 ‖h‖2 . (3.3.5)\nThe modified objective `Eq. (3.3.5)(·; ·) is λ-strongly convex and (1 + λ)-Lipschitz overH = {h : ‖h‖ ≤ 1} and thus satisfies the conditions of Theorem 1. Now, consider the same distribution over z = (x, α) used earlier: x = 0 and α is an i.i.d. sequence of uniform zero/one Bernoulli variables. Recall that almost surely we have a coordinate j that is never “observed”, namely such that ∀iαi[j] = 0. Consider a vector tej of magnitude 0 < t ≤ 1 in the direction of this coordinate. We have that L̂Eq. (3.3.5)(tej) = λ2 t\n2 (where L̂Eq. (3.3.5)(·) is the empirical risk w.r.t. `Eq. (3.3.5)(·)) but LEq. (3.3.5)(tej) = 12 t + λ 2 t\n2. Hence, letting LEq. (3.3.5)(·) denote the risk w.r.t. `Eq. (3.3.5)(·), we have that LEq. (3.3.5)(tej) − L̂Eq. (3.3.5)(tej) = t/2. In particular, we can set t = 1 and establish suph∈H(LEq. (3.3.5)(h)− L̂Eq. (3.3.5)(h)) ≥ 12 regardless of the sample size.\nWe see then that the empirical averages L̂Eq. (3.3.5)(h) do not converge uniformly to their expectations.\n25\nMoreover, the example above shows that there is no uniform convergence even in a local sense, namely over all hypotheses whose risk is close enough toL∗, or those close enough to the minimizer of `Eq. (3.3.5)(h;x, α).\nRegularization Vs Constrained Minimization\nThe technique of regularizing the objective function by adding a “bias” term is old and well known. In particular, adding ‖h‖2 is the so-called Tikhonov Regularization technique, which has been known for more than half a century (see [19]). However, the role of regularization in our case is very different than in familiar settings such as `2 regularization in SVMs and `1 regularization in LASSO. In those settings regularization serves to constrain our domain to a low-complexity domain (e.g., low-norm predictors), where we rely on uniform convergence. In fact, almost all learning guarantees that we are aware of can be expressed in terms of some sort of uniform convergence.\nIn our case, constraining the norm of h does not ensure uniform convergence. Consider the example `Eq. (3.3.1)(·) we have seen earlier. Even over a restricted domainHr = {h : ‖h‖ ≤ r}, for arbitrarily small r > 0, the empirical averages L̂(h) do not uniformly converge to L(h). Furthermore, consider replacing the regularization term λ ‖h‖2 with a constraint on the norm of ‖h‖, namely, solving the problem\nh̃r = arg min ‖h‖≤r L̂(h) (3.3.6)\nWe cannot solve the stochastic optimization problem by setting r in a distribution-independent way (i.e., without knowing the solution...). To see this, note that when x = 0 a.s. we must have r → 0 to ensure L(h̃r)→ L∗. However, if x = e1 a.s., we must set r → 1. No constraint will work for all distributions over Z = (X , α)! This sharply contrasts with traditional uses of regularization, where learning guarantees are typically stated in terms of a constraint on the norm rather than in terms of a parameter such as λ, and adding a regularization term of the form λ2 ‖h‖ 2 is viewed as a proxy for bounding the norm ‖h‖."
    }, {
      "heading" : "3.3.3 Contradiction to Vapnik?",
      "text" : "In Subsection 3.2.1, we discussed how Vapnik showed that uniform convergence is in fact necessary for learnability with the ERM. At first glance, this might seem confusing in light of the examples presented above, where we have problems learnable with the ERM without uniform convergence whatsoever.\nThe solution for this apparent paradox is that our examples are not “strictly consistent” in Vapnik’s sense. Recall that in order to exclude “trivial” cases, Vapnik defined strict consistency of empirical minimization as (in our notation):\n∀c ∈ R, inf h:L(h)≥c LS(h) −→ inf h:L(h)≥c L(h) , (3.3.7)\nwhere the convergence is in probability. This condition indeed ensures that L(ĥS) P→ L∗. Vapnik’s Key Theorem on Learning Theory [15, Theorem 3.1] then states that strict consistency of empirical minimization\n26\nis equivalent to one-sided 2 uniform convergence. In the example presented above, even though Theorem 1 establishes LEq. (3.3.5)(ĥS)\nP→ L∗, the consistency isn’t “strict” by the definition above. To see this, for any c > 0, consider the vector tej (where ∀iαi[j] = 0) with t = 2c. We have LEq. (3.3.5)(tej) = 12 t + λ 2 t 2 > c but L̂Eq. (3.3.5)(tej) = λ2 t 2 = 2λc2. Focusing on λ = 12 we get:\ninf LEq. (3.3.5)(h)≥c\nL̂Eq. (3.3.5)(h) ≤ c2 (3.3.8)\nalmost surely for any sample size n, violating the strict consistency requirement Eq. (3.3.7).\nWe emphasize that stochastic convex optimization is far from “trivial” in that there is no dominating hypothesis that will always be selected. Although for convenience of analysis we took x = 0, one should think of situations in which x is stochastic with an unknown distribution. This shows that uniform convergence is a sufficient, but not at all necessary, condition for consistency of empirical minimization in non-trivial settings."
    }, {
      "heading" : "3.4 Stability of Learning Rules",
      "text" : "In the previous section, we have shown that in the General Learning Setting, it is possible for problems to be learnable without uniform convergence, in sharp contrast to previously considered settings. The key underlying mechanism which allowed us to learn is stability. In this section, we study the connection between learnability and stability in greater depth, and show that stability can in fact characterize learnability. Also, we will see how various “common knowledge facts”, which we usually take for granted and are based on a “uniform convergence equivalent to learnability” assumption, do not hold in the General Learning Setting, and things can be much more delicate.\nWe will refer to settings where learnability is equivalent to uniform convergence as “supervised classification” settings. While supervised classification does not encompass all settings where this equivalence holds, most equivalence results refer to it either explicitly or implicitly (by reduction to a classification problem).\nWe start by giving the exact definition of the stability notions that we will use. As discussed earlier, there are many possible stability measures, some of which can be used to obtain results of a similar flavor to the ones below. The definition we use seems to be the most convenient for the goal of characterizing learnability in the General Learning Setting. In subsection 3.4.1, we provide a few illustrating examples to the subtle differences that can arise from slight variations in the stability measure.\nOur two stability notions are based on replacing one of the training sample instances. For a sample S of size n, let S(i) = {z1, ..., zi−1, z′i, zi+1, ..., zn} be a sample obtained by replacing the i-th observation of S with some different instance z′i. When not discussed explicitly, the nature of how z ′ i is obtained should be obvious from context.\n2“One-sided” meaning requiring only sup(L(h)− LS(h)) −→ 0, rather then sup |L(h)− LS(h)| −→ 0.\n27\nDefinition 14. A rule A is uniform-RO stable3 with rate stable(n), if for all possible S(i) and any z′ ∈ Z ,\n1 n n∑ i=1 ∣∣∣`(A(S(i)); z′)− `(A(S); z′)∣∣∣ ≤ stable(n). Definition 15. A rule A is average-RO stable with rate stable(n) under distributions D if∣∣∣∣∣ 1n n∑ i=1 ES∼Dn,(z′1,...,z′n)∼Dn [ `(A(S(i)); z′i)− `(A(S); z′i)\n]∣∣∣∣∣ ≤ stable(n). Note that this definition corresponds to assuming that the expected empirical risk of the learning rule converges to the expected risk - see Lemma 13.\nWe say that a rule is universally stable with rate stable(n), if the stability property holds with rate stable(n) for all distributions.\nClaim 3. Uniform-RO stability with rate stable(n) implies average-RO stability with rate stable(n).\nThe following subsection contains a brief literature survey on various definitions of stability and discusses and illustrates the differences between these various notions. The main results in this chapter are only based on the two definitions of stability we introduced so far and the following subsection is mainly for pedantic purposes and the reader may choose to skip it."
    }, {
      "heading" : "3.4.1 Comparison with Existing Literature and Other Notions of Stability",
      "text" : "The existing literature on stability in learning, briefly surveyed in Subsection 3.2.3, utilizes many different stability measures. All of them measure the amount of change in the algorithm’s output as a function of small changes to the sample on which the algorithm is run. However, they differ in how “output”, “amount of change to the output”, and “small changes to the sample” are defined. In Section ??, we used three stability measures. Roughly speaking, one measure (average-RO stability) is the expected change in the objective value on a particular instance, after that instance is replaced with a different instance. The second measure and third measure (uniform-RO stability and strongly-uniform-RO stability respectively) basically deal with the maximal possible change in the objective value with respect to a particular instance, by replacing a single instance in the training set. However, instead of measuring the objective value on a specific instance, we could have measured the change in the risk of the returned hypothesis, or any other distance measure between hypotheses. Instead of replacing an instance, we could have talked about adding or removing one instance from the sample, either in expectation or in some arbitrary manner. Such variations are common in the literature.\nTo relate our stability definitions to the ones in the literature, we note that our definitions of uniform-RO stability and strongly-uniform-RO stability are somewhat similar to uniform stability ([25]), which in our\n3RO is short for “replace-one”.\n28\nnotation is defined as supS,z maxi |`(A(S; z))− `(A(S\\i); z)|, where S\\i is the training sample S with instance zi removed. Compared to uniform-RO stability, here we measure maximal change over any particular instance, rather than average change over all instances in the training sample. Also, we deal with removing an instance rather than replacing it. Strongly-uniform-RO stability is more similar, with the only formal difference being removal vs. replacement of an instance. However, the results for uniform stability mostly assume deterministic learning rules, while in this work we have used strongly-uniform-RO stability solely in the context of randomized learning rules. For deterministic learning rules, the differences outlined above are sufficient to make uniform stability a strictly stronger requirement than uniform-RO stability, since it is easy to come up with learning problems and (non-symmetric) learning rules which are uniform-RO stable but not uniformly stable. Moreover, we show that uniform-RO stable AERM’s characterize learnability, while it is well known that uniformly stable AERM’s are not necessary for learnability (see [26]). For the same reason, our notion of strongly-uniform-RO stability is apparently too strong to characterize learnability when we deal with deterministic learning rules, as opposed to randomized learning rules.\nOur definition of average-RO stable is similar to “average stability” defined in [27], which in our notation is defined as ES∼Dn,z′1 [ `(A(S(i)); z1)− `(A(S); z1) ] . Compared to average-RO stability, the main difference is that the change in the objective value is measured with respect to z1 rather than an average over zi for all i, and stems from the assumption there that the learning algorithm is symmetric. Notice however that we do not make such an assumption.\nFor an elaborate study on other stability notions and their relationships, see [26].\nUnfortunately, many of the stability notions in the literature are incomparable, and even slight changes in the definition radically affect their behavior. WE shall now investigate these differences in more detail.\nLOO Stability vs. RO Stability\nThe stability definitions we saw introduced up to now were all based on the idea of replacing one instance in the training sample by another instance (e.g., “RO” or “replace-one” stability). An alternative set of definitions can be obtained based on removing one instance in the training sample (e.g., “LOO” or “leaveone-out” stability). Despite seeming like a small change, it turns out there is a considerable discrepancy in terms of the obtainable results, compared to RO stability. In this subsection, we wish to discuss these discrepancies, as well as show how small changes to the stability definition can materially affect its strength.\nSpecifically, we consider the following four LOO stability measures, each slightly weaker than the previous one. The first and last are similar to our notion of uniform-RO stability and average-RO stability respectively. However, we emphasize that RO stability and LOO stability are in general incomparable notions, as we shall see later on. Also, we note that some of these definitions appeared in previous literature. For instance, the notion of “all-i-LOO” below has been studied by several authors under different names [25, 28, 27]. The notation S\\i below refer to a training sample S with instance zi removed.\nDefinition 16. A rule A is uniform-LOO stable with rate stable(n) if for all samples S of n points and for\n29\nall i: ∣∣∣`(A(S\\i); zi)− `(A(S); zi)∣∣∣ ≤ stable(n). Definition 17. A rule A is all-i-LOO stable with rate stable(n) under distribution D if for all i:\nES∼Dn [∣∣∣`(A(S\\i); zi)− `(A(S); zi)∣∣∣] ≤ stable(n).\nDefinition 18. A rule A is LOO stable with rate stable(n) under distribution D if\n1 n n∑ i=1 ES∼Dn [∣∣∣`(A(S\\i); zi)− `(A(S); zi)∣∣∣] ≤ stable(n).\nDefinition 19. A rule A is on-average-LOO stable with rate stable(n) under distribution D if∣∣∣∣∣ 1n n∑ i=1 ES∼Dn [ `(A(S\\i); zi)− `(A(S); zi) ]∣∣∣∣∣ ≤ stable(n). While some of the definitions above might look rather similar, we show below that each one is strictly stronger than the other. Example 3 is interesting in its own right, since it presents a learning problem and an AERM that is universally consistent, but not LOO stable. While this is possible in the General Learning Setting, in supervised classification every such AERM has to be LOO stable (this is essentially proven in [28]).\nExample 1. There exists a learning problem with a universally consistent and all-i-LOO stable learning rule, but there is no universally consistent and uniform LOO stable learning rule.\nProof. This example is taken from [26]. Consider the hypothesis space {0, 1}, the instance space {0, 1}, and the objective function `(h, z) = |h− z|.\nIt is straightforward to verify that an ERM is a universally consistent learning rule. It is also universally\nall-i-LOO stable, because removing an instance can change the hypothesis only if the original sample had an equal number of 0’s and 1’s (plus or minus one), which happens with probability at most O(1/ √ n) where n\nis the sample size. However, it is not hard to see that the only uniform LOO stable learning rule, at least for\nlarge enough sample sizes, is a constant rule which always returns the same hypothesis h regardless of the\nsample. Such a learning rule is obviously not universally consistent.\nExample 2. There exists a learning problem with a universally consistent and LOO-stable AERM, which is not symmetric and is not all-i-LOO stable.\nProof. Let the instance space be [0, 1], the hypothesis space [0, 1] ∪ 2, and the objective function `(h, z) = 1{h=z}. Consider the following learning rule A: given a sample, check if the value z1 appears more than once in the sample. If no, return z1, otherwise return 2.\n30\nSince LS(2) = 0, and z1 returns only if this value constitutes 1/n of the sample, the rule above is an AERM with rate erm(n) = 1/n. To see universal consistency, let Pr z1 = p. With probability (1 − p)n−2, z1 /∈ {z2, . . . , zn}, and the returned hypothesis is z1, with L(z1) = p. Otherwise, the returned hypothesis is 2, with L(2) = 0. Hence ES [L(A(S))] ≤ p(1−p)n−2, which can be easily verified to be at most 1/(n−1), so the learning rule is consistent with rate cons(n) ≤ 1/(n − 1). To see LOO-stability, notice that our learning hypothesis can change by deleting zi, i > 1, only if zi is the only instance in z2, . . . , zn equal to z1. So stable(n) ≤ 2/n (in fact, LOO-stability holds even without the expectation). However, this learning rule is not all-i-LOO-stable. For instance, for any continuous distribution, |`(A(S\\1), z1) − `(A(S), z1)| = 1 with probability 1, so it obviously cannot be all-i-LOO-stable with respect to i = 1.\nExample 3. There exists a learning problem with a universally consistent (and on-average-LOO stable) AERM, which is not LOO stable.\nProof. Let the instance space, hypothesis space and objective function be as in Example 1. Consider the following learning rule, based on a sample S = (z1, . . . , zn): if ∑ i 1{zi=1}/n > 1/2 + √ log(4)/2n, return\n1. If ∑ i 1{zi=1}/n < 1/2− √ log(4)/2n, return 0. Otherwise, return Parity(S) = (z1 + . . . zn) mod 2.\nThis learning rule is an AERM, with erm(n) = √ 2 log(4)/n. Since we have only two hypotheses, we have uniform convergence of LS(·) to L(·) for any hypothesis. Therefore, our learning rule universally generalizes (with rate gen(n) = √ log(4/δ)/2n), and by Theorem 7, this implies that the learning rule is also universally consistent and on-average-LOO stable.\nHowever, the learning rule is not LOO stable. Consider the uniform distribution on the instance space. By Hoeffding’s inequality, | ∑ i 1{zi=1}/n − 1/2| ≤ √ log(4)/2n with probability at least 1/2 for any sample size n. In that case, the returned hypothesis is the parity function (even when we remove an instance from the sample, assuming n ≥ 3). When this happens, it is not hard to see that for any i,\n`(A(S), zi)− `(A(S\\i), zi) = 1{zi=1}(−1) Parity(S).\nThis implies that\nE\n[ 1\nn n∑ i=1 ∣∣∣(`(A(S\\i); zi)− `(A(S); zi))∣∣∣] (3.4.1) ≥ 1\n2 E\n[ 1\nn n∑ i=1 1{zi=1} ∣∣∣∣∣ √ log(4) 2n ≥ ∣∣∣ n∑ i=1 1{zi=1} n − 1 2 ∣∣∣]\n≥ 1 2\n( 1 2 − √ log(4) 2n ) −→ 1 4 ,\nwhich does not converge to zero with the sample size n. Therefore, the learning rule is not LOO stable.\nNote that the proof implies that on-average-LOO stability cannot be replaced even by something between\n31\non-average-LOO stability and LOO stability. For instance, a natural candidate would be\nES∼Dn [∣∣∣∣∣ 1n n∑ i=1 ( `(A(S\\i); zi)− `(A(S); zi) )∣∣∣∣∣ ] , (3.4.2)\nwhere the absolute value is now over the entire sum, but inside the expectation. In the example used in the proof, Eq. (3.4.2) is still lower bounded by Eq. (3.4.1), which does not converge to zero with the sample size.\nAfter showing that the hierarchy of definitions above is indeed strict, we turn to the question of what can be characterized in terms of LOO stability. In [31], we show a version of Theorem 5, which asserts that a problem is learnable if and only if there is an on-average-LOO stable AERM. However, on-average-LOO stability is qualitatively much weaker than the notion of uniform-RO stability used in Theorem 5 (see Definition 14). Rather, we would expect to prove a version of the theorem with the notion of unform-LOO stability or at least LOO stability, which are more analogous to uniform-RO stability. However, the proof of Theorem 5 does not work for these stability definitions (technically, this is because the proof relies on the sample size remaining constant, which is true for replacement stability, but not when we remove an instance as in LOO stability). We do not know if one can prove a version of Theorem 5 with an LOO stability notion stronger than on-average-LOO stability.\nOn the plus side, LOO stability allows us to prove the following interesting result, specific to ERM learning rules.\nTheorem 4. For an ERM the following are equivalent: • Universal LOO stability. • Universal consistency. • Universal generalization.\nIn particular, the theorem implies that LOO stability is a necessary property for consistent ERM learning rules. This parallels Theorem 7, which dealt with AERM’s in general, and used RO stability. As before, we do not know how to obtain something akin to Theorem 7 with RO stability."
    }, {
      "heading" : "3.5 Characterizing Learnability : Main Results",
      "text" : "Our overall goal is to characterize learnable problems (namely, problems for which there exists a universally consistent learning rule, as in Eq. (??)). That means finding some condition which is both necessary and sufficient for learnability. In the uniform convergence setting, such a condition is the stability of the ERM (under any of several possible stability measures, including both variants of RO-stability defined above). This is still sufficient for learnability in the General Learning Setting, but far from being necessary, as we have seen in Section ??.\nThe most important result in this section is a condition which is necessary and sufficient for learnability in the General Learning Setting:\n32\nTheorem 5. A learning problem is learnable if and only if there exists a uniform-RO stable, universally AERM learning rule.\nIn particular, if there exists a cons(n)-universally consistent rule, then there exists a rule that is stable(n)uniform-RO stable and universally erm(n)-AERM where:\nerm(n) = 3 cons(n 1/4) + 8B√\nn ,\nstable(n) = 2B√ n .\n(3.5.1)\nIn the opposite direction, if a learning rule is stable(n)-uniform-RO stable and universally erm(n)-AERM, then it is universally consistent with rate\ncons(n) ≤ stable(n) + erm(n)\nThus, while we have seen in Section ?? that the ERM rule might fail for learning problems which are in fact learnable, there is always an AERM rule which will work. In other words, when designing learning rules, we might need to look beyond empirical risk minimization, but not beyond AERM learning rules. On the downside, we must choose our AERM carefully, since not any AERM will work. This contrasts with supervised classification, where any AERM will work if the problem is learnable at all.\nHow do we go about proving this assertion? The easier part is showing sufficiency. Namely, that a stable AERM must be consistent (and generalizing). In fact, this holds both separately for any particular distribution Ds, and uniformly over all distributions:\nTheorem 6. If a rule is an AERM with rate erm(n) and average-RO stable (or uniform-RO stable) with rate stable(n) under D, then it is consistent and generalizes under D with rates\ncons(n) ≤ stable(n) + erm(n)\ngen(n) ≤ stable(n) + 2 erm(n) + 2B√n\nThe second part of Theorem 5 follows as a direct corollary. We note that close variants of Theorem 6 has already appeared in previous literature (e.g., [28] and [27]).\nThe harder part is showing that a uniform-RO stable AERM is necessary for learnability. This is done in several steps.\nFirst, we show that consistent AERMs have to be average-RO stable:\nTheorem 7. For an AERM, the following are equivalent: • Universal average-RO stability. • Universal consistency. • Universal generalization.\n33\nThe exact conversion rate of Theorem 7 is specified in the corresponding proof (Sub-section 3.7), and are all polynomial. In particular, an cons-universal consistent erm-AERM is average-RO stable with rate\nstable(n) ≤ erm(n) + 3 cons(n1/4) + 4B√n .\nNext, we show that if we seek universally consistent and generalizing learning rules, then we must consider only AERMs:\nTheorem 8. If a rule A is universally consistent with rate cons(n) and generalizing with rate gen(n), then it is universally an AERM with rate\nerm(n) ≤ gen(n) + 3 cons(n1/4) + 4B√ n\nNow, recall that learnability is defined as the existence of some universally consistent learning rule. Such a rule might not be generalizing, stable or even an AERM (see example 5 below). However, it turns out that if a universally consistent learning rule exist, then there is another learning rule for the same problem, which is generalizing (Lemma 22). Thus, by Theorems 7-8, this rule must also be average-RO stable AERM. In fact, by another application of Lemma 22, such an AERM must also be uniform-RO stable, leading to Theorem 5."
    }, {
      "heading" : "3.6 Randomization, Convexification, and a Generic Learning Rule",
      "text" : "In this section we show that when one considers randomized learning rules, it is possible to get stronger results and we even propose a generic randomized algorithm for statistical learning problem that guarantees non-trivial learning rate whenever the problem is learnable."
    }, {
      "heading" : "3.6.1 Stronger Results with Randomized Learning Rules",
      "text" : "The strongest result we were able to obtain for characterizing learnability so far is Theorem 5, which stated that a problem is learnable if and only if there exists a universally uniform-RO stable AERM. In fact, this result was obtained under the assumption that the learning rule A is deterministic: given a fixed sample S, A returns a single specific hypothesis h. However, we might relax this assumption and also consider randomized learning rules: given any fixed S, A(S) returns a distribution over the hypothesis classH.\nWith this relaxation, we will see that we can obtain a stronger version of Theorem 5, and even provide a generic learning algorithm (at least for computationally unbounded learners) which successfully learns any learnable problem.\nTo simplify notation, we will override the notations `(A(S), z),L(A(S)) andLS(A(S)) to mean Eh∼A(S) [`(h, z)], Eh∼A(S) [L(h)] and Eh∼A(S) [LS(h)]. In other words, A returns a distribution over H and `(A(S), z) for some fixed S, z is the expected loss of a random hypothesis picked according to that distribution, with\n34\nrespect to z. Similarly, L(A(S)) for some fixed S is the expected generalization error, and LS(A(S)) is the expected empirical risk on the fixed sample S. With this slight abuse of notation, all our previous definitions hold. For instance, we still define a learning rule A to be consistent with rate cons(n) if ES∼Dn [L(A(S))− L∗] ≤ cons(n), only now we actually mean\nES∼Dn [ Eh∼A(S) [L(h)− L∗] ] ≤ cons(n).\nThe definitions for AERM, generalization etc. also hold with this subtle change in meaning.\nAn alternative way to view randomization is as a method to linearize the learning problem. In other words, randomization implicitly replaces the arbitrary hypothesis class H by the space of probability distributions overH,\nM = { α : H → [0, 1] s.t. ∫ α[h] = 1 } ,\nand replaces the arbitrary function `(h; z) by a linear function in its first argument\n`(α; z) = Eh∼α [`(h, z)] = ∫ `(h; z)α[h] .\nLinearity of the loss and convexity of M are the key mechanism which allows us to obtain our stronger results. Moreover, if the learning problem is already convex (i.e., f is convex and H is covex), we can achieve the same results using a deterministic learning rule, as the following claim demonstrates:\nClaim 9. Assume that the hypothesis class H is convex subset of a vector space, such that Eh∼A(S) [h] is a well-defined element of H for any S. Moreover, assume that `(h; z) is convex in h. Then from any (possibly randomized) learning rule A, it is possible to construct a deterministic learning rule A′, such that `(A′(S), z) ≤ `(A(S), z) for any S, z. As a result, it also holds that LS(A′(S)) ≤ LS(A(S)) and L(A′(S)) ≤ L(A(S)).\nProof. Given a sample S, define A′(S; z) as the single hypothesis Eh∼A(S) [h]. The proof of the theorem is immediate by Jensen’s inequality: since `() is convex in its first argument,\n`(A′(S); z) = `(Eh∼A(S) [h] , z) ≤ Eh∼A(S) [`(h, z)] ,\nwhere the r.h.s. is in fact `(A(S), z) by the abuse of notation we have defined previously.\nAlthough linearization is the real mechanism at play here, we find it more convenient to display our results and proofs in the language of randomized learning rules.\nAllowing randomization allows us to obtain results with respect to the following very strong notion of stability4:\n4This definition of stability is very similar to the so-called “uniform stability”, discussed in [25], although [25] consider deterministic learning rules.\n35\nDefinition 20. A rule A is strongly-uniform-RO stable with rate stable(n) if for all samples S of n points, for all i, and any z′, z′i ∈ Z , it holds that∣∣∣`(A(S(i)); z′)− `(A(S); z′)∣∣∣ ≤ stable(n). The strengthening of Theorem 5 that we will prove here is the following:\nTheorem 10. A learning problem is learnable if and only if there exists a (possibly randomized) learning rule which is an always AERM and strongly-uniform-RO stable.\nCompared to Theorem 5, we have replaced universal AERM by the stronger notion of an always AERM, and uniform-RO stability by strongly-uniform-RO stability. This makes the result strong enough to formulate a generic learning algorithm, as we will see later on.\nThe theorem is an immediate consequence of Theorem 5 and the following lemma:\nLemma 11. For any deterministic learning rule A, there exists a randomized learning rule A′ such that:\n• For any D, if A is cons-consistent under D then A′ is cons(b √ nc) consistent under D. • A′ universally generalizes with rate 4B/ √ n.\n• If A is uniform-RO stable with rate stable(n), then A′ is strongly-uniform-RO stable with rate stable(b √ nc). • If A is universally cons-consistent, then A′ is an always AERM with rate 2 cons(b √ nc).\nMoreover, A′ is a symmetric learning rule (it does not depend on the order of elements in the sample on\nwhich it is applied)."
    }, {
      "heading" : "3.6.2 A Generic Learning Rule",
      "text" : "Recall that a symmetric learning rule A is such that A(S) = A(S′) whenever S, S′ are identical samples up to permutation. When we deal with randomized learning rules, we assume that the distribution of A(S) is identical to the distribution of A(S′). Also, let H̄ denote the set of all distributions onH. An element h̄ ∈ H̄ will be thought of as a possible outcome of a randomized learning rule.\nConsider the following learning rule: given a sample size n, find a minimizer over all symmetric5 functions A : Zn → H̄ of\nsup S∈Zn\n( LS(A(S))− LS(ĥS) ) + sup\nS∈Zn,z′ ∣∣∣`(A(S); z′)− `(A(S(i); z′)∣∣∣ , (3.6.1) with i being an arbitrary fixed element in {1, . . . , n}. Once such a function A is found, return An(S).\n5The algorithm would still work, with slight modifications, if we minimize over all functions - symmetric or not. However, the search space would be larger.\n36\nTheorem 12. If a learning problem is learnable (namely, there exist a universally consistent learning rule with rate cons(n)), the learning algorithm described above is universally consistent with rate\n4 cons(b √ nc) + 8B√\nn .\nThe main drawback of the algorithm we described is that it is completely infeasible: in practice, we cannot hope to efficiently perform minimization of Eq. (3.6.1) over all functions from Zn to H̄. Nevertheless, we believe it is conceptually important for three reasons: First, it hints that generic methods to develop learning algorithms might be possible in the General Learning Setting (similar to the more specific supervised classification setting); Second, it shows that stability might play a crucial role in the way such methods will work; And third, that stability might act in a similar manner to regularization. Indeed, Eq. (3.6.1) can be seen as a “regularized ERM” in the space of learning rules (i.e., functions from samples to hypotheses): if we take just the first term in Eq. (3.6.1), supS∈Zn ( LS(A(S))− LS(ĥS) ) , then its minimizer is trivially the ERM\nlearning rule. If we take just the second term in Eq. (3.6.1), supS∈Zn,z ∣∣`(A(S); z′)− `(A(S(i)); z′)∣∣, then its minimizers are trivial learning rules which return the same hypothesis irrespective of the training sample. Minimizing a sum of both terms forces us to choose a learning rule which is an “almost”-ERM but also stable - a learning rule which must exist if the problem is learnable at all, as Theorem 10 proves."
    }, {
      "heading" : "3.7 Detailed Results and Proofs",
      "text" : ""
    }, {
      "heading" : "3.7.1 Detailed Proof of Main Result (Section 3.5)",
      "text" : "In this subsection we provide proofs for the main results contained in Section 3.5. We first establish that for AERMs, average-RO stability and generalization are equivalent.\nEquivalence of Stability and Generalization\nIt will be convenient to work with a weaker version of generalization as an intermediate step: We say a rule A on-average generalizes with rate oag(n) under distribution D if for all n,\n|ES∼Dn [L(A(S))− LS(A(S))]| ≤ oag(n). (3.7.1)\nIt is straightforward to see that generalization implies on-average generalization with the same rate. We show that for AERMs, the converse is also true, and also that on-average generalization is equivalent to average-RO stability. This establishes the equivalence between generalization and average-RO stability (for AERMs).\nLemma 13 (on-average generalization⇔ average-RO stability). If A is on-average generalizing with rate oag(n) then it is average-RO stable with rate oag(n). If A is average-RO stable with rate stable(n) then it is on-average generalizing with rate stable(n).\n37\nProof. For any i, zi and z′i are both drawn i.i.d. from D, we have that\nES∼Dn [`(A(S); zi)] = ES∼Dn,z′i∼D [ `(A(S(i)); z′i) ] .\nHence,\nES∼Dn [LS(A(S))] = ES∼Dn [ 1\nn n∑ i=1 `(A(S); zi)\n]\n= 1\nn n∑ i=1 ES∼Dn [`(A(S); zi)]\n= 1\nn n∑ i=1 ES∼Dn,z′i∼D [ `(A(S(i)); z′i) ] Also note that L(A(S)) = Ez′i∼D [`(A(S); z ′ i)] = 1 n ∑n i=1 Ez′i∼D [`(A(S); z ′ i)]. Hence we can conclude that\nES∼Dn [L(A(S))− LS(A(S))] = 1\nn n∑ i=1 ES∼Dn,(z′1,...,z′n)∼Dn [ `(A(S); z′i)− `(A(S(i)); z′i) ] Hence we have the required result.\nFor the next result, we will need the following two short utility lemmas. Utility Lemma 14. For i.i.d. Xi, |Xi| ≤ B and X = 1n ∑n i=1Xi we have E [|X − E [X]|] ≤ B/ √ n.\nProof. E [|X − E [X]|] ≤ √ E [ |X − E [X]|2 ] ≤ √ Var[X] = √ Var[Xi]/n ≤ B/ √ n.\nUtility Lemma 15. Let X,Y be random variables s.t. X ≤ a.s. Y . Then E [|X|] ≤ |E [X]|+ 2E [|Y |].\nProof. E [|X|] = E [|(Y −X)− Y |] ≤ E [Y −X] + E [|Y |] ≤ |E [X]|+ 2 |E [Y ]| .\nLemma 16 (AERM + on-average generalization⇒ generalization). If A is an AERM with rate erm(n) and on-average generalizes with rate oag(n) underD, then A generalizes with rate oag(n)+2 erm(n)+ 2B√n under D.\nProof. Recall that L∗ = infh∈H L(h). For an arbitrarily small ν > 0, let hν be a fixed hypothesis such that L(hν) ≤ F ∗ + ν. Using respective optimalities of ĥS and L∗ we can bound:\nLS(A(S))− L(A(S))\n= LS(A(S))− LS(ĥS) + LS(ĥS)− LS(hν) + LS(hν)− L(hν) + L(hν)− L(A(S))\n≤ LS(A(S))− LS(ĥS) + LS(hν)− L(hν) + ν = Yν\n38\nWhere the final equality defines a new random variable Yν . By Lemma 14 and the AERM guarantee we have E [|Yν |] ≤ erm(n) +B/ √ n+ ν. From Lemma 15 we can conclude that\nE [|LS(A(S))− L(A(S))|] ≤ |E [LS(A(S))− L(A(S))]|+ 2E [|Yν |] ≤ oag(n) + 2 erm(n) + 2B√n + ν.\nNotice that the l.h.s. is a fixed quantity which does not depend on ν. Therefore, we can take ν in the r.h.s. to\nzero, and the result follows.\nCombining Lemma 13 and Lemma 16, we have now established the stability↔generalization parts of Theorem 6 and Theorem 7 (in fact, even a slightly stronger converse than in Theorem 7, as it does not require universality)."
    }, {
      "heading" : "A Sufficient Condition for Consistency",
      "text" : "It is fairly straightforward to see that generalization (or even on-average generalization) of an AERM implies its consistency:\nLemma 17 (AERM+generalization⇒consistency). If A is AERM with rate erm(n) and it on-average generalizes with rate oag(n) under D then it is consistent with rate oag(n) + erm(n) under D.\nProof. For any ν > 0, let hν be a hypothesis such that L(hν) ≤ L∗ + ν. We have\nE [L(A(S))− L∗] = E [L(A(S))− LS(hν) + ν]\n= E [L(A(S))− LS(A(S))] + E [LS(A(S))− LS(hν)] + ν ≤ E [L(A(S))− LS(A(S))] + E [ LS(A(S))− LS(ĥS) ] + ν\n≤ oag(n) + erm(n) + ν.\nSince this upper bound holds for any ν, we can take ν to zero, and the result follows.\nCombined with the results of Lemma 13, this completes the proof of Theorem 6 and the stability → consistency and generalization→ consistency parts of Theorem 7.\nConverse Direction\nLemma 13 already provides a converse result, establishing that stability is necessary for generalization. However, as it will turn out, in order to establish that stability is also necessary for universal consistency, we must prove that universal consistency of an AERM implies universal generalization. The assumption of universal consistency for the AERM is crucial here: mere consistency of an AERM with respect to a specific distribution does not imply generalization nor stability with respect to that distribution. The following example briefly illustrates this point.\n39\nExample 4. There exists a learning problem and a distribution on the instance space, such that the ERM (or any AERM) is consistent with rate cons(n) = 0, but does not generalize and is not average-RO stable (namely, gen(n), stable(n) = Ω(1)).\nProof. Let the instance space be [0, 1], the hypothesis space consist of all finite subsets of [0, 1], and define the\nobjective function as `(h, z) = 1{z/∈h}). Consider any continuous distribution on the instance space. Since the underlying distribution D is continuous, we have L(h) = 1 for any hypothesis h. Therefore, any learning rule (including any AERM) will be consistent with L(A(S)) = 1. On the other hand, the ERM here always\nachieves LS(ĥS) = 0, so any AERM cannot generalize, or even on-average-generalize (by Lemma 16), hence cannot be average-RO stable (by Lemma 13).\nThe main tool we use to prove our desired converse result is the following lemma. It is here that we crucially use the universal consistency assumption (i.e., consistency with respect to any distribution). Intuitively, it states that if a problem is learnable at all, then although the ERM rule might fail, its empirical risk is a consistent estimator of the minimal achievable risk.\nLemma 18 (Main Converse Lemma). If a problem is learnable, namely there exists a universally consistent rule A with rate cons(n), then under any distribution,\nE [∣∣∣LS(ĥS)− L∗∣∣∣] ≤ emp(n) where (3.7.2) emp(n) = 2 cons(n ′) + 2B√ n + 2Bn ′2 n\nfor any n′ such that 2 ≤ n′ ≤ n/2.\nProof. Let I = {I1, . . . , In′} be a random sample of n′ indexes in the range 1..n where each Ii is independently uniformly distributed, and I is independent of S. Let S′ = {zIi}n ′ i=1, i.e. a sample of size n ′ drawn from the uniform distribution over samples in S (with replacements). We first bound the probability that I\nhas no repeated indexes (“duplicates”):\nPr I has duplicates ≤ ∑n′ i=1(i− 1) n ≤ n ′2 2n (3.7.3)\nConditioned on not having duplicates in I , the sample S′ is actually distributed according to Dn′ , i.e. can be viewed as a sample from the original distribution. We therefore have by universal consistency:\nE [|L(A(S′))− L∗| | no dups] ≤ cons(n′) (3.7.4)\nBut viewed as a sample drawn from the uniform distribution over instances in S, we also have:\nES′ [∣∣∣LS(A(S′))− LS(ĥS)∣∣∣] ≤ cons(n′) (3.7.5)\n40\nConditioned on having no duplications in I , the set of those samples in S not chosen by I (i.e. S \\ S′) is independent of S′, and |S \\ S′| = n− n′, and so by Lemma 14:\nES [∣∣L(A(S′))− LS\\S′(A(S′))∣∣] ≤ B√\nn− n′ (3.7.6)\nFinally, if there are no duplicates, then for any hypothesis, and in particular for A(S′) we have:\n∣∣LS(A(S′))− LS\\S′(A(S′))∣∣ ≤ 2Bn′ n\n(3.7.7)\nCombining Eq. (3.7.4),Eq. (3.7.5),Eq. (3.7.6) and Eq. (3.7.7), accounting for a maximal discrepancy of B when we do have duplicates, and assuming 2 ≤ n′ ≤ n/2, we get the desired bound.\nEquipped with Lemma 18, we are now ready to show that universal consistency of an AERM implies universal generalization and that any universally consistent and generalizing rule must be an AERM. What we show is actually a bit stronger: that if a problem is learnable, and so Lemma 18 holds, then for any distribution D separately, consistency of an AERM under D implies generalization under D and also any consistent and generalizing rule under D must be an AERM.\nLemma 19 (learnable+AERM+consistent⇒generalizing). If Eq. (3.7.2) in Lemma 18 holds with rate emp(n), and A is an erm-AERM and cons-consistent under D, then it is generalizing under D with rate emp(n) + erm(n) + cons(n).\nProof.\nE [|LS(A(S))− L(A(S))|] ≤ E [∣∣∣LS(A(S))− LS(ĥS)∣∣∣]+ E [|L∗ − L(A(S))|] + E [∣∣∣LS(ĥS)− L∗∣∣∣]\n≤ erm(n) + cons(n) + emp(n) .\nLemma 20 (learnable+consistent+generalizing⇒AERM). If Eq. (3.7.2) in Lemma 18 holds with rate emp(n), and A is cons-consistent and gen-generalizing under D, then it is AERM under D with rate emp(n) + gen(n) + cons(n).\nProof.\nE [∣∣∣LS(A(S))− LS(ĥS)∣∣∣] ≤ E [|LS(A(S))− L(A(S))|] + E [|L(A(S))− L∗|] + E [∣∣∣L∗ − LS(ĥS)∣∣∣]\n≤ gen(n) + cons(n) + emp(n) .\n41\nLemma 19 establishes that universal consistency of an AERM implies universal generalization, and thus completes the proof of Theorem 7. Lemma 20 establishes Theorem 8. To get the rates in Subsection ??, we use n′ = n1/4 in Lemma 18.\nLemma 17, Lemma 19 and Lemma 20 together establish an interesting relationship:\nCorollary 21. For a (universally) learnable problem, for any distribution D and learning rule A, any two of the following imply the third : •A is an AERM under D. •A is consistent under D. •A generalizes under D.\nNote, however, that any one property by itself is possible, even universally:\n• In Subsection 3.3.1, we have discussed an example where the ERM learning rule is neither consistent nor generalizing, despite the problem being learnable.\n• In the next subsection (Example 5) we demonstrate a universally consistent learning rule which is neither generalizing nor an AERM.\n• A rule returning a fixed hypothesis always generalizes, but of course need not be consistent nor an AERM.\nIn contrast, for learnable supervised classification problems, it is not possible for a learning rule to be just universally consistent, without being an AERM and without generalization. Nor is it possible for a learning rule to be a universal AERM for a learnable problem, without being generalizing and consistent.\nCorollary 21 can also provide a certificate of non-learnability. In other words, for the problem in Example 4 we show a specific distribution for which there is a consistent AERM that does not generalize. We can conclude that there is no universally consistent learning rule for the problem, otherwise the corollary is violated.\nExistence of a Stable Rule\nTheorem 7 and Theorem 8, which we just completed proving, already establish that for AERMs, universal consistency is equivalent to universal average-RO stability. Existence of a universally average-RO stable AERM is thus sufficient for learnability. In order to prove that it is also necessary, it is enough to show that existence of a universally consistent learning rule implies existence of a universally consistent AERM. This AERM must then be average-RO stable by Theorem 7.\nWe actually show how to transform a consistent rule to a consistent and generalizing rule (Lemma 22 below). If this rule is universally consistent, then by Lemma 20 we can then conclude it must be an AERM, and by Lemma 13 it must be average-RO stable.\n42\nLemma 22. For any rule A there exists a rule A′, such that: •A′ universally generalizes with rate 3B√\nn .\n• For any D, if A is cons-consistent under D then A′ is cons(b √ nc) consistent under D. •A′ is uniformly-RO-stable with rate 2B√ n .\nProof. For a sample S of size n, let S′ be a sub-sample consisting of some b √ nc observation in S. To simplify the presentation, assume that b √ nc is an integer. Define A′(S) = A(S′). That is, A′ applies A to only √ n of the observation in S.\nA′ generalizes: We can decompose:\nLS(A(S ′))−L(A(S′)) = 1√\nn (LS′(A(S ′))− L(A(S′))) + (1− 1√ n )(LS\\S′(A(S ′))− L(A(S′)))\nThe first term can be bounded by 2B/ √ n. As for the second term, S \\ S′ is statistically independent of S′\nand so we can use Lemma 14 to bound its expected magnitude to obtain:\nE [|LS(A(S′))− L(A(S′))|] ≤ 2B√n + (1− 1√ n ) B√ n− √ n ≤ 3B√ n (3.7.8)\nA′ is consistent: If A is consistent, then:\nE [ L(A′(S))− inf\nh∈H L(h)\n] = E [ L(A(S′))− inf\nh∈H L(h)\n] ≤ cons( √ n)\nA′ is uniformly-RO-stable: Since A′ only uses the first √ n samples of S, for any i > √ n we have A′(S(i)) = A′(S) and so:\n1 n n∑ i=1 ∣∣∣`(A′(S(i)); z′)− `(A′(S); z′)∣∣∣ = 1 n √ n∑ i=1 ∣∣∣`(A′(S(i)); z′)− `(A′(S); z′)∣∣∣ ≤ 2B√ n\nProof of Converse in Theorem 5 If there exists a universally consistent rule with rate cons(n), by Lemma 22 there exists A′ which is cons( √ n)- universally consistent, 2B√\nn -generalizing and 2B√ n -uniformly-RO-stable.\nFurther by Lemma 20 and Lemma 18 (with n′ = n1/4), we can conclude that A′ is erm-universally AERM where,\nerm(n) ≤ 3 cons(n1/4) + 8B√ n .\nHence we get the specified rate for the converse direction. To see that if there exists a rule that is a universal AERM and stable it is consistent, we simply use Lemma 17.\n43\nAs a final note, the following example shows that while learnability is equivalent to the existence of stable and consistent AERM’s (Theorem 5 and Theorem 7), there might still exist other learning rules, which are neither stable, nor generalize, nor AERM’s. In this sense, our results characterize learnability, but do not characterize all learning rules which “work”.\nExample 5. There exists a learning problem with a universally consistent learning rule, which is not averageRO stable, generalizing nor an AERM.\nProof. Let the instance space be [0, 1]. Let the hypothesis space consist of all finite subsets of [0, 1], and the\nobjective function be the indicator function `(h, z) = 1{z∈h}. Consider the following learning rule: given a sample S ⊆ [0, 1], the learning rule checks if there are any two identical instances in the sample. If so, the learning rule returns the empty set ∅. Otherwise, it returns the sample.\nConsider any continuous distribution on [0, 1]. In that case, the probability of having two identical instances\nis 0. Therefore, the learning rule always returns a countable non-empty set A(S), with LS(A(S)) = 1, while LS(∅) = 0 (so it is not an AERM) and L(A(S)) = 0 (so it does not generalize). Also, `(A(S), zi) = 1 while `(A(S(i)), zi) = 0 with probability 1, so it is not average-RO stable either.\nHowever, the learning rule is universally consistent. If the underlying distribution is continuous on [0, 1], then\nthe returned hypothesis is S, which is countable hence , L(S) = 0 = infh L(h). For discrete distributions, let M1 denote the proportion of instances in the sample which appear exactly once, and letM0 be the probability mass of instances which did not appear in the sample. Using [32, Theorem 3], we have that for any δ, it holds with probability at least 1− δ over a sample of size n that\n|M0 −M1| ≤ O ( log(n/δ)√ n ) ,\nuniformly for any discrete distribution. If this occurs, then either M1 < 1, or M0 ≥ 1 − O(log(n/δ)/ √ n). But in the first event, we get duplicate instances in the sample, so the returned hypothesis is the optimal ∅, and in the second case, the returned hypothesis is the sample, which has a total probability mass of at most O(log(n/δ)/ √ n), and therefore L(A(S)) ≤ O(log(n/δ)/ √ n). As a result, regardless of the underlying distribution, with probability of at least 1− δ over the sample,\nL(A(S)) ≤ O (\nlog(n/δ)√ n\n) .\nSince the r.h.s. converges to 0 with n for any δ, it is easy to see that the learning rule is universally consistent.\n44"
    }, {
      "heading" : "3.7.2 Other Proofs",
      "text" : "Proof of Theorem 1. To prove the theorem, we use a stability argument. Denote\nF̂ (i)(h) = 1\nn `(h, z′i) +∑ j 6=i `(h, zj)  . the empirical average with zi replaced by an independently and identically drawn z′i, and consider its minimizer:\nĥ (i) S = arg min h∈H F̂ (i)(h).\nWe first use strong convexity and Lipschitz-continuity to establish that empirical minimization is stable in the\nfollowing sense:\n∀z ∈ Z, ∣∣∣`(ĥS , z)− `(ĥ(i)S , z)∣∣∣ ≤ 4L2λn . (3.7.9)\nWe have that\nL̂(ĥ (i) S )− L̂(ĥS)\n= `(ĥ\n(i) S , zi)− `(ĥS , zi)\nn +\n∑ j 6=i ( `(ĥ (i) S , zj)− `(ĥS , zj) ) n\n= `(ĥ\n(i) S , zi)− `(ĥS , zi)\nn + `(ĥS , z\n′ i)− `(ĥ (i) S , z ′ i)\nn + ( F̂ (i)(ĥ\n(i) S )− F̂ (i)(ĥS) )\n≤ |`(ĥ(i)S , zi)− `(ĥS , zi)| n + |`(ĥS , z′i)− `(ĥ (i) S , z ′ i)| n ≤ 2L n\n∥∥∥ĥ(i)S − ĥS∥∥∥ (3.7.10) where the first inequality follows from the fact that ĥ(i)S is the minimizer of F̂ (i)(h) and for the second inequality we use Lipschitz continuity. But from strong convexity of L̂(h) and the fact that ĥS minimizes L̂(h) we also have that\nL̂(ĥ (i) S ) ≥ L̂(ĥS) + λ 2 ∥∥∥ĥ(i)S − ĥS∥∥∥2 . (3.7.11) Combining Eq. (3.7.11) with Eq. (3.7.10) we get\n∥∥∥ĥ(i)S − ĥS∥∥∥ ≤ 4L/(λn) and combining this with Lipschitz continuity of f we obtain that Eq. (3.7.9) holds. Later in this chapter we show that a stable ERM is sufficient\nfor learnability. More formally, Eq. (3.7.9) implies that the ERM is uniform-RO stability (Definition 14) with rate stable(n) = 4L2/(λn) and therefore Theorem 6 implies that the ERM is consistent with rate ≤ stable(n), namely ES∼Dn [ L(ĥS)− L∗ ] ≤ 4L 2\nλn .\nThis concludes the proof.\n45\nWe now turn to the proof of Theorem 2.\nProof of Theorem 2. Let r(h; z) = λ2 ‖h‖ 2 + `(h; z) and let R(h) = Ez [r(h, z)]. Note that ĥλ is the empirical minimizer for the stochastic optimization problem defined by r(h; z).\nWe apply Theorem 1 to r(h; z), to this end note that since f is L-Lipschitz and ∀h ∈ H, ‖h‖ ≤ B we see that r is in fact L+ λB-Lipschitz. Applying Theorem 1, we see that\nλ 2 ∥∥∥ĥλ∥∥∥2 + ES [L(ĥλ)] = ES [R(ĥλ)] ≤ inf h R(h) + 4(L+ λB)2 λn\nNow note that infhR(h) ≤ infh∈H L(h) + λ2B 2 = L∗ + λ2B 2, and so we get that\nES [ L(ĥλ) ] ≤ inf\nh∈H L(h) +\nλ 2 B2 +\n4(L+ λB)2\nλn\n≤ inf h∈H\nL(h) + λ\n2 B2 +\n8L2\nλn +\n8λB2\nn\nPlugging in the value of λ given in the theorem statement we see that\nL(ĥλ) ≤ inf h∈H L(h) + 4\n√ L2B2\nn +\n32 n\n√ L2B2\nn\nThis gives us the required bound.\nProof of Theorem 4. Lemma 17 and Lemma 19 from subsection 3.7.1 already tell us that for ERM’s, uni-\nversal consistency is equivalent to universal generalization. Moreover, Lemma 16 implies that for ERM’s,\ngeneralization is equivalent to on-average generalization (see Eq. (3.7.1) for the exact definition). Thus, is\nleft to prove that for ERM’s, generalization implies LOO stability, and LOO stability implies on-average\ngeneralization. stability.\nFirst, suppose the ERM learning rule is generalizing with rate gen(n). Note that `(ĥS\\i ; zi) − `(ĥS ; zi) is\n46\nalways nonnegative. Therefore the LOO stability of the ERM can be upper bounded as follows:\n1 n n∑ i=1 E [ |`(ĥS\\i ; zi)− `(ĥS ; zi)| ] = 1\nn n∑ i=1 E [ `(ĥS\\i ; zi)− `(ĥS ; zi) ] = 1\nn n∑ i=1 E [ L(ĥS\\i) ] − E\n[ 1\nn n∑ i=1 `(ĥS ; zi)\n]\n≤ 1 n n∑ i=1 E [ LS\\i(ĥS\\i) + gen(n− 1) ] − E [ LS(ĥS) ] = gen(n− 1) + E [ 1\nn n∑ i=1 LS\\i(ĥS\\i)− LS(ĥS) ] ≤ gen(n− 1).\nFor the opposite direction, suppose the ERM learning rule is LOO stable with rate stable(n). Notice that we can get any sample of size n − 1 by picking a sample S of size n and discarding any instance i. Therefore, the on-average generalization rate of the ERM for samples of size n− 1 is equal to the following:∣∣∣E [L(ĥS\\i)− LS\\i(ĥS\\i)]∣∣∣\n= ∣∣∣∣∣ 1n n∑ i=1 E [ L(ĥS\\i)− LS\\i(ĥS\\i) ]∣∣∣∣∣ =\n∣∣∣∣∣ 1n n∑ i=1 E [ `(ĥS\\i ; zi) ] − 1 n n∑ i=1 E [ LS\\i(ĥS\\i) ]∣∣∣∣∣\nNow, note that for the ERM’s of S and S\\i we have ∣∣∣LS\\i(ĥS\\i)− LS(ĥS)∣∣∣ ≤ 2Bn . Therefore, we can upper bound the above by ∣∣∣∣∣ 1n n∑ i=1 E [ `(ĥS\\i ; zi) ] − E [ LS(ĥS)\n]∣∣∣∣∣+ 2Bn =\n∣∣∣∣∣ 1n n∑ i=1 E [ `(ĥS\\i ; zi)− `(ĥS , ; zi) ]∣∣∣∣∣ ≤ stable(n)\nusing the assumption that the learning rule is stable(n)-stable.\nProof of Lemma 11. Consider the learning rule A′ which given a sample S, returns a uniform distribution\n47\nover A(S′), where S′ ranges over all subsets of S of size b √ nc.\nThe fact that A′ is symmetric is trivial. We now prove the other assertions in the lemma.\nA′ is consistent: First note that L(A′(S)) = ES′ [L(A(S′))], and so:\nES [|L(A′(S))− L∗|] ≤ ES,S′ [|L(A(S′))− L∗|] = E[S′] [ ES|[S′] [|L(A(S′))− L∗|] ] where [S′] designates a choice of indices for S′. This decomposition of the random choice of S′ (e.g., first deciding on the indices and only then sampling S) allows us think of [S′] and S as statistically independent. Given a fixed choice of indices [S′], S′ is simply an i.i.d. sample of size b √ nc. Therefore, if A is consistent, |L(A(S′))− L∗| ≤ cons(b √ nc), this holds for any possible fixed [S′], and therefore\nE[S′] [ ES|[S′] [|L(A(S′))− L∗|] ] = E[S′] [ cons(b √ nc) ] ≤ cons(b √ nc).\nA′ generalizes: For convenience, let b(S, S′) = |LS(A(S′))− L(A(S′))|. Using similar arguments and notation as above:\nES [|LS(A′(S))− L(A′(S))|] ≤ E[S′] [ ES|[S′] [b(S, S′)] ] ≤ E[S′] [ ES|[S′] [ b √ nc n b(S′, S′) ] + ES|[S′] [( 1− b √ nc n ) b(S \\ S′, S′)\n]] ≤ E[S′] [ b √ nc n 2B + ( 1− b √ nc n ) B√ n− b √ nc+ 1 ] ,\nwhere the last line follows from Lemma 14 and the fact that b(S, S′) ≤ 2B for any S, S′. It is not hard to show that the expression above is at most 4B/ √ n, assuming n ≥ 1.\nA′ is strongly-uniform-RO stable: For any sample S, any i and replacement instance zi, and any instance z′, we have that∣∣∣`(A′(S(i)); z′)− `(A′(S); z′)∣∣∣ ≤ ES′ [∣∣∣`(A(S′(i)); z′)− `(A(S′); z′)∣∣∣] , where we take S′(i) in the expectation to mean S′ if i /∈ [S′]. Notice that if i /∈ [S′], then `(A(S′(i)); zi) − `(A(S′); zi) is trivially 0. Thus, we can upper bound the expression above by\nES′ [∣∣`(A(S′i); z′)− `(A(S′); z′)∣∣ ∣∣∣ i ∈ [S′]] .\n48\nSince S′ is chosen uniformly over all b √ nc-subsets of S, all permutations of [S′] are equally happen to occur,\nand therefore the above is equal to\nES′  1 b √ nc ∑ j∈S′ ∣∣∣`(A(S′(j)); z′)− `(A(S′); z′)∣∣∣  ≤ ES′ [ stable(b√nc)] = stable(b√nc).\nA′ is an always AERM: For any fixed sample S, we note that\n|LS(A′(S))− LS(ĥS)| = ES′ [ LS(A(S ′))− LS(ĥS) ]\n= ES′∼U(S)b√nc [ LS(A(S ′))− LS(ĥS) | no dups ] ,\nwhere U(S)b √ nc signifies the distribution of i.i.d. samples of size b √ nc, picked uniformly at random (with replacement) from b √ nc, and ’no dups’ signifies the event that no element in S was picked twice. By the law\nof total expectation, this is at most\nES′∼U(S)b√nc [ LS(A(S ′))− LS(ĥS) ]\nPr no dups .\nSince the learning rule A is universally consistent, it is in particular consistent with respect to the distribution U(S), and therefore the expectation in the expression above is at most cons(b √ nc). As to Pr no dups, an analysis identical to the one performed in the proof of Lemma 18 (see Eq. (3.7.3)) implies that it is at least 1− (b √ nc)2/n ≥ 1/2. Overall, we get that LS(A′(S))− LS(ĥS) ≤ 2 cons(b √ nc), so in particular\nES′∼U(S)b√nc [ LS(A(S ′))− LS(ĥS) ]\nPr no dups ≤ 2 cons(b\n√ nc),\nfrom which the claim follows.\nProof of Theorem 12. By Lemma 11, if a learning problem is learnable, there exists a (possibly randomized) symmetric learning rule A′, which is an always AERM and strongly-uniform-RO stable. More specifically,\nwe have that\nsup S∈Zn\n( LS(A ′(S))− LS(ĥS) ) ≤ 2 cons(b √ nc),\nas well as\nsup S∈Zn,z′ ∣∣∣`(A′(S); z′)− `(A′(S(i)); z′)∣∣∣ ≤ 4B√ n .\nIn particular, there exists some symmetric A : Zn → H̄, for which the expression in Eq. (3.6.1) is at most\n2 cons(b √ nc) + 4B√\nn .\n49\nTherefore, by definition, the A found satisfies\nsup S∈Zn\n( LS(An(S))− LS(ĥS) ) ≤ 2 cons(b √ nc) + 4B√\nn , (3.7.12)\nas well as\nsup S∈Zn ∣∣∣`(An(S); z′)− `(An(S(i)); z′)∣∣∣ ≤ 2 cons(b√nc) + 4B√ n . (3.7.13)\nIn Theorem 7, we have seen that a universally average-RO stable AERM learning rule has to be universally\nconsistent. The inequalities above essentially say that A is in fact both strongly-uniform-RO stable (and in\nparticular, universally average-RO stable) and an AERM, and thus is a universally consistent learning rule.\nFormally speaking, this is not entirely accurate, because A is defined only with respect to samples of size\nn, and hence is not formally a learning rule which can be applied to samples of any size. However, the\nanalysis we have done earlier in fact carries through also for learning rules A which are defined just on a\nspecific sample size n. In particular, the analysis of Lemma 13 and Lemma 17 hold verbatim for A (with\ntrivial modifications due to the fact that A is randomized), and together imply that since Eq. (3.7.12) and\nEq. (3.7.13) hold, then\nE [L(A(S))− L∗] ≤ 4 cons(b √ nc) + 8B√\nn .\nTherefore, our learning algorithm is consistent with rate 4 cons(b √ nc) + 8B√\nn ."
    }, {
      "heading" : "3.8 Discussion",
      "text" : "In this chapter we begun exploring the issue of statistical learnability in the General Setting, and uncovered important relationships between learnability and stability. However problems are left open and avenues left to explore, some of which are listed below.\nFirst, a natural question that might arise is whether it is possible to come up with well-known machine learning applications, where learnability is achievable despite uniform convergence failing to hold. Subsequently in a very recent work [33] it was shows that for multi-class learning problems with large number of classes, problems could still be learnable while uniform convergence fails and ERM approach may not be successful (at least not all ERM’s are good).\nIn Section 3.6.2, we have managed to obtain a completely generic learning algorithm: an algorithm which in principle allows us to learn any learnable problem. However, the algorithm suffers from the severe drawback that in general, it requires unbounded computational power and is not in any succinct form. Can we derive an algorithm in a simple form, or characterize classes of learning problems where our algorithm, or some other generic learning algorithm utilizing the notion of stability, can be written for instance, as a regularized ERM learning rule?\nOn a related vein, it would be interesting to develop learning algorithms (perhaps for specific settings rather than generic learning problems) which directly use stability in order to learn. Convex regularization is one\n50\nsuch mechanism, as discussed earlier to induce stability. Are there other mechanisms, which use the notion of stability in a different way?\nAnother issue is that even the existence of uniform-RO stable AERM (or strongly-uniform-RO stable, alwaysAERM allowing for convexity/randomization) is not as elegant and simple as having finite VC dimension or fat-shattering dimension. It would be very interesting to derive equivalent but more “combinatorial” conditions for learnability.\n51\nChapter 4\nOnline Learning/Optimization\nIn the online learning framework, the learner is faced with a sequence of data appearing at discrete time intervals. In contrast to statistical’ learning scenario where the learner is being evaluated after the sequence is completely revealed, in the online framework the learner is evaluated at every round. Furthermore, in the statistical learning scenario the data source is typically assumed to be drawn i.i.d.while in the online framework we relax or eliminate any stochastic assumptions on the data source. As such, the online learning problem can be phrased as a repeated two-player game between the learner (player) and the adversary (Nature).\nUnlike the statistical learning framework, there has been surprisingly little work on characterizing learnability and developing generic tools to obtain rates for the online learning framework. Littlestone [34] has shown that, in the setting of prediction of binary outcomes, a certain combinatorial property of the binaryvalued function class characterizes learnability in the realizable case (that is, when the outcomes presented by the adversary are given according to some function in the class F). The result has been extended to the non-realizable case by Shai Ben-David, Dávid Pál and Shai Shalev-Shwartz [35] who named this combinatorial quantity the Littlestone’s dimension. Coincident with [35], minimax analysis of online convex optimization yielded new insights into the value of the game, its minimax dual representation, as well as algorithm-independent upper and lower bounds [36, 7]. In this chapter we will build tools analogous to the ones we have to analyze statistical learning problems like Rademacher complexity, covering numbers etc that work for online learning framework.\nSection 4.1 introduces the problem at hand formally and provides various key definitions like that of online learning algorithm. Section 4.2 formally defines value of the online learning game and uses it to define learnability of an online learning problem and provides the main minimax theorem which is key in getting many results. In Section 4.3 we formally define the sequential Rademacher complexity and shows that it can be used to bound the value and hence get bounds on optimal rates for online learning problems. Structural properties of this complexity measure is also provided. This is perhaps the most important tool we introduce in this chapter. Section 4.4 introduces sequential covering numbers and sequential fat-shattering dimension and shows relation between them and how they can be used to bound the sequential Rademacher complexity.\n52\nSection 4.5 shows how these complexity tools provide a martingale uniform convergence story for online learning. Section 4.6 shows how these complexity measures can be used to characterize online learnability of supervised learning problems and goes on to provide a generic algorithm for online supervised learning problems. Section 4.7 provides various examples illustrating how the results can be used to obtain bounds for various online learning problems."
    }, {
      "heading" : "4.1 The Online Learning Problem",
      "text" : "The online learning problem is a continual learning process that proceeds in rounds where in each round adversary picks an instance, learner in turn picks a hypothesis. At the end of the round, the learner pays loss for picking the particular hypothesis against the instance chosen by the adversary for that round. Specifically the online learning protocol can be written as :\nOnline Learning Protocol :\nfor t = 1 to n Learner picks hypothesis ht ∈ H̄ Adversary simultaneously picks instance zt ∈ Z Learner pays loss `(ht, zt) end for\nNotice that unlike the statistical learning framework, the instances need not be selected statistically according to some fixed distribution. The adversary at round t can select the instance zt in an adversarial worst case fashion based on previous instances z1, . . . , zt−1 and based on previous hypotheses h1, . . . ,ht−1 selected by the learner. The automatic question that will rise in the reader’s mind would be what is the goal of the learner in this online learning framework. The goal we consider for this online learning framework at the end of n rounds is to have low “regret” w.r.t. the best single hypothesis from target classH that the learner could have picked at hind-sight after knowing z1, . . . , zn. That is, the regret after n rounds is defined as :\nRn ((h1, z1), . . . , (hn, zn)) = 1\nn n∑ t=1 `(ht, zt)− inf h∈H 1 n n∑ t=1 `(h, zt) (4.1.1)\nOn the Importance of Randomization : Unlike the statistical learning framework, due to the adversarial nature of the online learning framework, randomization of learning rules become necessary even for very simple (non-convex) problems. To illustrate this let us consider the simple problem of binary classification w.r.t. target hypothesis class H consisting of exactly two functions, the constant mapping to 1 and the constant mapping to −1. Now if we consider only deterministic rule, the adversary (knowing the learning rule of the learner), at each round can pick label yt to be opposite of what the learner predicts. At the end of n rounds, the average loss suffered by the learner is 1n ∑n t=1 `(ht, (xt, yt)) = 1. However infh∈H 1 n ∑n t=1 `(h, (xt, yt)) ≤ 1 2 Hence regret is lower bounded by 1/2. This example can easily be extended to other non-convex (in H) supervised learning problems. Thus we see that in the online learning\n53\nframework considering randomized learning rules is imperative to get useful results. However whenever H̄ is a vector space and loss ` is convex in its first argument, simple application of Jensen’s inequality shows that it is enough to consider only deterministic learning rules.\nJust like we introduced the notion of ”Statistical Learning Rule” to refer to the way the learner picks hypothesis for statistical learning problems, we now formally define online learning rules to refer to learner’s strategy for an online learning problem. However as discussed in the previous paragraph, we shall right from the start define learning rule to be a randomized one. Definition 21. A “Randomized Online Learning Rule” A : ⋃ n∈N∪{} ( H̄n ×Zn ) 7→ ∆(H̄) is a mapping from sequences of hypothesis, instance pair in H̄ × Z to the set of all Borel distributions over hypothesis set H̄.\nGiven a “Randomized Online Learning Rule” A, the way the learner uses this for picking the hypothesis for each round is as follows. On any round 1, learner simply sampling h1 ∼ A({}) which is some fixed distribution over the set H̄. Further recursively at each round t, the learner uses the learning rule on instances z1, . . . , zt−1 seen so far and hypotheses h1, . . . ,ht−1 sampled so far to pick the hypothesis for current round as ht ∼ A(h1:t−1, z1:t−1). For a randomized learning algorithm our aim will be to ensure that under expectation over randomization, the regret of the learner is small.\nWe shall often refer to the the “Randomized Online Learning Rule” as player/learner’s strategy as well. Further, whenever the output of the learning rule is deterministic, ie. whenever for each input sequence, the learning rule picks a particular hypothesis with probability one, we will refer to the rule as a “Deterministic Online Learning Rule”. Notice that if the learning rule is deterministic then at any round t, giving hypotheses h1, . . . ,ht−1 as argument to the learning is redundant as they can be calculated using just the instances. Hence we see that when we talk of “Deterministic Learning Rule”, it is no different in form from “Statistical Learning Rules” which are mapping from sequence of instances to hypothesis set H̄."
    }, {
      "heading" : "4.2 Online Learnability and the Value of the Game",
      "text" : "How can one define learnability in the online learning framework? Of course, we will refer to a problem as online learnable if there exists a randomized online learning algorithm which can guarantee diminishing expected regret against any strategy of the adversary. We can use the concept of value of a game to succinctly write down that it means for a problem to be online learnable. We will assume that ∆(H̄), the set of all Borel probability measures on H̄ is weakly compact. Note that if H̄ is a compact set or is the unit ball of a reflexive Banach space then one can guarantee that ∆(H̄) is automatically weakly compact. Hence this restriction is automatically true in most practical cases. We consider randomized learners who predicts a distribution qt ∈ ∆(H̄) on every round. We can define the value of the game as\nVn(H,Z) = inf q1∈∆(H̄) sup z1∈Z E h1∼q1 · · · inf qn∈∆(H̄) sup zn∈Z E hn∼qn\n[ 1\nn n∑ t=1 `(ht, zt)− inf h∈H 1 n n∑ t=1 `(h, zt)\n] (4.2.1)\n54\nwhere ht has distribution qt. We consider here the adaptive adversary who gets to choose instance zt at round t based on the history of moves h1:t−1 and z1:t−1.\nThe above definition is stated in the extensive form, but can be equivalently written in a strategic form. Just like we used A to describe the learner’s strategy, analogously we can define the adversarial strategy as a sequence of τ : ⋃ n∈N (Hn ×Zn) 7→ ∆(Z) where ∆(Z) refers to the set of all Borel probability distribution on Z . The value can then be written as\nVn(H,Z) = inf A sup τ E\n{ 1\nn n∑ t=1 `(ht, zt)− inf h∈H 1 n n∑ t=1 `(h, zt)\n} . (4.2.2)\nwhere it is understood that each ht and zt are successively drawn according to law A(h1:t−1, z1:t−1) and τ(h1:t−1, z1:t−1) respectively. While the strategic notation is more succinct, it hides the important sequential structure of the problem. This is the reason why we opt for the more explicit, yet more cumbersome, extensive form. We are now ready to formally define online learnability of a problem.\nDefinition 22. We say that a problem is online learnable with respect to the given instance space Z against target hypothesis setH if\nlim sup n→∞\nVn(H,Z) = 0 .\nThe first key step is to appeal to the minimax theorem and exchange the pairs of infima and suprema in Eq. (4.2.1). This dual formulation is easier to analyze because the choice of the player comes after the choice of the mixed strategy of the adversary. We remark that the minimax theorem holds under a very general assumption of weak compactness of ∆(H̄). The assumptions on H̄ that translate into weak compactness of ∆(H̄) are discussed in Appendix ??. Compactness under weak topology allows us to appeal to Theorem 23 stated below.\nTheorem 23. Let H̄ and Z be the sets satisfying the necessary conditions for the minimax theorem to hold, then\nVn(H,Z) = inf q1∈∆(H̄) sup z1∈Z E h1∼q1 · · · inf qn∈∆(H̄) sup zn∈Z E hn∼qn\n[ 1\nn n∑ t=1 `(ht, zt)− inf h∈H 1 n n∑ t=1 `(h, zt)\n]\n= sup p1∈∆(Z) E z1∼p1 . . . sup pn∈∆(Z) E zn∼pn\n[ 1\nn n∑ t=1 inf ht∈H̄ E zt∼pt [`(ht, zt)]− inf h∈H 1 n n∑ t=1 `(h, zt)\n] (4.2.3)\nThe question of learnability in the online learning model is now reduced to the study of Vn(H,Z), taking Eq. Eq. (4.2.3) as the starting point. In particular, under our definition, showing that the value grows sublinearly with n is equivalent to showing learnability.\nOne of the key notions introduced in this chapter is the complexity which we term Sequential Rademacher complexity. A natural generalization of Rademacher complexity [37, 38, 39], the sequential analogue possesses many of the nice properties of its classical cousin. The properties are proved in Section 4.3.1 and then used to show learnability for many of the examples in Section ??. The first step, however, is to show\n55\nthat Sequential Rademacher complexity upper bounds the value of the game. This is the subject of the next section."
    }, {
      "heading" : "4.3 Sequential Rademacher Complexity",
      "text" : "We propose the following definition of sequential Rademacher complexity of any function class F ⊂ RZ . The key difference from the classical notion is the dependence of the sequence of data on the sequence of signs (Rademacher random variables). As shown in the sequel, this dependence captures the sequential nature of the problem.\nDefinition 23. The Sequential Rademacher Complexity of a function class F ⊆ RZ is defined as\nRseqn (F) = sup z E [ sup f∈F 1 n n∑ t=1 tf(zt( )) ]\nwhere the outer supremum is taken over all Z-valued trees (z) of depth n and = ( 1, . . . , n) is a sequence of i.i.d. Rademacher random variables.\nIn statistical learning, Rademacher complexity is shown to control uniform deviations of means and expectations, and this control is key for learnability in the “batch” setting. We now show that Sequential Rademacher complexity upper-bounds the value of the game, suggesting its importance for online learning (see Section ?? for a lower bound).\nTheorem 24. The minimax value of a randomized game is bounded as\nVn(H,Z) ≤ 2Rseqn (F)\nwhere the function class F is given by, F = {z 7→ `(h, z) : h ∈ H} .\nProof. From Eq. Eq. (4.2.3),\nVn(H,Z) = sup p1 Ez1∼p1 . . . sup pn Ezn∼pn\n[ 1\nn n∑ t=1 inf ht∈H̄ Ezt∼pt [`(ht, zt)]− inf h∈H 1 n n∑ t=1 `(h, zt)\n]\n= sup p1 Ez1∼p1 . . . sup pn Ezn∼pn [ sup h∈H { 1 n n∑ t=1 inf ht∈H̄ Ezt∼pt [`(ht, zt)]− 1 n n∑ t=1 `(h, zt) }]\n≤ sup p1 Ez1∼p1 . . . sup pn Ezn∼pn [ sup h∈H { 1 n n∑ t=1 Ezt∼pt [`(h, zt)]− 1 n n∑ t=1 `(h, zt) }] (4.3.1)\nThe last step, in fact, is the first time we deviated from keeping equalities. The upper bound is obtained by\n56\nreplacing each infimum by a particular choice h. Now renaming variables we have,\nVn(H,Z) = sup p1 Ez1∼p1 . . . sup pn Ezn∼pn [ sup h∈H { 1 n n∑ t=1 Ez′t∼pt [`(h, z ′ t)]− 1 n n∑ t=1 `(h, zt) }]\n≤ sup p1 Ez1∼p1 . . . sup pn Ezn∼pn\n[ Ez′1∼p1 . . .Ez′n∼pn sup\nh∈H\n{ 1\nn n∑ t=1 `(h, z′t)− 1 n n∑ t=1 `(h, zt)\n}]\n≤ sup p1 Ez1,z′1∼p1 . . . sup pn Ezn,z′n∼pn [ sup h∈H { 1 n n∑ t=1 `(h, z′t)− 1 n n∑ t=1 `(h, zt) }] .\nwhere the last two steps are using Jensen inequality for the supremum.\nBy the Key Technical Lemma (see Lemma 25 below) with φ(u) = u,\nsup p1 Ez1,z′1∼p1 . . . sup pn Ezn,z′n∼pn [ sup h∈H { 1 n n∑ t=1 `(h, z′t)− `(h, zt) }]\n≤ sup z1,z′1 E 1 . . . sup zn,z′n E n [ sup h∈H 1 n n∑ t=1 t (`(h, z ′ t)− `(h, zt)) ]\nThus,\nVn(F) ≤ sup z1,z′1 E 1 . . . sup zn,z′n E n [ sup h∈H 1 n n∑ t=1 t (`(h, z ′ t)− `(hzt)) ]\n≤ sup z1,z′1 E 1 . . . sup zn,z′n E n [ sup h∈H { 1 n n∑ t=1 t`(h, z ′ t) } + sup h∈H { 1 n n∑ t=1 − t`(h, zt) }]\n= 2 sup z1 E 1 . . . sup zn E n [ sup h∈H 1 n n∑ t=1 t`(h, zt) ]\nNow, we need to move the suprema over zt’s outside. This is achieved via an idea similar to skolemization in logic. We basically exploit the identity\nE 1:t−1 [ sup zt G( 1:t−1, zt) ] = sup zt E 1:t−1 [G( 1:t−1, zt( 1:t−1))]\nthat holds for any G : {±1}t−1 ×Z 7→ R. On the right the supremum is over functions zt : {±1}t−1 → Z . Using this identity once, we get,\nVn(F) ≤ 2\nn sup z1,z2\n{ E 1, 2 [ sup z3 . . . sup zn { E n [ sup h∈H { 1`(h, z1) + 2`(h, z2( 1)) + n∑ t=3 t`(h, zt) }]} . . . ]}\n57\nNow, use the identity n− 2 more times to successively move the supremums over z3, . . . , zn outside, to get\nVn(F) ≤ 2\nn sup\nz1,z2,...,zn\nE 1,..., n [ sup h∈H { 1`(h, z1) + n∑ t=2 t`(h, zt( 1:t−1)) }]\n= 2 sup z E 1,..., n [ sup h∈H { 1 n n∑ t=1 t`(h, zt( )) }]\nwhere the last supremum is over Z-valued trees of depth n. Thus we have proved the required statement.\nTheorem 24 relies on the following technical lemma, which will be used again in Section ??. Its proof requires considerably more work than the classical symmetrization proof [40, 39] due to the non-i.i.d. nature of the sequences.\nLemma 25 (Key Technical Lemma). Let (z1, . . . , zn) ∈ Zn be a sequence distributed according to D and let (z′1, . . . , z ′ n) ∈ Zn be a tangent sequence. Let φ : R 7→ R be a measurable function. Then\nsup p1 E z1,z′1∼p1 . . . sup pn E zn,z′n∼pn\n[ φ ( sup h∈H 1 n n∑ t=1 ∆h(zt, z ′ t) )]\n≤ sup z1,z′1 E 1 . . . sup zn,z′n E n\n[ φ ( sup h∈H 1 n n∑ t=1 t∆h(zt, z ′ t) )]\nwhere 1, . . . , n are independent (of each other and everything else) Rademacher random variables and ∆h(xt, x ′ t) = 1 n (`(h, z ′ t)− `(h, zt)). The inequality also holds when an absolute value of the sum is introduced on both sides.\nBefore proceeding, let us give some intuition behind the attained bounds. Theorem 23 establishes an upper bound on the value of the game in terms of a stochastic process on Z . In general, it is difficult to get a handle on the behavior of this process. The key idea is to relate this process to a symmetrized version, and then pass to a new process obtained by fixing a binary tree z and then following a path in z using i.i.d. coin flips. In some sense, we are replacing the σ-algebra generated by the random process of Theorem 23 by a simpler process generated by Rademacher random variables and a tree z. It can be shown that the original process and the simpler process are in fact close in a certain sense, yet the process generated by the Rademacher random variables is much easier to work with. It is precisely due to symmetrization that the trees we consider in this work are binary trees and not full game trees. Passing to binary trees allows us to define covering numbers, combinatorial parameters, and other analogues of the classical notions from statistical learning theory."
    }, {
      "heading" : "4.3.1 Structural Results",
      "text" : "Being able to bound complexity of a function class by a complexity of a simpler class is of great utility for proving bounds. In statistical learning theory, such structural results are obtained through properties of Rademacher averages [39, 38]. In particular, the contraction inequality due to Ledoux and Talagrand [41,\n58\nCorollary 3.17], allows one to pass from a composition of a Lipschitz function with a class to the function class itself. This wonderful property permits easy convergence proofs for a vast array of problems.\nWe show that the notion of Sequential Rademacher complexity also enjoys many of the same properties. In Section ??, the effectiveness of the results is illustrated on a number of examples.\nThe next lemma bounds the Sequential Rademacher complexity for the product of function classes.\nLemma 26. Let F = F1 × . . . × Fk where each Fj ⊂ [−1, 1]Z . Also let φ : Rk × Z 7→ R be such that φ(·, z) is L-Lipschitz w.r.t. ‖ · ‖∞ norm for any z ∈ Z . Then we have that\nRseqn (φ ◦ F) ≤ 8L ( 1 + 4 √ 2 log3/2(en2) ) k∑ j=1 Rseqn (Fj)\nas long as Rseqn (Fj) ≥ 1 for each j.\nAs a special case of the result, we get a sequential counterpart of the Ledoux-Talagrand [41] contraction inequality.\nLemma 27. Fix a class F ⊆ [−1, 1]Z with Rseqn (F) ≥ 1 and a function φ : R × Z 7→ R. Assume, for all z ∈ Z , φ(·, z) is a Lipschitz function with a constant L.\nRseq(φ(F)) ≤ 8L ( 1 + 4 √ 2 log3/2(en2) ) ·Rseqn (F)\nwhere φ(F) = {z 7→ φ(f(z), z) : f ∈ F}.\nWe remark that the lemma above encompasses the case of a Lipschitz φ : R 7→ R, as stated in [41, 38]. However, here we get an extra logarithmic factor in n which is absent in the classical case. Whether the same can be proved in the sequential case remains an open question.\nWe state another useful corollary of Lemma 26.\nCorollary 28. For a fixed binary function b : {±1}k 7→ {±1} and classes F1, . . . ,Fk of {±1}-valued functions,\nRseqn (g(F1, . . . ,Fk)) ≤ O ( log3/2(n) ) k∑ j=1 Rseqn (Fj)\nIn the next proposition, we summarize some useful properties of Sequential Rademacher complexity (see [39, 38] for the results in the i.i.d. setting)\nProposition 29. Sequential Rademacher complexity satisfies the following properties.\n1. If F ⊂ G, then Rseqn (F) ≤ Rseqn (G).\n2. Rseqn (F) = Rseq(conv(F)).\n59\n3. Rseqn (cF) = |c|Rseqn (F) for all c ∈ R.\n4. For any h, Rseqn (F + h) = Rseqn (F) where F + h = {f + h : f ∈ F}."
    }, {
      "heading" : "4.4 Sequential Covering Number and Combinatorial Parameters",
      "text" : "In statistical learning theory, learnability for binary classes of functions is characterized by the VapnikChervonenkis combinatorial dimension [42]. For real-valued function classes, the corresponding notions are the scale-sensitive dimensions, such as Pγ [43, 44]. For online learning, the notion characterizing learnability for binary prediction in the realizable case has been introduced by Littlestone [34] and extended to the non-realizable case of binary prediction by Shai Ben-David, Dávid Pál and Shai Shalev-Shwartz [35]. Next, we define the Littlestone’s dimension [34, 35] and propose its scale-sensitive versions for real-valued function classes. In the sequel, these combinatorial parameters are shown to control the growth of covering numbers on trees. In the setting of prediction, the combinatorial parameters are shown to exactly characterize learnability (see Section ??).\nDefinition 24. AZ-valued tree z of depth d is shattered by a function classF ⊆ {±1}Z if for all ∈ {±1}d, there exists f ∈ F such that f(zt( )) = t for all t ∈ [d]. The Littlestone dimension Ldim(F ,Z) is the largest d such that F shatters an Z-valued tree of depth d.\nDefinition 25. An Z-valued tree z of depth d is α-shattered by a function class F ⊆ RZ , if there exists an R-valued tree s of depth d such that\n∀ ∈ {±1}d, ∃f ∈ F s.t. ∀t ∈ [d], t(f(zt( ))− st( )) ≥ α/2\nThe tree s is called the witness to shattering. The fat-shattering dimension fatseqα (F ,Z) at scale α is the largest d such that F α-shatters an Z-valued tree of depth d.\nWith these definitions it is easy to see that fatseqα (F ,Z) = Ldim(F ,Z) for a binary-valued function class F ⊆ {±1}Z for any 0 < α ≤ 2.\nWhen Z and/or F is understood from the context, we will simply write fatseqα or fat seq α (F) instead of fatseqα (F ,Z). Furthermore, we will write fat seq α (F , z) for fat seq α (F , Img(z)). In other words, fat seq α (F , z) is the largest d such that F α-shatters a tree y of depth d with Img(y) ⊆ Img(z).\nLet us mention that if trees z are defined by constant mappings zt( ) = zt, the combinatorial parameters coincide with the Vapnik-Chervonenkis dimension and with the scale-sensitive dimension Pγ . Therefore, the notions we are studying are strict “temporal” generalizations of the VC theory.\nAs in statistical learning theory, the combinatorial parameters are only useful if they can be shown to capture that aspect of F which is important for learnability. In particular, a “size” of a function class is known to be related to complexity of learning from i.i.d. data., and the classical way to measure “size” is through a cover or of a packing set. We propose the following definitions for online learning.\n60\nDefinition 26. A set V of R-valued trees of depth n is an α-cover (with respect to `p-norm) of F ⊆ RZ on a tree z of depth n if\n∀f ∈ F , ∀ ∈ {±1}n ∃v ∈ V s.t.\n( 1\nn n∑ t=1\n|vt( )− f(zt( ))|p )1/p ≤ α\nThe covering number of a function class F on a given tree z is defined as\nNseqp (α,F , z) = min{|V | : V is an α− cover w.r.t. `p-norm of F on z}.\nFurther define Nseqp (α,F , n) = supz Nseqp (α,F , z), the maximal `p covering number of F over depth n trees.\nIn particular, a set V of R-valued trees of depth n is a 0-cover of F ⊆ RZ on a tree z of depth n if\n∀f ∈ F , ∀ ∈ {±1}n ∃v ∈ V s.t. vt( ) = f(zt( ))\nWe denote by Nseq(0,F , z) the size of a smallest 0-cover on z and Nseq(0,F , n) = supz Nseq(0,F , z).\nLet us discuss a subtle point. The 0-cover should not be mistaken for the size |F(z)| of the projection of F onto the tree z, and the same care should be taken when dealing with α-covers. Let us illustrate this with an example. Consider a tree z of depth n and suppose for simplicity that |Img(z)| = 2n− 1, i.e. the values of z are all distinct. Suppose F consists of 2n−1 binary-valued functions defined as zero on all of Img(z) except for a single value of Img(zn). In plain words, each function is zero everywhere on the tree except for a single leaf. While the projection F(z) has 2n−1 distinct trees, the size of a 0-cover is only 2. It is enough to take an all-zero function g0 along with a function g1 which is zero on all of Img(z) except Img(zn) (i.e. on the leaves). It is easy to verify that g0(z) and g1(z) provide a 0-cover for F on z, and therefore, unlike |F(z)|, the size of the cover does not grow with n. The example is encouraging: our definition of a cover captures the fact that the function class is “simple” for any given path.\nNext, we naturally propose a definition of a packing.\nDefinition 27. A set V of R-valued trees of depth n is said to be α-separated if\n∀v ∈ V, ∃ ∈ {±1}n s.t. ∀w ∈ V \\ {v}\n( 1\nn n∑ t=1\n|vt( )−wt( )|p )1/p > α\nThe weak packing number Dp(α,F , z) of a function class F on a given tree z is the size of the largest α-separated subset of {f(z) : f ∈ F}.\nDefinition 28. A set V of R-valued trees of depth n is said to be strongly α-separated if\n∃ ∈ {±1}n s.t. ∀v,w ∈ V,v 6= w\n( 1\nn n∑ t=1\n|vt( )−wt( )|p )1/p > α\n61\nThe strong packing number Mp(α,F , z) of a function class F on a given tree z is the size of the largest strongly α-separated subset of {f(z) : f ∈ F}.\nNote the distinction between the packing number and the strong packing number. For the former, it must be that every member of the packing is α-separated from every other member on some path. For the latter, there must be a path on which every member of the packing is α-separated from every other member. This distinction does not arise in the classical scenario of “batch” learning. We observe that if a tree z is defined by constant mappings zt = xt, the two notions of packing and strong packing coincide, i.e. Dp(α,F , z) = Mp(α,F , z). The following lemma gives a relationship between covering numbers and the two notions of packing numbers. The form of this should be familiar, except for the distinction between the two types of packing numbers.\nLemma 30. For any F ⊆ RZ , any Z-valued tree z of depth n, and any α > 0\nMp(2α,F , z) ≤ Nseqp (α,F , z) ≤ Dp(α,F , z).\nIt is important to note that the gap between the two types of packing can be as much as 2n."
    }, {
      "heading" : "4.4.1 A Combinatorial Upper Bound",
      "text" : "We now relate the combinatorial parameters introduced in the previous section to the size of a cover. In the binary case (k = 1 below), a reader might notice a similarity of Theorems 31 and 33 to the classical results due to Sauer [45], Shelah [46] (also, Perles and Shelah), and Vapnik and Chervonenkis [42]. There are several approaches to proving what is often called the Sauer-Shelah lemma. We opt for the inductive-style proof (e.g. Alon and Spencer [47]). Dealing with trees, however, requires more work than in the VC case .\nTheorem 31. Let F ⊆ {0, . . . , k}Z be a class of functions with fatseq2 (F) = d. Then\nNseq∞ (1/2,F , n) ≤ d∑ i=0 ( n i ) ki ≤ (ekn)d .\nFurthermore, for n ≥ d d∑ i=0 ( n i ) ki ≤ ( ekn d )d .\nArmed with Theorem 31, we can approach the problem of bounding the size of a cover at an α scale by a discretization trick. For the classical case of a cover based on a set points, the discretization idea appears in [43, 48]. When passing from the combinatorial result to the cover at scale α in Corollary 32, it is crucial that Theorem 31 is in terms of fatseq2 (F) and not fat seq 1 (F). This point can be seen in the proof of Corollary 32 (also see [48]): the discretization process can assign almost identical function values to discrete values which differ by 1. This explains why the combinatorial result of Theorem 31 is proved for the 2-shattering dimension.\n62\nWe now show that the covering numbers are bounded in terms of the fat-shattering dimension.\nCorollary 32. Suppose F is a class of [−1, 1]-valued functions on Z . Then for any α > 0, any n > 0, and any Z-valued tree z of depth n,\nNseq1 (α,F , z) ≤ N seq 2 (α,F , z) ≤ Nseq∞ (α,F , z) ≤\n( 2en\nα\n)fatseqα (F)\nWith a proof similar to Theorem 31, a bound on the 0-cover can be proved in terms of the fatseq1 (F) combinatorial parameter. Of particular interest is the case k = 1, when fatseq1 (F) = Ldim(F).\nTheorem 33. Let F ⊆ {0, . . . , k}Z be a class of functions with fatseq1 (F) = d. Then\nNseq(0,F , n) ≤ d∑ i=0 ( n i ) ki ≤ (ekn)d .\nFurthermore, for n ≥ d d∑ i=0 ( n i ) ki ≤ ( ekn d )d .\nIn particular, the result holds for binary-valued function classes (k = 1), in which case fatseq1 (F) = Ldim(F).\nWhen bounding deviations of means from expectations uniformly over the function class, the usual approach proceeds by a symmetrization argument [49] followed by passing to a cover of the function class and a union bound (e.g. [39]). Alternatively, a more refined chaining analysis integrates over covering at different scales (e.g. [50]). By following the same path, we are able to prove a number of similar results for our setting. In the next section we present a bound similar to Massart’s finite class lemma [51, Lemma 5.2], and in the following section this result will be used when integrating over different scales for the cover."
    }, {
      "heading" : "4.4.2 Finite Class Lemma and the Chaining Method",
      "text" : "Lemma 34. For any finite set V of R-valued trees of depth n we have that\nE [ max v∈V n∑ t=1 tvt( ) ] ≤ √√√√2 log(|V |) max v∈V max ∈{±1}n n∑ t=1 vt( )2\nA simple consequence of the above lemma is that if F ⊆ [0, 1]Z is a finite class, then for any given tree z we have that\nE [ max f∈F 1 n n∑ t=1 tf(zt( )) ] ≤ 1 n E [ max v∈F(z) n∑ t=1 tvt( ) ] ≤ √ 2 log |F| n .\nNote that if f ∈ F is associated with an “expert”, this result combined with Theorem 24 yields a bound given\n63\nby the exponential weighted average forecaster algorithm (see [52]). In Section ?? we discuss this case in more detail. However, as we show next, Lemma 34 goes well beyond just finite classes and can be used to get an analog of Dudley entropy bound [17] for the online setting through a chaining argument.\nDefinition 29. The Integrated complexity of a function class F ⊆ [−1, 1]Z is defined as\nDseqn (F) = inf α\n{ 4α+ 12 ∫ 1 α √ log N2(δ,F , n) n dδ } .\nTo prove the next theorem, we consider covers of the classF at different scales that form a geometric progression. We zoom into a given function f ∈ F using covering elements at successive scales. This zooming in procedure is visualized as forming a chain that consists of links connecting elements of covers at successive scales. The Rademacher complexity of F can then be bounded by controlling the Rademacher complexity of the link classes, i.e. the class consisting of differences of functions from covers at neighbouring scales. This last part of the argument is the place where our proof becomes a bit more involved than the classical case.\nTheorem 35. For any function class F ⊆ [−1, 1]Z ,\nRseqn (F) ≤ Dseqn (F) .\nIf a fat-shattering dimension of the class can be controlled, Corollary 32 together with Theorem 35 yield an upper bound on the value.\nWe can now show that, in fact, the two complexity measures Rseqn (F) and Dseqn (F) are equivalent, up to a logarithmic factor. Before stating this result formally, we prove the following lemma which asserts that the fat-shattering dimensions at “large enough” scales cannot be too large.\nLemma 36. For any β > 2nR seq n (F), we have that fat seq β (F) < n.\nThe following lemma complements Theorem 35.\nLemma 37. For any function class F ⊆ [−1, 1]Z , we have that\nDseqn (F) ≤ 8 Rseqn (F) ( 1 + 4 √ 2 log3/2 ( en2 ))\nas long as Rseqn (F) ≥ 1n ."
    }, {
      "heading" : "4.5 Martingale Uniform Convergence",
      "text" : "As we discussed in the previous chapter, in the statistical learning setting learnability of supervised learning problem is equivalent to the so called uniform Glivenko-Cantelli property (uniform convergence) of the class. The property refers to the empirical averages converging to expected value of the function for any fixed\n64\ndistribution (samples drawn i.i.d.) and uniformly over the function class almost surely. Tools like classical Rademacher complexity, statistical covering numbers and statistical fat-shattering dimension can be used in analyzing rate of uniform convergence of empirical average to expected value of the function for samples drawn i.i.d from any fixed distribution. Analogously sequential counterparts of these complexity measures can be used to bound rate of uniform convergence of average value of the function to average conditional expectation of the function values for arbitrary distributions over sequence of random variables. In fact in the proof of Theorem 24 we already encountered this general martingale uniform convergence in expectation. Specifically Equation 4.3.1 which we showed is bounded by the sequential Rademacher complexity. We now formally define Universal Uniform Convergence which is analogous to the usual definition of uniform Glivenko-Cantelli property for general dependent processes and in this section we will show how the sequential complexity measures provide tools for bounding this martingale version of uniform convergence.\nDefinition 30. A function class F satisfies a Universal Uniform Convergence if for all α > 0,\nlim N→∞ sup D\nPD ( sup n≥N sup f∈F 1 n ∣∣∣∣∣ n∑ t=1 (f(zt)− Et−1[f(zt)]) ∣∣∣∣∣ > α ) = 0\nwhere the supremum is over distributions D over infinite sequences (x1, . . . , xn, . . .)\nWe remark that the notion of uniform Glivenko-Cantelli classes is recovered if the supremum is taken over i.i.d. distributions. The theorem below shows that finite fat shattering dimension at all scales is a sufficient condition for Universal Uniform Convergence.\nTheorem 38. LetF be a class of [−1, 1]-valued functions. If fatseqα (F) is finite for all α > 0, thenF satisfies Universal Uniform Convergence.\nThe proof follows from the Lemma 39 and Lemma 40 below, while Lemma 41 is an even stronger version of Lemma 40. We remark that Lemma 39 is the “in-probability” version of sequential symmetrization technique of Theorem 24 and Lemma 41 is the “in-probability” version of Theorem 35.\nLemma 39. Let F be a class of [−1, 1]-valued functions. Then for any α > 0\nPD\n( 1\nn sup f∈F ∣∣∣∣∣ n∑ t=1 (f(zt)− Et−1[f(zt)]) ∣∣∣∣∣ > α ) ≤ 4 sup z P ( 1 n sup f∈F ∣∣∣∣∣ n∑ t=1 tf(zt( )) ∣∣∣∣∣ > α/4 )\nLemma 40. Let F be a class of [−1, 1]-valued functions. For any Z-valued tree z of depth n and α > 0\nP\n( 1\nn sup f∈F ∣∣∣∣∣ n∑ t=1 tf(zt( )) ∣∣∣∣∣ > α/4 ) ≤ 2N1(α/8,F , z)e−nα 2/128 ≤ 2 ( 16en α )fatseq α/8 e−nα 2/128\nNext, we show that the sequential Rademacher complexity is, in some sense, the “right” complexity measure even when one considers high probability statements.\n65\nLemma 41. Let F be a class of [−1, 1]-valued functions and suppose fatseqα (F) is finite for all α > 0. Then for any θ > √ 8/n, for any Z-valued tree z of depth n,\nP ( sup f∈F ∣∣∣∣∣ 1n n∑ t=1 tf(zt( )) ∣∣∣∣∣ > 128(1 + θ√n log3/2(2n)) ·Rseqn (F) )\n≤ P ( sup f∈F ∣∣∣∣∣ 1n n∑ t=1 tf(zt( )) ∣∣∣∣∣ > infα>0 { 4α+ 12θ ∫ 1 α √ logNseq∞ (δ,F , n) n dδ }) ≤ Le−nθ 2 4\nwhere L is a constant such L > ∑∞ j=1 N seq ∞ (2 −j ,F , n)−1 .\nWhile throughout this chapter we are mostly concerned with expected versions of minimax regret and the corresponding complexities, the above lemmas can be employed to give an analogous in-probability treatment. To obtain such in-probability statements, the value is defined as the minimax probability of regret exceeding a threshold."
    }, {
      "heading" : "4.6 Charecterizing Learnability of Supervised Learning Problem",
      "text" : "In this section we study the specific case of online supervised learning problem. In this setting, the instance space Z is of form Z = X × Y where X is some arbitrary input space and Y ⊂ R. We shall assume that Y ⊆ [−1, 1] (of course the bound of 1 can be changed to an arbitrary value). The target hypothesis set H for the supervised learning problem corresponds to a set of functions that map input instances from X to some predicted label in [−1, 1], that is H ⊂ [−1, 1]X . H̄ ⊂ [−1, 1]X can be an arbitrary superset of H and the results in fact hold even if H̄ = H (ie. proper learning setting). The loss function we consider is for the form\n`(h, (x, y)) = |h(x)− y|\nIn the online supervised learning problem at each round t, the player picks hypothesis ht ∈ H̄ and the adversary provides input target pair (xt, yt) and the player suffers loss |ht(xt)−yt|. Note that if H̄ ⊆ {±1}X and each yt ∈ {±1} then the problem boils down to binary classification problem.\nThough we use the absolute loss in this section, it is easy to see that all the results hold (with modified rates) for any loss `(h(x), y) which is such that for all h, x and y,\nφ(`(ŷ, y)) ≤ |ŷ − y| ≤ Φ(`(ŷ, y))\nwhere Φ and φ are monotonically increasing functions. For instance the squared loss is a classic example.\nTo formally define the value of the online supervised learning game, fix a set of labels Y ⊆ [−1, 1]. For the sake of brevity, we shall use the notation VSn(H) = Vn(H,X × Y). Binary classification is, of course, a special case when Y = {±1} and H̄ ⊆ {±1}X . In that case, we simply use VBinaryn (H) for VSn(H).\n66\nProposition 42. For the supervised learning game played with a target hypothesis class H ⊆ [−1, 1]X , for any n ≥ 2\n1 4 √ 2 sup α\n{ α √ nmin {fatseqα (H), n} } ≤ 1\n2 VSn (H) ≤ Dseqn (H) ≤ inf α\n{ 4α+\n12√ n ∫ 1 α √ fatseqβ (H) log ( 2en β ) dβ } ≤ 58 log\n3 2 n Rseqn (H) . (4.6.1)\nMoreover, the lower bound Rseqn (H) ≤ VSn(H) on the value of the supervised game also holds.\nThe proposition above implies that finiteness of the fat-shattering dimension is necessary and sufficient for learnability of a supervised game. Further, all the complexity notions introduced so far are within a logarithmic factor from each other whenever the problem is learnable. These results are summarized in the next theorem.\nTheorem 43. For any target hypothesis classH ⊆ [−1, 1]X , the following statements are equivalent\n1. Target hypothesis classH is online learnable in the supervised setting.\n2. For any α > 0, fatseqα (H) is finite.\nMoreover, if the function class is online learnable, then the value of the supervised game VSn(H), the Sequential Rademacher complexity Rseqn (H), and the Integrated complexity Dseqn (H) are within a multiplicative factor of O(log3/2 n) of each other.\nProof. The equivalence of 1 and 2 follows directly from Proposition 42. As for relating the various complexity measures, note that again by Proposition 42, Rseqn (H) ≤ VSn(H), VSn(H) ≤ 2Dseqn (H) and Dseqn (H) ≤ 58 log\n3 2 nRseqn (H). Hence VSn(H) and Dseqn (H) are sandwiched between Rseqn (H) andO(log 3/2 n)Rseqn (H) which concludes the proof.\nCorollary 44. For the binary classification game played with function class F we have that\nK1 √ nmin {Ldim(H), n} ≤ VBinaryn (H) ≤ K2 √ n Ldim(H) log n\nfor some universal constants K1,K2.\nWe wish to point out that the lower bound of Proposition 42 holds for arbitrary class H̄ that is a superset of the target hypothesis class H. Since a proper learning rule can always be seen also as an improper learning rule, we trivially have that if class is properly online learnable in the supervised setting then it is improperly online learnable too. However by the above mentioned fact that the lower bound of Proposition 42 holds for arbitrary class H̄, we also have the non-trivial reverse implication that : if a class is improperly online learnable in the supervised setting, it is online learnable.\n67\nIt is natural to ask whether being able to learn in the online model is different from learning in a batch model (in the supervised setting). The standard example (e.g. [34, 35]) is the class of step functions on a bounded interval, which has a VC dimension 1, but is not learnable in the online setting. Indeed, it is possible to verify that the Littlestone’s dimension is not bounded. Interestingly, the closely-related class of “ramp” functions (modified step functions with a Lipschitz transition between 0’s and 1’s) is learnable in the online setting (and in the batch case). We extend this example as follows. By taking a convex hull of step-up and step-down functions on a unit interval, we arrive at a class of functions of bounded variation, which is learnable in the batch model, but not in the online learning model. However, the class of Lipschitz functions of bounded variation is learnable in both models. Online learnability of the latter class is shown with techniques analogous to Section 4.7.5."
    }, {
      "heading" : "4.6.1 Generic Algorithm for Supervised Learning Problem",
      "text" : "We shall now present a generic improper learning algorithm for the online supervised setting that achieves a low regret bound whenever the function class is online learnable. For any α > 0 define an α-discretization of the [−1, 1] interval asBα = {−1+α/2,−1+3α/2, . . . ,−1+(2k+1)α/2, . . .} for 0 ≤ k and (2k+1)α ≤ 4. Also for any a ∈ [−1, 1] define bacα = argmin\nr∈Bα |r − a|. For a set of functions V ⊆ H, any r ∈ Bα and\nx ∈ X define V (r, x) = {h ∈ V | h(x) ∈ (r − α/2, r + α/2]}\nAlgorithm 1 Fat-SOA Algorithm (F , α) V1 ← F for t = 1 to n do Rt(x) = {r ∈ Bα : fatseqα (Vt(r, x)) = maxr′∈Bα fat seq α (Vt(r\n′, x))} For each x ∈ X , let ht(x) = 1|Rt(x)| ∑ r∈Rt(x) r Play ht and receive (xt, yt) if |ht(xt)− yt| ≤ α then Vt+1 = Vt else Vt+1 = Vt(bytcα, xt)\nend if end for\nLemma 45. Let H ⊆ [−1, 1]X be a function class with finite fatseqα (H). Suppose the learner is presented with a sequence (x1, y1), . . . , (xn, yn), where yt = h(xt) for some fixed h ∈ H unknown to the player. Then for ht’s computed by the Algorithm 1 it must hold that\nT∑ t=1 1{|ht(xt)−yt|>α} ≤ fat seq α (H).\nLemma 45 proves a bound on the performance of Algorithm 1 in the realizable setting. We now provide an\n68\nalgorithm for the agnostic setting. We achieve this by generating “experts” in a way similar to [35]. Using these experts along with the exponentially weighted average (EWA) algorithm we shall provide the generic algorithm for online supervised learning. The EWA (Algorithm 3) and its regret bound are provided in the appendix for completeness (p. 101).\nAlgorithm 2 Expert (F , α, 1 ≤ i1 < . . . < iL ≤ n, Y1, . . . , YL) V1 ← F for t = 1 to n do Rt(x) = {r ∈ Bα : fatseqα (Vt(r, x)) = maxr′∈Bα fat seq α (Vt(r\n′, x))} For each x ∈ X , let f ′t(x) = 1|Rt(x)| ∑ r∈Rt(x) r if t ∈ {i1, . . . , iL} then ∀x ∈ X ,ht(x) = Yj where j is s.t. t = ij Play ht and receive xt Vt+1 = Vt(ht(xt), xt) else Play ht = h′t and receive xt Vt+1 = Vt\nend if end for\nFor each L ≤ fatseqα (H) and every possible choice of 1 ≤ i1 < . . . < iL ≤ n and Y1, . . . , YL ∈ Bα we generate an expert. Denote this set of experts as En. Each expert outputs a function ht ∈ H at every round n. Hence each expert e ∈ En can be seen as a sequence (e1, . . . , en) of mappings et : X t−1 7→ H. The total number of unique experts is clearly\n|En| = fatseqα∑ L=0 ( n L ) (|Bα| − 1)L ≤ ( 2n α )fatseqα Lemma 46. For any h ∈ H there exists an expert e ∈ En such that for any t ∈ [n],\n|h(xt)− e(x1:t−1)(xt)| ≤ α\nProof. By Lemma 45, for any function h ∈ H, the number of rounds on which |ht(xt)−h(xt)| > α for the output of the fat-SOA algorithm ht is bounded by fatseqα (H). Further on each such round there are |Bα| − 1 other possibilities. For any possible such sequence of “mistakes”, there is an expert that predicts the right\nlabel on those time steps and on the remaining time agrees with the fat-SOA algorithm for that target function. Hence we see that there is always an expert e ∈ En such that\n|h(xt)− e(x1:t−1)(xt)| ≤ α\nTheorem 47. For any α > 0 if we run the exponentially weighted average (EWA) algorithm with the set En\n69\nof experts then the expected regret of the algorithm is bounded as\nE [Rn] ≤ α+\n√ fatseqα log ( 2n α ) n\nProof. For any α ≥ 0 if we run EWA with corresponding set of experts En then we can guarantee that regret w.r.t. best expert in the set En is bounded by √ nfatseqα log ( 2n α ) . However by Lemma 46 we have that the regret of the best expert in En w.r.t. best function in function class H is at most αn. Combining we get the required result.\nThe above theorem holds for a fixed α. To provide a regret statement that optimizes over α we consider αi’s of form 2−i and assign weights pi = 6π2 i\n−2 to experts generated in above theorem for each αi and run EWA on the entire set of experts with these initial weights. Hence we get the following corollary.\nCorollary 48. LetH ⊆ [−1, 1]X . The expected regret of the algorithm described above is bounded as\nE [Rn] ≤ inf α α+ √ fatseqα log ( 2n α ) n + ( 3 + 2 log log ( 1 α )) √ n "
    }, {
      "heading" : "4.7 Examples",
      "text" : ""
    }, {
      "heading" : "4.7.1 Example: Margin Based Regret",
      "text" : "In the classical statistical setting, margin bounds provide guarantees on expected zero-one loss of a classifier based on the empirical margin zero-one error. These results form the basis of the theory of large margin classifiers (see [53, 54]). Recently, in the online setting, margin bounds have been shown through the concept of margin via the Littlestone dimension [35]. We show that our machinery can easily lead to margin bounds for the binary classification games for general function classes F based on their sequential Rademacher Complexity. We use ideas from [54] to do this.\nProposition 49. For any target hypothesis class H ⊂ [−1, 1]Z , there exists a randomized online algorithm A such that for any sequence z1, . . . , zn where each zt = (xt, yt) ∈ Z × {±1}, played by the adversary,\nE\n[ 1\nn n∑ t=1 Eht∼A(z1:t−1) [ 1{ht(xt)yt<0} ]] ≤ inf γ>0  infh∈H 1n n∑ t=1 1{h(xt)yt<γ} + 4 γ Rseqn (H) + ( 3 + log log ( 1 γ )) √ n "
    }, {
      "heading" : "4.7.2 Example : Neural Networks",
      "text" : "We provide below a bound on sequential Rademacher complexity for classic multi-layer neural networks thus showing they are learnable in the online setting. The model of neural network we consider below and the\n70\nbounds we provide are analogous to the ones considered in the batch setting in [38]. We now consider a k-layer 1-norm neural network. To this end let function class F1 be given by\nF1 = x 7→∑ j w1jxj ∣∣∣ ‖w‖1 ≤ B1 \nand further for each 2 ≤ i ≤ k define\nFi = x 7→∑ j wijσ (fj(x)) ∣∣∣ ∀j fj ∈ Fi−1, ‖wi‖1 ≤ Bi  Proposition 50. Say σ : R 7→ [−1, 1] is L-Lipschitz, then\nRseqn (Fk) ≤ ( k∏ i=1 2Bi ) Lk−1X∞ √ 2 log d n\nwhere X∞ is such that ∀x ∈ Z , ‖x‖∞ ≤ X∞ and Z ⊂ Rd"
    }, {
      "heading" : "4.7.3 Example: Decision Trees",
      "text" : "We consider here the supervised learning game where adversary provides instances from input space X and binary labels ±1 corresponding to the input and the player plays decision trees of depth no more than d with decision functions from set C ⊂ {±1}X of binary valued functions. The following proposition shows that there exists a randomized learning algorithm which under certain circumstances could have low regret for the supervised learning (binary) game played with class of decision trees of depth at most d with decision functions from C. The proposition is analogous to the one in [38] considered in the batch (classical) setting.\nProposition 51. Denote by T the class of decision trees of depth at most d with decision functions in"
    }, {
      "heading" : "C. There exists a randomized online learning algorithm A such that for any sequence of instances z1 =",
      "text" : "(x1, y1), . . . , zn = (xn, yn) ∈ (X × {±1})n played by the adversary,\nE\n[ 1\nn n∑ t=1 Eτt∼A(z1:t−1) [ 1{τt(xt) 6=yt} ]] ≤ inf τ∈T 1 n n∑ t=1 1{τ(xt)6=yt}\n+O (∑ l min ( C̃n(l), d log 3/2(n) Rseqn (H) ) + (3 + 2 log(Nleaf))√ n )\nwhere C̃n(l) denotes the number of instances which reach the leaf l and are correctly classified in the decision tree t that minimizes ∑n t=1 1{τ(xt)6=yt} and let Nleaf be the number of leaves in this tree.\n71"
    }, {
      "heading" : "4.7.4 Example: Online Transductive Learning",
      "text" : "Let F be a class of functions from Z to R. Let\nN̂∞(α,F) = min { |G| : G ⊆ RZ s.t. ∀f ∈ F ∃g ∈ G satisfying ‖f − g‖∞ ≤ α } . (4.7.1)\nbe the `∞ covering number at scale α, where the cover is pointwise on all of Z . It is easy to verify that\n∀n, N∞(α,F , n) ≤ N̂∞(α,F) (4.7.2)\nIndeed, let G be a minimal cover of F at scale α. We claim that the set V = {vg = g(z) : g ∈ G} of R-valued trees is an `∞ cover of F on z. Fix any ∈ {±1}n and f ∈ F , and let g ∈ G be such that ‖f − g‖∞ ≤ α. Then clearly |vgt ( )− f(zt( ))| for any 1 ≤ t ≤ n, which concludes the proof.\nThis simple observation can be applied in several situations. First, consider the problem of transductive learning, where the set Z = {z1, . . . , zm} is a finite set. To ensure online learnability, it is sufficient to consider an assumption on the dependence of N̂∞(α,F) on α. An obvious example of such a class is a VC-type class with N̂∞(α,F) ≤ (c/α)d for some c which can depend on m. Assume that F ⊂ [−1, 1]Z . Substituting this bound on the covering number into\nDseqn (F) = inf α\n{ 4α+ 12 ∫ 1 α √ log N2(δ,F , n) n dδ }\nand choosing α = 0, we observe that the value of the supervised game is upper bounded by 2Dseqn (F) ≤ 4 √\nd log c n by Proposition 42. It is easy to see that if m is fixed and the problem is learnable in the batch (e.g.\nPAC) setting, then the problem is learnable in the online transductive model.\nIn the transductive setting considered by Kakade and Kalai [55], it is assumed that m ≤ n and F are binaryvalued. If F is a class with VC dimension d, the Sauer-Shelah lemma ensures that the `∞ cover is smaller than (em/d)d ≤ (en/d)d. Using the previous argument with c = en, we obtain a bound of 4 √ dn log(en) for the value of the game, matching [55] up to a constant 2.\nWe also consider the problem of prediction of individual sequences, which has been studied both in information theory and in learning theory. In particular, in the case of binary prediction, Cesa-Bianchi and Lugosi [56] proved upper bounds on the value of the game in terms of the (classical) Rademacher complexity and the (classical) Dudley integral. The particular assumption made in [56] is that experts are static. That is, their prediction only depends on the current round, not on the past information. Formally, we define static experts as mappings f : {1, . . . , n} 7→ Y = [−1, 1], and let F denote a class of such experts. Defining Z = {1, . . . , n} puts us in the setting considered earlier withm = n. We immediately obtain 4 √ dn log(en), matching the results on [56, p. 1873]. We mention that the upper bound in Theorem 4 in [56] is tighter by a log n factor if a sharper bound on the `2 cover is considered. Finally, for the case of a finite number of experts, clearly N̂∞ ≤ N which gives the classical O( √ n logN) bound on the value of the game [52].\n72"
    }, {
      "heading" : "4.7.5 Example: Isotron",
      "text" : "Recently, Kalai and Sastry [57] introduced a method called Isotron for learning Single Index Models (SIM). These models generalize linear and logistic regression, generalized linear models, and classification by linear threshold functions. For brevity, we only describe the Idealized SIM problem from [57]. In its “batch” version, we assume that the data is revealed at once as a set {(xi, yi)}nt=1 ∈ Rd × R where yt = u(〈w,xt〉) for some unknown w ∈ Rd of bounded norm and an unknown non-decreasing u : R 7→ R with a bounded Lipschitz constant. Given this data, the goal is to iteratively find the function u and the direction w, making as few mistakes as possible. The error is measured as 1n ∑n t=1(hi(xt) − yt)2, where hi(x) = ui(〈wi,x〉) is the iterative approximation found by the algorithm on the ith round. The elegant computationally efficient method presented in [57] is motivated by Perceptron, and a natural open question posed by the authors is whether there is an online variant of Isotron. Before even attempting a quest for such an algorithm, we can ask a more basic question: is the (Idealized) SIM problem even learnable in the online framework? After all, most online methods deal with convex functions, but u is only assumed to be Lipschitz and non-decreasing. We answer the question easily with the tools we have developed.\nWe are interested in online learnability of the above described problem. Specifically, the setting of the problem is a supervised learning one where instance set is Z = X × Y where X = B2 (the unit Euclidean ball in Rd) and Y = [−1, 1]. The target hypothesis set is given by H = U × B2 where U = {u : [−1, 1] 7→ [−1, 1] : u is 1-Lipschitz}. Finally the loss function is the squared loss, that is given hypothesis h = (u,x) and instance z = (x, y), the loss function if given by `(h, z) = (u(〈x,w〉)− y)2. It is evident that the loss class F associated with the problem is a composition with three levels: the squared loss, the Lipschitz non-decreasing function, and the linear function. That is\nF = {(x, y) 7→ (y − u(〈w,x〉))2 | u ∈ U , ‖w‖2 ≤ 1} (4.7.3)\nThe proof of the following Proposition boils down to showing that the covering number of the class does not increase much under these compositions.\nProposition 52. The target classH = U ×B2 is online learnable in the supervised setting. Moreover,\nVn(H,X × Y) = O √ log3 n n  .\n73"
    }, {
      "heading" : "4.8 Detailed Proofs and More Results",
      "text" : ""
    }, {
      "heading" : "4.8.1 Proofs",
      "text" : "Proof of Theorem 23. For brevity, denote ψ(z1:n) = infh∈H 1n ∑n t=1 `(h, zt). The first step in the proof is to appeal to the minimax theorem for every couple of inf and sup:\nVn(H,Z) = inf q1∈∆(H̄) sup p1∈∆(Z) E h1∼q1 z1∼p1 . . . inf qn∈∆(H̄) sup pn∈∆(Z) E hn∼qn zn∼pn\n{ 1\nn n∑ t=1 `(ht, zt)− ψ(z1:n)\n}\n= sup p1∈∆(Z) inf q1∈∆(H̄) E h1∼q1 z1∼p1 . . . sup pn∈∆(Z) inf qn∈∆(H̄) E hn∼qn zn∼pn\n{ 1\nn n∑ t=1 `(ht, zt)− ψ(z1:n)\n}\n= sup p1∈∆(Z) inf h1∈H̄ Ex1∼p1 . . . sup pn∈∆(Z) inf hn∈H̄ Ezn∼pn\n{ 1\nn n∑ t=1 `(ht, zt)− ψ(z1:n)\n}\nFrom now on, it will be understood that zt has distribution pt. Moving the expectation with respect to zn and then the infimum with respect to hn inside the expression, we arrive at,\n= sup p1∈∆(Z) inf h1∈H̄ E z1 . . . sup pn−1 inf hn−1∈H̄ E zn−1 sup pn∈∆(Z)\n{ 1\nn n−1∑ t=1 `(ht, zt) + 1 n inf hn∈H̄ Ezn`(hn, zn)− Eznψ(z1:n)\n}\n= sup p1∈∆(Z) inf h1∈H̄ Ez1 . . . sup pn−1 inf hn−1 Ezn−1 sup pn∈∆(Z) Ezn\n[ 1\nn n−1∑ t=1 `(ht, zt) + 1 n inf hn∈H̄ Ezn`(hn, zn)− ψ(z1:n)\n]\nRepeating the procedure for step n− 1,\n= sup p1∈∆(Z) inf h1∈H̄ E z1 . . . sup pn−1 inf hn−1 E zn−1\n[ 1\nn n−1∑ t=1 `(ht, zt) + sup pn∈∆(Z) Ezn [ 1 n inf hn∈H̄ Ezn`(hn, zn)− ψ(z1:n) ]]\n= sup p1∈∆(Z) inf h1∈H̄ E z1 . . . sup pn−1∈∆(Z)\n{ 1\nn n−2∑ t=1 `(ht, zt) + 1 n [ inf hn−1∈H̄ Ezn−1`(hn−1, zn−1) ] +Ezn−1 sup\npn\nE zn\n[ 1\nn inf hn∈H̄ Ezn`(hn, zn)− ψ(z1:n) ]} = sup p1∈∆(Z) inf h1∈H̄ Ez1 . . . sup pn∈∆(Z) Ezn { 1 n n−2∑ t=1 `(ht, zt) + 1 n n∑ t=n−1 inf ht∈H̄ Ezt`(ht, zt)− ψ(z1:n) }\nContinuing in this fashion for n− 2 and all the way down to t = 1 proves the theorem.\nProof of the Key Technical Lemma (Lemma 25). We start by noting that since zn, z′n are both drawn from\n74\npn,\nEzn,z′n∼pn\n[ Φ ( n∑ t=1 ∆h(zt, z ′ t) )] = Ezn,z′n∼pn [ Φ ( n−1∑ t=1 ∆h(zt, z ′ t) + ∆h(zn, z ′ n) )]\n= Ez′n,zn∼pn\n[ Φ ( n−1∑ t=1 ∆h(zt, z ′ t) + ∆h(zn, z ′ n) )]\n= Ezn,z′n∼pn\n[ Φ ( n−1∑ t=1 ∆h(zt, z ′ t)−∆h(zn, z′n) )] ,\nwhere the last line is by antisymmetry of ∆h. Since the first and last lines are equal, they are both equal to their average and hence\nEzn,z′n∼pn\n[ Φ ( n−1∑ t=1 ∆h(zt, z ′ t) )] = Ezn,z′n∼pn [ E n [ Φ ( n−1∑ t=1 ∆h(zt, z ′ t) + n∆h(zn, z ′ n) )]] .\nHence we conclude that\nsup pn Ezn,z′n∼pn\n[ Φ ( n∑ t=1 ∆h(zt, z ′ t) )]\n= sup pn Ezn,z′n∼pn\n[ E n [ Φ ( n−1∑ t=1 ∆h(zt, z ′ t) + n∆h(zn, z ′ n) )]]\n≤ sup zn,z′n E n\n[ Φ ( n−1∑ t=1 ∆h(zt, z ′ t) + n∆h(zn, z ′ n) )] .\nUsing the above and noting that zn−1, z′n−1 are both drawn from pn−1 and hence similar to previous step introducing Rademacher variable n−1 we get that\nsup pn−1 Ezn−1,z′n−1∼pn−1 sup pn Ezn,z′n∼pn\n[ Φ ( n∑ t=1 ∆h(zt, z ′ t) )]\n≤ sup pn−1 Ezn−1,z′n−1∼pn−1 [ sup zn,z′n E n [ Φ ( n−1∑ t=1 ∆h(zt, z ′ t) + n∆h(zn, z ′ n) )]]\n= sup pn−1 Ezn−1,z′n−1∼pnE n−1 [ sup zn,z′n E n [ Φ ( n−2∑ t=1 ∆h(zt, z ′ t) + n−1∆h(zn−1, z ′ n−1) + n∆h(zn, z ′ n) )]]\n≤ sup zn−1,z′n−1 E n−1 [ sup zn,z′n E n [ Φ ( n−2∑ t=1 ∆h(zt, z ′ t) + n−1∆h(zn−1, z ′ n−1) + n∆h(zn, z ′ n) )]] .\nProceeding in similar fashion introducing Rademacher variables all the way upto 1 we finally get the required\n75\nstatement that\nsup p1 Ez1,z′1∼p1 . . . sup pn Ezn,z′n∼pn\n[ Φ ( n∑ t=1 ∆h(zt, z ′ t) )]\n≤ sup z1,z′1\n{ E 1 [ . . . sup\nzn,z′n\n{ E n [ Φ ( n∑ t=1 t∆h(zt, z ′ t) )]} . . . ]}\nProof of Lemma 26. Without loss of generality assume that the Lipschitz constantL = 1 because the general case follows by scaling φ. Now note that by Theorem 35 we have that\nRseqn (φ ◦ F) ≤ inf α\n{ 4α+ 12 ∫ 1 α √ log N2(δ, φ ◦ F , n) n dδ } (4.8.1)\nNow we claim that we can bound\nlog N2(δ, φ ◦ F , n) ≤ k∑ j=1 log N∞(δ,Fj , n)\nTo see this we first start by noting that√√√√ 1 n n∑ t=1 (φ(f(zt( )), zt( ))− φ(vt( ), zt( )))2\n≤ √√√√ 1 n n∑ t=1 max j ( fj(zt( )))− vjt ( ) )2 ≤ √ max t∈[n] max j ( fj(zt( )))− vjt ( )\n)2 ≤ max j∈[k],t∈[n] |fj(zt( )))− vjt ( )|\nNow suppose we have V1, . . . , Vk that are minimal `∞-covers forF1, . . . ,Fk on the tree z at level δ. Consider the set:\nVφ = {vφ : v ∈ V1 × . . .× Vk}\nwhere vφ is the tree such that (vφ)t( ) = φ(vt( ), zt( )). Then, for any f = (f1, . . . , fk) ∈ F (with\n76\nrepresentatives (v1, . . . ,vk) ∈ V1 × . . .× Vk) and any ∈ {±1}n, we have,√√√√ 1 n n∑ t=1 (φ(f(zt( )), zt( ))− (vφ)t( ))2\n= √√√√ 1 n n∑ t=1 (φ(f(zt( )), zt( ))− φ(vt( ), zt( )))2\n≤ max j∈[k] max t∈[n] |fj(zt( )))− vjt ( )| ≤ δ\nThus we see that Vφ is an `∞-cover at scale δ for φ ◦ F on z. Hence\nlog N2(δ, φ ◦ F , n) ≤ log N∞(δ, φ ◦ F , n)\n≤ log(|V |) = k∑ j=1 log(|Vj |)\n= k∑ j=1 log N∞(δ,Fj , n)\nas claimed. Now using this in Equation 4.8.1 we have that\nRseqn (φ ◦ F) ≤ inf α 4α+ 12 ∫ 1 α √∑k j=1 log N∞(δ,Fj , n) n dδ  ≤ inf\nα 4α+ 12 k∑ j=1 ∫ 1 α √ log N∞(δ,Fj , n) n dδ  ≤\nk∑ j=1 inf α\n{ 4α+ 12 ∫ 1 α √ log N∞(δ,Fj , n) n dδ }\nNow applying Lemma37 we conclude, as required, that\nRseqn (φ ◦ F) ≤ 8 ( 1 + 4 √ 2 log3/2(en2) ) k∑ j=1 Rseqn (Fj)\nas long as Rseqn (Fj) ≥ 1 for each j.\nProof of Corollary 28. We first extend the binary function b to a function b̄ to any x ∈ Rk as follows :\nb̄(x) =\n{ (1− ‖x− a‖∞)b(a) if ‖x− a‖∞ < 1 for some a ∈ {±1}k\n0 otherwise\nFirst note that b̄ is well-defined since all points in the k-cube are separated by L∞ distance 2. Further\n77\nnote that b̄ is 1-Lipschitz w.r.t. the L∞ norm and so applying Lemma 26 we conclude the statement of the corollary.\nProof of Proposition 29. The most difficult of these is Part 4, which follows immediately by Lemma 27 by taking φ(·, z) there to be simply φ(·). The other items follow similarly to Theorem 15 in [39] and we provide the proofs for completeness. Note that, unlike Rademacher complexity defined in [39], Sequential\nRademacher complexity does not have the absolute value around the sum.\nPart 1 is immediate because for any fixed tree z and fixed realization of { i},\nsup f∈F n∑ t=1 tf(zt( )) ≤ sup f∈G n∑ t=1 tf(zt( )) ,\nNow taking expectation over and supremum over z completes the argument.\nTo show Part 2, first observe that, according to Part 1,\nRseqn (F) ≤ Rseqn (conv(F)) .\nNow, any h ∈ conv(F) can be written as h = ∑m j=1 αjfj with ∑m j=1 αj = 1, αj ≥ 0. Then, for fixed tree z and sequence , n∑ t=1 th(zt( ) = m∑ j=1 αj n∑ t=1 tfj(zt( ) ≤ sup f∈F n∑ t=1 tf(zt( ))\nand thus\nsup h∈conv(F) n∑ t=1 th(zt( ) ≤ sup f∈F n∑ t=1 tf(zt( ) .\nTaking expectation over and supremum over z completes the proof.\nTo prove Part 3, first observe that the statement is easily seem to hold for c ≥ 0. That is, Rseqn (cF) = cRseqn (F) follows directly from the definition. Hence, it remains to convince ourselves of the statement for c = −1. That is, Rseqn (−F) = Rseqn (F). To prove this, consider a tree zR that is a reflection of z. That is, zRt ( ) = zt(− ) for all t ∈ [n]. It is then enough to observe that\nE [ sup f∈−F n∑ t=1 tf(zt( )) ] = E [ sup f∈F n∑ t=1 − tf(zt( )) ]\n= E [ sup f∈F n∑ t=1 tf(zt(− )) ] = E [ sup f∈F n∑ t=1 tf(z R t ( )) ]\nwhere we used the fact that and − have the same distribution. As z varies over all trees, zR also varies over all trees. Hence taking the supremum over z above finishes the argument.\n78\nFinally, for Part 5,\nsup f∈F { n∑ t=1 t (f + h) (zt( )) } = { sup f∈F n∑ t=1 tf(zt( )) } + { n∑ t=1 th(zt( )) }\nNote that, since h(zt( )) only depends on 1:t−1, we have\nE [ th(zt( ))] = E 1:t−1 [E [ t| 1:t−1]h(zt( )] = 0 .\nThus,\nRseqn (F + h) = Rseqn (F) .\nProof of Lemma 34. For any λ > 0, we invoke Jensen’s inequality to get\nM(λ) := exp { λE [ max v∈V n∑ t=1 tvt( ) ]} ≤ E [ exp { λmax v∈V n∑ t=1 tvt( ) }]\n≤ E [ max v∈V exp { λ n∑ t=1 tvt( ) }] ≤ E [∑ v∈V exp { λ n∑ t=1 tvt( ) }]\nWith the usual technique of peeling from the end,\nM(λ) ≤ ∑ v∈V E 1,..., n [ n∏ t=1 exp {λ tvt( 1:t−1)} ]\n= ∑ v∈V E 1,..., n−1 [ n−1∏ t=1 exp {λ tvt( 1:t−1)} × ( exp {λvn( 1:n−1)}+ exp {−λvn( 1:n−1)} 2 )]\n≤ ∑ v∈V E 1,..., n−1 [ n−1∏ t=1 exp {λ tvt( 1:t−1)} × exp { λ2vn( 1:n−1) 2 2 }]\nwhere we used the inequality 12 {exp(a) + exp(−a)} ≤ exp(a 2/2), valid for all a ∈ R. Peeling off the second term is a bit more involved:\nM(λ) ≤ ∑ v∈V E 1,..., n−2 [ n−2∏ t=1 exp {λ tvt( 1:t−1)}×\n1 2\n( exp {λvn−1( 1:n−2)} exp { λ2vn(( 1:n−2, 1)) 2\n2 } + exp {−λvn−1( 1:n−2)} exp { λ2vn(( 1:n−2,−1))2\n2\n})]\n79\nConsider the term inside:\n1 2\n( exp {λvn−1( 1:n−2)} exp { λ2vn(( 1:n−2, 1)) 2\n2\n} + exp {−λvn−1( 1:n−2)} exp { λ2vn(( 1:n−2,−1))2\n2 }) ≤ max\nn−1\n( exp { λ2vn(( 1:n−2, n−1)) 2\n2\n}) exp {λvn−1( 1:n−2)}+ exp {−λvn−1( 1:n−2)}\n2\n≤ max n−1\n( exp { λ2vn(( 1:n−2, n−1)) 2\n2\n}) exp { λ2vn−1( 1:n−2) 2\n2 } = exp { λ2 max n−1∈{±1} ( vn−1( 1:n−2) 2 + vn( 1:n−1) 2 )\n2\n}\nRepeating the last steps, we show that for any i,\nM(λ) ≤ ∑ v∈V E 1,..., i−1 [ i−1∏ t=1 exp {λ tvt( 1:t−1)} × exp { λ2 max i... n−1∈{±1} ∑n t=i vt( 1:t−1) 2 2 }]\nWe arrive at\nM(λ) ≤ ∑ v∈V exp { λ2 max 1... n−1∈{±1} ∑n t=1 vt( 1:t−1) 2 2 }\n≤ |V | exp { λ2 maxv∈V max ∈{±1}n ∑n t=1 vt( ) 2\n2\n}\nTaking logarithms on both sides, dividing by λ and setting λ = √\n2 log(|V |) maxv∈V max ∈{±1}n ∑n t=1 vt( ) 2 we conclude\nthat\nE 1,..., n [ max v∈V n∑ t=1 tvt( ) ] ≤ √√√√2 log(|V |) max v∈V max ∈{±1}n n∑ t=1 vt( )2\nProof of Lemma 30. We prove the first inequality. Let {w1, . . . ,wM} be a largest strongly 2α-separated set of F(z) with M = Mp(2α,F , z). Let {v1, . . . ,vN} be a smallest α-cover of F on z with N = Nseqp (α,F , z). For the sake of contradiction, assume M > N . Consider a path ∈ {±1}n on which all the trees {w1, . . . ,wM} are (2α)-separated. By the definition of a cover, for any wi there exists a tree vj such that (\n1 n n∑ t=1 |vjt ( )−wit( )|p )1/p ≤ α.\nSince M > N , there must exist distinct wi and wk, for which the covering tree vj is the same for the given path . By triangle inequality, ( 1\nn n∑ t=1 |wit( )−wkt ( )|p )1/p ≤ 2α,\n80\nwhich is a contradiction. We conclude that M ≤ N .\nNow, we prove the second inequality. Consider a maximal α-packing V ⊆ F(z) of size Dp(α,F , z). Since this is a maximal α-packing, for any f ∈ F , there is no path on which f(z) is α-separated from every member of the packing. In other words, for every path ∈ {±1}n, there is a member of the packing v ∈ V such that\n( 1\nn n∑ t=1\n|vt( )− f(zt( ))|p )1/p ≤ α\nwhich means that the packing V is a cover.\nProof of Theorem 31. For any d ≥ 0 and n ≥ 0, define the function\ngk(d, n) = d∑ i=0 ( n i ) ki.\nIt is not difficult to verify that this function satisfies the recurrence\ngk(d, n) = gk(d, n− 1) + kgk(d− 1, n− 1)\nfor all d, n ≥ 1. To visualize this recursion, consider a k × n matrix and ask for ways to choose at most d columns followed by a choice among the k rows for each chosen column. The task can be decomposed into (a) making the d column choices out of the first n − 1 columns, followed by picking rows (there are gk(d, n − 1) ways to do it) or (b) choosing d − 1 columns (followed by row choices) out of the first n − 1 columns and choosing a row for the nth column (there are kgk(d − 1, n − 1) ways to do it). This gives the recursive formula.\nIn what follows, we shall refer to an L∞ cover at scale 1/2 simply as a 1/2-cover. The theorem claims that the size of a minimal 1/2-cover is at most gk(d, n). The proof proceeds by induction on n+ d.\nBase: For d = 1 and n = 1, there is only one node in the tree, i.e. the tree is defined by the constant z1 ∈ Z . Functions in F can take up to k + 1 values on z1, i.e. Nseq(0,F , 1) ≤ k + 1 (and, thus, also for the 1/2-cover). Using the convention ( n 0 ) = 1, we indeed verify that gk(1, 1) = 1 + k = k + 1. The same calculation gives the base case for n = 1 and any d ∈ N. Furthermore, for any n ∈ N if d = 0, then there is no point which is 2-shattered by F . This means that functions in F differ by at most 1 on any point of Z . Thus, there is a 1/2 cover of size 1 = gk(0, n), verifying this base case.\nInduction step: Suppose by the way of induction that the statement holds for (d, n− 1) and (d− 1, n− 1). Consider any tree z of depth n with fatseq2 (F , z) = d. Define the partition F = F0 ∪ . . . ∪ Fk with Fi = {f ∈ F : f(z1) = i} for i ∈ {0, . . . , k}, where z1 is the root of z. Let n = |{i : fatseq2 (Fi, z) = d}|.\nSuppose first, for the sake of contradiction, that fatseq2 (Fi, z) = fat seq 2 (Fj , z) = d for |i − j| ≥ 2. Then there exist two trees z and v of depth d which are 2-shattered by Fi and Fj , respectively, and with\n81\nImg(z), Img(v) ⊆ Img(z). Since functions within each subset Fi take on the same values on z1, we conclude that z1 /∈ Img(z), z1 /∈ Img(v). This follows immediately from the definition of shattering. We now join the two shattered z and v trees with z1 at the root and observe that Fi ∪ Fj 2-shatters this resulting tree of depth d+ 1, which is a contradiction. Indeed, the witness R-valued tree s is constructed by joining the two witnesses for the 2-shattered trees z and v and by defining the root as s1 = (i+ j)/2. It is easy to see that s is a witness to the shattering. Given any ∈ {±1}d+1, there is a function f i ∈ Fi which realizes the desired separation under the signs ( 2, . . . , d+1) for the tree z and there is a function f j ∈ Fj which does the same for v. Depending on 1 = +1 or 1 = −1, either f i or f j realize the separation over .\nWe conclude that the number of subsets of F with fat-shattering dimension equal to d cannot be more than two (for otherwise at least two indices will be separated by 2 or more). We have three cases: n = 0, n = 1,\nor n = 2, and in the last case it must be that the indices of the two subsets differ by 1.\nFirst, consider any Fi with fatseq2 (Fi, z) ≤ d− 1, i ∈ {0, . . . , k}. By induction, there are 1/2-covers V ` and V r ofFi on the subtrees z` and zr, respectively, both of size at most gk(d−1, n−1). Informally, out of these 1/2-covers we can create a 1/2-cover V for Fi on z by pairing the 1/2-covers in V ` and V r. The resulting cover of Fi will be of size gk(d− 1, n− 1). Formally, consider a set of pairs (v`,vr) of trees, with v` ∈ V `, vr ∈ V r and such that each tree in V ` and V r appears in at least one of the pairs. Clearly, this can be done using at most gk(d − 1, n − 1) pairs, and such a pairing is not unique. We join the subtrees in every pair (v`,vr) with a constant i as the root, thus creating a set V of trees, |V | ≤ gk(d−1, n−1). We claim that V is a 1/2-cover for Fi on z. Note that all the functions in Fi take on the same value i on z1 and by construction v1 = i for any v ∈ V . Now, consider any f ∈ Fi and ∈ {±1}n. Without loss of generality, assume 1 = −1. By assumption, there is a v` ∈ V ` such that |v`t( 2:n)− f(zt+1( 1:n))| ≤ 1/2 for any t ∈ [n− 1]. By construction v` appears as a left subtree of at least one tree in V , which, therefore, matches the values of f for 1:n. The same argument holds for 1 = +1 by finding an appropriate subtree in V r. We conclude that V is a 1/2-cover of Fi on z, and this holds for any i ∈ {0, . . . , k} with fatseq2 (Fi, z) ≤ d− 1. Therefore, the total size of a 1/2-cover for the union ∪i:fatseq2 (Fi,z)≤d−1Fi is at most (k+ 1−n)gk(d− 1, n− 1). If n = 0, the induction step is proven because gk(d− 1, n− 1) ≤ gk(d, n− 1) and so the total size of the constructed cover is at most\n(k + 1)gk(d− 1, n− 1) ≤ gk(d, n− 1) + kgk(d− 1, n− 1) = gk(d, n).\nNow, consider the case n = 1 and let fatseq2 (Fi, z) = d. An argument exactly as above yields a 1/2-cover for Fi, and this cover is of size at most gk(d, n− 1) by induction. The total 1/2-cover is therefore of size at most\ngk(d, n− 1) + kgk(d− 1, n− 1) = gk(d, n).\nLastly, for n = 2, suppose fatseq2 (Fi, z) = fat seq 2 (Fj , z) = d for |i − j| = 1. Let F ′ = Fi ∪ Fj . Note that fatseq2 (F ′, z) = d. Just as before, the 1/2-covering for z can be constructed by considering the 1/2-covers\n82\nfor the two subtrees. However, when joining any (v`,vr), we take (i+ j)/2 as the root. It is straightforward to check that the resulting cover is indeed an 1/2-cover, thanks to the relation |i − j| = 1. The size of the constructed cover is, by induction, gk(d, n− 1), and the induction step follows. This concludes the induction proof, yielding the main statement of the theorem.\nFinally, the upper bound on gk(d, n) is\nd∑ i=1 ( n i ) ki ≤ ( kn d )d d∑ i=1 ( n i )( d n )i ≤ ( kn d )d( 1 + d n )n ≤ ( ekn d )d whenever n ≥ d.\nProof of Theorem 33. The proof is very close to the proof of Theorem 31, with a few key differences. As before, for any d ≥ 0 and n ≥ 0, define the function gk(d, n) = ∑d i=0 ( n i ) ki.\nThe theorem claims that the size of a minimal 0-cover is at most gk(d, n). The proof proceeds by induction on n+ d.\nBase: For d = 1 and n = 1, there is only one node in the tree, i.e. the tree is defined by the constant z1 ∈ Z . Functions in F can take up to k+ 1 values on z1, i.e. Nseq(0,F , 1) ≤ k+ 1. Using the convention ( n 0 ) = 1, we indeed verify that gk(1, 1) = 1 + k = k + 1. The same calculation gives the base case for n = 1 and any d ∈ N. Furthermore, for any n ∈ N if d = 0, then there is no point which is 1-shattered by F . This means that all functions in F are identical, proving that there is a 0-cover of size 1 = gk(0, n).\nInduction step: Suppose by the way of induction that the statement holds for (d, n− 1) and (d− 1, n− 1). Consider any tree z of depth n with fatseq1 (F , z) = d. Define the partition F = F0 ∪ . . . ∪ Fk with Fi = {f ∈ F : f(z1) = i} for i ∈ {0, . . . , k}, where z1 is the root of z.\nWe first argue that fatseq1 (Fi, z) = d for at most one value i ∈ {0, . . . , k}. By the way of contradiction, suppose we do have fatseq1 (Fi, z) = fat seq 1 (Fj , z) = d for i 6= j. Then there exist two trees z and v of depth d 1-shattered by Fi and Fj , respectively, and with Img(z), Img(v) ⊆ Img(z). Since functions within each subset Fi take on the same values on z1, we conclude that z1 /∈ Img(z), z1 /∈ Img(v). This follows immediately from the definition of shattering. We now join the two shattered z and v trees with z1 at the root and observe that Fi ∪ Fj 1-shatters this resulting tree of depth d + 1, which is a contradiction. Indeed, the witness R-valued tree s is constructed by joining the two witnesses for the 1-shattered trees z and v and by defining the root as s1 = (i + j)/2. It is easy to see that s is a witness to the shattering. Given any\n∈ {±1}d+1, there is a function f i ∈ Fi which realizes the desired separation under the signs ( 2, . . . , d+1) for the tree z and there is a function f j ∈ Fj which does the same for v. Depending on 1 = +1 or 1 = −1, either f i or f j realize the separation over .\nWe conclude that fatseq1 (Fi, z) = d for at most one i ∈ {0, . . . , k}. Without loss of generality, assume fatseq1 (F0, z) ≤ d and fat seq 1 (Fi, z) ≤ d − 1 for i ∈ {1, . . . , k}. By induction, for any Fi, i ∈ {1, . . . , k},\n83\nthere are 0-covers V ` and V r of Fi on the subtrees z` and zr, respectively, both of size at most gk(d − 1, n− 1). Out of these 0-covers we can create a 0-cover V for Fi on z by pairing the 0-covers in V ` and V r. Formally, consider a set of pairs (v`,vr) of trees, with v` ∈ V `, vr ∈ V r and such that each tree in V ` and V r appears in at least one of the pairs. Clearly, this can be done using at most gk(d−1, n−1) pairs, and such a pairing is not unique. We join the subtrees in every pair (v`,vr) with a constant i as the root, thus creating a set V of trees, |V | ≤ gk(d− 1, n− 1). We claim that V is a 0-cover for Fi on z. Note that all the functions in Fi take on the same value i on z1 and by construction v1 = i for any v ∈ V . Now, consider any f ∈ Fi and ∈ {±1}n. Without loss of generality, assume 1 = −1. By assumption, there is a v` ∈ V ` such that v`t( 2:n) = f(zt+1( 1:n)) for any t ∈ [n− 1]. By construction v` appears as a left subtree of at least one tree in V , which, therefore, matches the values of f for 1:n. The same argument holds for 1 = +1 by finding an appropriate subtree in V r. We conclude that V is a 0-cover of Fi on z, and this holds for any i ∈ {1, . . . , k}.\nTherefore, the total size of a 0-cover for F1∪ . . .∪Fk is at most kgk(d−1, n−1). A similar argument yields a 0-cover for F0 on z of size at most gk(d, n− 1) by induction. Thus, the size of the resulting 0-cover of F on z is at most\ngk(d, n− 1) + kgk(d− 1, n− 1) = gk(d, n),\ncompleting the induction step and yielding the main statement of the theorem.\nThe upper bound on gk(d, n) appears in the proof of Theorem 31.\nProof of Corollary 32. The first two inequalities follow by simple comparison of norms. It remains to prove the bound for the `∞ covering. For any α > 0 define an α-discretization of the [−1, 1] interval as Bα = {−1 +α/2,−1 + 3α/2, . . . ,−1 + (2k+ 1)α/2, . . .} for 0 ≤ k and (2k+ 1)α ≤ 4. Also for any a ∈ [−1, 1] define bacα = argmin\nr∈Bα |r − a| with ties being broken by choosing the smaller discretization point. For a\nfunction f : Z 7→ [−1, 1] let the function bfcα be defined pointwise as bf(x)cα, and let bFcα = {bfcα : f ∈ F}. First, we prove that Nseq∞ (α,F , z) ≤ Nseq∞ (α/2, bFcα, z). Indeed, suppose the set of trees V is a minimal α/2-cover of bFcα on z. That is,\n∀fα ∈ bFcα, ∀ ∈ {±1}n ∃v ∈ V s.t. |vt( )− fα(zt( ))| ≤ α/2\nPick any f ∈ F and let fα = bfcα. Then ‖f − fα‖∞ ≤ α/2. Then for all ∈ {±1}n and any t ∈ [n]\n|f(zt( ))− vt( )| ≤ |f(zt( ))− fα(zt( ))|+ |fα(zt( ))− vt( )| ≤ α,\nand so V also provides an L∞ cover at scale α.\nWe conclude that Nseq∞ (α,F , z) ≤ Nseq∞ (α/2, bFcα, z) = Nseq∞ (1/2,G, z) where G = 1αbFcα. The functions of G take on a discrete set of at most b2/αc + 1 values. Obviously, by adding a constant to all the functions in G, we can make the set of values to be {0, . . . , b2/αc}. We now apply Theorem 31 with an upper\n84\nbound ∑d i=0 ( n i ) ki ≤ (ekn)d which holds for any n > 0. This yields Nseq∞ (1/2,G, z) ≤ (2en/α) fatseq2 (G). It remains to prove fatseq2 (G) ≤ fat seq α (F), or, equivalently (by scaling) fat seq 2α (bFcα) ≤ fat seq α (F). To this end, suppose there exists an R-valued tree z of depth d = fatseq2α (bFcα) such that there is an witness tree s with\n∀ ∈ {±1}d, ∃fα ∈ bFcα s.t. ∀t ∈ [d], t(fα(zt( ))− st( )) ≥ α\nUsing the fact that for any f ∈ F and fα = bfcα we have ‖f − fα‖∞ ≤ α/2, it follows that\n∀ ∈ {±1}d, ∃f ∈ F s.t. ∀t ∈ [d], t(f(zt( ))− st( )) ≥ α/2\nThat is, s is a witness to α-shattering by F . Thus for any z,\nNseq∞ (α,F , z) ≤ Nseq∞ (α/2, bFcα, z) ≤ ( 2en\nα\n)fatseq2α (bFcα) ≤ ( 2en\nα\n)fatseqα (F)\nProof of Theorem 35. Define β0 = 1 and βj = 2−j . For a fixed tree z of depth n, let Vj be an `2-cover at scale βj . For any path ∈ {±1}n and any f ∈ F , let v[f, ]j ∈ Vj the element of the cover such that√√√√ 1\nn n∑ t=1 |v[f, ]jt ( )− f(zt( ))|2 ≤ βj\nBy the definition such a v[f, ]j ∈ Vj exists, and we assume for simplicity this element is unique (ties can be broken in an arbitrary manner). Thus, f 7→ v[f, ]j is a well-defined mapping for any fixed and j. As before, v[f, ]jt denotes the t-th mapping of v[f, ] j . For any t ∈ [n], we have\nf(zt( )) = f(zt( ))− v[f, ]Nt ( ) + N∑ j=1 (v[f, ]jt ( )− v[f, ] j−1 t ( ))\nwhere v[f, ]0t ( ) = 0. Hence,\nE [ sup f∈F n∑ t=1 tf(zt( )) ] = E [ sup f∈F n∑ t=1 t ( f(zt( ))− v[f, ]Nt ( ) + N∑ j=1 (v[f, ]jt( )− v[f, ] j−1 t ( )) )]\n= E [ sup f∈F n∑ t=1 t ( f(zt( ))− v[f, ]Nt ( ) ) + n∑ t=1 t ( N∑ j=1 (v[f, ]jt( )− v[f, ] j−1 t ( )) )]\n≤ E [ sup f∈F n∑ t=1 t ( f(zt( ))− v[f, ]Nt ( ) )] + E [ sup f∈F n∑ t=1 t ( N∑ j=1 (v[f, ]jt( )− v[f, ] j−1 t ( )) )] (4.8.2)\n85\nThe first term above can be bounded via the Cauchy-Schwarz inequality as\nE [ sup f∈F n∑ t=1 t ( f(zt( ))− v[f, ]Nt ( ) )] ≤ n E [ sup f∈F n∑ t=1 t√ n ( f(zt( ))− v[f, ]Nt ( ) ) √ n ] ≤ n βN .\nThe second term in Eq. (4.8.2) is bounded by considering successive refinements of the cover. The argument, however, is more delicate than in the classical case, as the trees v[f, ]j , v[f, ]j−1 depend on the particular path. Consider all possible pairs of vs ∈ Vj and vr ∈ Vj−1, for 1 ≤ s ≤ |Vj |, 1 ≤ r ≤ |Vj−1|, where we assumed an arbitrary enumeration of elements. For each pair (vs,vr), define a real-valued tree w(s,r) by\nw (s,r) t ( ) = vst ( )− vrt ( ) if there exists f ∈ F s.t. vs = v[f, ]j ,vr = v[f, ]j−10 otherwise. for all t ∈ [n] and ∈ {±1}n. It is crucial that w(s,r) can be non-zero only on those paths for which vs and vr are indeed the members of the covers (at successive resolutions) close to f(z( )) (in the `2 sense) for some f ∈ F . It is easy to see that w(s,r) is well-defined. Let the set of trees Wj be defined as\nWj = { w(s,r) : 1 ≤ s ≤ |Vj |, 1 ≤ r ≤ |Vj−1| } Now, the second term in Eq. (4.8.2) can be written as\nE sup f∈F n∑ t=1 t N∑ j=1 (v[f, ]jt ( )− v[f, ] j−1 t ( ))  ≤ N∑ j=1 E [ sup f∈F n∑ t=1 t(v[f, ] j t ( )− v[f, ] j−1 t ( )) ]\n≤ N∑ j=1 E [ max w∈Wj n∑ t=1 twt( ) ]\nThe last inequality holds because for any j ∈ [N ], ∈ {±1}n and f ∈ F there is some w(s,r) ∈ Wj with v[f, ]j = vs, v[f, ]j−1 = vr and\nvst ( )− vrt ( ) = w (s,r) t ( ) ∀t ≤ n.\nClearly, |Wj | ≤ |Vj | · |Vj−1|. To invoke Lemma 34, it remains to bound the magnitude of all w(s,r) ∈ Wj along all paths. For this purpose, fix w(s,r) and a path . If there exists f ∈ F for which vs = v[f, ]j and vr = v[f, ]j−1, then w(s,r)t ( ) = v[f, ] j t − v[f, ]\nj−1 t for any t ∈ [n]. By triangle inequality√√√√ n∑\nt=1\nw (s,r) t ( ) 2 ≤ √√√√ n∑ t=1 (v[f, ]jt ( )− f(zt( )))2+ √√√√ n∑ t=1 (v[f, ]j−1t ( )− f(zt( )))2 ≤ √ n(βj+βj−1) = 3 √ nβj .\n86\nIf there exists no such f ∈ F for the given and (s, r), then w(s,r)t ( ) is zero for all t ≥ to, for some 1 ≤ to < n, and thus √√√√ n∑\nt=1\nw (s,r) t ( ) 2 ≤ √√√√ n∑ t=1 w (s,r) t ( ′)2\nfor any other path ′ which agrees with up to to. Hence, the bound√√√√ n∑ t=1 w (s,r) t ( ) 2 ≤ 3 √ nβj\nholds for all ∈ {±1}n and all w(s,r) ∈Wj .\nNow, back to Eq. (4.8.2), we put everything together and apply Lemma 34:\nE [ sup f∈F n∑ t=1 tf(zt( )) ] ≤ n βN + √ n N∑ j=1 3βj √ 2 log(|Vj | |Vj−1|)\n≤ n βN + √ n N∑ j=1 6βj √ log(|Vj |)\n≤ n βN + 12 √ n N∑ j=1 (βj − βj+1) √ logN2(βj ,F , z)\n≤ n βN + 12 ∫ β0 βN+1 √ n log N2(δ,F , z) dδ\nwhere the last but one step is because 2(βj − βj+1) = βj . Now for any α > 0, pick N = sup{j : βj > 2α}. In this case we see that by our choice of N , βN+1 ≤ 2α and so βN = 2βN+1 ≤ 4α. Also note that since βN > 2α, βN+1 = βN2 > α. Hence dividing throughout by n we conclude that\nRseqn (F) ≤ inf α\n{ 4α+ 12 ∫ 1 α √ log N2(δ,F , n) n dδ }\nProof of Theorem 38. Let (x′1, . . . , x′n) be a sequence tangent to (x1, . . . , xn). Recall the notation Et−1 [f(x′t)] =\n87\nE {f(x′t)|x1, . . . , xt−1}. By Chebychev’s inequality, for any f ∈ F ,\nPD\n[ 1\nn ∣∣∣∣∣ n∑ t=1 (f(x′t)− Et−1 [f(x′t)]) ∣∣∣∣∣ > α/2∣∣∣ x1, . . . , xn ] ≤ E [ ( ∑n t=1 (f(x ′ t)− Et−1 [f(x′t)])) 2 ∣∣∣x1, . . . , xn] n2α2/4\n=\n∑n t=1 E [ (f(x′t)− Et−1 [f(x′t)]) 2 ∣∣x1, . . . , xn] n2α2/4\n≤ 4n n2α2/4 = 16 nα2 .\nThe second step is due to the fact that the cross terms are zero:\nE { (f(x′t)− Et−1 [f(x′t)]) (f(x′s)− Es−1 [f(x′s)]) ∣∣x1, . . . , xn} = 0 .\nHence\ninf f∈F\nPD\n[ 1\nn ∣∣∣∣∣ n∑ t=1 (f(x′t)− Et−1[f(x′t)]) ∣∣∣∣∣ ≤ α/2 ∣∣∣ x1, . . . , xn ] ≥ 1− 16 nα2\nWhenever α2 ≥ 32n we can conclude that\ninf f∈F\nPD\n[ 1\nn ∣∣∣∣∣ n∑ t=1 (f(x′t)− Et−1[f(x′t)]) ∣∣∣∣∣ ≤ α/2 ∣∣∣ x1, . . . , xn ] ≥ 1 2\nNow given a fixed x1, ..., xn let f∗ be the function that maximizes 1n | ∑n t=1 (f(xt)− Et−1[f(x′t)])|. Note that f∗ is a deterministic choice given x1, ..., xn. Hence\n1 2 ≤ inf f∈F PD\n[ 1\nn ∣∣∣∣∣ n∑ t=1 (f(x′t)− Et−1[f(x′t)]) ∣∣∣∣∣ ≤ α/2 ∣∣∣ x1, . . . , xn ]\n≤ PD\n[ 1\nn ∣∣∣∣∣ n∑ t=1 (f∗(x′t)− Et−1[f∗(x′t)]) ∣∣∣∣∣ ≤ α/2∣∣∣ x1, . . . , xn ]\nLetA = { (x1, . . . , xn) ∣∣ 1 n supf∈F | ∑n t=1 f(xt)− Et−1 [f(x′t)] | > α } . Since the above inequality holds for any x1, . . . , xn we can assert that\n1 2 ≤ PD\n[ 1\nn ∣∣∣∣∣ n∑ t=1 (f∗(x′t)− Et−1[f∗(x′t)]) ∣∣∣∣∣ ≤ α/2∣∣∣(x1, . . . , xn) ∈ A ]\n88\nHence we conclude that\n1 2 PD [ sup f∈F 1 n ∣∣∣∣∣ n∑ t=1 (f(xt)− Et−1[f(x′t)]) ∣∣∣∣∣ > α ]\n≤ PD\n[ 1\nn ∣∣∣∣∣ n∑ t=1 (f∗(x′t)− Et−1[f∗(x′t)]) ∣∣∣∣∣ ≤ α/2 ∣∣∣ (x1, . . . , xn) ∈ A ]\n× PD\n[ 1\nn sup f∈F ∣∣∣∣∣ n∑ t=1 (f(xt)− Et−1[f(x′t)]) ∣∣∣∣∣ > α ]\n≤ PD\n[ 1\nn ∣∣∣∣∣ n∑ t=1 (f∗(xt)− f∗(x′t)) ∣∣∣∣∣ > α/2 ]\n≤ PD\n[ 1\nn sup f∈F ∣∣∣∣∣ n∑ t=1 (f(xt)− f(x′t)) ∣∣∣∣∣ > α/2 ]\nNow we apply Lemma 25 with φ(u) = 1{u>α/2} and ∆f (xt, x′t) = f(xt)− f(x′t),\nE [ 1{supf∈F | ∑n t=1 f(xt)−f(x′t)|≥α/2} ] ≤ sup x1,x′1 { E 1 [ . . . sup xn,x′n { E n [ 1{supf∈F | ∑n t=1 t(f(xt)−f(x′t))|≥α/2} ]} . . . ]} (4.8.3)\nThe next few steps are similar to the proof of Theorem 24. Since\nsup f∈F ∣∣∣∣∣ n∑ t=1 t (f(xt)− f(x′t)) ∣∣∣∣∣ ≤ supf∈F ∣∣∣∣∣ n∑ t=1 tf(xt) ∣∣∣∣∣+ supf∈F ∣∣∣∣∣ n∑ t=1 tf(x ′ t) ∣∣∣∣∣ it is true that\n1{supf∈F | ∑n t=1 t(f(xt)−f(x′t))|≥α/2} ≤ 1{supf∈F | ∑n t=1 tf(xt)|≥α/4} + 1{supf∈F | ∑n t=1 tf(x ′ t)|≥α/4}\nThe right-hand side of Eq. Eq. (4.8.3) then splits into two equal parts:\nsup x1\n{ E 1 [ . . . sup\nxn\n{ E n [ 1{supf∈F | ∑n t=1 tf(xt)|≥α/4} ]} . . . ]} + sup\nx′1\n{ E 1 [ . . . sup\nx′n\n{ E n [ 1{supf∈F | ∑n t=1 tf(x ′ t)|≥α/4} ]} . . .\n]}\n= 2 sup x1\n{ E 1 [ . . . sup\nxn\n{ E n [ 1{supf∈F | ∑n t=1 tf(xt)|≥α/4} ]} . . . ]}\n89\nMoving to the tree representation,\nPD\n[ 1\nn sup f∈F ∣∣∣∣∣ n∑ t=1 (f(xt)− f(x′t)) ∣∣∣∣∣ > α/2 ] ≤ 2 sup z E [ 1{ 1n supf∈F | ∑n t=1 tf(zt( ))|>α/4} ] = 2 sup\nz P\n[ 1\nn sup f∈F ∣∣∣∣∣ n∑ t=1 tf(zt( )) ∣∣∣∣∣ > α/4 ]\nWe can now conclude that\nPD\n[ 1\nn sup f∈F ∣∣∣∣∣ n∑ t=1 (f(xt)− Et−1[f(xt)]) ∣∣∣∣∣ > α ] ≤ 4 sup z P [ 1 n sup f∈F ∣∣∣∣∣ n∑ t=1 tf(zt( )) ∣∣∣∣∣ > α/4 ]\nFix an Z-valued tree z of depth n. By assumption fatseqα (F) < ∞ for any α > 0. Let V be a minimum `1-cover of F over z at scale α/8. Corollary 32 ensures that\n|V | = N1(α/8,F , z) ≤ ( 16en\nα\n)fatseqα 8\nand for any f ∈ F and ∈ {±1}n, there exists v[f, ] ∈ V such that\n1 n n∑ t=1 |f(zt( ))− v[f, ]t( )| ≤ α/8\non the given path . Hence\nP\n[ 1\nn sup f∈F ∣∣∣∣∣ n∑ t=1 tf(zt( )) ∣∣∣∣∣ > α/4 ]\n= P\n[ 1\nn sup f∈F ∣∣∣∣∣ n∑ t=1 t (f(zt( ))− v[f, ]t( ) + v[f, ]t( )) ∣∣∣∣∣ > α/4 ]\n≤ P\n[ 1\nn sup f∈F ∣∣∣∣∣ n∑ t=1 t (f(zt( ))− v[f, ]t( )) ∣∣∣∣∣+ 1n supf∈F ∣∣∣∣∣ n∑ t=1 tv[f, ]t( ) ∣∣∣∣∣ > α/4 ]\n≤ P\n[ 1\nn sup f∈F ∣∣∣∣∣ n∑ t=1 tv[f, ]t( ) ∣∣∣∣∣ > α/8 ]\nFor fixed = ( 1, . . . , n),\n1 n sup f∈F ∣∣∣∣∣ n∑ t=1 tv[f, ]t( ) ∣∣∣∣∣ > α/8 =⇒ 1n maxv∈V ∣∣∣∣∣ n∑ t=1 tvt( ) ∣∣∣∣∣ > α/8\n90\nand, therefore, for any z,\nP\n[ 1\nn sup f∈F ∣∣∣∣∣ n∑ t=1 tf(zt( )) ∣∣∣∣∣ > α/4 ] ≤ P [ 1 n max v∈V ∣∣∣∣∣ n∑ t=1 tvt( ) ∣∣∣∣∣ > α/8 ]\n≤ ∑ v∈V P\n[ 1\nn ∣∣∣∣∣ n∑ t=1 tvt( ) ∣∣∣∣∣ > α/8 ] ≤ 2|V |e−nα 2/128 ≤ 2 ( 16en α )fatseq α/8 e−nα 2/128\nHence we conclude that for any D\nPD\n[ 1\nn sup f∈F ∣∣∣∣∣ n∑ t=1 (f(xt)− Et−1[f(xt)]) ∣∣∣∣∣ > α ] ≤ 8 ( 16en α )fatseq α/8 e−nα 2/128\nNow applying Borel-Cantelli lemma proves the required result as\n∞∑ n=1 8 ( 16en α )fatseq α/8 e−nα 2/128 <∞ .\nProof of Proposition 42. Using Theorem 35 we get the bound Rseqn (FS) ≤ Dseqn (FS). Further the fact that absolute loss is 1-Lipschitz implies that an cover of class H is also an cover for loss class FS and so Dseqn (FS) ≤ Dseqn (H). This gives the first upper bound of value in terms of Dseqn (H). The second inequality in the upper bound is a direct consequence of using Corollary 32 in Dseqn (H). Now before we prove the final inequality in the upper bound we first prove the lower bound because we shall use ideas from the lower bound\nto get the final inequality in the upper bound.\nFor the lower bound, we use a construction similar to [35]. We construct a particular distribution which induces a lower bound on regret for any algorithm. For any α ≥ 0 by definition of fat-shattering dimension, there exists a tree z of depth d = fatseqα (H) that can be α-shattered byH. For simplicity, we assume n = kd where k is some non-negative integer, and the case n ≤ d is discussed at the end of the proof. Now, define the jth block of time Tj = {(j − 1)k + 1, . . . , jk}.\nNow the strategy of Nature (Adversary) is to first pick ̃ ∈ {±1}n independently and uniformly at random. Further let ∈ {±1}d be defined as j = sign (∑ t∈Tj ̃t ) for 1 ≤ j ≤ d, the block-wise modal sign of ̃. Now note that by definition of α-shattering, there exists a witness tree s such that for any ∈ {±1}d there exists h ∈ H with j(h (zj( )) − sj( )) ≥ α/2 for all 1 ≤ j ≤ d. Now let the random sequence (x1, y1), . . . , (xn, yn) be defined by xt = zj( ) for all t ∈ Tj and j ∈ {1, . . . , d} and yt = ̃t. In the remainder of the proof we show that any algorithm suffers large expected regret.\nNow consider any player strategy (possibly randomized) making prediction ŷt ∈ [−1, 1] at round t. Note that if we consider block j, yt = ̃t is ±1 uniformly at random. This means that irrespective of what ŷt the player\n91\nplays, the expectation over ̃t of the loss the player suffers at round t is\nẼt |ŷt − yt| = 1\nHence on block j, the expected loss accumulated by any player is k and so for any player strategy (possibly\nrandomized),\nE [ n∑ t=1 |ŷt − yt| ] = d∑ j=1 k = dk = n (4.8.4)\nOn the other hand since xt = zj( ), we know that there always exists a function for any ∈ {±1}d, say h such that j(h (zj( ))− sj( )) ≥ α/2. Hence\nE [ inf h∈H n∑ t=1 |h(xt)− yt| ] ≤ d∑ j=1 E ∑ t∈Tj |h (xt)− yt|  =\nd∑ j=1 E ∑ t∈Tj |h (zj( ))− yt|  ≤\nd∑ j=1 E  max cj∈[sj( )+ j α2 , j ] ∑ t∈Tj |cj − yt|  where the last step is because for all of block j, h (zj( )) does not depend on t and lies in the interval1 [sj( ) + j α 2 , j ] (i.e. the majority side) and so by replacing it by the maximal cj in the same interval for that block we only make the quantity bigger. Now for a block j, define the number of labels that match\nthe sign of j (the majority) as Mj = ∑ t∈Tj 1{yt= j}. Since yt = ̃t ∈ {±1}, observe that the function\ng(cj) = ∑ t∈Tj |cj − yt| is linear on the interval [−1, 1] with its minimum at the majority sign j . Hence, the maximum over [sj( ) + j α2 , j ] must occur at cj = sj( ) + j α 2 . Substituting,\nmax cj∈[sj( )+ j α2 , j ] ∑ t∈Tj |cj − yt| = Mj ∣∣∣sj( ) + j α 2 − j ∣∣∣+ (k −Mj) ∣∣∣sj( ) + j α 2 + j ∣∣∣ = Mj ∣∣∣ jsj( ) + α 2 − 1 ∣∣∣+ (k −Mj) ∣∣∣ jsj( ) + α 2 + 1 ∣∣∣\n= Mj ( 1− jsj( )− α\n2\n) + (k −Mj) ( 1 + jsj( ) + α\n2 ) = k + (k − 2Mj) ( jsj( ) + α\n2 ) 1We use the convention that [a, b] stands for [b, a] whenever a > b.\n92\nHence,\nE [ inf h∈H n∑ t=1 |h(xt)− yt| ] ≤ dk + d∑ j=1 E [ jsj( )(k − 2Mj) + α 2 (k − 2Mj) ]\n= dk + d∑ j=1 E [ jsj( )(k − 2Mj)] + α 2 d∑ j=1 E [k − 2Mj ]\nFurther note that k − 2Mj = −| ∑ t∈Tj ̃t| and so j(k − 2Mj) = − ∑ t∈Tj ̃t and so the expectation\nE [ jsj( )(k − 2Mj)] = E [ Ẽk(j−1)+1:jk [ jsj( )(k − 2Mj)] ] = 0\nbecause sj( ) is independent of ̃t for t ∈ Tj . Hence we see that\nE [ inf h∈H n∑ t=1 |h(xt)− yt| ] ≤ dk + α 2 d∑ j=1 E [k − 2Mj ] (4.8.5)\nCombining Equations Eq. (4.8.4) and Eq. (4.8.5) we can conclude that for any player strategy,\nE [ n∑ t=1 |ŷt − yt| ] − E [ inf h∈H n∑ t=1 |h(xt)− yt| ] ≥ α 2 d∑ j=1 E [2Mj − k]\n= α\n2 E  d∑ j=1 ∣∣∣∣∣∣ ∑ t∈Tj ̃t ∣∣∣∣∣∣  = α 2 d∑ j=1 E ∣∣∣∣∣∣ ∑ t∈Tj ̃t ∣∣∣∣∣∣ ≥ αd2 √ k 2 = α √ nd 8 = α √ n fatseqα 8 (4.8.6)\nby Khinchine’s inequality (e.g. [52, Lemma A.9]), yielding the theorem statement for n ≥ fatseqα . For the case of n < fatseqα , the proof is the same with k = 1 and the depth of the shattered tree being n, yielding a lower bound of αn/ √ 8. Dividing throughout by n completes the lower bound.\nNow we move to the final inequality in the upper bound, but before we proceed notice that since yt are Rademacher random variables. Hence from Equation 4.8.6 we see that\nα √ fatseqα 8 n ≤ E [ 1 n n∑ t=1 |ŷt − yt| ] − E [ inf h∈H 1 n n∑ t=1 |h(xt)− yt| ]\n= E [ sup h∈H 1 n n∑ t=1 (1− |h(xt)− yt|) ]\n= E [ sup h∈H 1 n n∑ t=1 ̃th(xt) ]\n= E [ sup h∈H 1 n n∑ t=1 ̃th(z ′ t(̃)) ]\n93\nWhere z′t(̃) = zd tk e( ) where each j = sign (∑ t∈Tj ̃t ) (ie. xt’s can be seen as nodes of a tree formed by taking the z tree which is of depth d and making it into a depth d tree by expanding each node of the tree into\na subtree of depth k). Hence we conclude that :\nα √ fatseqα 8 n ≤ E [ 1 n n∑ t=1 |ŷt − yt| ] − E [ inf h∈H 1 n n∑ t=1 |h(xt)− yt| ]\n= E [ sup h∈H 1 n n∑ t=1 ̃th(z ′ t(̃)) ]\n≤ sup z E [ sup h∈H 1 n n∑ t=1 th(zt( )) ] = Rseqn (H)\nThus effectively we have shown that supα α √ min{fatseqα ,n} 8 n ≤ R seq n (H). Using this we see that if α̂ is the\nsolution to fatseqα̂ = n then we have that α̂ ≤ Rseqn (H) and for any β > α̂, √ fatseqβ n ≤ 2 √ 2 Rseqn (H) β . Hence using this we conclude that,\ninf α\n{ 4α+\n12√ n ∫ 1 α √ fatseqβ (H) log ( 2en β ) dβ } ≤ 4α̂+ 12 ∫ 1 α̂ 2 √\n2 Rseqn (H) √ log ( 2en β ) β dβ\n≤ 4Rseqn (H) + 36 √ 2 Rseqn (H) √ log(n) ∫ 1 α̂ 1 β dβ ≤ 4Rseqn (H) + 36 √ 2 Rseqn (H) log 3 2 (n) ≤ 58 Rseqn (H) log 3 2 (n)\nThis concludes the upper bound. To show that Rseqn (H) ≤ VSn (H), consider the adversary strategy where adversary picks three x of depth n. Now at round 1 the adversary presents as instance x1 = x1 and y1 = 1 where 1 is a Rademacher random variable. Now at round 2 the adversary presents instance x2 = x2( 1) and y2 = 2 where again 2 is a Rademacher random variable. In a similar fashion at round t adversary presents instance xt = xt( 1:t−1) and yt = t. Therefore we see that\nVSn(H) ≥ E\n[ 1\nn n∑ t=1 |ŷt − t|\n] − E [ inf h∈H 1 n n∑ t=1 |h(xt( ))− t| ]\n= 1− E [ inf h∈H 1 n n∑ t=1 |h(xt( ))− t| ]\n= E [ inf h∈H 1 n n∑ t=1 (1− |h(xt( ))− t|) ]\n= E [ inf h∈H 1 n n∑ t=1 th(xt( )) ]\n94\nSince choice of tree x is arbitrary we conclude that VSn(H) ≥ Rseqn (H)\nProof of Lemma 45. First, we claim that for any x ∈ X , fatseqα (Vt(r, x)) = fat seq α (Vt) for at most two r, r ′ ∈ Bα.2 Further if there are two such r, r′ ∈ Bα then r, r′ are consecutive elements of Bα (i.e. |r − r′| = α). Suppose, for the sake of contradiction, that fatseqα (Vt(r, x)) = fat seq α (Vt(r ′, x)) = fatseqα (Vt) for distinct r, r′ ∈ Bα that are not consecutive (i.e. |r−r′| ≥ 2α). Then let s = (r+r′)/2 and without loss of generality suppose r > r′. By definition for any h ∈ Vt(r, x),\nh(x) ≥ r − α/2 = (r′ + r)/2 + (r − r′)/2− α/2 ≥ s+ α/2\nAlso for any h′ ∈ Vt(r′, x) we also have,\nh′(x) ≤ r′ + α/2 = (r′ + r)/2 + (r′ − r)/2 + α/2 ≤ s− α/2\nLet v and v′ be trees of depth fatseqα (Vt) α-shattered by Vt(r, x) and Vt(r ′, x), respectively. To get a contradiction, form a new tree v′′ of depth fatseqα (Vt) + 1 by joining trees v and v ′ with the constant function v′′1 = x as the root and v and v ′ as the lest and right subtrees respectively. It is straightforward that this tree is shattered by Vt(r, x) ∪ Vt(r′, x), a contradiction.\nNotice that the times t ∈ [n] for which |ht(xt)− yt| > α are exactly those times when we update current set Vt+1. We shall show that whenever an update is made, fatseqα (Vt+1) < fat seq α (Vt) and hence claim that the total number of times |ht(xt)− yt| > α is bounded by fatseqα (F).\nAt any round we have three possibilities. First is when fatseqα (Vt(r, xt)) < fat seq α (Vt) for all r ∈ Bα. In this case, clearly, an update results in fatseqα (Vt+1) = fat seq α (Vt(bytcα, xt)) < fat seq α (Vt).\nThe second case is when fatseqα (Vt(r, xt)) = fat seq α (Vt) for exactly one r ∈ Bα. In this case the algorithm chooses ht(xt) = r. If the update is made, |ht(xt) − yt| > α and thus bytcα 6= ht(xt). We can conclude that\nfatseqα (Vt+1) = fat seq α (Vt(bytcα, xt)) < fat seq α (Vt(ht(xt), xt)) = fat seq α (Vt)\nThe final case is when fatseqα (Vt(r, xt)) = fat seq α (Vt(r ′, xt)) = fat seq α (Vt) and |r − r′| = α. In this case, the algorithm chooses ht(xt) = r+r ′\n2 . Whenever yt falls in either of these two consecutive intervals given by r\nor r′, we have |ht(xt) − yt| ≤ α, and hence no update is made. Thus, if an update is made, bytcα 6= r and bytcα 6= r′. However, for any element or Bα other than r, r′, the fat shattering dimension is less than that of Vt. That is\nfatseqα (Vt+1) = fat seq α (Vt(bytcα, xt)) < fat seq α (Vt(r, xt)) = fat seq α (Vt(r ′, xt)) = fat seq α (Vt).\n2The argument should be compared to the combinatorial argument in Theorem 31.\n95\nWe conclude that whenever we update, fatseqα (Vt+1) < fat seq α (Vt), and so we can conclude that algorithm’s prediction is more than α away from yt on at most fatseqα (F) number of rounds.\nProof of Corollary 48. For the choice of weights pi = 6π2 i −2 we see from Proposition 53 that for any i,\nE [Rn] ≤ αi + √√√√ fatseqαi log ( 2nαi ) n + 1√ n (3 + 2 log(i))\nNow for any α > 0 let iα be such that α ≤ 2−iα and for any i < iα, α > 2−iα . Using the above bound on expected regret we have that\nE [Rn] ≤ αiα + √√√√ fatseqαiα log ( 2nαiα ) n + 1√ n (3 + 2 log(iα))\nHowever for our choice of iα we see that iα ≤ log(1/α) and further αiα ≤ α. Hence we conclude that\nE [Rn] ≤ α+\n√ fatseqα log ( 2n α ) n + 1√ n ( 3 + 2 log log ( 1 α )) Since choice of α was arbitrary we take infimum and get the result.\nProof of Proposition 49. Fix a γ > 0 and use loss\n`(ŷ, y) =  1 ŷy ≤ 0 1− ŷy/γ 0 < ŷy < γ 0 ŷy ≥ γ\nFirst note that since the loss is 1/γ-Lipschitz, we can use Theorem 24 and the Rademacher contraction Lemma 27 to show that for each γ > 0 there exists a randomized strategy Aγ such that\nE [ n∑ t=1 Eht∼Aγt (z1:t−1) [`(ht(xt), yt)] ] ≤ inf h∈H n∑ t=1 `(h(xt), yt) + 2 γ Rseqn (H)\nNow note that the loss is lower bounded by the Zero-one loss 1{ŷy<0} and is upper bounded by the margin Zero-one loss 1{ŷy<γ}. Hence we see that for this strategy,\nE [ n∑ t=1 Eht∼Aγt (z1:t−1) [ 1{ht(xt)yt<0} ]] ≤ inf h∈H n∑ t=1 1{h(xt)yt<γ} + 2 γ Rseqn (H) (4.8.7)\nHence for each fixed γ for randomized strategy given by Aγ we have the above bound. Now we discretize over γ’s as γi = 1/2i and using the output of the randomized strategies Aγ1 ,Aγ2 , . . . that attain the regret\n96\nbounds given in Eq. (4.8.7) as experts and running experts algorithm given in Algorithm 3 with initial weight for expert i as pi = 6π2i2 then using Proposition 53 we get that for this randomized strategy A, such that for any i\nE [ n∑ t=1 Eht∼A(z1:t−1) [ 1{ht(xt)yt<0} ]] ≤ inf h∈H n∑ t=1 1{h(xt)yt<γi} + 2 γi Rseqn (H) + √ n ( 1 + 2 log ( iπ√ 6 ))\nNow for any γ > 0 let iγ be such that γ ≤ 2−iγ and for any i < iγ , γ > 2−iγ . Then using the above bound we see that\nE [ n∑ t=1 Eht∼A(z1:t−1) [ 1{ht(xt)yt<0} ]] ≤ inf h∈H n∑ t=1 1{h(xt)yt<2γ} + 2 γ Rseqn (H) + √ n ( 1 + 2 log ( iπ√ 6 ))\nHowever note that iγ ≤ log(1/γ) and so we can conclude that\nE [ n∑ t=1 Eht∼A(z1:t−1) [ 1{ht(xt)yt<0} ]] ≤ inf h∈H n∑ t=1 1{h(xt)yt<2γ}+ 2 γ Rseqn (H)+ √ n ( 1 + 2 log ( π log(1/γ)√ 6 ))\nDividing throughout by n concludes the proof.\nProof of Proposition 50. We shall prove that for any i ∈ [k],\nRseqn (Fi) ≤ 2LBiRseqn (Fi−1)\n97\nTo see this note that\nRseqn (Fi) = 1\nn sup z\nE  sup wi:‖wi‖1≤Bi ∀jfj∈Fi−1 n∑ t=1 t ∑ j wijσ (fj(zt( )))  \n≤ 1 n sup z E  sup wi:‖wi‖1≤Bi ∀jfj∈Fi−1 ‖wi‖1 max j ∣∣∣∣∣ n∑ t=1 tσ (fj(zt( ))) ∣∣∣∣∣  (Hölder’s inequality)\n≤ 1 n sup z E\n[ Bi sup\nf∈Fi−1 ∣∣∣∣∣ n∑ t=1 tσ (f(zt( ))) ∣∣∣∣∣ ]\n= 1\nn sup z\nE [ Bi sup\nf∈Fi−1 max { n∑ t=1 tσ (f(zt( ))) ,− n∑ t=1 tσ (f(zt( ))) }]\n= 1\nn sup z\nE [ Bi max { sup\nf∈Fi−1 n∑ t=1 tσ (f(zt( ))) , sup f∈Fi−1 n∑ t=1 − tσ (f(zt( )))\n}]\n≤ 1 n sup z E\n[ Bi sup\nf∈Fi−1 n∑ t=1 tσ (f(zt( )))\n] + sup\nz E\n[ Bi sup\nf∈Fi−1 n∑ t=1 − tσ (f(zt( )))\n] (σ(0) = 0 and 0 ∈ Fi)\n= 2Bi n sup z E\n[ sup\nf∈Fi−1 n∑ t=1 tσ (f(zt( )))\n] (Proposition 29)\n≤ 2BiL n sup z E\n[ sup\nf∈Fi−1 n∑ t=1 tf(zt( ))\n] (Lemma 27)\n= 2BiLR seq n (Fi−1) (4.8.8)\nTo finish the proof we note that\nRseqn (F1) = sup z E\n[ sup\nw∈Rd:‖w‖1≤B1\n1 n n∑ t=1 tw >zt( )\n]\n≤ sup z E\n[ sup\nw∈Rd:‖w‖1≤B1 ‖w‖1 ∥∥∥∥∥ 1n n∑ t=1 tzt( ) ∥∥∥∥∥ ∞ ]\n≤ B1 sup z E [ max i∈[d] { 1 n n∑ t=1 tzt( )[i] }]\nNote that the instances x ∈ Z are vectors in Rd and so for a given instance tree z, for any i ∈ [d], z[i] given by only taking the ith co-ordinate is a valid real valued tree. Hence using Lemma 34 we conclude that\nRseqn (F1) ≤ B1 sup z E [ max i∈[d] { 1 n n∑ t=1 tzt( )[i] }]\n≤ B1\n√ 2X2∞ log d\nn\n98\nUsing the above and Equation 4.8.8 we conclude the proof.\nProof of Proposition 51. For a tree of depth d, the indicator function of a leaf is a conjunction of no more than d decision functions. More specifically, if the decision tree consists of decision nodes chosen from a class C of binary-valued functions, the indicator function of leaf l (which takes value 1 at a point x if x reaches l, and 0 otherwise) is a conjunction of dl functions from C, where dl is the depth of leaf l. We can represent the function computed by the tree as the sign of\ng(x) = ∑ l wlσl dl∧ i=1 cl,i(x)\nwhere the sum is over all leaves l, wl > 0, ∑ l wl = 1, σl ∈ {±1} is the label of leaf l, cl,i ∈ C, and the conjunction is understood to map to {0, 1}. Now note that if we fix some L > 0 then we see that the loss\nφL(α) =  1 if α ≤ 0 1− Lα if 0 < α ≤ 1/L 0 otherwise\nis L-Lipschitz and so by Theorem 24 and Lemma 27 we have that for every L > 0, there exists a randomized strategy AL for the player, such that for any sequence z1 = (x1, y1), . . . , zn = (xn, yn),\nE [ n∑ t=1 Eτt∼AL(z1:t−1) [φL(ytτt(xt)] ] ≤ inf τ∈T n∑ t=1 φL(ytτ(xt)) + LR seq n (T )\nNow note that φL upper bounds the step function and so\nE [ n∑ t=1 Eτt∼AL(z1:t−1) [ 1{τt(xt)6=yt} ]] ≤ inf τ∈T n∑ t=1 φL(ytτ(xt)) + LR seq n (T )\nNow say τ∗ ∈ T is the minimizer of ∑n t=1 1{τ(xt) 6=yt} then note that\nn∑ t=1 φL(ytτ ∗(xt)) = n∑ t=1 1{τ(xt) 6=yt} + ∑ l C̃n(l)φL(wl)\n≤ n∑ t=1 1{τ∗(xt) 6=yt} + ∑ l C̃n(l) max(0, 1− Lwl)\n≤ n∑ t=1 1{τ∗(xt) 6=yt} + ∑ l max ( 0, (1− Lwl)C̃n(l) )\n= inf τ∈T n∑ t=1 1{τ(xt) 6=yt} + ∑ l max ( 0, (1− Lwl)C̃n(l) )\n99\nHence we see that\nE [ n∑ t=1 Eτt∼AL(z1:t−1) [ 1{τt(xt)6=yt} ]] ≤ inf τ∈T n∑ t=1 1{τ(xt)6=yt} + ∑ l max ( 0, (1− Lwl)C̃n(l) )\nNow if we discretize over L as Li = i for all i ∈ N and run experts algorithm 3 with output of randomized strategies, AL1 ,AL2 , . . . as our experts and weight of expert i with pi = 6 π2 i −2 so that ∑ i pi = 1 then we get that for this randomized strategy A, we have from Proposition 53 that for all L ∈ N,\nE [ n∑ t=1 Eτt∼A(z1:t−1) [ 1{τt(xt) 6=yt} ]]\n≤ inf τ∈T n∑ t=1 1{τ(xt)6=yt} + ∑ l max ( 0, (1− Lwl)C̃n(l) ) + LRseqn (T ) + √ n+ 2 √ n log(Lπ/ √ 6)\nNow we pick L = |{l : C̃n(l) > 2Rseqn (T )}| =: Nleaf and also pick wl = 0 if C̃n(l) ≤ 2Rseqn (T ) and wl = 1/L otherwise. Hence we see that\nE [ n∑ t=1 Eτt∼A(z1:t−1) [ 1{τt(xt)6=yt} ]] ≤ inf τ∈T n∑ t=1 1{τ(xt) 6=yt} + ∑ l C̃n(l)1{C̃n(l)≤2Rseqn (T )}\n+ 2Rseqn (T ) ∑ l 1{C̃n(l)>2Rseqn (T )} + √ n+ 2 √ n log(Nleafπ/ √ 6)\n= inf τ∈T n∑ t=1 1{τ(xt) 6=yt} + ∑ l min(C̃n(l), 2R seq n (T )) + √ n ( 1 + 2 log(Nleafπ/ √ 6) )\nNow finally we can apply Corollary 28 to bound Rseqn (T ) ≤ dO(log 3/2 n) Rseqn (H) and thus conclude the proof by plugging this into the above."
    }, {
      "heading" : "4.8.2 Exponentially Weighted Average (EWA) Algorithm on Countable Experts",
      "text" : "We consider here a version of the exponentially weighted experts algorithm for countable (possibly infinite) number of experts and provide a bound on the expected regret of the randomized algorithm. The proof of the result closely follows the finite case (e.g. [52, Theorem 2.2]).\nSay we are provided with countable experts E1, E2, . . . where each expert can herself be thought of as a randomized/deterministic player strategy which, given history, produces an element of F at round t. Here we also assume that F ⊂ [0, 1]Z contains only non-negative functions (corresponds to loss class). Denote by f it the function output by expert i at round t given the history. The EWA algorithm we consider needs access to the countable set of experts and also needs an initial weighting on each expert p1, p2, . . . such that ∑ i pi = 1.\n100\nAlgorithm 3 EWA (E1, E2, . . ., p1, p2, . . .) Initialize each w1i ← pi for t = 1 to n do\nPick randomly an expert i with probability wti Play ft = f ti Receive zt Update for each i, wt+1i = wtie −ηfti (zt)∑\ni w t ie −ηft i (zt)\nend for\nProposition 53. For the exponentially weighted average forecaster (Algorithm 3) with η = n−1/2 yields\nE [ n∑ t=1 ft(zt) ] ≤ n∑ t=1 f ti (zt) + √ n 8 + √ n log (1/pi)\nfor any i ∈ N. Proof. Define Wt = ∑ i pie −η ∑t j=1 f j i (zt). Then note that\nlog ( Wt Wt−1 ) = log (∑ i pie −η ∑t j=1 f j i (zt) Wt−1 ) = log (∑ i wt−1i e −ηfti (zt) )\nNow using Hoeffding’s inequality (see [52, Lemma 2.2]) we have that\nlog ( Wt Wt−1 ) ≤ −η ∑ i wt−1i f t i (zt) + η2 8 = −ηE [ft(zt)] + η2 8\nSumming over t we get\nlog(Wn)− log(W0) = n∑ t=1 log ( Wt Wt−1 ) ≤ −ηE [ n∑ t=1 ft(zt) ] + nη2 8 (4.8.9)\nNote that W0 = ∑ i pi = 1 and so log(W0) = 0. Also note that for any i ∈ N,\nlog(Wn) = log (∑ i pie −η ∑n t=1 f t i (zt) ) ≥ log ( p −η ∑n t=1 f t i (zt) i ) = log(pi)− η n∑ t=1 f ti (zt)\nHence using this with Equation 4.8.9 we see that\nlog(pi)− η n∑ t=1 f ti (zt) ≤ −ηE [ n∑ t=1 ft(zt) ] + nη2 8\n101\nRearranging we get\nE [ n∑ t=1 ft(zt) ] ≤ n∑ t=1 f ti (zt) + ηn 8 + 1 η log ( 1 pi )\nUsing η = 1√ n we get the desired bound.\nProof of Proposition 52. First, by the classical result of Kolmogorov and Tihomirov [58], the class G of all bounded Lipschitz functions has small metric entropy: log N̂∞(α,G) = Θ(1/α). For the particular class of non-decreasing 1-Lipschitz functions, it is trivial to verify that the entropy is in fact bounded by 2/α. Next, consider the class F = {〈w, x〉 | ‖w‖2 ≤ 1} over the Euclidean ball. By Proposition ??, Rseqn (F) ≤√ 2 n . Using the lower bound of Proposition 42, fat seq α ≤ 64/α2 whenever α > 8/ √ n. This implies that Nseq∞ (α,F , n) ≤ (2en/α)64/α 2 whenever α > 8/ √ n. Note that this bound does not depend on the ambient dimension of Z .\nNext, we show that a composition of G with any small class F ⊂ [−1, 1]Z also has a small cover. To this end, suppose Nseq∞ (α,F , n) is the covering number for F . Fix a particular tree z and let V = {v1, . . . ,vN} be an `∞ cover of F on z at scale α. Analogously, let W = {g1, . . . , gM} be an `∞ cover of G with M = N̂∞(α,G). Consider the class G ◦ F = {g ◦ f : g ∈ G, f ∈ F}. The claim is that {g(v) : v ∈ V, g ∈ W} provides an `∞ cover for G ◦ F on z. Fix any f ∈ F , g ∈ G and ∈ {±1}n. Let v ∈ V be such that maxt∈[n] |f(zt( )) − vt( )| ≤ α, and let g′ ∈ W be such that ‖g − g′‖∞ ≤ α. Then, using the fact that functions in G are 1-Lipschitz, for any t ∈ [n],\n|g(f(zt( )))− g′(vt( ))| ≤ |g(f(zt( )))− g′(f(zt( ))|+ |g′(f(zt( ))− g′(vt( ))| ≤ 2α .\nHence, Nseq∞ (2α,G ◦ F , n) ≤ N̂∞(α,G)×Nseq∞ (α,F , n).\nFinally, we put all the pieces together. By Lemma 27, the Sequential Rademacher complexity ofH is bounded by 4 times the Sequential Rademacher complexity of the class\nG ◦ F = {u(〈w, x〉) | u : [−1, 1] 7→ [−1, 1] is 1-Lipschitz , ‖w‖2 ≤ 1}\nsince the squared loss is 4-Lipschitz on the space of possible values. The latter complexity is then bounded\nby\nDseqn (G ◦ F) ≤ 32√ n + 12 ∫ 1 8/ √ n √ log Nseq(δ,G ◦ F , n) n dδ ≤ 32√ n + 12 ∫ 1 8/ √ n √ 2 nδ + 64 nδ2 log(2en)dδ . We conclude that the value of the game Vn(H,Z × Y) = O( √ log3 n n ).\n102"
    }, {
      "heading" : "4.9 Discussion",
      "text" : "While in this chapter we introduced tools for analyzing rates for online learning problems analogous to the various complexity measures for statistical learning framework, as we saw in the previous chapter, for statistical learning framework these tools fail to characterize learnability in general. Similar situation is true for the online setting too. While these tools characterize learnability of online supervised learning problems and can also be used to obtain rates for online convex learning problems indirectly, in general they could fail to characterize learnability general of online learning problems.\nIn the previous chapter we then turned to the notion of online stability to characterize learnability and even provided a generic learning algorithm. Is there some notion of stability that can be used to characterize learnability in the online learning framework? Can we provide a generic algorithm for general learning problems in the online framework?\nAnother interesting avenue to explore is the question of fast rates for online learning problems. In the statistical learning framework the notion of Localized Rademacher complexity introduced in [59] can often be used to obtain fast rates. Just like we provided an analog to Rademacher complexity for online learning, can we provide an analog of localized complexity measures, specifically a local sequential Rademacher complexity that can then be used to obtain fast rates for online learnign problems?\n103\nPart II\nConvex Problems : Oracle Efficient Learning/Optimization\n104\nChapter 5\nConvex Learning and Optimization Problem Setup\nIn the first part of this dissertation we mainly focused on the question of learnability (and learning rates) in both statistical and online settings, that is whether the problems were at all learnable using some algorithm. We did not take into consideration tractability of the learning rules we considered and whether the problem is learnable using some efficient learning algorithm. The generic learning rules/algorithms presented in the first part are not at all tractable. In this part of the dissertation, we try to address the issue of tractability of learning algorithms in both statistical and online learning settings. To do this, we restrict ourselves to so called convex learning or optimization problems. In this chapter we introduce the convex learning problems we will encounter in the second part of this dissertation and associated notations."
    }, {
      "heading" : "5.1 Convex Problems",
      "text" : "Let us now give the basic setup for the convex learning and optimization problems we consider in the second part of this dissertation. Of course when we say convex problem, we mean that the set of target hypothesisH is a convex set and for each given instance z ∈ Z , the loss function `(h, z) is convex in h. To describe more formal, we consider an arbitrary real vector space B and denote its dual by B?. Now the target hypothesis class H ⊂ B we shall consider throughout will be a convex and centrally symmetric subset of B. Also consider the set X ⊂ B? to be a bounded, convex and centrally symmetric subset of the dual B?. The role of set X will become clear in the following paragraph. It will be convenient for us, to relate the notion of a convex centrally symmetric sets to their corresponding (semi)norms. To do this, recall the definition of the Minkowski functional of a set K of a real vector space B. It is defined as\n‖v‖K := inf {α > 0 : v ∈ αK}\n105\nNow it can be seen that if K is convex and centrally symmetric (i.e. K = −K), then ‖·‖K is a semi-norm. Further, for instance in Rd, if the set K is bounded then ‖·‖K is in fact a norm. Our assumption on the sets H and X ensure that ‖·‖H and ‖·‖X (the Minkowski functionals of the sets H and X ) are semi-norms. For simplicity we shall further assume that ‖·‖H and ‖·‖X are in fact norms. Even though we do this for simplicity, we remark that all our results go through for semi-norms too. We use X ? andH? to represent the duals of X andH respectively, i.e. the unit balls of the dual norms ‖·‖∗X and ‖·‖ ∗ H.\nAs mentioned we consider convex learning and optimization problem where target set H is the unit ball of norm ‖·‖H and for each instance z ∈ Z the loss `(·, z) is convex. Now the convex problems we consider are of three flavors. The first case we consider is the one where, for each instance z ∈ Z , the sub-gradients of the convex function `(·, z) belong to the set X . Notice that when X = H? this case exactly corresponds to convex 1-lipschitz problems. Most prior work on online learning, statistical learning and convex optimization problems considers this case whenH is the unit ball of some Banach space, and X is the unit ball of the dual space. However, we analyze the general problem where X ∈ B? is not necessarily the dual ball of H . The second flavor of problems we consider are the ones where for each z ∈ Z , the function `(·, z) is in fact uniformly convex w.r.t. norm ‖·‖X? . The final flavor of problems we consider are problems where for each z ∈ Z , the function `(·, z) are non-negative, convex and smooth w.r.t. norm ‖·‖X . The next section formally defines the key set of convex function classes we consider for the convex learning and optimization problems throughout the second part."
    }, {
      "heading" : "5.2 Various Convex Learning/Optimization Problems",
      "text" : "Below we provide examples of various convex learning/optimization problems we consider in this work.\nExample 6 (Lipschitz Convex functions). The set Z = ZLip(X ) correspond to the set of all convex loss functions such that for any z ∈ Z and h ∈ H, ∇h`(h, z) ∈ X where X is some set in the dual vector space. In short\nZLip(X ) = {z : `(·, z) is convex and ∀h ∈ H,∇`(h, z) ∈ X}\nExample 7 (Linear functions). The set Z = Zlin consists of linear functions on H̄ from the set X .\nZlin(X ) = {z : `(·, z) = 〈x, ·〉 where x ∈ X}\nExample 8 (Supervised Learning with Linear predictors). The set Z = Zsup(X ) consists of functions of form\nZsup(X ) = {z : `(h, z) = | 〈h,x〉 − y| where x ∈ X , y ∈ [−b, b]}\nIn the first part we introduced generic supervised learning problem with absolute loss and arbitrary function class for prediction that mapped input x to reals. At first glance the above class might seem specific given that the predictor is always linear. However at second glance, if we consider supervised learning with absolute loss and we require loss function for every instance to be convex in h, then necessarily the predictor has to\n106\nbe linear. To see this note that we need |h(x) − y| to be convex in h, when both y = 1 and y = −1 which basically means predictor has to be both convex and concave in h and so is linear. The same argument can be extended to other common margin losses like squared loss, logistic loss etc. Given a convex loss function φ one can also more generally define a class Zφ = {z : `(h, z) = φ(〈x,h〉 , y) : x ∈ X , y ∈ [−b, b], } for any 1-Lipschitz loss function φ : R × R 7→ R, and this class would also be a subset of ZLip. In fact, this setting includes supervised learning fairly generally, including problems such as multitask learning and matrix completion, where in all cases X specifies the data domain1.\nExample 9 (Non-Negative Smooth Convex Loss). The set Z corresponds to non-negative convex functions that are smooth w.r.t. to the norm ‖·‖X? , that is\nZsmt(H)(X ) = { z : `(·, z) ≥ 0 is convex and ∀h,h′ ∈ H̄, ‖∇`(h, z)−∇`(h′, z)‖X ≤ H ‖h− h ′‖X? }\nA subset of the above instance class are cases of supervised learning problems Zφ where φ is non-negative and smooth function on the reals like logistic loss, smoothed hinge loss and squared loss. In the chapters to come we show how for non-negative smooth convex instances one can get faster learning rates in both online and statistical convex learning frameworks when optimal loss itself is small.\nExample 10 (Regularized Convex Loss). The set Z consists of functions of form\nZreg(X ) = {z : `(·, z) = φ(·, z) +R(h) where φ(·, z) is convex, ∀h ∈ H,∇φ(h, z) ∈ X and R is convex}\nThe class Zreg(X ) captures regularized convex objective classes where R : H̄ 7→ R is a convex regularizer used to enforce structure or prior into the learning problem. Commonly regularizer chosen are strongly convex or more generally uniformly convex. The following instance class captures more specifically these classes.\nExample 11 (Uniformly Convex Loss). The set Z corresponds to uniformly convex functions,\nZucvx(σ,q)(X ) = {z : `(·, z) = φ(·, z) +R(·) is (σ, q)-uniformly convex w.r.t. ‖·‖X? ,∀z ∈ Z ∇φ(·, z) ∈ X}\nIn the above example, we assume that ` is uniformly convex but however we assumes that ` can be decomposed as `(·, z) = φ(·, z)+R(·) and only assumes that∇φ(·, z) ∈ X . The reason we did not directly assume in the above that ∇` is itself in X is so that we can capture many regularized learning problems where the regularizer R is not Lipschitz and so ` is not Lipschitz but however the loss function of interest φ in these examples are Lipschitz.\nAnother class of important loss functions are non-negative smooth convex loss functions. Linear predictors with logistic loss ,squared loss are common examples of such problems. The following example captures such classes.\n1Note that any convex supervised learning problem can necessarily be viewed as linear classification with some convex constraintH on the predictors.\n107\nExample 12 (Bounded Convex functions). The set Z = Zbnd corresponds ot the set of all convex loss functions that are that are bounded by b onH, that is\nZbnd = {z : `(·, z) is convex and bounded by b onH}\nRemark 54. It must be noted that for convex optimization problem, both in the online and statistical setting, we can use Jensen’s inequality to show that for every randomized algorithm there exists a deterministic\nalgorithm (that play the expected action of the randomized algorithm) that achieves learning rates that are\nat most as bad as that of the randomized algorithm. Hence for the convex optimization problem it suffices to\nonly consider deterministic learning algorithms.\nOwing to the above remark, we see that since we only need to consider deterministic learning algorithms, both in statistical and online convex learning settings, any learning algorithm A is specified by a mapping A : ⋃ n∈NZn−1 7→ H̄. Also note that while we assumed that the setH was convex and centrally symmetric and for ease even assumed thatH is the unit ball of norm ‖·‖H, no such assumptions are made on hypothesis set H̄ other than that it containsH and so it could even be all of B."
    }, {
      "heading" : "5.3 Discussion",
      "text" : "Notice that the sets H and X that we consider are arbitrary convex centrally symmetric sets and need not be related to each other a priori. While the special case of when H = X ? is what is usually encountered in majority of the theoretical analysis existing literature, in many applications H and X are not dual to each other. Here we provide a generic theoretical analysis of the non-dual case. Note that when H = X ?, ZLip(X ) corresponds to usual convex Lipschitz problem, Zsmt(X ) corresponds to usual smooth convex class and finally Zucvx(σ,2)(X ) corresponds to σ-strongly convex functions. Overall, the convex learning and optimization problems that we introduced in this chapter and will study in the remaining chapters cover majority of the problems considered in previous works. Perhaps the one case not covered in this thesis is the case of exp-concave loss functions for which in finite (low) dimensional cases, one can get faster learning rates in both online and statistical learning cases.\n108\nChapter 6\nMirror Descent Methods\nPerhaps one of the most popular convex optimization algorithm most readers would be familiar with is the gradient descent algorithm. The gradient descent algorithm proceeds by starting with an initial point and iteratively updating it by taking steps in the direction of the negative gradient of the function to optimize at the current point. The gradient descent method is a natural algorithm for problems in Euclidean space. The mirror descent algorithm [1] is a natural generalization of gradient descent method for general convex learning problems. Section 6.1 describes the basic update step of the mirror descent algorithm. Section ?? provides bounds on regret of mirror descent method for generic online convex learning problems, online smooth convex learning problems and online uniformly convex learning problems. The section 6.3 which follows shows how mirror descent algorithm can be used for statistical convex learning problem and associated learning guarantee. Section 6.4 shows how mirror descent can also be used for offline convex optimization problems. Following that Section 6.5 provides proofs of all the results of this chapter and finally we conclude this chapter with some discussion in Section 6.6."
    }, {
      "heading" : "6.1 The Mirror Descent Update",
      "text" : "Given a strictly convex function Ψ : B 7→ R, the Mirror Descent algorithm, AMD is given by the update\nht+1 = argmin h∈H̄\n∆Ψ (h|ht) + η 〈∇`(ht, zt),h− ht〉 (6.1.1)\nor equivalently h′t+1 = ∇Ψ∗ (∇Ψ(ht)− η∇`(ht, zt)) , ht+1 = argmin h∈H̄\n∆Ψ ( h ∣∣h′t+1) (6.1.2)\nwhere ∆Ψ (h|h′) := Ψ(h) − Ψ(h′) − 〈∇Ψ(h′),h− h′〉 is the Bregman divergence and Ψ∗ is the convex conjugate of Ψ. As an example notice that when Ψ(h) = 12 ‖h‖ 2 2 then we get back the online gradient descent algorithm. It is worth noting that the perceptron algorithm can be viewed as a conservative variant of online gradient descent with hinge loss function. Also when H is the d dimensional simplex and Ψ(h) =\n109\n∑d i=1 hi log(1/hi), then we get the multiplicative weights update algorithm. In general the function Ψ used in mirror descent is often referred to as the proxy-function.\nThe mirror descent algorithm is an O(1), memory single pass, first order method that only needs some sub-gradient for each update. Often times in practice, each mirror descent update step has time complexity same as that of calculating a single gradient and hence overall runtime is linear in number of rounds (for online learning case) or number of samples (statistical learning case).This makes the mirror descent algorithm attractive from a computational viewpoint.\nBefore we proceed we would like to point out that in the setting we consider, the hypothesis set from which learner is allowed to pick, H̄ ⊂ B, need not be the same as the target hypothesis set H and only needs to be a superset. Of course when H̄ = H then the update above corresponds to the usual mirror descent update. However when H̄ is all of B then notice that the projection step is mute and essentially the update becomes,\nht+1 = ∇Ψ∗ (∇Ψ(ht)− η∇`(ht, zt)) .\nThis is especially attractive because in many machine learning applications while we would like to do as well as the best hypothesis from some target class, we don’t really care if the hypothesis learner picks itself is selected from this target hypothesis set as long as it gives good results. Hence if we set up the problem such that H̄ = B then we avoid extra computational time on projection step which could at times be expensive."
    }, {
      "heading" : "6.2 Online Mirror Descent",
      "text" : "In this section we describe the online mirror descent algorithm AMD : ⋃ n∈NZn−1 7→ H̄ which simply uses the mirror descent update given in the previous section and returns the ht’s in each round. That is, AMD({}) = h1 and further for any t ∈ N and any z1, . . . , zt−1 ∈ Z ,\nAMD(z1, . . . , zt−1) = ht .\nA key tool in the analysis mirror descent is the notion of strong convexity or more generally uniform convexity of the function Ψ. Recall the definition of uniform convexity :\nDefinition 31. A function Ψ : B → R is said to be q-uniformly convex w.r.t. ‖ · ‖ if for any h,h′ ∈ B:\n∀α∈[0,1] Ψ (αh + (1− α)h′) ≤ αΨ(h) + (1− α)Ψ(h′)− α(1−α)q ‖h− h ′‖q\n110\nWe are interested in bounding the regret of the of the mirror descent algorithm given by,\nRn(AMD, z1, . . . , zn) := 1\nn n∑ t=1 `(AMD(z1:t−1), zt)− inf h∈H 1 n n∑ t=1 `(h, zt)\n= 1\nn n∑ t=1 `(ht, zt)− inf h∈H 1 n n∑ t=1 `(h, zt) .\nNow assuming we can find an appropriate q-uniformly convex function on B, below we provide bounds on regret of mirror descent method for generic online convex learning problems, non-negative smooth convex learning problems and uniformly convex learning problems.\nConvex Losses with Sub-gradients in X : We first start with the case when the convex costs at each round are such that their sub-gradients lie in the set X . Note that instance sets ZLip(X ), Zsupp(X ) and Zlin(X ) are examples of instance classes that fall in this set.\nLemma 55. Let Ψ : B 7→ R be non-negative and q-uniformly convex w.r.t. norm ‖·‖X∗ . For the Mirror Descent algorithm with this Ψ, using h1 = argmin h∈H Ψ(h) and η = ( suph∈H Ψ(h) nB )1/p we can guarantee that\nfor any z1, . . . , zn s.t. 1n ∑n t=1 ‖∇`(·, zt)‖ p X ≤ 1 (where p = q q−1 ),\nR(AMD, z1, . . . , zn) ≤ 2 ( suph∈HΨ(h)\nn\n) 1 q\n.\nNote that in our case we have that for each z ∈ z, ∇`(·, z) ∈ X , i.e. ‖∇`(·, z)‖X ≤ 1, and so certainly 1 n ∑n t=1 ‖∇`(·, zt‖ p X ≤ 1.\nNon-Negative Smooth Convex Losses : Next we deal with the case when the convex costs in each round need not be such that their sub gradients are from set X but rather are such that the costs are non-negative and H-smooth w.r.t. to the norm ‖·‖X . That is :\n∀z ∈ Z,∀h,h′ ∈ H̄, ‖∇`(h, z)−∇`(h′, z)‖X ≤ H ‖h− h ′‖X? .\nFor non-negative smooth losses, one has a property called self-bounding property that for any h and any z,\n‖∇`(h, z)‖X ≤ √ 4H`(h, z)\nIn [60] we had made the observation that any non-negative smooth convex loss satisfies this above self bounding property. Shalev-Shwartz [61] showed that the self bounding property can be used this to provide optimistic rates on regret of mirror gradient descent for the dual case and using strongly convex function. By optimistic rates we refer to rates that improve when average loss of best hypothesis is small. The following lemma provides regret bounds for mirror descent algorithm with optimistic rates using similar lines of\n111\nreasoning as in[60, 61].\nLemma 56. Let Ψ : B 7→ R be non-negative and q-uniformly convex w.r.t. norm ‖·‖X∗ . For any L∗ ≥ 0, using the Mirror Descent algorithm with function Ψ for making updates, using h1 = argmin\nh∈H Ψ(h) and\nη =  ( p suph∈H Ψ(h) n )1/p 1√ 4HL∗ if L∗ ≥ 16H p2/p ( suph∈H Ψ(h) n )2/q ( p 2 ) p 2 1 4H ( suph∈H Ψ(h) n ) 2−p p otherwise\nwe can guarantee that for any sequence z1, . . . , zn ∈ Z and any h? ∈ H such that 1n ∑n t=1 `(h ?, zt) ≤ L∗,\nR(AMD, z1, . . . , zn) ≤ √ 64HL∗ ( suph∈HΨ(h)\nn\n)1/q + 40H ( suph∈HΨ(h)\nn\n)2/q .\nUniformly Convex Losses : We now consider the case when loss functions are uniformly convex, specifically (σ, q′)-uniformly convex. Up to now in this chapter we considered H̄ to be any superset ofH including H̄ = H. For the case of uniformly convex loss instance classZucvx(σ,q)(X ) alone we will assume that H̄ = B so that we don’t need a projection step and h′t = ht in the mirror descent update. Under this setting we have the following upper bound for regret of mirror descent algorithm.\nLemma 57. Let Ψ : B 7→ R be non-negative and q-uniformly convex w.r.t. norm ‖·‖X∗ . Let ψ : B 7→ R be non-negative and q′ uniformly convex w.r.t. norm ‖·‖X∗ . For each t ∈ [n], define\nΨ̃t(·) = 1\nη Ψ(·) +R(·) + σ t ψ(·)\nThen for the choice of\nη =  ( suph∈H Ψ(h) n )1/p if n ≥ ( (2− p′)σp′−1 suph∈HΨ1/q(h) ) 1 2−p′−1/p\n∞ otherwise\nwe can guarantee that for any sequence z1, . . . , zn ∈ Zucvx(σ,q′), if q′ > 2 :\nR(AMD, z1, . . . , zn) ≤ min\n{ 2 (suph∈HΨ(h ?)) 1/q\nn1/q ,\n2\n(2− p′)σp′−1np′−1\n} + suph∈HR(h)\nn .\nwhere p′ = q ′\nq′−1 and for q ′ = 2 : R(AMD, z1, . . . , zn) ≤ 2 lognσn + suph∈H R(h) n .\nChoice of Ψ : Construction I The Mirror Descent bound suggests that as long as we can find an appropriate function Ψ that is uniformly convex w.r.t. ‖·‖∗X we can get a diminishing regret guarantee using Mirror\n112\nDescent. This suggests constructing the following function:\nΨ̃q := argmin ψ:ψ is q-uniformly convex\nw.r.t. ‖·‖X∗ onH and ψ≥0\nsup h∈H Ψ(h) . (6.2.1)\nIf no q-uniformly convex function exists then Ψ̃q = ∞ is assumed by default. The above function is in a sense the best choice for the Mirror Descent bound in Eq. (55). The question then is: when can we find such appropriate functions and what is the best rate we can guarantee using Mirror Descent?"
    }, {
      "heading" : "6.3 Stochastic Mirror Descent",
      "text" : "In the previous section we saw that mirror descent can be used successfully for online convex learning. In general, especially for convex problems, any online method can be converted into an algorithm for statistical learning with same learning rate guarantee and this is often referred to as online to batch conversion. Refer to [62] for more details about online to batch conversion. Hence one can also use the mirror descent method for statistical convex learning problems. This algorithm is often referred to as stochastic mirror descent algorithm. The stochastic mirror descent algorithm is given as follows :\nAMD({}) = h1, ∀t ∈ [n], AMD(z1, . . . , zt) = 1\nt t∑ i=1 hi .\nProposition 58. For any hypothesis set H̄ ⊂ B, target hypothesis classH ⊂ B and convex learning problem specified by instance space Z and any fixed distribution D on instance space Z :\nES∼Dn [ LD ( AMD(z1:n) ) − inf\nh∈H LD(h)\n] ≤ ES∼Dn [Rn(AMD, z1, . . . , zn)]\nProof. Note that\nES∼Dn [Rn(AMD, z1, . . . , zn)] = ES∼Dn [ 1\nn n∑ t=1 `(AMD(z1:t−1), zt)− inf h∈H 1 n n∑ t=1 `(h, zt)\n]\n= 1\nn n∑ t=1 ES∼Dn [`(AMD(z1:t−1), zt)]− inf h∈H LD(h)\n= 1\nn n∑ t=1 ES∼Dn [Ezt∼D [`(AMD(z1:t−1), zt)]]− inf h∈H LD(h)\n= 1\nn n∑ t=1 ES∼Dn [LD(AMD(z1:t−1))]− inf h∈H LD(h)\n= ES∼Dn [ 1\nn n∑ t=1 LD(AMD(z1:t−1))− inf h∈H LD(h)\n]\n113\n≥ ES∼Dn [ LD ( 1\nn n∑ t=1 AMD(z1:t−1)\n) − inf\nh∈H LD(h)\n]\n= ES∼Dn [ LD ( AMD(z1:n) ) − inf\nh∈H LD(h) ] where the inequality step above is due to Jensen’s inequality. Thus we get the statement of the proposition.\nOwing to the above proposition we see that one can get the same learning guarantees for statistical convex learning problems as for the online counterpart. Specifically we get the following lemmas for statistical convex learning problems.\nLemma 59. Let Ψ : B 7→ R be non-negative and q-uniformly convex w.r.t. norm ‖·‖X∗ . For the Mirror Descent algorithm with this Ψ, using h1 = argmin h∈H Ψ(h) and η = ( suph∈H Ψ(h) nB )1/p we can guarantee that for any distribution D over Z s.t. Ez∼D [‖∇`(·, z)‖pX ] ≤ 1 we have that :\nR(AMD, z1, . . . , zn) ≤ 2 ( suph∈HΨ(h)\nn\n) 1 q\n. (wherep = qq−1 )\nProof. The statement follows by using Proposition 58 along with line of proof in Lemma 55.\nSimilar bound can be given for stochastic convex learning of non-negative smooth convex losses as follows :\nLemma 60. Let Ψ : B 7→ R be non-negative and q-uniformly convex w.r.t. norm ‖·‖X∗ . For any L∗ ≥ 0, using the stochastic Mirror Descent algorithm with function Ψ for making updates, using h1 = argmin\nh∈H Ψ(h)\nand\nη =  ( p suph∈H Ψ(h) n )1/p 1√ 4HL∗ if L∗ ≥ 16H p2/p ( suph∈H Ψ(h) n )2/q ( p 2 ) p 2 1 4H ( suph∈H Ψ(h) n ) 2−p p otherwise\nwe can guarantee that for any distribution D over Z such that infh∈H L(h) ≤ L∗,\nL(AMD)− inf h∈H\nL(h) ≤ √ 64HL∗ ( suph∈HΨ(h)\nn\n)1/q + 40H ( suph∈HΨ(h)\nn\n)2/q .\nProof. The statement follows by using Proposition 58 along with line of proof in Lemma 56.\nFinally bounds for stochastic learning of uniformly convex losses can also be given.\n114\nLemma 61. Let Ψ : B 7→ R be non-negative and q-uniformly convex w.r.t. norm ‖·‖X∗ . Let ψ : B 7→ R be non-negative and q′ uniformly convex w.r.t. norm ‖·‖X∗ . For each t ∈ [n], define\nΨ̃t(·) = 1\nη Ψ(·) +R(·) + σ t ψ(·)\nThen for the choice of\nη =  ( suph∈H Ψ(h) n )1/p if n ≥ ( (2− p′)σp′−1 suph∈HΨ1/q(h) ) 1 2−p′−1/p\n∞ otherwise\nwe can guarantee that for any distribution D over instance space Zucvx(σ,q′)(X ), if q′ > 2 :\nL(AMD)− inf h∈H L(h) ≤ min\n{ 2 (suph∈HΨ(h ?)) 1/q\nn1/q ,\n2\n(2− p′)σp′−1np′−1\n} + suph∈HR(h)\nn .\nwhere p′ = q ′\nq′−1 and for q ′ = 2 : L(AMD)− infh∈H L(h) ≤ 2 lognσn + suph∈H R(h) n .\nProof. The statement follows by using Proposition 58 along with line of proof in Lemma 57."
    }, {
      "heading" : "6.4 Mirror Descent for Offline Optimization",
      "text" : "Notice that one can think of offline convex optimization as a special case of stochastic convex optimization where the set of distributions we consider are point distributions on a single instance of the instance space Z . Using this observation we see that AMD can directly be used for offline convex optimization problems with same guarantee on sub-optimality. In this thesis, for the problem of offline convex optimization we only consider the instance class ZLip(X ). The following corollary is a direct consequence of Lemma 59.\nCorollary 62. Let Ψ : B 7→ R be non-negative and q-uniformly convex w.r.t. norm ‖·‖X∗ . For the Stochastic Mirror Descent algorithm with this Ψ, using h1 = argmin h∈H Ψ(h) and η = ( suph∈H Ψ(h) nB )1/p we can guarantee that for any instance z ∈ ZLip, we have that :\n`(AMD(∇`(h1, z), . . . ,∇`(hn, z)), z)− inf h∈H\n`(h, z) ≤ 2 ( suph∈HΨ(h)\nn\n) 1 q\n. (wherep = qq−1 )\n115"
    }, {
      "heading" : "6.5 Detailed Proofs",
      "text" : "Proof of Lemma 55 (generalized MD guarantee). Note that for any h? ∈ H,\nη ( n∑ t=1 `(ht, zt)− n∑ t=1 `(h?, zt) ) ≤ n∑ t=1 〈η∇`(ht, zt),ht − h?〉\n= n∑ t=1 (〈 η∇`(ht, zt),ht − h′t+1 〉 + 〈 ∇`(ht, zt),h′t+1 − h? 〉) =\nn∑ t=1 (〈 η∇`(ht, zt),ht − h′t+1 〉 + 〈 ∇Ψ(ht)−∇Ψ(h′t+1),h′t+1 − h? 〉) ≤\nn∑ t=1 ( ‖η∇`(ht, zt)‖X ∥∥ht − h′t+1∥∥X? + 〈∇Ψ(ht)−∇Ψ(h′t+1),h′t+1 − h?〉) ≤\nn∑ t=1 ( ηp p ‖∇`(ht, zt)‖pX + 1 q ∥∥ht − h′t+1∥∥qX? + 〈∇Ψ(ht)−∇Ψ(h′t+1),h′t+1 − h?〉)\nUsing simple manipulation we can show that\n〈∇Ψ(ht)−∇Ψ(ht+1),ht+1 − h?〉 = ∆Ψ (h?|ht)−∆Ψ (h?|ht+1)−∆Ψ (ht+1|ht)\nwhere given any h,h′ ∈ B,\n∆Ψ (h|h′) := Ψ(h)−Ψ(h′)− 〈∇Ψ(h′),h− h′〉\nis the Bregman divergence between h and h′ w.r.t. function Ψ. Hence,\nη ( n∑ t=1 `(ht, zt)− n∑ t=1 `(h?, zt) )\n≤ n∑ t=1 ( ηp p ‖∇`(ht, zt)‖pX + 1 q ∥∥ht − h′t+1∥∥qX? + 〈∇Ψ(ht)−∇Ψ(h′t+1),h′t+1 − h?〉)\n= n∑ t=1 ( ηp p ‖∇`(ht, zt)‖pX + 1 q ∥∥ht − h′t+1∥∥qX? + ∆Ψ (h?|ht)−∆Ψ (h?∣∣h′t+1)−∆Ψ (h′t+1∣∣ht))\n≤ n∑ t=1 ( ηp p ‖∇`(ht, zt)‖pX + 1 q ∥∥ht − h′t+1∥∥qX? + ∆Ψ (h?|ht)−∆Ψ (h?|ht+1)−∆Ψ (h′t+1∣∣ht))\n= n∑ t=1 ( ηp p ‖∇`(ht, zt)‖pX + 1 q ∥∥ht − h′t+1∥∥qX? −∆Ψ (h′t+1∣∣ht))+ ∆Ψ (h?|h1)−∆Ψ (h?|hn+1) ≤\nn∑ t=1 ( ηp p ‖∇`(ht, zt)‖pX + 1 q ∥∥ht − h′t+1∥∥qX? −∆Ψ (h′t+1∣∣ht))+ Ψ(h?)\n116\nNow since Ψ is q-uniformly convex w.r.t. ‖·‖X? , for any h,h′ ∈ B?, ∆Ψ (h′|h) ≥ 1 q ‖h− h ′‖qX? . Hence we conclude that\nn∑ t=1 `(ht, zt)− n∑ t=1 `(h?, zt) ≤ ηp−1 p n∑ t=1 ‖∇`(ht, zt)‖pX + Ψ(h?) η\n≤ η p−1Bn\np +\nsuph∈HΨ(h)\nη\n≤ η p−1Bn\np +\nsuph∈HΨ(h)\nη\nPlugging in the value of η = ( suph∈H Ψ(h)\nnB\n)1/p we get :\nn∑ t=1 `(ht, zt)− n∑ t=1 `(h?, zt) ≤ 2 ( sup h∈H Ψ(h) )1/q (Bn)1/p\ndividing throughout by n conclude the proof.\nProof of Lemma 56. Note that for any h? ∈ H,\nη ( n∑ t=1 `(ht, zt)− n∑ t=1 `(h?, zt) ) ≤ n∑ t=1 〈η∇`(ht, zt),ht − h?〉\n= n∑ t=1 (〈 η∇`(ht, zt),ht − h′t+1 〉 + 〈 η∇`(ht, zt),h′t+1 − h? 〉) =\nn∑ t=1 (〈 η∇`(ht, zt),ht − h′t+1 〉 + 〈 ∇Ψ(ht)−∇Ψ(h′t+1),h′t+1 − h? 〉) ≤\nn∑ t=1 ( ‖η∇`(ht, zt)‖X ∥∥ht − h′t+1∥∥X? + 〈∇Ψ(ht)−∇Ψ(h′t+1),h′t+1 − h?〉) ≤\nn∑ t=1 ( ηp p ‖∇`(ht, zt)‖pX + 1 q ∥∥ht − h′t+1∥∥qX? + 〈∇Ψ(ht)−∇Ψ(h′t+1),h′t+1 − h?〉)\nUsing simple manipulation we can show that\n〈∇Ψ(ht)−∇Ψ(ht+1),ht+1 − h?〉 = ∆Ψ (h?|ht)−∆Ψ (h?|ht+1)−∆Ψ (ht+1|ht)\nwhere given any h,h′ ∈ B,\n∆Ψ (h|h′) := Ψ(h)−Ψ(h′)− 〈∇Ψ(h′),h− h′〉\n117\nis the Bregman divergence between h and h′ w.r.t. function Ψ. Hence,\nη ( n∑ t=1 `(ht, zt)− n∑ t=1 `(h?, zt) )\n≤ n∑ t=1 ( ηp p ‖∇`(ht, zt)‖pX + 1 q ∥∥ht − h′t+1∥∥qX? + 〈∇Ψ(ht)−∇Ψ(h′t+1),h′t+1 − h?〉)\n= n∑ t=1 ( ηp p ‖∇`(ht, zt)‖pX + 1 q ∥∥ht − h′t+1∥∥qX? + ∆Ψ (h?|ht)−∆Ψ (h?∣∣h′t+1)−∆Ψ (h′t+1∣∣ht))\n≤ n∑ t=1 ( ηp p ‖∇`(ht, zt)‖pX + 1 q ∥∥ht − h′t+1∥∥qX? + ∆Ψ (h?|ht)−∆Ψ (h?|ht+1)−∆Ψ (h′t+1∣∣ht))\n= n∑ t=1 ( ηp p ‖∇`(ht, zt)‖pX + 1 q ∥∥ht − h′t+1∥∥qX? −∆Ψ (h′t+1∣∣ht))+ ∆Ψ (h?|h1)−∆Ψ (h?|hn+1) ≤\nn∑ t=1 ( ηp p ‖∇`(ht, zt)‖pX + 1 q ∥∥ht − h′t+1∥∥qX? −∆Ψ (h′t+1∣∣ht))+ Ψ(h?) Now since Ψ is q-uniformly convex w.r.t. ‖·‖X? , for any h,h′ ∈ B?, ∆Ψ (h′|h) ≥ 1 q ‖h− h\n′‖qX? . Hence we conclude that\nn∑ t=1 `(ht, zt)− n∑ t=1 `(h?, zt) ≤ ηp−1 p n∑ t=1 ‖∇`(ht, zt)‖pX + Ψ(h?) η\nNow by smoothness of objective, using the Lemma ?? we have that each ‖∇`(ht, zt)‖X ≤ √ 4H`(ht, zt). Using this in the above we get that,\n1 n n∑ t=1 `(ht, zt)− 1 n n∑ t=1 `(h?, zt) ≤ ηp−1 p 1 n n∑ t=1 (4H`(ht, zt)) p 2 + 1 n Ψ(h?) η\n≤ (4H) p 2 ηp−1\np\n( 1\nn n∑ t=1 `(ht, zt)\n) p 2\n+ 1\nn\nΨ(h?)\nη\n≤ (4H) p 2 ηp−1\np\n( 1\nn n∑ t=1 `(ht, zt)− 1 n n∑ t=1 `(h?, zt)\n) p 2\n+ (4H)\np 2 ηp−1 p\n( 1\nn n∑ t=1 `(h?, zt)\n) p 2\n+ 1\nn\nΨ(h?)\nη\nWe now use the fact that if for any x,B ≥ 0 if x ≤ Bxα + A for some α ∈ ( 12 , 1] then as long as B < A1−α, x ≤ A1−BAα−1 . Using this with x = 1 n ∑n t=1 `(ht, zt)− 1 n ∑n t=1 `(h ?, zt), B = (4H) p 2 ηp−1 p and A = 1n Ψ(h?) η + (4H) p 2 ηp−1 p ( 1 n ∑n t=1 `(h ?, zt) )p/2 we conclude that for any η such that,\n(4H) p 2 ηp−1\np ≤ 1 2  1 n Ψ(h?) η + (4H) p 2 ηp−1 p ( 1 n n∑ t=1 `(h?, zt) )p/21− p 2 , (6.5.1)\n118\nwe will have that :\n1 n n∑ t=1 `(ht, zt)− 1 n n∑ t=1 `(h?, zt) ≤ 1 n Ψ(h?) η + (4H) p 2 ηp−1 p\n( 1 n ∑n t=1 `(h ?, zt) )p/2\n1− (4H) p 2 ηp−1\np\n( 1 n Ψ(h?) η + (4H) p 2 ηp−1 p ( 1 n ∑n t=1 `(h ?, zt) )p/2) p2−1\n(6.5.2)\n≤ 2  1 n Ψ(h?) η + (4H) p 2 ηp−1 p ( 1 n n∑ t=1 `(h?, zt) )p/2 (6.5.3) To this end we now choose the step size as\nη =  ( pΨ(h?) n )1/p 1√ 4H n ∑n t=1 `(h ?,zt) if 1n ∑n t=1 `(h ?, zt) ≥ 16Hp2/p ( Ψ(h?) n )2/q ( p 2 ) p 2 1 4H ( Ψ(h?) n ) 2−p p otherwise\nIt is easy to verify that the choice of η above satisfies condition in Equation 6.5.1. To see this note that if we plug in η = ( pΨ(h?) n )1/p 1√\n4H n ∑n t=1 `(h ?,zt) into condition in Equation 6.5.1 and rearrange we get that\n1 n ∑n t=1 `(h ?, zt) ≥ 16Hp2/p ( Ψ(h?) n )2/q . Thus whenever 1n ∑n t=1 `(h ?, zt) ≥ 16Hp2/p ( Ψ(h?) n )2/q , the choice of\nη satisfies the condition. On the other hand, the choice η = ( p 2 ) p 2 1 4H ( Ψ(h?) n ) 2−p p was in the first place derived by making (4H) p 2 ηp−1\np ≤ 1 2\n( 1\nn\nΨ(h?)\nη ) which is got by zeroing out the second term in the condition. Hence we conclude that this choice of η always\nsatisfies the condition in Equation 6.5.1. Now we plug in this choice of η into the bound in Equation 6.5.2. Note that whenever η = ( p Ψ(h?)\nn\n)1/p 1√\n4H n ∑n t=1 `(h ?,zt) , then simply plugging in this choice of η into the\nbound in Equation 6.5.2, we get\n1 n n∑ t=1 `(ht, zt)− 1 n n∑ t=1 `(h?, zt) ≤ √ 64 H Ψ1/q(h?)\n√ 1 n ∑n t=1 `(h ?, zt)\np1/pn1/q\n≤ √√√√64H n n∑ t=1 `(h?, zt) ( Ψ(h?) n )1/q (6.5.4)\nOn the other hand when\nη = (p\n2\n) p 2 1\n4H\n( Ψ(h?)\nn\n) 2−p p\n119\nplugging into bound in Equation 6.5.2 we get,\n1 n n∑ t=1 `(ht, zt)− 1 n n∑ t=1 `(h?, zt) ≤ 2  4H(p 2 ) p 2 ( Ψ(h?) n )2/q + (4H) 2−p 2 ( p 2 ) p(p−1) 2 ( Ψ(h?) n ) (2−p)(p−1) p p ( 1 n n∑ t=1 `(h?, zt) )p/2\nHowever note that we pick η = ( p 2 ) p 2 1 4H ( Ψ(h?) n ) 2−p p only when 1n ∑n t=1 `(h ?, zt) < 16H p2/p ( Ψ(h?) n )2/q and so plugging this inequality we get,\n1 n n∑ t=1 `(ht, zt)− 1 n n∑ t=1 `(h?, zt) ≤ 2  4H( p 2 ) p 2 ( Ψ(h?) n )2/q + 2p−24H( p 2 ) 4−p(p−1) 2 ( Ψ(h?) n )2/q ≤ 40H ( Ψ(h?)\nn\n)2/q (6.5.5)\nCombining the above and bound in Equation 6.5.4 we conclude that for this choice of η,\n1 n n∑ t=1 `(ht, zt)− 1 n n∑ t=1 `(h?, zt) ≤ max  √√√√64H n n∑ t=1 `(h?, zt) ( Ψ(h?) n )1/q , 40H ( Ψ(h?) n )2/q ≤ √√√√64H n n∑ t=1 `(h?, zt) ( Ψ(h?) n )1/q + 40H ( Ψ(h?) n )2/q\nProof of Lemma 57. Note that for any h? ∈ H,\nn∑ t=1 `(ht, zt)− n∑ t=1 `(h?, zt) ≤ n∑ t=1 〈∇`(ht, zt),ht − h?〉 − σ q′ ‖ht − h?‖q ′ X?\n= n∑ t=1 〈 ∇`(ht, zt),ht − h′t+1 〉 + 〈 ∇`(ht, zt),h′t+1 − h? 〉 − σ q′ ‖ht − h?‖q ′ X?\n= n∑ t=1 〈 ∇`(ht, zt),ht − h′t+1 〉 + 〈 ∇Ψ̃t(ht)−∇Ψ̃t(h′t+1),h′t+1 − h? 〉 − σ q′ ‖ht − h?‖q ′ X?\nUsing simple manipulation we can show that〈 ∇Ψ̃t(ht)−∇Ψ̃t(ht+1),ht+1 − h? 〉 = ∆Ψ̃t (h ?|ht)−∆Ψ̃t (h ?|ht+1)−∆Ψ̃t (ht+1|ht)\n120\nHence, n∑ t=1 `(ht, zt)− n∑ t=1 `(h?, zt) ≤ n∑ t=1 〈 ∇`(ht, zt),ht − h′t+1 〉 + 〈 ∇Ψ̃t(ht)−∇Ψ̃t(h′t+1),h′t+1 − h? 〉 − σ q′ ‖ht − h?‖q ′ X?\n≤ n∑ t=1 〈 ∇`(ht, zt),ht − h′t+1 〉 + ∆Ψ̃t (h ?|ht)−∆Ψ̃t ( h? ∣∣h′t+1)−∆Ψ̃t (h′t+1∣∣ht)− σq′ ‖ht − h?‖q′X?\n≤ n∑ t=1 〈 ∇`(ht, zt),ht − h′t+1 〉 + ∆Ψ̃t (h ?|ht)−∆Ψ̃t (h ?|ht+1)−∆Ψ̃t ( h′t+1 ∣∣ht)− σ q′ ‖ht − h?‖q ′ X?\n≤ n∑ t=1 (〈 ∇`(ht, zt),ht − h′t+1 〉 −∆Ψ̃t ( h′t+1 ∣∣ht))+ ∆Ψ̃1 (h?|h1)− σq′ ‖h1 − h?‖q′X? +\nn∑ t=2 ( ∆Ψ̃t (h ?|ht)−∆Ψ̃t−1 (h ?|ht)− σ q′ ‖ht − h?‖q ′ X? )\n≤ n∑ t=1 (〈 ∇`(ht, zt),ht − h′t+1 〉 −∆Ψ̃t ( h′t+1 ∣∣ht))+ ∆Ψ̃1 (h?|h1)− σ∆ψ (h?|h1) +\nn∑ t=2 ( ∆Ψ̃t (h ?|ht)−∆Ψ̃t−1 (h ?|ht)− σ∆ψ (h?|ht) ) =\nn∑ t=1 (〈 ∇`(ht, zt),ht − h′t+1 〉 −∆Ψ̃t ( h′t+1 ∣∣ht))+ 1 η ∆Ψ (h ?|h1) + ∆R (h?|h1)\n= n∑ t=1 ( 〈∇`(ht, zt),ht − ht+1〉 −∆Ψ̃t (ht+1|ht) ) + 1 η ∆Ψ (h ?|h1) + ∆R (h?|h1)\n= n∑ t=1 ( 〈∇φ(ht, zt),ht − ht+1〉 −∆ Ψ η +σtψ (ht+1|ht) +R(ht)−R(ht+1) ) + 1 η ∆Ψ (h ?|h1)\n+ ∆R (h ?|h1)\n≤ n∑ t=1 (〈 ∇φ(ht, zt),ht − h′t+1 〉 − 1 η q ∥∥h? − h′t+1∥∥qX? − σ tq′ ∥∥h? − h′t+1∥∥q′X? ) + Ψ(h?) η\n+R(h?)−R(hn+1)\n≤ n∑ t=1 inf ut+vt=∇φ(ht,zt) { ηp−1 p ‖ut‖pX +\n1\np′ σp′−1 tp′−1 ‖vt‖p\n′ X\n} + Ψ(h?)\nη +R(h?)\nWhere in the steps above we used the fact that for any functions F and G, ∆G+F (·|·) = ∆G (·|·) + ∆F (·|·) and that fact that for any function F that is q-uniformly convex w.r.t. ‖·‖X? , for any h,h′ ∈ B?, ∆F (h′|h) ≥ 1 q ‖h− h\n′‖qX? . The final step is due to Fenchel Young inequality. Now we upper bound the summation term by replacing each infimum over decompositions of∇`(ht, zt) into any arbitrary vectors ut and vt to vectors of specific form, ut = (1− α)∇φ(ht, zt) and vt = α∇φ(ht, zt) for some α ∈ [0, 1]. Hence we get for any\n121\nα ∈ [0, 1],\nn∑ t=1 `(ht, zt)− n∑ t=1 `(h?, zt) ≤ n∑ t=1\n( ηp−1(1− α)p\np ‖∇φ(ht, zt)‖pX +\nαp ′\np′ σp′−1 tp′−1 ‖∇φ(ht, zt)‖p\n′ X\n)\n+ Ψ(h?)\nη +R(h?)\n≤ η p−1(1− α)pn\np +\nαp ′\np′ σp′−1 n∑ t=1 1 tp′−1 + Ψ(h?) η +R(h?)\n≤ η p−1(1− α)pn\np +\nαp ′ p′ σp′−1 n2−p ′ 2− p′ + Ψ(h?) η +R(h?)\n≤ ηp−1(1− α)pn+ α p′ σp′−1 n2−p ′ 2− p′ + Ψ(h?) η +R(h?) ≤ ηp−1(1− α)pn+ α p′\nσp′−1 n2−p\n′\n2− p′ +\nsuph∈HΨ(h)\nη + sup h∈H R(h)\nUsing α = 1 whenever n ≥ ( (2− p′)σp′−1Ψ1/q(h?) ) 1 2−p′−1/p and α = 0 otherwise and picking\nη =  ( suph∈H Ψ(h) n )1/p if n ≥ ( (2− p′)σp′−1 suph∈HΨ1/q(h) ) 1 2−p′−1/p\n∞ otherwise\nwe get that n∑ t=1 `(ht, zt)− n∑ t=1 `(h?, zt) ≤ min { 2 sup h∈H Ψ1/q(h)n1/p + sup h∈H R(h), 2 (2− p′)σp′−1 n2−p ′ + sup h∈H R(h) }\nDividing throughout by n concludes the proof."
    }, {
      "heading" : "6.6 Discussion",
      "text" : "The mirror descent algorithm with uniformly convex Ψ functions were introduced by Nemirovski and Yudin in [1] for offline convex optimization. Specific upper bounds for offline convex optimization of ZLip dual case when H is the unit `p ball and X is the dual of H are provided in [1]. For online convex optimization problem, online gradient descent (Euclidean case) was proposed by Zinkevich in[30]. Faster rates when the losses are strongly convex in the Euclidean case for online gradient descent was proposed in [63]. Mirror descent for general strongly convex objectives with log n/n rates was proposed and analyzed in [64]. While in all the above the set X in the corresponding problems are same as dual of set H and H = H̄, in this chapter we consider the generic case and provide bounds for the non-dual case for online and statistical convex learning and for offline convex optimization. One fact to pay attention to is that the upper bounds are provided assuming one can find appropriate function Ψ that is q-uniformly convex w.r.t. norm ‖·‖X? .\n122\nOfcourse the immediate question that arises is “When can one find such functions Ψ and are the bounds got using such Ψ optimal?”. The next three chapters deals with this question for online and statistical convex learning problems and for offline convex optimization problems.\n123\nChapter 7\nOptimality of Mirror Descent for Online Convex Learning Problem\nIn this chapter we will show that the mirror descent method is universal and near optimal for online convex learning problems. Very roughly, the main result we show in this chapter can be stated as :\nFor any online convex learning problem, if some online learning algorithm can guarantee a regret bound of Raten, then mirror descent algorithm can guarantee regret bounded as Õ(Raten). Of course in the remainder of the chapter we will exactly quality this result and show optimality of mirror descent for online learning when losses are s.t. gradients are in set X , for non-negative smooth convex losses and for uniformly convex losses.\nBefore we proceed we start by noticing that owing to Remark 54, it suffices to only consider Deterministic learning algorithms. As a result, the online convex learning problem can be viewed as a multi-round game where on round t, the learner first picks a vector ht ∈ H̄. Next, the adversary picks instance zt ∈ Z where Z is a class of instances specifying some set of convex functions. At the end of the round, the learner pays instantaneous cost `(ht, zt). Recall that a deterministic online learning algorithm A for the problem is specified by the mapping A : ⋃ n∈NZn−1 7→ H̄. We shall represent the regret of an algorithm A for a given sequence of instances z1, . . . , zn by the shorthand :\nRn(A, z1, . . . , zn) := 1\nn n∑ t=1 `(A(z1:t−1), zt)− inf h∈H 1 n n∑ t=1 `(h, zt) .\nThe goal of the learner as before, is to minimize the regret at the end of n rounds.\nIn Chapter 4 since we considered randomized online learning algorithms we had to be careful in defining the value of the game in Equation 4.2.1. Since for convex learning problems it suffices to only consider deterministic learning algorithms, it is easier to write down the value of the game for these problems. The value of the online convex learning problem can we written as the best possible guarantee on regret against\n124\nany sequence of instances that any algorithm can enjoy. Formally the value can be written as :\nVn(H,Z) = inf A sup z1:n∈Z Rn(A, z1, . . . , zn) (7.0.1)\nIn the Section 7.1 we will see that for online convex learning problems introduced in previous chapter, a problem is online learnable if and only if it is learnable using a gradient-based online learning algorithm. We will also see that value of the linear game acts as key in characterizing optimal learning rates of various other convex learning problems and hence will focus on that. In Chapter 6 we describe the online mirror descent algorithm and provide guarantees for various problems. However these guarantees relied on our ability to be able to pick appropriate uniformly convex function to use with the mirror descent algorithm. In section 7.2 we show how the concept of martingale type (a generalization of it as per our need) captures closely the value of linear game and hence can be used to closely characterize rates for the various convex learning problems. In Section 7.3 we extend Pisier’s result [65] to show that martingale type of the problem can be used to ensure existence of an appropriate uniformly convex function. Subsequently in Section 7.4 we put it all together and establish that for the convex problems we consider, there exists appropriate uniformly convex funciton so that mirror descent with this function and right step size is always near optimal (upto to log factors). Thus we establish universality and nearo optimality of mirror descent. This is also shown for non-negative smooth convex losses and certain uniformly convex losses. In Section 7.5 we provide several examples of commonly encountered convex learning problems and establish rates for these problems using mirror descent. Section 7.6 provides detailed proofs for results in this chapter and finally we conclude with some discussion in Section 7.7."
    }, {
      "heading" : "7.1 Value of the Linear Game",
      "text" : "The value of online learning learning problem plays an important role in characterizing optimal rates of various online convex learning problems. In this section we show how the value of the linear online learning game is related to value of other online convex learning games and also introduce some necessary definitions to build towards showing the main result of this chapter. The following lemma shows how one can use online algorithms for linear problems for other online convex learning problems (problems where sub-gradients are in X ).\nLemma 63. Let A be any online learning algorithm for linear learning problems specified by instance set Zlin. Using this, for any convex learning problem specified by instance set Z such that for any h ∈ H, ∇h`(h, z) ∈ X , one can construct a new gradient-based learning algorithm AO 1st such that for any z1, . . . , zn ∈ Z , Rn(A\nO1st , z1, . . . , zn) ≤ sup z∗1 ,...,z ∗ n∈Zlin Rn(A, z ∗ 1 , . . . , z ∗ n)\nA direct consequence of the above lemma is the following corollary that shows that value of the linear game upper bounds value of other online convex learning problems and is in fact equal to the value of the supervised\n125\nlearning game and online convex Lipschitz learning game.\nCorollary 64. For any convex learning problem specified by instance set Z which is s.t. ∀h ∈ H, z ∈ Z : ∇h`(h, z) ∈ X , we have that,\nVn(H,Z) ≤ Vn(H,Zlin(X ))\nFurthermore Vn(H,ZLip(X )) = Vn(H,Zsup(X )) = Vn(H,Zlin(X ))\nThe equality in the above corollary can also be extended to most other commonly occurring convex loss function classes like say the hinge loss class and logistic learning loss class with some extra constant factors. As we see from the above result, the value of the online learning problem for linear instance class Zlin(X ) is critical in upper and lower bounds on rates of various other convex learning problems. In fact as we will sees later the value of the linear game also plays an important role in characterizing rates of smooth and uniformly convex online learning problems. Owing to this, for any p ∈ [1, 2] we define constant :\nVp := inf { V ∣∣∣ ∀n ∈ N,Vn(H,Zlin(X )) ≤ V n−(1− 1p )} (7.1.1)\nNotice that Vp characterizes optimal rate for online linear learning problems (and hence supervised and Lipschitz classes too) up to polynomial.\nThe main aim of this chapter is to show near optimality of mirror descent algorithm. To this end, similar to Vp for each p ∈ [1, 2] we can define:\nMDp := inf\n{ D : ∃Ψ, η s.t. ∀n ∈ N, sup\nz1:n∈Z Rn(AMD, z1:n) ≤ Dn−(1− 1 p )\n} (7.1.2)\nwhere the Mirror Descent algorithm in the above definition is run with the corresponding Ψ and η. The constant MDp is a characterization of the best guarantee the Mirror Descent algorithm can provide by choosing the best Ψ and η.\nA simple consequence of the definitions of Vp and MDp is the following proposition.\nProposition 65. For any p ∈ [1, 2]: Vp ≤ MDp"
    }, {
      "heading" : "7.2 Value and Martingale Type",
      "text" : "In [7], it was shown that the concept of the Martingale type (also sometimes called the Haar type) of a Banach space and optimal rates for online convex optimization problem, where X andH are duals of each other, are closely related. In this section we extend the classic notion of Martingale type of a Banach space (see for instance [65]) to one that accounts for the pair (H?,X ). Before we proceed with the definitions we would like to introduce a few necessary notations. First, throughout we shall use ∈ {±1}N to represent infinite sequence of signs drawn uniformly at random (i.e. each i has equal probability of being +1 or −1). Also\n126\nthroughout (xn)n∈N represents an B? valued tree of infinite depth, that is a sequence of mappings where each xn : {±1}n−1 7→ B?. We are now ready to give the extended definition of Martingale type (or M-type) of a pair (H?,X ).\nDefinition 32. A pair (H?,X ) of subsets of a vector space B? is said to be of M-type p if there exists a constant C ≥ 1 such that for all B?-valued tree x of infinite depth and any x0 ∈ B? :\nsup n\nE [∥∥∥∥∥x0 + n∑ i=1 ixi( ) ∥∥∥∥∥ p\nH?\n] ≤ Cp ‖x0‖pX +∑ n≥1 E [‖xn( )‖pX ]  (7.2.1) The concept is called Martingale type because ( nxn( ))n∈N is a martingale difference sequence and it can be shown that rate of convergence of martingales in Banach spaces is governed by the rate of convergence of martingales of the form Zn = x0 + ∑n i=1 ixi( ) (which are incidentally called Walsh-Paley martingales). We point the reader to [65, 66] for more details. Further, for any p ∈ [1, 2] we also define,\nCp := inf C ∣∣∣∣∣∣ ∀x0 ∈ B?, ∀x, supn E [∥∥∥∥∥x0 + n∑ i=1 ixi( ) ∥∥∥∥∥ p\nH?\n] ≤ Cp ‖x0‖pX +∑ n≥1 E ‖xn( )‖pX  Cp is useful in determining if the pair (H?,X ) has Martingale type p.\nBy the results in Chapter ?? (using it specifically for linear class) we have the following theorem:\nTheorem 66. For anyH ∈ B and any X ∈ B? and any n ≥ 1,\nsup x\nE [∥∥∥∥∥ 1n n∑ i=1 ixi( ) ∥∥∥∥∥ H? ] ≤ Vn(H,X ) ≤ 2 sup x E [∥∥∥∥∥ 1n n∑ i=1 ixi( ) ∥∥∥∥∥ H? ]\nwhere the supremum above is over B?-valued tree x of infinite depth.\nOur main interest here will is in establishing that low regret implies Martingale type. To do so, we start with the above theorem to relate value of the online convex optimization game to rate of convergence of martingales in the Banach space. We then extend the result of Pisier in [65] to the “non-matching” setting combining it with the above theorem to finally get :\nLemma 67. If for some r ∈ (1, 2] there exists a constant D > 0 such that for any n,\nVn(H,X ) ≤ Dn−(1− 1 r )\nthen for all s < r, we can conclude that any x0 ∈ B? and any B?-valued tree x of infinite depth will satisfy :\nsup n\nE [∥∥∥∥∥x0 + n∑ i=1 ixi( ) ∥∥∥∥∥ s\nH?\n] ≤ ( 1104 D\n(r − s)2 )s‖x0‖sX +∑ i≥1 E [‖xi( )‖sX ]  That is, the pair (H,X ) is of martingale type s.\n127\nThe following corollary is an easy consequence of the above lemma.\nCorollary 68. For any p ∈ [1, 2] and any p′ < p : Cp′ ≤ 1104 Vp(p−p′)2"
    }, {
      "heading" : "7.3 Martingale Type and Uniform Convexity",
      "text" : "The classical notion of Martingale type plays a central role in the study of geometry of Banach spaces. In [65], it was shown that a Banach space has Martingale type p (the classical notion) if and only if uniformly convex functions with certain properties exist on that space (w.r.t. the norm of that Banach space). In this section, we extend this result and show how the Martingale type of a pair (H?,X ) are related to existence of certain uniformly convex functions. Specifically, the following theorem shows that the notion of Martingale type of pair (H?,X ) is equivalent to the existence of a non-negative function that is uniformly convex w.r.t. the norm ‖·‖X? .\nLemma 69. If, for some p ∈ (1, 2], there exists a constant C > 0, such that for all B?-valued tree x of infinite depth and any x0 ∈ B?:\nsup n\nE [∥∥∥∥∥x0 + n∑ i=1 ixi( ) ∥∥∥∥∥ p\nH?\n] ≤ Cp ‖x0‖pX +∑ n≥1 E [‖xn( )‖pX ]  (i.e. (H?,X ) has Martingale type p), then there exists a convex function Ψ : B 7→ R+ with Ψ(0) = 0, that is q-uniformly convex w.r.t. norm ‖·‖X? s.t. ∀h ∈ B, 1 q ‖h‖ q X? ≤ Ψ(h) ≤ Cq q ‖h‖ q H.\nDefine,\nDp := inf {( sup h∈H Ψ(h) ) p−1 p ∣∣∣∣∣ Ψ : H 7→ R+ is pp−1 -uniformly convex w.r.t. ‖·‖X∗ ,Ψ(0) = 0 }\nThe following corollary follows directly from the above lemma.\nCorollary 70. For any p ∈ [1, 2], Dp ≤ Cp.\nThe proof of Lemma 69 goes further and gives a specific uniformly convex function Ψ satisfying the desired requirement (i.e. establishing Dp ≤ Cp) under the assumptions of the previous lemma:\nΨ∗q(x) := sup  1Cp supn E [∥∥∥∥∥x + n∑ i=1 ixi( ) ∥∥∥∥∥ p\nH?\n] − ∑ i≥1 E [ ‖xi( )‖pX ] , Ψq := (Ψ∗q)∗ . (7.3.1) where the supremum above is over B?-valued tree x of infinite depth and p = qq−1 .\n128"
    }, {
      "heading" : "7.4 Main Result : Optimality of Online Mirror Descent",
      "text" : "In previous chapter we argued that if we can find an appropriate uniformly convex function to use with the mirror descent algorithm, one can guarantee diminishing regret. However the pending question there was when one can find such a function and what is the rate one can gaurantee. In Section ?? we introduced the extended notion of Martingale type of a pair (H?,X ) and how it related to the value of the game. In Section ??, we saw how the concept of M-type related to existence of certain uniformly convex functions. We can now combine these results to show that the mirror descent algorithm is a universal online learning algorithm for convex learning problems. Specifically we show that whenever a problem is online learnable, the mirror descent algorithm can guarantee near optimal rates:\nTheorem 71. If for some constant V > 0 and some q ∈ [2,∞), Vn(H,X ) ≤ V n− 1 q for all n, then for any n > eq−1, there exists regularizer function Ψ and step-size η, such that the regret of the mirror descent algorithm using Ψ against any z1, . . . , zn ∈ ZLip chosen by the adversary is bounded as:\nRn(AMD, z1, . . . , zn) ≤ 6002V log2(n) n− 1 q (7.4.1)\nProof. Combining Mirror descent guarantee in Lemma 55, Lemma 69 and the lower bound in Lemma 67 with s = qq−1 − 1 log(n) we get the above statement.\nThe above Theorem tells us that, with appropriate Ψ and learning rate η, mirror descent will obtain regret at most a factor of 6002 log2(n) from the best possible worst-case upper bound. We would like to point out that the constant V in the value of the game appears linearly and there is no other problem or space related hidden constants in the bound.\nThe following figure summarizes the relationship between the various constants. The arrow mark from Cp′ to Cp indicates that for any n, all the quantities are within log2 n factor of each other.\nWe now provide some general guidelines that will help us in picking out appropriate function Ψ for mirror descent. First we note that though the function Ψq in the construction Eq. (7.3.1) need not be such that (qΨq(h))\n1/q is a norm, with a simple modification as noted in [66] we can make it a norm. This basically tells us that the pair (H,X ) is online learnable, if and only if we can sandwich a q-uniformly convex norm inbetween X ? and a scaled version of H (for some q <∞). Also note that by definition of uniform convexity, if any function Ψ is q-uniformly convex w.r.t. some norm ‖·‖ and we have that ‖·‖ ≥ c ‖·‖X , then Ψ(·) cq is q-uniformly convex w.r.t. norm ‖·‖X . These two observations together suggest that given pair (H,X ) what\n129\nwe need to do is find a norm ‖·‖ in between ‖·‖?X and C ‖·‖H (C <∞, smaller the C better the bound ) such that ‖·‖q is q-uniformly convex w.r.t ‖·‖."
    }, {
      "heading" : "7.4.1 Smooth Loss Case",
      "text" : "Lemma 72. Given any L∗ ∈ (0, 34 ] and any n ∈ N, for any online convex learning algorithm A, there exists instances z1, . . . , zn ∈ Zsmt(1) such that, infh∈H 1n ∑n t=1 `(h, zt) ≤ L∗ and\nRn(A, z1:n) ≥ 1\nn sup x\nE ∥∥∥∥∥∥ nL∗∑ t=1 txt( ) ∥∥∥∥∥∥ H?  ≥ L∗ 2 VnL∗(H,Zlin(X ))\nThe following theorem shows the almost optimality of mirror descent for non-negative smooth convex objectives. The upper bound on the regret of the mirror descent algorithm also shows how having small L∗ helps reduce regret.\nTheorem 73. If for some V > 0 and q ∈ [2,∞) we have that\nVn(H,Zsmt(1)) ≤ V\nn1/q\nthen for any L∗ > 0 and n > eq−1 there exists regularizer function Ψ and step-size η such that regret of the mirror descent algorithm with these Ψ and η against any z1, . . . , zn ∈ Zsmt(H) s.t. infh∈H 1n ∑n t=1 `(h, zt) ≤"
    }, {
      "heading" : "L∗ is bounded as :",
      "text" : "Rn(AMD, z1, . . . , zn) ≤ 48016V\n√ HL∗ log2 n\nn 1 q\n+ (37960V )2H log4 n\nn 2 q\nProof. Using Lemma 72 with L∗ = 34 with the premise of the theorem we have that for any n,\nV3n/4(H,Zlin(X )) ≤ 8V 3n1/q ≤ 8V 3(3n/4)1/q\nand so we can conclude that for any m, Vm(H,Zlin(X )) ≤ 8V3m1/q . Combining Mirror descent guarantee for smooth loss in Lemma 56, Lemma 69 and the lower bound in Lemma 67 with s = qq−1 − 1 log(n) we get the above statement.\nThe above theorem shows that the mirror descent can achieve a near optimal rate in terms of dependence on n. However a closer look at the above Theorem and Lemma also reveals that we can in fact also capture a tighter dependence on L∗. In fact when q = 2, (that is when we get 1/ √ n rates), we can show that the dependence on L∗ of mirror descent is tight.\nTo see this assume that algorithm A is a minimax optimal algorithm and is such that for any L∗ ∈ (0, 3/4] and n the regret of the algorithm against any z1, . . . , zn ∈ Zsmt(1) s.t. infh∈H 1n ∑n t=1 `(h, zt) ≤ L∗ is\n130\nbounded as :\nR(A, z1, . . . , zn) ≤ V L∗\na\nn1/q +\nV\nn1/b\nwhere V, a, b > 0 and q ∈ [2,∞) with b < q.\nUsing the guarantee of the algorithm and Lemma 72 we see that\nVnL∗(H,Zlin(X )) ≤ 2V L∗\na−1\nn1/q +\n2V\nL∗n1/b\nHence using notation m = nL∗ we conclude that for any m,\nVm(H,Zlin(X )) ≤ 2V L∗\na−1+ 1q\nm1/q +\n2V\nL∗ 1− 1bm1/b\n(7.4.2)\nFrom the above inequality we can conclude that a ≥ 1 − 1q = 1 p . This is because if a < 1 p , then since the above inequality holds for all L∗, by picking L∗ to minimize the above inequality we can conclude that Vm(H,Zlin(X )) = o ( 1\nm1/q\n) . Hence Theorem 73 we can conclude that regret of the mirror descent algorithm\nfor any z1, . . . , zn ∈ Zsmt(1)(X ) is bounded by o ( 1 m1/q ) which is a contradiction since A is minimax optimal but can only guarantee bound of order 1/n1/q . Hence we can conclude that a ≤ 1/p.\nNow note that by the guarantee of algorithm A we can conclude that\nVn(H,Zsmt(1)(X )) ≤ 2V\nn1/q\nand so by Theorem 73 we can conclude that there exists regularizer function Ψ and step-size η such that regret of the mirror descent algorithm with these Ψ and η against any z1, . . . , zn ∈ Zsmt(H) s.t. infh∈H 1n ∑n t=1 `(h, zt) ≤ L∗ is bounded as :\nRn(AMD, z1, . . . , zn) ≤ 96032V\n√ HL∗ log2 n\nn 1 q\n+ (75920V )2H log4 n\nn 2 q\nThus we can conclude that while the best guarantee of any algorithm A is,\nR(A, z1, . . . , zn) ≤ V L∗\n1/p\nn1/q +\nV\nn1/b ,\nthe regret of mirror descent can be bounded as\nRn(AMD, z1, . . . , zn) ≤ 96032V\n√ HL∗ log2 n\nn 1 q\n+ (75920V )2H log4 n\nn 2 q\nNotice that when q = 2, for any L∗ > V n1/q , guarantee of mirror descent is not only optimal in terms of n but also in terms of L∗.\n131"
    }, {
      "heading" : "7.4.2 Uniformly Convex Loss Case",
      "text" : "Lemma 74. For any σ > 0 and q′ ∈ (2,∞) we have that,\nVn(H,Zucvx(σ,q′)(X )) ≥ 1\n2σp′−1Dp ′\np′\n( sup u E [∥∥∥∥∥ 1n n∑ t=1 tut( ) ∥∥∥∥∥ H? ])p′ ≥ 1 2σp′−1Dp ′ p′ (Vn(H,Zlin(X )))p ′\nwhere the supremum is over all X -valued trees u of depth n.\nFor simplicity we are going to assume now that σ = 1. The following theorem shows that for certain q′, mirror descent achieves optimal rate.\nTheorem 75. Given a q′, if there exists V > 0 and a > 0 such that :\nVn(H,Zucvx(1,q′)) ≤ V\nna\nthen for any n > eq−1 there exists regularizer function Ψ and step-size η such that regret of the mirror descent algorithm with these Ψ and η against any z1, . . . , zn ∈ Zucvx(1,q′) or the case when q′ > 2 is bounded as :\nRn(AMD, z1, . . . , zn) ≤ min\n{ 12008(2V ) 1 p′Dp′ log 2 n\nn a p′\n, 2\n(2− p′)np′−1\n} + suph∈HR(h)\nn .\nand for q′ = 2 is bounded as : Rn(AMD, z1, . . . , zn) ≤ 2 lognn + suph∈H R(h) n .\nNotice that when q′ =∞, Dp′ = D1 = 1 and so by above theorem,\nRn(AMD, z1, . . . , zn) ≤ 24016V log2 n\nna +\nsuph∈HR(h)\nn .\nHence for this case mirror descent is near optimal.\nNext notice that by lower bound in lemma 74 and assumption of above theorem, we have\nVn(H,Zlin(X )) ≤ (2V )\n1 p′Dp′\nn a p′\nNow define p? = sup{p : Vp < ∞}. By above inequality and definition of p? we see that if for any p′, a p′ > 1 q? , then it should be true that (2V ) 1 p′Dp′ =∞. In other words, it has to be true that for any p′, a ≤ p ′\nq? . Hence we can conclude that as p′ → p?, any algorithm will have a regret of order at least 1\nn p? q? = 1 np?−1 .\nHowever mirror descent guarantee in Theorem 75 shows that mirror descent algorithm achieves regret at most\nRn(AMD, z1, . . . , zn) ≤ 2\n(2− p?)np?−1 +\nsuph∈HR(h)\nn\n132\nwhen p? < 2 and\nRn(AMD, z1, . . . , zn) ≤ 2 log n\nn +\nsuph∈HR(h)\nn\nwhen p? = 2. Hence we can conclude that for the case when p′ → p? again mirror descent is optimal. (The reason we are taking limit of p′ → p? is because one may not even be able to find q? uniformly convex functions on the given space that is bounded). This specifically shows that mirror descent is near optimal for strongly convex objectives."
    }, {
      "heading" : "7.5 Examples",
      "text" : "We demonstrate our results on several online learning problems, specified byH and X ."
    }, {
      "heading" : "7.5.1 Example : `p non-dual pairs",
      "text" : "It is usual in the literature to consider the case whenH is the unit ball of the `p norm in some finite dimension d while X is taken to be the unit ball of the dual norm `q where p, q are Hölder conjugate exponents. Using the machinery developed in this chapter, it becomes effortless to consider the non-dual case when H is the unit ball Bp1 of some `p1 norm while X is the unit ball Bp2 for arbitrary p1, p2 in [1,∞]. We shall use q1 and q2 to represent Holder conjugates of p1 and p2. Before we proceed we first note that for any r ∈ (1, 2], ψr(h) := 12(r−1)‖h‖ 2 r is 2-uniformly w.r.t. norm ‖·‖r (see for instance [61]). On the other hand by Clarkson’s inequality, we have that for r ∈ (2,∞), ψr(h) := 2 r r ‖h‖ r r is r-uniformly convex w.r.t. ‖·‖r. Putting it together we see that for any r ∈ (1,∞), the function ψr defined above, is Q-uniformly convex w.r.t ‖·‖r for Q = max{r, 2}. The basic technique idea is to be to select ψr based on the guidelines in the end of the previous section. Finally we show that using ψ̃r := d Qmax{ 1q2− 1 r ,0}ψr in Mirror descent Lemma 55 yields the bound that for any z1, . . . , zn ∈ ZLip:\nRn(AMD, z1, . . . , zn) ≤ 2 max{2, 1√ 2(r−1) }dmax{ 1 q2 − 1r ,0}+max{ 1 r− 1 p1 ,0}\nn1/max{r,2}\nThe following table summarizes the scenarios where a value of r = 2, i.e. a rate of D2/ √ n, is possible, and lists the corresponding values of D2 (up to numeric constant of at most 16):\np1 Range p2 Range D2 Treating as dual in ‖ · ‖p1 1 ≤ p1 ≤ 2 p2 < 2 1 1/ √ p1 − 1 1 ≤ p1 ≤ 2 2 ≤ p2 ≤ p1p1−1 √ p2 − 1 1/ √ p1 − 1 1 ≤ p1 ≤ 2 p1p1−1 < p2 d p2−1 p2 − 1 p1 / √ p1 − 1 d p2−1 p2 − 1 p1 / √ p1 − 1 p1 > 2 p2 < 2 d 1 2 − 1 p1 d 1 2 − 1 p1 p1 > 2 p2 ≥ 2 d p2−1 p2 − 1 p1 d p2−1 p2 − 1 p1\n1 ≤ p1 ≤ 2 p2 =∞ √ log(d) √ log(d)\nNote that the first two rows are dimension free, and so apply also in infinite-dimensional settings, whereas in\n133\nthe other scenarios, D2 is finite only when the dimension is finite. An interesting phenomena occurs when d is∞, p1 > 2 and q2 ≥ p1. In this case D2 =∞ and so one cant expect a rate of O( 1√n ). However we have Dp2 < 16 and so can still get a rate of n − 1q2 .\nBall et al [67] tightly calculate the constants of strong convexity of squared `p norms, establishing the tightness ofD2 when p1 = p2. By extending their constructions it is also possible to show tightness (up to a factor of 16) for all other values in the table. Also, Agarwal et al [68] recently showed lower bounds on the sample complexity of stochastic optimization when p1 = ∞ and p2 is arbitrary—their lower bounds match the last two rows in the table."
    }, {
      "heading" : "7.5.2 Example : Non-dual Schatten norm pairs in finite dimensions",
      "text" : "Exactly the same analysis as above can be carried out for Schatten p-norms, i.e. when H = BS(p1), X = BS(p2) are the unit balls of Schatten p-norm (the p-norm of the singular values) for matrix of dimensions d1×d2. We get the same results as in the table above (as upper bounds onD2), with d = min{d1, d2}. These results again follow using similar arguments as `p case and tight constants for strong convexity parameters of the Schatten norm from [67]."
    }, {
      "heading" : "7.5.3 Example : Non-dual group norm pairs in finite dimensions",
      "text" : "In applications such as multitask learning, groups norms such as ‖h‖q,1 are often used on matrices h ∈ Rk×d where (q, 1) norm means taking the `1-norm of the `q-norms of the columns of h. Popular choices include q = 2,∞. Here, it may be quite unnatural to use the dual norm (p,∞) to define the space X where the data lives. For instance, we might want to consider H = B(q,1) and X = B(∞,∞) = B∞. In such a case we can calculate that D2(H,X ) = Θ(k1− 1 q √ log(d)) using Ψ(h) = 1q+r−2 ‖h‖ 2 q,r where r = log d log d−1 ."
    }, {
      "heading" : "7.5.4 Example : Max Norm",
      "text" : "Max-norm has been proposed as a convex matrix regularizer for application such as matrix completion [69]. In the online version of the matrix completion problem at each time step one element of the matrix is revealed, corresponding to X being the set of all matrices with a single element being 1 and the rest 0. Since we need X to be convex we can take the absolute convex hull of this set and use X to be the unit element-wise `1 ball. Its dual is ‖W‖X? = maxi,j |Wi,j |. On the other hand given a matrix W , its max-norm is given by ‖W‖max = minU,V :W=UV > (maxi ‖Ui‖2) ( maxj ‖Vj‖2 ) . The set H is the unit ball under the max norm. As noted in [70] the max-norm ball is equivalent, up to a factor two, to the convex hull of all rank one sign matrices. Let us now make a more general observation.\nProposition 76. LetH = abscvx({h1, . . . ,hK}). The Minkowski norm for thisH is given by\n‖h‖H := inf α1,...,αK :h= ∑K i=1 αihi K∑ i=1 |αi|\n134\nIn this case, for any q ∈ (1, 2], if we define the norm :\n‖h‖H,q = inf α1,...,αK :h= ∑K i=1 αihi ( K∑ i=1 |αi|q )1/q\n(7.5.1)\nthen the function Ψ(h) = 12(q−1) ‖h‖ 2 H,q is 2-uniformly convex w.r.t. ‖·‖H,q . Further if we use q = logK logK−1 , then suph∈H √ Ψ(h) = O( √ logK).\nProof of the above proposition is similar to proof of strong convexity of `q norms. For the max norm case as noted before the norm is equivalent to the norm got by the taking the absolute convex hull of the set of all rank one sign matrices. Cardinality of this set is of course 2N+M . Hence using the above proposition and noting that X ? is the unit ball of | · |∞ we see that Ψ is obviously 2-uniformly convex w.r.t. ‖·‖X? and so we get a regret bound O (√ M+N n ) . This matches the stochastic (PAC) learning guarantee [70], and is the first guarantee we are aware of for the max norm matrix completion problem in the online setting."
    }, {
      "heading" : "7.5.5 Example : Interpolation Norms",
      "text" : "Another interesting setting is when the set H is got by interpolating between unit balls of two other norms ‖·‖H1 and ‖·‖H2 . Specifically one can considerH to be the unit ball of two such interpolated norms, the first type of interpolation norm is given by,\n‖h‖H = ‖h‖H1 + ‖h‖H2 (7.5.2)\nThe second type of interpolation norm one can consider is given by\n‖h‖H = inf h1+h2=h\n( ‖h1‖H1 + ‖h2‖H2 ) (7.5.3)\nIn learning problems such interpolation norms are often used to induce certain structures or properties into the regularization. For instance one might want sparsity along with grouping effect in the linear predictors for which elastic-net type regularization introduced by Zou and Hastie [71] (this is captured by interpolation of the first type between `1 and `2 norms). Another example is in matrix completion problems when we would like the predictor matrix to be decomposable into sum of sparse and low rank matrices as done by Chanrdasekaran et. al [72] (here one can use the interpolation norm of second type to interpolate between trace norm and element wise `1 norm). Another example where interpolation norms of type two are useful are in multi-task learning problems (with linear predictors) as done by Jalali et. al [73]. The basic idea is that the matrix of linear predictors can is decomposed into sum of two matrices one with for instance low entry-wise `1 norm and other with low B(2,∞) group norm (group sparsity).\nWhile in these applications the set H used is obtained through interpolation norms, it is typically not natural for the set X to be the dual ball of H but rather something more suited to the problem at hand. For instance, for the elastic net regularization case, the set X usually considered are either the vectors with bounded `∞\n135\nnorm or bounded `2. Similarly for the [73] case X could be either matrices with bounded entries or some other natural assumption that suits the problem.\nIt can be shown that in general for any interpolation norm of first type specified in Equation 7.5.2,\nD2(H,X ) ≤ 2 min{D2(H1,X ), D2(H2,X )} (7.5.4)\nSimilarly for the interpolation norm of type two one can in general show that,\nD2(H,X ) ≤ 1\n2 max{D2(H1,X ), D2(H2,X )} (7.5.5)\nUsing the above bounds one can get regret bounds for mirror descent algorithm with appropriate Ψ and step size η for specific examples like the ones mentioned.\nThe bounds given in Equations Eq. (7.5.4) and Eq. (7.5.5) are only upper bounds and it would be interesting to analyze these cases in more detail and also to analyze interpolation between several norms instead of just two."
    }, {
      "heading" : "7.6 Detailed Proofs",
      "text" : "Proof of Lemma 63. Let A : ⋃ n∈N Xn 7→ H be any proper learning algorithm for the linear learning problem specified by instance set Zlin(X ). One can use this learning algorithm to construct a new gradient-based learning algorithm AO 1st : ⋃ n∈N Xn 7→ H as follows, for any t ∈ N and any x1, . . . ,xt ∈ X ,\nAO 1st (z1, . . . , zt) = A(∇`(h1, z1), . . . ,∇`(ht, zt)) .\nThat is, at the end of each round the learning algorithm for linear problem is fed with a sub-gradient of the\nloss at the hypothesis ht played on that round t and thus ht+1 is selected using this algorithm. Note that for\n136\nany h? ∈ H, by convexity of the loss function,\n1 n n∑ t=1 ` ( AO 1st (∇`(h1, z1), . . . ,∇`(ht−1, zt−1)) , zt ) − 1 n n∑ t=1 ` (h?, zt)\n≤ 1 n n∑ t=1 〈 ∇` ( AO 1st (∇`(h1, z1), . . . ,∇`(ht−1, zt−1)) , zt ) ,AO 1st (∇`(h1, z1), . . . ,∇`(ht−1, zt−1))− h? 〉\n= 1\nn n∑ t=1 〈 x∗t ,A ( x∗1, . . . ,x ∗ t−1 ) − h? 〉 ≤ 1 n n∑ t=1 〈 x∗t ,A ( x∗1, . . . ,x ∗ t−1 )〉 − inf h∈H 1 n n∑ t=1 〈x∗t ,h〉\n≤ sup x1,...,xn∈X\n{ 1\nn n∑ t=1 〈xt,A (x1, . . . ,xt−1)〉 − inf h∈H 1 n n∑ t=1 〈xt,h〉\n}\nwhere in the above we used the notation x∗t = ∇`(ht, zt) Thus we can conclude that the new first-order oracle-based learning algorithm enjoys the same regret guarantee as the algorithm A enjoys on linear learning\nproblems. From this we conclude the lemma statement.\nProof of Corollary 64. First by Lemma 63 we have that for any algorithm A for linear learning problem, there exists an oracle based learning algorithm AO 1st such that\nsup z1,...,zn∈Z\nRn(A O1st , z1:n) ≤ sup\nz∗1 ,...,zn∈Zlin Rn(A, z\n∗ 1:n)\nand so\nVn(H,Z) ≤ Vn(H,Zlin(X ))\nAs for the set of equalities for specific classes, Zsup, ZLip and Zlin, note that since each of these classes are such that sub-gradients of losses belong to X , we have that\nVn(H,Zsup(X )) ≤ Vn(H,ZLip(X )) ≤ Vn(H,Zlin(X ))\nOn the other hand, Zlin(X ) ⊂ ZLip(X ) and so Vn(H,ZLip(X )) ≥ Vn(H,Zlin(X )) and hence we can conclude that\nVn(H,ZLip(X )) = Vn(H,Zlin(X )) .\nSimilarly for the supervised learning problem specified by instance set Zsup, note that if adversary always\n137\npick targets yt = −b = − suph∈H,x∈X 〈h,x〉 then for any x1, . . . ,xn ∈ X ,\n1 n n∑ t=1 |〈xt,ht〉 − yt| − inf h∈H 1 n n∑ t=1 |〈xt,h〉 − yt| = 1 n n∑ t=1 〈xt,ht〉+ b− inf h∈H 1 n ( n∑ t=1 〈xt,h〉+ b )\n= 1\nn n∑ t=1 〈xt,ht〉 − inf h∈H 1 n n∑ t=1 〈xt,h〉\nThus we can conclude that Vn(H,Zsup(X )) ≥ Vn(H,Zlin(X )) and so, Vn(H,Zsup(X )) = Vn(H,Zlin(X )). Thus we conclude the corollary.\nLemma 77. Let 1 < p ≤ 2 and C > 0 be fixed constants, the following statements are equivalent :\n1. For all B?-valued tree x of infinite depth and any x0 ∈ B?:\nsup n\nE [∥∥∥∥∥x0 + n∑ i=1 ixi( ) ∥∥∥∥∥ p\nH?\n] ≤ Cp ‖x0‖pX +∑ n≥1 E [‖xn( )‖pX ]  2. There exist a non-negative convex function Ψ on B with Ψ(0) = 0, that is q-uniformly convex w.r.t.\nnorm ‖·‖X? and for any h ∈ B, 1 q ‖h‖ q X? ≤ Ψ(h) ≤ Cq q ‖h‖ q H.\nProof. For any x ∈ B? define Ψ∗ : B? 7→ R as\nΨ∗(x) := sup   1 Cp sup n E [∥∥∥∥∥x + n∑ i=1 ixi( ) ∥∥∥∥∥ p\nH?\n] − ∑ i≥1 E [‖xi( )‖pX ]  where the supremum is over B?-valued tree x of infinite depth such that, sup\nn E [ ‖x + ∑n i=1 xi‖ p H? ] < ∞.\nSince supremum of convex functions is a convex function, it is easily verified that Ψ∗(·) is convex. Note that by the definition of M-type in Equation 7.2.1, we have that for any x0 ∈ B?, Ψ∗(x0) ≤ ‖x0‖pX . On the other hand, note that by considering the sequence of constant mappings, xi = 0 for all i ≥ 1, we get that for any x0 ∈ B?,\nΨ∗(x0) = sup   1 Cp sup n E [∥∥∥∥∥x0 + n∑ i=1 ixi( ) ∥∥∥∥∥ p\nH?\n] − ∑ i≥1 E [‖xi( )‖pX ]  ≥ 1Cp ‖x0‖pH? Thus we can conclude that for any x ∈ B?, 1Cp ‖x‖ p H? ≤ Ψ∗(x) ≤ ‖x‖ p X .\nFor any x0,y0 ∈ B?, by definition of Ψ∗(x0) and Ψ∗(y0), for any γ > 0, there exist B?-valued trees x and\n138\ny of infinite depth s.t. :\nΨ∗(x0) ≤  1 Cp sup n E [∥∥∥∥∥x0 + n∑ i=1 ixi( ) ∥∥∥∥∥ p\nH?\n] − ∑ i≥1 E [‖xi( )‖pX ] + γ and\nΨ∗(y (j) 0 ) ≤  1 Cp sup n E [∥∥∥∥∥y0 + n∑ i=1 iyi( ) ∥∥∥∥∥ p\nH?\n] − ∑ i≥1 E [‖yi( )‖pX ] + γ In fact in the above two inequalities if the supremum over n were achieved at some finite n0, by replacing the original sequence by one which is identical up to n0 and for any i > n0 using xi( ) = 0 (and similarly yi( ) = 0), we can in fact conclude that using these x’s and y’s instead,\nΨ∗(x0) ≤  1 Cp E ∥∥∥∥∥∥x0 + ∑ i≥1 ixi( ) ∥∥∥∥∥∥ p\nH?\n−∑ i≥1 E [‖xi( )‖pX ] + γ (7.6.1) and\nΨ∗(y (j) 0 ) ≤  1 Cp E ∥∥∥∥∥∥y0 + ∑ i≥1 iyi( ) ∥∥∥∥∥∥ p\nH?\n−∑ i≥1 E [‖yi( )‖pX ] + γ (7.6.2) Now consider a sequence formed by taking z0 = x0+y02 and further let\nz1 =\n( 1 + 0\n2\n) x0 − y0\n2 +\n( 1− 0\n2\n) y0 − x0\n2 = 0(x0 − y0)\nand for any i ≥ 2, define\nzi =\n( 1 + 0\n2\n) i−1xi−1( ) + ( 1− 0\n2\n) i−1yi−1( )\nwhere 0 ∈ {±1} is drawn uniformly at random. That is essentially at time i = 1 we flip a coin and decide\n139\nto go with tree x with probability 1/2 and y with probability 1/2. Clearly using the tree z, we have that, Ψ∗ ( x0 + y0\n2\n) = sup\nz   1 Cp sup n E [∥∥∥∥∥x0 + y02 + n∑ i=1 zi( 0, ) ∥∥∥∥∥ p\nH?\n] − ∑ i≥1 E [‖zi( 0, )‖pX ] 1/p  p\n≥ 1 Cp E ∥∥∥∥∥∥z0 + ∑ i≥1 zi( 0, ) ∥∥∥∥∥∥ p\nH?\n−∑ i≥1 E [‖zi( 0, )‖pX ]\n= 1\nCp\nE [∥∥∥x0 +∑i≥1 ixi( )∥∥∥pH?]+ E [∥∥∥y0 +∑i≥1 iyi( )∥∥∥pH?]\n2 − ∑ i≥1 E [‖zi( 0, )‖pX ]\n= 1\nCp\nE [∥∥∥x0 +∑i≥1 ixi( )∥∥∥pH?]+ E [∥∥∥y0 +∑i≥1 iyi( )∥∥∥pH?]\n2 − ∑ i≥1 E [‖zi( 0, )‖pX ]\n= 1\nCp\nE [∥∥∥x0 +∑i≥1 ixi( )∥∥∥pH?]+ E [∥∥∥y0 +∑i≥1 iyi( )∥∥∥pH?]\n2 − ∥∥∥∥x0 − y02 ∥∥∥∥p X − ∑ i≥2 E [‖zi( 0, )‖pX ]\n= 1\nCp\nE [∥∥∥x0 +∑i≥1 ixi( )∥∥∥pH?]+ E [∥∥∥y0 +∑i≥1 iyi( )∥∥∥pH?]\n2 − ∥∥∥∥x0 − y02 ∥∥∥∥p X\n− ∑ i≥1 E [‖xi( )‖pX ] + E [‖yi( )‖ p X ] 2\n=\n1 CpE ∥∥∥x0 +∑i≥1 ixi( )∥∥∥pH? −∑i≥1 E‖xi( )‖pX + 1CpE∥∥∥y0 +∑i≥1 iyi( )∥∥∥pH? −∑i≥1 E‖yi( )‖pX 2\n− ∥∥∥∥x0 − y02 ∥∥∥∥p X\n≥ Ψ ∗(x0) + Ψ ∗(y0) 2 − ∥∥∥∥x0 − y02 ∥∥∥∥p X − γ\nwhere the last step is obtained by using Equations 7.6.1 and 7.6.2. Since γ was arbitrary taking limit we\nconclude that for any x0 and y0,\nΨ∗(x0) + Ψ ∗(y0)\n2 ≤ Ψ∗\n( x0 + y0\n2\n) + ∥∥∥∥x0 − y02 ∥∥∥∥p X\nHence we have shown the existence of a convex function Ψ∗ that is p-uniformly smooth w.r.t. norm ‖·‖X such that 1Cp ‖·‖ p H? ≤ Ψ∗(·) ≤ ‖·‖ p X . Using convex duality we can conclude that the convex conjugate Ψ of function Ψ∗, is q-uniformly convex w.r.t. norm ‖ · ‖X? and is such that ‖·‖qX ≤ Ψ(·) ≤ Cq ‖·‖ q H. That 2 implies 1 can be easily verified using the smoothness property of Ψ∗.\nThe following sequence of four lemma’s give us the essentials towards proving Lemma 67. They use similar\n140\ntechniques as in [65].\nLemma 78. Let 1 < r ≤ 2. If there exists a constant D > 0 such that any x0 ∈ B? and any B?-valued tree x of infinite depth satisfies :\n∀n ∈ N, E [∥∥∥∥∥x0 + n∑ i=1 ixi( ) ∥∥∥∥∥ H? ] ≤ D(n+ 1)1/r sup 0≤i≤n sup ‖xi( )‖X\nthen for all p < r and αp = 20Dr−p we can conclude that any x0 ∈ B ? and any B?-valued tree x of infinite depth will satisfy :\nsup n\nE [∥∥∥∥∥x0 + n∑ i=1 ixi( ) ∥∥∥∥∥ H? ] ≤ αp sup ∑ i≥0 ‖xi( )‖pX 1/p\nProof. To begin with note that in the definition of type, if the supremum over n were achieved at some finite\nn0, then by replacing the original sequence by one which is identical up to n0 and then on for any i > n0 using xi( ) = 0 would only tighten the inequality. Hence it suffices to only consider such sequences. Further to prove the statement we only need to consider finite such sequences (ie. sequences such that there exists\nsome n so that for any i > n, xi = 0) and show that the inequality holds for every such n (every such sequence).\nRestricting ourselves to such finite sequences, we now use the shorthand, S = sup ( ∑n i=0 ‖xi( )‖ p X ) 1/p. Now define\nIk( ) = { i ≥ 0 ∣∣ S 2(k+1)/p < ‖xi( )‖X ≤ S2k/p } , T (k) 0 ( ) = inf{i ∈ Ik( )} and ∀m ∈ N, T (k)m ( ) = inf{i > T (k) m−1( ), i ∈ Ik( )}\nNote that for any ∈ {±1}N, Sp ≥ ∑ i∈Ik( ) ‖xi( )‖pX > Sp |Ik( )| 2(k+1)\nand so we get that sup |Ik( )| < 2k+1. From this we conclude that\n141\nE [∥∥∥∥∥x0 + n∑ i=1 ixi( ) ∥∥∥∥∥ H? ] ≤ ∑ k≥0 E ∥∥∥∥∥∥ ∑ i∈Ik( ) ixi( ) ∥∥∥∥∥∥ H?  = ∑ k≥0 E ∥∥∥∥∥∥ ∑ i≥0 T (k) i ( ) x T (k) i ( ) ∥∥∥∥∥∥ H?\n ≤ ∑ k≥0 ( D sup {|Ik( )|1/r} sup { sup i∈Ik( ) ‖xi( )‖X } )\n≤ ∑ k≥0\n( D 2(k+1)/r sup sup\ni∈Ik( ) ‖xi( )‖X ,∞\n)\n≤ ∑ k≥0 ( D 2(k+1)/r 2−k/pS ) = D 21/r\n∑ k≥0 2k( 1 r− 1 p ) S\n≤ 2D 1− 2( 1 r− 1 p ) S ≤ 2D 1− 2−(r−p)/4 S ≤ 12D r − p S\n= αp sup ( n∑ i=0 ‖xi( )‖pX )1/p\nLemma 79. Let 1 < r ≤ 2. If there exists a constant D > 0 such that any x0 ∈ B? and any B?-valued tree x of infinite depth satisfies :\n∀n ∈ N, E [∥∥∥∥∥x0 + n∑ i=1 ixi( ) ∥∥∥∥∥ H? ] ≤ D(n+ 1)1/r sup 0≤i≤n sup ‖xi( )‖X\nthen for any p < r, any x0 ∈ B? and any B?-valued tree x of infinite depth :\nP ( sup n ∥∥∥∥∥x0 + n∑ i=1 ixi( ) ∥∥∥∥∥ H? > c ) ≤ 2 (αp c )p/(p+1)‖x0‖pX +∑ i≥1 E [‖xi( )‖pX ] 1/(p+1)\nProof. For any x0 ∈ B? and B?-valued tree x of infinite depth define\nVn( ) = n∑ i=0 ‖xi( )‖pX\n142\nFor appropriate choice of a > 0 to be fixed later, define stopping time\nτ( ) = inf {n ≥ 0|Vn+1 > ap}\nNow for any c > 0 we have,\nP ( sup n ∥∥∥∥∥x0 + n∑ i=1 ixi( ) ∥∥∥∥∥ H? > c ) ≤ P(τ( ) <∞) + P ( τ( ) =∞, sup n ∥∥∥∥∥ n∑ i=0 ixi( ) ∥∥∥∥∥ H? > c )\n≤ P(τ( ) <∞) + P τ( ) > 0, sup n ∥∥∥∥∥∥x0 + n∧τ( )∑ i=1 ixi( ) ∥∥∥∥∥∥ H? > c  (7.6.3) As for the first term in the above equation note that\nP(τ( ) <∞) = P(sup n Vn > a\np) ≤ ‖x0‖pX +\n∑ i≥1 E [‖xi( )‖ p X ]\nap (7.6.4)\nTo consider the second term of Equation 7.6.3 we note that ( 1{τ( )>0}(x0 + ∑n∧τ( ) i=1 ixi( )) ) n≥0 is a valid\nmartingale (stopped process) and hence, (∥∥∥1{τ( )>0}(x0 +∑n∧τ( )i=1 ixi( ))∥∥∥H?)n≥0 is a sub-matingale. Hence by Doob’s inequality we conclude that,\nP T > 0, sup n ∥∥∥∥∥∥x0 + n∧τ( )∑ i=1 ixi( ) ∥∥∥∥∥∥ H? > c  ≤ 1 c sup n E ∥∥∥∥∥∥1{τ( )>0} x0 + n∧τ( )∑ i=1 ixi( ) ∥∥∥∥∥∥ H?  Applying conclusion of the previous lemma we get that\nP T > 0, sup n ∥∥∥∥∥∥x0 + n∧τ( )∑ i=1 ixi( ) ∥∥∥∥∥∥ H? > c  ≤ αp c sup 1{τ( )>0} ‖x0‖pX + τ( )∑ i=1 ‖xi( )‖pX 1/p\n≤ αp c (ap)1/p = αp a c\nPlugging the above and Equation 7.6.4 into Equation 7.6.3 we conclude that:\nP ( sup n ∥∥∥∥∥x0 + n∑ i=1 ixi( ) ∥∥∥∥∥ H? > c ) ≤ ‖x0‖pX + ∑ i≥1 E [‖xi( )‖ p X ] ap + αp a c\nUsing a = ( c αp ( ‖x0‖pX + ∑ i≥1 E [‖xi( )‖ p X ] ))1/(p+1) we conclude that\nP ( sup n ∥∥∥∥∥x0 + n∑ i=1 ixi( ) ∥∥∥∥∥ H? > c ) ≤ 2 (αp c )p/(p+1)‖x0‖pX +∑ i≥1 E [‖xi( )‖pX ] 1/(p+1)\nThis conclude the proof.\n143\nLemma 80. Let 1 < r ≤ 2. If there exists a constant D > 0 such that any x0 ∈ B? and any B?-valued tree x of infinite depth satisfies :\n∀n ∈ N, E [∥∥∥∥∥x0 + n∑ i=1 ixi( ) ∥∥∥∥∥ H? ] ≤ D(n+ 1)1/r sup 0≤i≤n sup ‖xi( )‖X\nthen for any p < r, any x0 ∈ B? and B?-valued tree x of infinite depth will satisfy :\nsup λ>0\nλp P ( sup n ∥∥∥∥∥x0 + n∑ i=1 ixi( ) ∥∥∥∥∥ H? > λ )\n≤ max 4 p+1p αp ‖x0‖pX +∑\ni≥1\nE [‖xi( )‖pX ]  1p , 22p+3 log(2) αpp ‖x0‖pX +∑\ni≥1\nE [‖xi( )‖pX ]  \nProof. We shall use Proposition 8.53 of Pisier’s notes which is restated below to prove this lemma. To this end consider any x0 ∈ B? and any B?-valued tree x of infinite depth. Given an ∈ {±1}N, for any j ∈ [M ] and i ∈ N let (j)i = (i−1)M+j . Let z0 = x0 M−1/p and define the sequence (zi)i≥1 as follows, for any k ∈ N given by k = j + (i− 1)M where j ∈ [M ] and i ∈ N,\nzk( ) = xi( (j)) M−1/p\nClearly,\n‖z0‖pX + ∑ k≥1 E [‖zk( )‖pX ] = ‖x0‖ p X + 1 M M∑ j=1 ∑ k≥1 E [∥∥∥xk( (j))∥∥∥p X ] = ‖x0‖pX +\n∑ i≥1 E [‖xi( )‖pX ]\nBy previous lemma we get that for any c > 0,\nP ( sup n ∥∥∥∥∥z0 + n∑ i=1 izi( ) ∥∥∥∥∥ H? > c ) ≤ 2 (αp c )p/(p+1)‖z0‖pX +∑ i≥1 E [‖zi( )‖pX ] 1/(p+1)\n= 2 (αp c )p/(p+1)‖x0‖pX +∑ i≥1 E [‖xi( )‖pX ] 1/(p+1)\nNote that\nsup n ∥∥∥∥∥z0 + n∑ i=1 izi( ) ∥∥∥∥∥ H? = M−1/p sup j∈[M ] sup n ∥∥∥∥∥x0 + n∑ i=1 (j) i xi( (j)) ∥∥∥∥∥ H?\n144\nHence we conclude that\nP ( sup j∈[M ] M−1/p sup n ∥∥∥∥∥x0 + n∑ i=1 (j) i xi( (j)) ∥∥∥∥∥ H? > c ) ≤ 2 (αp c ) p (p+1) ‖x0‖pX +∑ i≥1 E [‖xi( )‖pX ]  1(p+1) For any j ∈ [M ], defining Z(j) = supn ∥∥∥x0 +∑ni=1 (j)i xi( (j))∥∥∥H? and using Proposition 82 we conclude that for any c > 0,\nsup λ>0\nλp P ( sup n ∥∥∥∥∥x0 + n∑ i=1 ixi( ) ∥∥∥∥∥ H? > λ )\n≤ max c, 2cp log  1\n1− 2 (αp c ) p (p+1) ( ‖x0‖pX + ∑ i≥1 E [‖xi( )‖ p X ] ) 1 (p+1)\n \nPicking\nc = 4 p+1 p αp ‖x0‖pX +∑ i≥1 E [‖xi( )‖pX ] 1/p\nwe conclude that\nsup λ>0\nλp P ( sup n ∥∥∥∥∥x0 + n∑ i=1 ixi( ) ∥∥∥∥∥ H? > λ )\n≤ max 4 p+1p αp ‖x0‖pX +∑\ni≥1\nE [‖xi( )‖pX ]  1p , 22p+3 log(2) αpp ‖x0‖pX +∑\ni≥1\nE [‖xi( )‖pX ]  \nLemma 81. Let 1 < r ≤ 2. If there exists a constant D > 0 such that any x0 ∈ B? and any B?-valued tree x of infinite depth satisfies :\n∀n ∈ N, E [∥∥∥∥∥x0 + n∑ i=1 ixi( ) ∥∥∥∥∥ H? ] ≤ D(n+ 1)1/r sup 0≤i≤n sup ‖xi( )‖X\nthen for all p < r, we can conclude that any x0 ∈ B? and any B?-valued tree x of infinite depth will satisfy :\nsup n\nE [∥∥∥∥∥x0 + n∑ i=1 ixi( ) ∥∥∥∥∥ p\nH?\n] ≤ ( 1104 D\n(r − p)2 )p‖x0‖pX +∑ i≥1 E [‖xi( )‖pX ]  That is the pair (H,X ) is of martingale type p.\nProof. Given any p < r pick r > p′ > p, due to the homogeneity of the statement we need to prove, w.l.o.g.\n145\nwe can assume that\n‖x0‖p ′ X + ∑ i≥1 E [ ‖xi( )‖p ′ X ] = 1\nHence by previous lemma, we can conclude that\nsup λ>0\nλp ′ P ( sup n ∥∥∥∥∥x0 + n∑ i=1 ixi( ) ∥∥∥∥∥ H? > λ ) ≤ p′22p ′+3 log(2) αp ′ p′ ≤ (32 αp′) p′ (7.6.5)\nHence,\nE [ sup n ∥∥∥∥∥x0 + n∑ i=1 ixi( ) ∥∥∥∥∥ p\nH?\n] ≤ inf a>0 { ap ′ + p ∫ ∞ a λp−1P ( sup n ∥∥∥∥∥x0 + n∑ i=1 ixi( ) ∥∥∥∥∥ H? > λ ) dλ }\n≤ inf a>0\n{ ap + p(32 αp′) p′ ∫ ∞ a λp−1−p ′ dλ } ≤ inf a>0 { ap + p(32 αp′) p′ [ λp−p ′ p− p′ ]∞ a }\n≤ inf a>0\n{ ap + (46 αp′) p′ a p−p′\np′ − p\n}\n= 2 (46 αp′)\np\n(p′ − p)p/p′ ≤ 2 (46 αp)\np\n(p′ − p)p/p′\nSince ‖x0‖p ′ X + ∑ i≥1 E [ ‖xi( )‖p ′ X ] = 1 and p′ > p, we can conclude that ‖x0‖pX + ∑ i≥1 E [‖xi( )‖ p X ] ≥ 1 and so\nE [ sup n ∥∥∥∥∥x0 + n∑ i=1 ixi( ) ∥∥∥∥∥ p\nH?\n] ≤ 2 (46 αp) p\n(p′ − p)p/p′ ‖x0‖pX +∑ i≥1 E [‖xi( )‖pX ]  ≤ 2(46 αp) p\n(p′ − p) ‖x0‖pX +∑ i≥1 E [‖xi( )‖pX ]  Since p′ can be chosen arbitrarily close to r, taking the limit we can conclude that\nE [ sup n ∥∥∥∥∥x0 + n∑ i=1 ixi( ) ∥∥∥∥∥ p\nH?\n] ≤ 2(46 αp) p\n(r − p) ‖x0‖pX +∑ i≥1 E [‖xi( )‖pX ] \n146\nRecalling that αp = 12Dr−p we conclude that\nE [ sup n ∥∥∥∥∥x0 + n∑ i=1 ixi( ) ∥∥∥∥∥ p\nH?\n] ≤ (\n1104 D\n(r − p)(p+1)/p )p‖x0‖pX +∑ i≥1 E [‖xi( )‖pX ]  ≤ ( 1104 D\n(r − p)2 )p‖x0‖pX +∑ i≥1 E [‖xi( )‖pX ]  This concludes the proof.\nWe restate below a proposition from Pisier’s note (in [66])\nProposition 82 (Proposition 8.53 of [66]). Consider a random variable Z ≥ 0 and a sequence Z(1), Z(2), . . . drawn iid from some distribution. For some 0 < p <∞, 0 < δ < 1 and R > 0,\nsup M≥1\nP (\nsup m≤M\nM−1/pZ(m) > R ) ≤ δ =⇒ sup\nλ>0 λp P (Z > λ) ≤ max\n{ R, 2Rp log ( 1\n1− δ\n)}\nProof of Lemma 67. By Theorem 66 and our assumption that Vn(H,X ) ≤ Dn−(1−1/r), we have that for any B?-valued tree x of infinite depth and any n ≥ 1,\nE\n[ 1\nn ∥∥∥∥∥ n∑ i=1 ixi( ) ∥∥∥∥∥ H? ] ≤ Dn−(1− 1 r )\nHence we can conclude for any B?-valued tree x of infinite depth and any n ≥ 1,\nE [∥∥∥∥∥ n∑ i=1 ixi( ) ∥∥∥∥∥ H? ] ≤ Dn 1r sup 1≤i≤n sup ‖xi( )‖X\nHence for any x0 ∈ B?, we have that\nE [∥∥∥∥∥x0 + n∑ i=1 ixi( ) ∥∥∥∥∥ H? ] ≤ E [∥∥∥∥∥ n∑ i=1 ixi( ) ∥∥∥∥∥ H? ] + ‖x0‖H?\n≤ E [∥∥∥∥∥ n∑ i=1 ixi( ) ∥∥∥∥∥ H? ] +D ‖x0‖X ≤ Dn 1r sup 1≤i≤n sup ‖xi( )‖X +D ‖x0‖X ≤ 2D(n+ 1) 1r sup 0≤i≤n sup ‖xi( )‖X\nNow applying Lemma 81 with s = p completes the proof.\n147\nProof of Lemma 72. We now show that the bound got by the Mirror Descent algorithm is tight. First assume without loss of generality that supx∈X ,h∈H 〈h,x〉 = 1. Consider the 1-smooth convex loss function.\nφ(z, y) =\n{ |z − y| − 14 if |z − y| > 1 2\n(z − y)2 otherwise\nThis is basically a smoothed version of the absolute loss. Consider the smooth convex objective\n`(h, (x, y)) = φ(〈h,x〉 , y)\ngiven by instances x ∈ X and y ∈ [−1, 1]. Now before we proceed we recall from [5] that the value of the online learning game is equal to :\nVn(H,X )\n= sup p1 E (x1,y1)∼p1 . . . sup pn E (xn,yn)∼pn\n[ 1\nn n∑ t=1 inf ht∈H̄ E (xt,yt)∼pt [φ(〈ht,xt〉 , yt)]− inf h∈H 1 n n∑ t=1 φ(〈h,xt〉 , yt)\n]\n≥ E (x1,y1)∼p∗1 . . . E (xn,yn)∼p∗n\n[ 1\nn n∑ t=1 inf ht∈H̄ E (xt,yt)∼p∗t [φ(〈ht,xt〉 , yt)]− inf h∈H 1 n n∑ t=1 φ(〈h,xt〉 , yt)\n] (7.6.6)\nWhere p∗1, . . . , p ∗ n is the distribution on X × [−1, 1] described as follows :\n• For the first n−m rounds, p∗t deterministically sets xt = 0 and yt = 0.\n• For the remaining m rounds, that is for t > n − m, we first consider a B?-valued tree u of infinite depth. The distribution p∗t picks yt = t−n+m where each i ∼ Unif{±1} are Rademacher random variables. xt is chosen by setting xt = ut−n−m( 1, . . . , t−n−m−1).\nNow notice that on the first n−m round any algorithm suffers no regret. Using this particular distribution in the lower bound in Equation 7.6.6 we get that :\nVn(H,X ) ≥ 1\nn E(x1,y1)∼p∗1 . . .E(xn,yn)∼p∗n\n[ n∑\nt=n−m+1 inf ht∈H E(xt,yt)∼p∗t [φ(〈ht,xt〉 , yt)]− infh∈H n∑ t=n−m+1 φ(〈h,xt〉 , yt)\n]\nNow we shall select the tree u such that for any n and any ∈ {±1}n−1, ‖un( )‖X ≤ 12 . Hence we see that for any choice of h ∈ H, and any t > n−m, | 〈h,xt〉 | ≤ 12 . Hence we can conclude that for any t < n−m, and any h ∈ H,\nφ(〈h,xt〉 , yt) = | 〈h,xt〉 − yt| − 1\n4 =\n3 4 − yt 〈h,xt〉\nFurther notice that for any t > n−m, since yt ∼ Unif{±1} we have that,\nE(xt,yt) [φ(〈h,xt〉 , yt)] = 3\n4\n148\nPlugging this in the lower bound lower bound to the value, we get that\nVn(H,X ) ≥ 1\nn E(x1,y1)∼p∗1 . . .E(xn,yn)∼p∗n [ sup h∈H n∑ t=n−m+1 ( 3 4 − φ(〈h,xt〉 , yt) )]\n= 1\nn E(x1,y1)∼p∗1 . . .E(xn,yn)∼p∗n [ sup h∈H n∑ t=n−m+1 ( 3 4 − ( 3 4 − yt 〈h,xt〉 ))]\n= 1\nn E(x1,y1)∼p∗1 . . .E(xn,yn)∼p∗n [ sup h∈H n∑ t=n−m+1 ( t−n+m 〈h,xt〉) ]\n= 1\nn E [ sup h∈H 〈 h, m∑ i=1 iui( ) 〉]\n= 1\nn E [∥∥∥∥∥ m∑ i=1 iui( ) ∥∥∥∥∥ H ]\nSpecifically picking m = L∗n concludes the proof.\nProof of Lemma 74. Consider the functions of form `(h, zt) = 〈xt,h〉 + σψ(h). Recall from [5] that the value of the online learning game is equal to :\nV(H,Zucvx(σ,q′)(X )) = sup p1 Ex1∼p1 . . . sup pn Exn∼pn\n[ 1\nn n∑ t=1 inf ht∈H Ext∼pt [`(ht, zt)]− inf h∈H 1 n n∑ t=1 ft(h)\n]\n≥ Ex1∼p∗1 . . .Exn∼p∗n\n[ 1\nn n∑ t=1 inf ht∈H Ext∼p∗t [`(ht, zt)]− infh∈H 1 n n∑ t=1 `(h, zt)\n]\n= Ex1∼p∗1 . . .Exn∼p∗n\n[ 1\nn n∑ t=1 inf ht∈H Ext∼p∗t [〈ht,xt〉] + σψ(ht)− infh∈H\n{ 1\nn n∑ t=1 〈h,xt〉+ σψ(h)\n}]\nWhere p∗1, . . . , p ∗ n is a distribution on X specified as follows : We consider a X -valued tree u of infinite depth and the distribution p∗t picks xt = tut( ) where each t ∼ Unif{±1} are Rademacher random variables.\n149\nHence we conclude that\nV(H,Zucvx(σ,q′)(X )) ≥ E\n[ 1\nn n∑ t=1 inf ht∈H E t [ t 〈ht, tut( )〉] + σψ(ht)− inf h∈H\n{ 1\nn n∑ t=1 〈h, tut( )〉+ σψ(h)\n}]\n≥ E [ sup h∈H {〈 h,− 1 n n∑ t=1 tut( ) 〉 − σψ(h) }]\n≥ E [ sup h∈H {〈 h,− 1 n n∑ t=1 tut( ) 〉 − Dq ′ p′σ q′ ‖h‖q ′ H }]\n= 1\np′ σp′−1Dp ′\np′\nE ∥∥∥∥∥ 1n n∑ t=1 tut( ) ∥∥∥∥∥ p′\nH?  ≥ 1 p′ σp′−1Dp ′\np′\n( E [∥∥∥∥∥ 1n n∑ t=1 tut( ) ∥∥∥∥∥ H? ])p′\nSince choice of X -valued the tree u is arbitrary we can take supremum over all such trees which concludes the proof."
    }, {
      "heading" : "7.7 Discussion",
      "text" : "We first note that using mirror descent with uniformly convex function (as opposed to strongly convex) is not new and has been used in optimization setting in [1]. The key result of this chapter is to establish universality and near optimality of mirror descent for online learning problems. As we showed it is even optimal for smooth learning problems and some uniformly convex learning problems. While the classic definition of martingale type and the associated results are for dual pairs, in this chapter we extended results by [65] to handle non-dual scenario. Also the proofs have been slightly modified to obtain right dependence on the constants since these constants could be dimension dependent and hence its important to keep track of them.\n150\nChapter 8\nOptimality of Mirror Descent for Statistical Convex Learning Problems\nIn the previous chapter we considered convex learning problems in the online learning framework and showed near optimality of mirror descent algorithm for learning rates for online convex learning problems. In this chapter we consider these convex learning problems in statistical learning framework. We show that in fact for most commonly occurring convex learning problems, stochastic mirror descent is in fact near optimal in terms of both sample complexity and efficiency.\nTo mirror the notation and analysis in previous chapter, given any target hypothesisH and any instance class Z let us define\nV iidn (H,Z) := inf A sup D∈∆(Z)\nES∼Dn [ LD(A(S))− inf\nh∈H LD(h)\n] .\nHowever unlike the online learning setting for the infimum over the learning algorithm above we shall only consider proper learning algorithms (and as mentioned earlier it suffices to only consider deterministic algorithms), that is H̄ = H. Notice that V iidn (H,Z) is essentially the same as term cons(n) we introduced in chapter 3. We introduce this notation to mirror the results in previous chapter and because in this chapter the sets H and Z we are referring to is rather important and so we prefer to explicitly show this. Also as in the previous chapter the linear learning problem is central to obtaining many of the lower bounds and throughout this chapter we shall use the notation\nFlin(H,X ) = {x 7→ 〈h,x〉 : h ∈ H}\nto represent the linear loss class Flin(H,X ) ⊂ RX .\nIn Section 8.1 we provide lower bounds on learning rates for various convex learning problems (including for smooth losses) using the statistical Rademacher complexity of the linear class Flin(H,X ). While all the lower bounds are provided based on statistical Rademacher complexity of the linear function class, in\n151\nSection 8.2, analogous to previous chapter we show that the concept of Rademacher type and the Rademacher Complexity of the linear function class are closely related. In chapter 6 we described the stochastic mirror descent algorithm and provided guarantees for it for various statistical convex learning problems. As we saw in the previous chapter these rates are characterized by Martingale type. However the lower bounds in Section 8.2 are in terms of statistical Rademacher complexity or equivalently characterized by Rademacher type. In general, it turns out that Rademacher type and Martingale types do not match and so in general the upper bound on performance of stochastic mirror descent do not match lower bound. However in Section 8.3 we show that for most reasonable cases, the concepts of Rademacher type and Martingale types do in fact match and so we use this result to argue that stochastic mirror descent is near optimal even for statistical learning problems. In the same section we also show that stochastic mirror descent algorithm is optimal also in terms of number of gradient access and for certain supervised learning problems, optimal in terms of computational complexity. In the succeeding section 8.4 we review the examples we saw in the previous chapter to see how mirror descent is indeed optimal. Section 8.5 provides detailed proof of the results in this chapter and we finally conclude the chapter with some discussions in Section 8.6."
    }, {
      "heading" : "8.1 Lower Bounds for Statistical Learning Rates",
      "text" : "We would like to point out here that linear instance class Zlin is a subclass of the Lipschitz class ZLip. Hence any lower bound on learning rates for linear class is also a lower bound for the convex Lipschitz class. Similarly, if we consider the supervised learning class with label y always being b = − suph∈H,x∈X 〈h,x〉 then we see that `(h; (x, y)) = 〈h,x〉 − b. Hence we see that in excess risk for any distribution D on x’s with y being deterministically set to b is same as excess risk of linear class with distribution D on x’s. Hence we see that lower bound on linear class is also a lower bound on supervised learning class. The following proposition formalizes this.\nProposition 83. For any hypothesis setH ⊂ B, any X ⊂ B? and > 0\nV iidn (H,ZLip(X )) ≥ V iidn (H,Zlin(X )) and Vn(H,Zsup(X )) ≥ Vn(H,Zlin(X )) .\nOwing to the fact that optimal learning rates for the linear class provide lower bounds for convex lipschitz and supervised convex learning problems, we define for each p ∈ [1, 2] the constant V iidp analogous to the definition of Vp in the previous chapter.\nV iidp := inf { V ∣∣∣ ∀n ∈ N,V iidn (H,Zlin(X )) ≤ V n−(1− 1p )} (8.1.1)\nIn the previous chapter we saw that the value of the linear game was closely related to the sequential Rademacher complexity of the linear class, Rseqn (Flin(H,X )). We will see an analogous relationship in the statistical setting with the worst case statistical Rademacher complexity. Recall that for the linear class\n152\nspecified by setsH ⊂ B and X ⊂ B?, the worst case statistical Rademacher complexity is given by\nRiidn (Flin(H,X )) = sup x1,...,xn∈X E [ sup h∈H 1 n n∑ i=1 i 〈h,xi〉 ] = sup x1,...,xn∈X E [∥∥∥∥∥ 1n n∑ i=1 ixi ∥∥∥∥∥ H? ]\nwhere ∈ {±1}n are iid Rademacher random variables. In the following lemma we show that the learning rate of the linear class can in turn be bounded by the worst case statistical Rademacher complexity of the linear class.\nLemma 84. For anyH ⊂ B and X ⊂ B?,\nV iidn (H,Zlin(X )) ≥ Riid2n(Flin(H,X ))− 1\n2 Riidn (Flin(H,X ))\nwhere n = |S|.\nWhile the above lemma along Proposition 83 lower bounds learning rates for convex lipschitz, supervised and linear problems by Rademacher complexity of linear class, for linear and supervised learning problems we can also show that Rademacher complexity of linear class can be used to upper bound the optimal learning rates. The following proposition which is a direct consequence of results from [38] show that the learning rates for linear and supervised convex learning problems are upper bounded by the statistical Rademacher complexity.\nProposition 85. [38] For any set X ∈ B? if Z(X ) is one of either Zlin(X ) or Zsup(X ), then for any n ∈ N,\nsup D∈∆(Z(X ))\nES∼Dn [ LD(AERM(S))− inf\nh∈H LD(h)\n] ≤ 2Riidn (Flin(X ))\nand hence : V iidn (H,Z(X )) ≤ 2Riidn (Flin(H,X )) .\nProof. The inequality of linear class is a direct consequence of symmetrization (see [38]). The inequality for the supervised learning class Zsup(X ) additionally uses the Lipschitz contraction property (Theorem 10 (4) of [38] ) since the absolute loss is 1-Lipschitz.\nWhile the above proposition bounds the learning rates of linear and supervised convex learning classes using Rademacher complexity of the linear class we don’t know if such a result is true for the convex Lipschitz class (unlike the online case).\nWhile Lemma 84 provides lower bound on learning rates of linear learning problems in terms of statistical Rademacher complexity of linear class, the RHS there involves the difference of two Rademacher complexity terms. The following theorem shows that learning rates for these convex learning problems provide direct upper bounds on statistical Rademacher complexity of the linear class that captures all polynomial dependences right.\n153\nTheorem 86. Given any target hypothesis set H ⊂ B and instance space X ⊂ B?, let Z(X ) be one of ZLip(X ), Zsup(X ) or Zlin(X ). If for some q ∈ [2,∞) and V > 0,\nV iidn (H,Z(X )) ≤ V\nn1/q\nthen,\n∀n ∈ N, Riid2n(Flin(H,X )) ≤ 5V\n(2n) 1 q\n."
    }, {
      "heading" : "8.1.1 Lower Bounds for Smooth Losses",
      "text" : "The following lemma lower bounds learning rate for non-negative smooth convex learning problems and also captures dependence on expected loss of the optimal hypothesis in target class H. This result is an analog to Lemma 72 of previous chapter.\nLemma 87. Given H ⊂ B, X ⊂ B? and L∗ ∈ (0, 3/4] for any learning algorithm A, there exists a distribution D over instances in Zsmt(1)(X ) s.t. infh∈H LD(h) ≤ L∗ and\nES [ LD(A(S))− inf\nh∈H LD(h)\n] ≥ L ∗\n4\n( RiidnL∗(Flin(H,X ))− 1\n2 RiidnL∗(Flin(H,X ))\n)"
    }, {
      "heading" : "8.2 Optimal Rates and Rademacher Type",
      "text" : "In this section we extend the classic notion of Rademacher type of a Banach space (see for instance [74]) to one that accounts for the pair (H?,X ). The results of this section are analogous to the results in Section 7.2.\nDefinition 33. A pair (H?,X ) of subsets of a vector space B? is said to be of Rademacher type p if there exists a constant C ≥ 1 such that for any n ∈ N and any x1,x2, . . . ,xn ∈ B? :\nE [∥∥∥∥∥ n∑ i=1 ixi ∥∥∥∥∥ p\nH?\n] ≤ Cp ( n∑ i=1 ‖xi‖pX ) (8.2.1)\nIt can be shown that rate of convergence of i.i.d. random variables in Banach spaces is governed by the above notion of Rademacher type of the associated Banach space. We point the reader to [66] for more details. Further, for any p ∈ [1, 2] we also define constant C iidp , analogous to the definition of Cp in previous chapter.\nC iidp := inf { C ∣∣∣∣∣ ∀n ∈ N,∀x1, . . . ,xn ∈ B?, E [∥∥∥∥∥ n∑ i=1 ixi ∥∥∥∥∥ p\nH?\n] ≤ Cp ( n∑ i=1 ‖xn‖pX )}\nC iidp is useful in determining if the pair (H?,X ) has Rademacher type p.\nThe following lemma is an analog to Lemma 67 of previous section.\n154\nLemma 88. If for some r ∈ (1, 2] there exists a constant D > 0 such that for any n,\nRiidn (Flin(H,X )) = sup x1,...,xn∈X E [∥∥∥∥∥ 1n n∑ i=1 ixi ∥∥∥∥∥ H? ] ≤ Dn−(1− 1r )\nthen for all p < r, we can conclude that for any x1, . . . ,xn ∈ B? :\nE [∥∥∥∥∥ n∑ i=1 ixi ∥∥∥∥∥ p\nH?\n] ≤ ( 12 √ 2 D\n(r − p) )p( n∑ i=1 ‖xi‖pX )\nThat is, the pair (H?,X ) is of Rademacher type p.\nCorollary 89. Given any pair (H?,X ) and any p and p′ < p, we have that\n(p−p′) 60 √ 2 C iidp′ ≤ V iidp ≤ 2C iidp\nProof. Owing to Proposition 85 and definition of V iidp using Jensen’s inequality, we get that V iid p ≤ 2C iidp . The second inequality is a consequence of using the above Lemma 88 with Theorem 86 and the definitions of C iidp and V iid p .\nThe following figure summarizes the relationship between V iidp and C iid p . The arrow mark from C iid p′ to C iid p indicates that for any n, the quantities are within log n factor of each other."
    }, {
      "heading" : "8.3 Main Result : Optimality of Stochastic Mirror Descent",
      "text" : "In Section 8.1 we provided lower bounds on learning rates of convex learning problems in the statistical learning framework. In previous chapter we provided upper bounds on learning rate of stochastic mirror descent algorithm for statistical convex learning problems. A natural question that arises is whether the stochastic mirror descent algorithm is optimal in terms of learning rates or efficiency or both and whether they match the lower bounds obtained.\nIt turns out that in general even for convex learning problems, this is not true. A problem could be statistically learnable but not online learnable. However in this section we will see that for large class of convex learning problems (in fact most of the commonly occurring ones) it is true that online methods are near optimal for statistical convex learning problems in terms of both rates and efficiency. In the previous we saw that mirror descent algorithm was near optimal and the key to showing this result is depicted in Figure 7.1. In this\n155\nchapter again in Section 8.2 we obtained an analogous result depicted in Figure 8.1. The attractive feature in the online learning scenario was that optimal learning rates and hence concept of martingale type was closely connected to existence of appropriate uniformly convex function which then we could use with mirror descent to get near optimal guarantees. In the statistical case we only have the connection between optimal rates (or at least lower bounds) and the concept of Rademacher type. By the definitions of Cp and C iidp , it is easy to see that C iidp ≤ Cp and due to online to batch conversion we can also infer that V iidp ≤ Vp. However assume that we could get a reverse bound, this would then imply that online learning methods are optimal at least up to logarithmic factors. Such optimality of learning rate is essentially captured in the following theorem.\nTheorem 90. Let Z(X ) stand for one of Zlin(X ), Zsup(X ) or ZLip(X ). If there exists some constant G ≥ 1 such that for any 1 < p′ < r ≤ 2\nCp′ ≤ G C iidr\nand it is true that for some V > 0 and p ∈ (1,∞),\n∀n ∈ N, V iidn (H,Z(X )) ≤ V\nn1/q\nthen there exists function Ψ and step size η using which the stochastic mirror descent algorithm enjoys the\nlearning guarantee\n∀n ∈ N, sup D∈∆(Z(X ))\nES∼Dn [ LD(AMD(S))− inf\nh∈H LD(h)\n] ≤ 30010GV log 3 n\nn1/q\nProof. Using Theorem 86 we first get a bound on Riidn (Flin). Next using Corollary 89 we see that for any p′ < p, C iidp′ ≤ 60 √ 2V p−p′ . Using the assumption that Cp ≤ G C iid p we get bound on Cp′ and from here on using exactly the same proof as that of theorem 71 we get the bound on the regret. To convert it to bound on excess\nrisk we can use Proposition 58.\nEven for the smooth loss case one can show that if for any 1 < p′ < r ≤ 2 Cp′ ≤ G C iidr then online mirror descent is near optimal as the following theorem shows.\nTheorem 91. If there exists some constant G ≥ 1 such that for any 1 < p′ < r ≤ 2\nCp′ ≤ G C iidr\nand it is true that for some V > 0 and p ∈ (1,∞),\n∀n ∈ N, V iidn (H,Zsmt(1)(X )) ≤ V\nn1/q ,\nthen there exists function Ψ and step size η using which the mirror descent algorithm followed by online to batch conversion technique, that is the algorithm AMD, enjoys the guarantee that for any L∗ ∈ (0, 3/4] and\n156\nany distribution D ∈ ∆(Zsmt(H)(X )) s.t. infh∈H LD(h) ≤ L∗,\nES∼Dn [ LD(AMD(S))− inf\nh∈H LD(h)\n] ≤ 240080 G V √ HL∗ log3 n\nn1/q +\n(189800 G V )2H log6 n\nn2/q\nProof. Again the proof is similar to previous two proofs. The main difference is that at the very first step we use Lemma 87 with L∗ = 3/4 to bound Riidn (Flin). Also in the step before last (ie. just before the online to batch conversion), instead of using Theorem 71 we instead use Theorem 73. Also after the online to batch\nstep note that\nES∼Dn [\ninf h∈H\n1 n n∑ t=1 `(h, zt)\n] ≤ inf\nh∈H LD(h) ≤ L∗\nAlso note that the discussion after Theorem 73 in the previous chapter for the online setting also applies here and specifically when q = 2 we also get tightness w.r.t. dependence in L∗\nOwing to the above two theorems the main condition we are now looking to satisfy is that there exists some constant G ≥ 1 such that for any 1 < p′ < r ≤ 2\nCp′ ≤ G C iidr\nRemark 92. In some cases we might only be able to prove that there exists some constant G ≥ 1 such that for any 1 < p′ < r ≤ 2\nCp′ ≤ G\nr − p′ C iidr\nsuch an inequality is also fine and basically all the above three theorems still hold with the only modification\nthat the logarithmic factors in the theorems increase by one more power.\nIn the following subsections we will show that for a large class of spaces such G exists (or owing to above remark with additional (r − p′)−1 factor). From this we can infer that mirror descent is near optimal for convex learning optimization problems in these spaces."
    }, {
      "heading" : "8.3.1 Banach Lattices",
      "text" : "In this subsection we show that if the Banach space specified by norm ‖·‖H? is a Banach lattice, then one can relate martingale type and Rademacher type constants Cp and C iidp to within constant factor of each other.\nDefinition 34 (Banach Lattice [75]). A partially ordered Banach space is called a Banach lattice provided :\n1. For any x,x′, x̃ ∈ B?, x x′ implies that x + x̃ x′ + x̃\n2. ax 0 for every x 0 and non-negative scalar a.\n157\n3. For all x,x′ ∈ B?, there exists a least upper bound (l.u.b.) represented by x ∨ x′.\n4. For any x,x′ ∈ B? such that |x| |x′|, we have that ‖x‖ ≤ ‖x′‖ (where the absolute value |x| is defines as |x| = x ∨ (−x)).\nBefore we proceed we notice that all `p spaces with partial order given by, x y if anf only if on each co-ordinate i, xi ≤ yi is a Banach lattice. In fact all the examples we saw in previous chapters were Banach lattices under appropriate partial order. In fact one could safely say that most Banach spaces one would encounter in machine learning applications would be Banach lattices. Hence the results in this section are very general from a practical view-point.\nNow we introduce the notation that for any p ∈ [1,∞), we will use ( ∑n i=1 |xi|p) 1/p to represent the vector,\n( n∑ i=1 |xi|p )1/p := l.u.b. { n∑ i=1 aixi ∣∣∣∣∣(a1, . . . , an) ∈ R, n∑ i=1 |ai|q ≤ 1 }\nwhere q = pp−1 . Notice that if xi’s were reals this would we simply the `p norm. Here it is a vector though. The main technology behind proving results about Banach lattices arises from the so called functional calculus over banach lattices introduced by Krivine [76] (See [75]). The basic idea is a theorem (theorem 1.d.1 in [75]) which roughly states that if we prove any inequality involving continuous degree 1, homogenous equations involving finite number of real valued variables, then the same inequality is true with of course appropriate changes like ≤ replaced by and absolute value replaced by the lattice version and so on.\nAt first glance the statements we would like to prove would involve expectation over Rademacher random variables and trees (of depth n). However simple observation that expectation over Rademacher are finite averages and that the tree of depth n (involving the n mappings) can be expanded to the 2n − 1 variables involved we see automatically that this general technology can be used to prove results that involve trees and expectations w.r.t. ’s too. Detailed proof and associated definitions are provided in Section 8.5. We delineate the main results below.\nBefore we proceed we first define below the notion of co-type of a Banach space (again extended to the non-dual case, see ?? for details about classical definition).\nDefinition 35. A pair (H?,X ) of subsets of a vector space B? is said to be of Rademacher co-type q if there exists a constant C ≥ 1 such that for any x1,x2, . . . ,xn ∈ B? :(\nn∑ i=1 ‖xi‖qX\n) ≤ CqE [∥∥∥∥∥ n∑ i=1 ixi ∥∥∥∥∥ q\nH?\n] (8.3.1)\nThe following lemma shows that if H? formed a Banach lattice and were of some finite co-type r then for any p, Cp can be bounded by a constant factor times C iidp .\nLemma 93. If (B?, ‖·‖?H) is a Banach lattice and the pair (H?,H?) is of co-type r for some r ∈ [2,∞) with\n158\nsome constant C̃r, then for any p ∈ (1, 2] we have that\nCp ≤ 723r2C̃r√ p− 1 C iidp\nThe above Lemma shows that as long as H? has a lattice structure and is of finite co-type, G is bounded by 723r2C̃r√\np−1 and so for any such case owing to the theorems in the beginning of this section we automatically can conclude optimality of mirror descent.\nDual Learning Problem :\nIn Lemma 93 we needed thatH? had finite co-type. Though this is a fairly weak condition we still needed to verify that this was satisfied before we used the result. However if we consider the dual learning problem, that is the case when X is the dual ball of H then it turns out that this condition is automatically satisfied as long as the problem is statistically learnable (if it is not we anyway are not in a position to give any meaningful bounds). This is due to the celebrated result of Maurey and Pisier [77] which (in the dual case) assures that any space with non-trivial type also has finite co-type. The following corollary which uses a result by Konig and Tzafriri [? ] shows the exact relationship between Cp and C iidp for Banach lattices.\nCorollary 94. For the dual learning problem with X = H?, if (B?, ‖·‖?H) is a Banach lattice, then for any p ∈ (1, 2] we have that\nCp ≤ 723√ p− 1 (3C iidp ) 2q+1\nwhere q = pp−1 .\nProof. Note that for the dual problem X = H? and by definition, (H?,H?) has Rademacher type p with constant C iidp . However by Theorem 3 of [? ] we have that the pair (H,H) is of co-type 2 + (2C iidp )q with constant 2 (for co-type constant of 2 refer proof of the theorem). Hence using this in Lemma 93 with r = 2 + (2C iidp ) q and C̃r = 2 and simplifying yields the required statement.\nThus for dual learning problems if the Banach space is a Banach Lattice then online methods (specifically mirror descent) is always near optimal in terms of dependence on n for learning rate and in terms of dependence on for oracle complexity. More specifically, for a dual learning problem on a Banach lattice, if for instance optimal rate is V/ √ n then mirror descent will guarantee a rate of order V 5 log2(n)/ √ n."
    }, {
      "heading" : "8.3.2 Decoupling Inequalities",
      "text" : "Another way to guarantee that Cp ≤ GC iidp for some finiteG is by using the so called decoupling inequalities (see [78, 79] for more details).\n159\nDefinition 36. We say that a Banach space with norm ‖·‖?H satisfies p-decoupling inequality with constant B > 0 if :\nE [∥∥∥∥∥ n∑ i=1 ixi( ) ∥∥∥∥∥ p\nH?\n] ≤ Bp E , ′ [∥∥∥∥∥ n∑ i=1 i ′ ixi( ) ∥∥∥∥∥ p\nH?\n] (8.3.2)\nwhere ′1, . . . , ′ n are Rademacher random variables (drawn independent of 1, . . . , n).\nWe would like to point out that the above definition is not the same as the decoupling inequalities in [78, 79] where the above needs to be true for all martingale difference sequences where as above we only consider Walsh-Paley martingales. However since above condition is weaker, all positive results for the stronger definition also hold for the above definition. In any case, the below lemma shows that if a Banach space satisfies decoupling inequality, then there exists G <∞ through which we can upper bound Cp using C iidp′ .\nLemma 95. If the Banach space equipped with norm ‖·‖H? satisfies 1-decoupling inequality with some constant B > 0, then it is true that for all 1 < p′ < r ≤ 2,\nCp′ ≤ 92B\n(r − p′) C iidr\nFirst we would like to point out that if a Banach space satisfies p-decoupling inequality for some p ∈ [1,∞) with constant D, then it satisfies 1-decoupling inequality with constant KpD where Kp only depends on p (see Theorem 4.1 of [79]). Hence we can talk of space satisfying decoupling inequality without refering to exponent (of course the exponent plays a role in the constant) .Several spaces we commonly encounter satisfy the decoupling inequality. It can be shown that called Unconditional Martingale difference (UMD) Banach spaces always satisfy decoupling inequality. These spaces include all Lp, `p Schatten p norms for p ∈ (1,∞). However while every UMD space satisfies decoupling for instance the `1 space while satisfying the decoupling inequality is not UMD. Using some of the results in [79, 80, 81, 82, 83] we can conclude that for many interesting spaces we commonly encounter, there in fact even exists a universal 1-decoupling constant, call itBR. (This constantBR is the one referred to asDR in [79]). The following proposition proved in [79] (see also [80]) is particularly useful especially to provide decoupling inequalities for group norms and interpolation norms.\nProposition 96 (Corollaries 4.6 [79]). If Y is a Banach space that satisfies p-decoupling inequality with constant B, then for any σ-finite measure space (S,Σ, µ) and any p ∈ [1,∞), the space X = Lp(S;Y ) satisfies p-decoupling inequality with same constant B.\nFrom the above we immediately see that `p spaces satisfy p-decoupling inequality with universal constant BR (the space Y in this case is the reals). We can also use the above result to estimate decoupling constants for group norm by taking the the above Proposition space Y to be the `p space corresponding to the inner index of the group norm (mixed norm). In Corollary 4.11 of [79] it has been shown that for p ≥ log2(d), the p-decoupling constant of `d∞ spaces if bounded by 2BR. In [81] it has been shown that a large class of Orlicz and Rearrangement invariant function spaces satisfy 1-decoupling inequality with universal constant\n160\nBR combining these result with the above proposition one can obtain decoupling inequalities for even more examples.\nHence overall, the following figure captures the scenario for the statistical convex learning problems."
    }, {
      "heading" : "8.3.3 Optimality of Mirror Descent in Terms of Efficiency",
      "text" : "In this section we argue that mirror descent is not only optimal in terms of learning rate but also in terms of efficiency. Specifically we are interested in achieving excess risk less than some target sub-optimality > 0, that is, we are interested in coming up with learning algorithm A such that\nsup D∈∆(Z)\nES∼Dn [ LD(A(S))− inf\nh∈H LD(h)\n] ≤ .\nNow there are two main question one can ask, the first is the minimum sample size required by any algorithm to be able to ensure the above guarantee on sub-optimality. This question is essentially the question about optimal learning rates and we already showed optimality of stochastic mirror descent. The second question one can ask is the computational time required by any algorithm given any amount of requested samples to ensure the above sub-optimality guarantee. Note that in general giving exact computational complexity is cumbersome and even impossible at full generality. A good proxy for computational complexity is the number of gradient calculations or calculation or some kind of other local information like hessians or higher order derivative. Note that mirror descent is a sub-gradient based method which in each iteration only calculates one gradient. Since we already established that mirror descent is optimal for many statistical convex learning problem, we can easily conclude that mirror descent is also optimal in terms of number of gradient calculations needed to achieve a target sub-optimality.\nAs mentioned, in general getting a handle on exact computational complexity is hard. However for a few important class of supervised learning problems with absolute loss (can also be extended to hinge loss, logistic loss etc.) one can show that mirror descent is optimal even in terms of exact computational complexity. To see this consider a supervised learning problems where instances given are S = (x1, y1), . . . , (xn, yn). Let d be the intrinsic dimensionality of the sample S (in vector space B). Then the total time required just to read the sample is dn. However note that for many cases like when H/X are various unit balls of `p norms\n161\nor Lp norms or group norms then as we will see in the next section, mirror descent update step has tipple complexity of order d and so since mirror descent is a single pass algorithm that goes over samples one by one, its time complexity when run once over the sample is again of order nd. However we already argued that the learning rate of mirror descent is near optimal and so sample size n required for getting sub-optimality of at most is also near optimal. Hence we can conclude that mirror descent algorithm for these cases have time complexity of same order as time complexity needed just to read the minimum required sample. Hence mirror descent for these cases is near optimal even in terms of exact computational time."
    }, {
      "heading" : "8.4 Examples",
      "text" : "First of all, we start by noticing that all the examples we considered in the previous chapter were Banach Lattices with finite co-type and so for all these problems by Theorems 90, 104 and 91 stochastic mirror descent technique is near optimal in terms of both rates and number of gradient access, for convex Lipschitz problems, supervised learning problems and non-negative smooth problems. We will further see below that for these examples we don’t just get near optimality but that in fact Cp and C iidp are within fixed numeric constant factor of each other. Hence we see that beyond the logarithmic factor there aren’t any space dependent hidden factors even."
    }, {
      "heading" : "8.4.1 Example : `p non-dual pairs",
      "text" : "In the previous chapter we gave characterization of constant D2 (corresponding to 1/ √ n rates in online setting) for the `p pairs. Recall the setting, that is H is the unit ball of `dp1 ball and X is the dual ball of `dp2 norm. It turns out that for `p norms, when p ∈ [1, 2] we have a universal constant for 1-decoupling by Proposition 96. On the other hand, when p ∈ (2,∞), the type constant of `p spaces is of order √ p and so we can conclude that the table is essentially tight even for the statistical learning setting because martingale type constant (Cp) and type constant (C iidp ) are within fixed numeric constant factor of each other."
    }, {
      "heading" : "8.4.2 Example : Non-dual Schatten norm pairs in finite dimensions",
      "text" : "As we showed in the previous chapter, the constants Dp (Cp, Vp etc.) for Schatten norms were same as those for `p norms in the online learning setting. However as we saw in the previous section, constants for `p norms match in statistical and online frameworks. But since lower bounds for `p case can be converted to atleast same lower bound on Schatten norm case (by diagonalizing). We can again conclude that rates of Mirror descent in statistical case are near optimal and moreover that Cp and C iidp are within fixed numeric constant factor of each other.\n162"
    }, {
      "heading" : "8.4.3 Example : Non-dual group norm pairs in finite dimensions",
      "text" : "For the group norm case using Decoupling inequalities for `p along with Proposition 95 we can show again that Cp and C iidp are within constant factor of each other which guarantees tightness of mirror descent for for these problems in the statistical setting."
    }, {
      "heading" : "8.4.4 Computational Efficiency Issues",
      "text" : "Up to now we used number of gradient calculations to argue that mirror descent is optimal in terms of efficiency. Notice that we showed optimality using mirror descent algorithm which has a simple update step that at every round uses only previous hypothesis and gradient of loss at the hypothesis. Hence the time complexity of the update at each round is of the order of effective dimension (as an example in the `p case it is linear in d). Thus once can translate in these cases oracle complexity to time complexity of the algorithm. An for `p cases one can even show that time complexity is near optimal. However this is not always the case, it might be that the complexity of update step of mirror descent (which depends on Ψ) is large that time complexity blows up. This can be seen for instance in the max norm example. In the previous chapter when we considered the max norm example, the function Ψ we considered in Equation 7.5.1 had a summation over 2M+N elements for matrix of size M ×N . This is of course prohibitive to use in practice. However notice that while oracle complexity is still tight, for matrix completion problem in the statistical learning framework, one can use a constrained minimization approach (minimize average loss subject to constraint on max norm or equivalently max norm regularization) and using an SDP approach this can be done in time polynomial in the matrix size. This shows a disparity between oracle complexity and actual time complexity. It is an interesting open question whether there is a polynomial time algorithm for max norm based learning problem in the online framework. In [84], in the transductive online setting for max norm it is shown that once can again use the SDP to obtain a poly-time algorithm. An even stronger flavor of questioning is whether one can provide a mirror descent (or variant) algorithm for learning with max-norm that works in time polynomial in matrix size."
    }, {
      "heading" : "8.5 Detailed Proofs",
      "text" : "Proof of Lemma 84. First we pick elements x1, . . . ,x2n ∈ X and then draw Rademacher random variables ∈ {±1}2n uniformly at random. We shall now use this to construct a distribution D over instances which is the one we shall use for the lower bound. Specifically, given a draw ∈ {±1}2n consider the distribution D to be the uniform distribution over the set { 1x1, . . . , 2nx2n}. Now consider a S of size n drawn iid from the distribution D . Now note that for any learning algorithm A,\nsup D\nES [ LD(A(S))− inf\nh∈H LD(h)\n] ≥ sup\nx1,...,x2n\nE ES∼Dn [ LD (A(S))− inf\nh∈H LD (h)\n]\n163\nSince D is the uniform distribution over set { 1x1, . . . , 2nx2n} one can rewrite sampling S from the distribution as follows : First we sample n numbers, t1, . . . , tn uniformly at random from the set [2n]. Next, the sample S is given by S = t1xt1 , . . . , tnxtn . Hence we can rewrite the above inequality as\nsup D\nES [ LD(A(S))− inf\nh∈H LD(h)\n] ≥ sup\nx1,...,x2n\nE Et1,...,tn∼unif([2n]) [ LD (A(S))− inf\nh∈H LD (h) ] Now let the set J ⊂ [2n] be the set\nJ = {i ∈ [2n] : i ∈ {t1, . . . , tn}}\nthat is the set of indices i such that xi appeared at least once in the sample S provided to the learner. Also let Jc ∈ [2n] stand for the complement of the set J . For linear class, for the distribution D note that for any h ∈ H,\nLD (h) = 1\n2n 2n∑ i=1 〈h, ixi〉\nHence we see that,\nsup D\nES [ LD(A(S))− inf\nh∈H LD(h)\n] ≥ sup\nx1,...,x2n∈X E Et1,...,tn∼unif([2n])\n[ LD (A(S))− inf\nh∈H LD (h)\n]\n= sup x1,...,x2n∈X E Et1,...,tn∼unif([2n]) [ sup h∈H 1 2n 2n∑ i=1 〈h,− ixi〉 − 1 2n 2n∑ i=1 〈A(S),− ixi〉 ]\n= 1\n2n sup x1,...,x2n∈X E Et1,...,tn∼unif([2n]) [∥∥∥∥∥ 2n∑ i=1 ixi ∥∥∥∥∥ H? − ∑ i∈J 〈A(S),− ixi〉 − ∑ i∈Jc 〈A(S),− ixi〉 ]\n= 1\n2n sup x1,...,x2n∈X E t1,...,tn∼unif([2n])\n[ E [∥∥∥∥∥ 2n∑ i=1 ixi ∥∥∥∥∥ H? ] − E [∑ i∈J 〈A(S),− ixi〉 ] − E [∑ i∈Jc 〈A(S),− ixi〉 ]]\nNow we notice that given t1, . . . , tn ∈ [2n], the sets J and Jc are fixed and S only consists of ixi s.t. i ∈ J . Thus, S is only a function of i where i ∈ J . Hence,\nE [〈 A(S),−\n∑ i∈Jc ixi 〉] = E J [ E Jc [〈 A(S),− ∑ i∈Jc ixi 〉]] = E J [〈 A(S),−E Jc [∑ i∈Jc ixi ]〉] = 0\n164\nwhere we use the shorthand J to refer to Rademacher random variables i where i ∈ J and Jc refers to the remaining i’s not in J . Hence we see that\nsup D\nES [ LD(A(S))− inf\nh∈H LD(h) ] ≥ 1\n2n sup x1,...,x2n∈X E t1,...,tn∼unif([2n])\n[ E [∥∥∥∥∥ 2n∑ i=1 ixi ∥∥∥∥∥ H? ] − E [∑ i∈J 〈A(S),− ixi〉 ]]\n≥ 1 2n sup x1,...,xn∈X E t1,...,tn∼unif([2n])\n[ E [∥∥∥∥∥ 2n∑ i=1 ixi ∥∥∥∥∥ H? ] − E [∥∥∥∥∥∑ i∈J ixi ∥∥∥∥∥ H? ]]\n= sup x1,...,x2n∈X\n{ E [∥∥∥∥∥ 12n 2n∑ i=1 ixi ∥∥∥∥∥ H? ] − 1 2n E t1,...,tn∼unif([2n]) E [∥∥∥∥∥∑ i∈J ixi ∥∥∥∥∥ H? ]}\n≥ sup x1,...,x2n∈X E [∥∥∥∥∥ 2n∑ i=1 ixi ∥∥∥∥∥ H? ] − 1 2n E t1,...,tn∼unif([2n]) sup x1,...,x|J|∈X E ∥∥∥∥∥∥ |J|∑ i=1 ixi ∥∥∥∥∥∥ H?  Note that since sample size |S| = n, we have that |J | ≤ n. Therefore,\nsup x1,...,x|J|∈X\nE [∥∥∥∥∥∑ i∈J ixi ∥∥∥∥∥ H? ] ≤ sup x1,...,xn∈X E [∥∥∥∥∥ n∑ i=1 ixi ∥∥∥∥∥ H? ]\n(the above is obvious because worst case we can always put x|J|+1, . . . ,xn = 0 so the two are equal). Thus we conclude that :\nsup D\nES [ LD(A(S))− inf\nh∈H LD(h)\n] ≥ sup\nx1,...,x2n∈X E [∥∥∥∥∥ 2n∑ i=1 ixi ∥∥∥∥∥ H? ] − 1 2n E t1,...,tn∼unif([2n]) sup x1,...,x|J|∈X E ∥∥∥∥∥∥ |J|∑ i=1 ixi ∥∥∥∥∥∥ H?  ≥ sup\nx1,...,x2n∈X E [∥∥∥∥∥ 2n∑ i=1 ixi ∥∥∥∥∥ H? ] − 1 2n sup x1,...,xn∈X E [∥∥∥∥∥ n∑ i=1 ixi ∥∥∥∥∥ H? ]\n= Riid2n(Flin(H,X ))− 1\n2 Riidn (Flin(H,X ))\nThis conclude the lemma.\nProof of Theorem 86. Applying the upper bound guaranteed by the assumption of the theorem and using the lower bound from lemma 84 we get that for any n ∈ N:\nRiid2n(Flin(H,X )) ≤ 1\n2 Riidn (Flin(H,X )) +\nV\nn1/q\nExpanding the recursive inequality on the right above we get that\nRiid2n(Flin(H,X )) ≤ V\nn1/q\n( 1 + 1\n21/p +\n1\n22/p + . . .\n) ≤ 2 1/p\n21/p − 1 V n1/q\n165\nThus we conclude the first inequality in the theorem statement. As for the upper bound on the fat shattering dimension, we have that whenever β ≤ Riidn (Flin(H,X )),\nfatiidβ (Flin(H,X )) ≤ n ( Riidn (Flin(H,X )) )2 β2 ≤ n\nHence we conclude that for all n such thatRiidn (Flin(H,X )) ≥ β, fat iid β (Flin(H,X )) ≤ n. In other words,\nfatiidβ (Flin(H,X )) ≤ inf{n : Riidn (Flin(H,X )) ≥ β} inf{2n : Riid2n(Flin(H,X )) ≥ β}\nHowever since we already proved that for all n ∈ N,Riid2n(Flin(H,X )) ≤ 5Vn1/q we can conclude that for any β > 0,\nfatiidβ (Flin(H,X )) ≤ ( 5V\nβ\n)q .\nThis concludes the theorem.\nProof of Lemma 87. Consider instance space Z = X × [−1, 1] and the non-negative function φ : R × [−1, 1] 7→ R+ that is 1-smooth (in its first argument) given by :\nφ(z, y) = { (z − y)2 if |z − y| ≤ 12 |z − y| − 14 otherwise\nNow the loss function we consider is given by `(h, (x, y)) = φ(〈h,x〉 , y). Since φ is 1-smooth in its first argument, we see that\n‖∇`(h, (x, y))−∇`(h′, (x, y))‖X = |∂φ(〈h,x〉 , y)− ∂φ(〈h ′,x〉 , y)| ‖x‖X ≤ | 〈h− h ′,x〉 | ‖x‖X ≤ ‖h− h′‖X? ‖x‖ 2 X ≤ ‖h− h ′‖X?\nThus we conclude that the loss is 1-smooth and so belongs to class Zsmooth. The distribution we pick for showing the lower bound is a slight modification of the one used in Lemma 84. We start by picking elements x1, . . . ,x2nL∗ ∈ X and then draw Rademacher random variables ∈ {±1}2nL ∗ uniformly at random. We shall now use this to construct a distribution D′ over instances which is the one we shall use for the lower bound. Specifically, given a draw ∈ {±1}2nL∗ consider the distribution D on X to be the uniform distribution over the set { 1x1, . . . , 2nL∗x2nL∗}. Now the distribution D′ is the distribution on Z = X × [−1, 1] to be the one that picks (0, 0) with probability 1 − L∗ and with probability L∗, draws input instance x ∈ X i.i.d. from distribution D and deterministically picks label y = −1 (or whatever suph∈H,x∈X 〈h,x〉 is with appropriate scaling changes but for simplicity let us assume its bounded by 1) .\n166\nNote that for any h ∈ H,\nLD′ (h) = L ∗ED [ | 〈h,x〉 − y| − 1\n4\n] = 1\n2n 2nL∗∑ i=1 ( 〈h, ixi〉 − 1− 1 4 ) .\nHence we see that for any ∈ {±1}2n,\nLD′ (A(S))− infh∈HLD ′ (h) =\n1\n2n 2nL∗∑ i=1 〈A(S), ixi〉 − inf h∈H 1 2n 2nL∗∑ i=1 〈h, ixi〉\nNow consider sample S of size n drawn iid from the distribution D′ . Note that for any learning algorithm A,\nsup D\nES [ LD(A(S))− inf\nh∈H LD(h)\n] ≥ sup\nx1,...,x2nL∗ E ES∼D′ n [ LD′ (A(S))− infh∈HLD ′ (h) ]\n= sup x1,...,x2nL∗\nE ES∼D′\n[ 1\n2n 2nL∗∑ i=1 〈A(S), ixi〉 − inf h∈H 1 2n 2nL∗∑ i=1 〈h, ixi〉\n]\n= sup x1,...,x2nL∗\nE ES∼D′\n[ 1\n2n 2nL∗∑ i=1 〈A(S), ixi〉+ ∥∥∥∥∥ 12n 2nL∗∑ i=1 ixi ∥∥∥∥∥ H? ]\nProceeding in similar fashion as in the proof of Lemma 84, we see that one can rewrite sampling S from the distribution as follows : First we sample m from binomial distribution Binomial(L∗, n) (represents the samples for which y 6= 0). Next, numbers t1, . . . , tm is drawn uniformly at random from the set [2nL∗]. Hence we get the inequality :\nsup D\nES [ LD(A(S))− inf\nh∈H LD(h) ] ≥ sup\nx1,...,x2nL∗ E E m∼Binomial(n,L∗) Et1,...,tm∼Unif([2nL∗])\n[ 1\n2n 2nL∗∑ i=1 〈A(S), ixi〉+ ∥∥∥∥∥ 12n 2nL∗∑ i=1 ixi ∥∥∥∥∥ H? ]\n= sup x1,...,x2nL∗ E m∼Binomial(n,L∗) Et1,...,tm∼Unif([2nL∗])\n[ E [〈 A(S), 1\n2n 2nL∗∑ i=1 ixi\n〉] + E [∥∥∥∥∥ 12n 2nL∗∑ i=1 ixi ∥∥∥∥∥ H? ]]\nNow let the set J ⊂ [2nL∗] be the set J = {i ∈ [2nL∗] : i ∈ {t1, . . . , tm}} that is the set of indices i such that (xi,−1) appeared at least once in the sample S provided to the learner. Also let Jc ∈ [2nL∗] stand for the complement of the set J . Following the same line of proof as in Lemma 84 noting that S only depends\non the ’s that occur in the sample S, we can see that\nE [〈 A(S), 1\n2n 2nL∗∑ i=1 ixi\n〉] ≥ −E [∥∥∥∥∥ 12n∑ i∈J ixi ∥∥∥∥∥ H? ]\n167\nand so we conclude that :\nsup D\nES [ LD(A(S))− inf\nh∈H LD(h) ] ≥ sup\nx1,...,x2nL∗ E m∼Binomial(n,L∗) Et1,...,tm∼Unif([2nL∗])\n[ E [∥∥∥∥∥ 12n 2nL∗∑ i=1 ixi ∥∥∥∥∥ H? ] − E [∥∥∥∥∥ 12n∑ i∈J ixi ∥∥∥∥∥ H? ]]\n≥ sup x1,...,x2nL∗ Em∼Binomial(n,L∗) E [ ∥∥∥∥∥ 12n 2nL∗∑ i=1 ixi ∥∥∥∥∥ H? ] − sup x1,...,xm∈X E ∥∥∥∥∥∥ 12n min{m,2L∗n}∑ i=1 ixi ∥∥∥∥∥∥ H?  However note that with probability 1/2, m ≤ nL∗ and so we have that\nsup D\nES [ LD(A(S))− inf\nh∈H LD(h)\n] ≥ 1\n2 sup x1,...,x2nL∗ E [∥∥∥∥∥ 12n 2nL∗∑ i=1 ixi ∥∥∥∥∥ H? ] − sup x1,...,xnL∗∈X E [∥∥∥∥∥ 12n L∗n∑ i=1 ixi ∥∥∥∥∥ H? ]\n= L∗\n2\n( Riid2L∗n(Flin(H,X ))− 1\n2 RiidL∗n(Flin(H,X )) ) This conclude the lemma.\nProof of Lemma 88. First, since both sides below are homogenous, the premise of the lemma can be rewritten as, for all n ∈ N and all x1, . . . ,xn ∈ B?,\nE [∥∥∥∥∥ n∑ i=1 ixi ∥∥∥∥∥ H? ] ≤ D n 1r max i∈[n] ‖xi‖X (8.5.1)\nLet S = ( ∑n i=1 ‖xi‖ p X ) 1/p, define\nIk := { i ≥ 1 ∣∣ S 2(k+1)/p < ‖xi‖X ≤ S2k/p } , T (k) 0 := inf{i ∈ Ik} and ∀m ∈ N, T (k)m := inf{i > T (k) m−1, i ∈ Ik}\nNote that, Sp ≥ ∑ i∈Ik ‖xi‖ p X > Sp |Ik| 2(k+1) and so we get that |Ik| < 2k+1. From this, using the premise in\n168\nEquation 8.5.1 we conclude that\nE [∥∥∥∥∥ n∑ i=1 ixi ∥∥∥∥∥ H? ] ≤ ∑ k≥0 E ∥∥∥∥∥∑ i∈Ik ixi ∥∥∥∥∥ H?  = ∑ k≥0 E ∥∥∥∥∥∥ ∑ i≥0 T (k) i x T (k) i ∥∥∥∥∥∥ H?  ≤ ∑ k≥0 ( D {|Ik|1/r}{sup i∈Ik ‖xi‖X } )\n≤ ∑ k≥0 ( D 2(k+1)/r sup i∈Ik ‖xi‖X ,∞ ) ≤ ∑ k≥0 ( D 2(k+1)/r 2−k/pS\n) = D 21/r\n∑ k≥0 2k( 1 r− 1 p ) S ≤ 2D 1− 2( 1 r− 1 p ) S ≤ 12D r − p S\n= 12D\nr − p ( n∑ i=0 ‖xi‖pX )1/p\nWe conclude the proof by using Kahane Inequality (see [85]) which asserts that for any p ∈ [1, 2],\n( E [∥∥∥∥∥ n∑ i=1 ixi ∥∥∥∥∥ p\nH?\n])1/p ≤ √ 2 E [∥∥∥∥∥ n∑ i=1 ixi ∥∥∥∥∥ H? ]\nProof of Proposition 58. Since we are dealing with convex loss, using Jensen’s inequality we have that\nLD\n( 1\nn n∑ t=1 A(z1:t) ) ≤ 1 n n∑ t=1 LD (A(z1:t)) .\n169\nHence we see that ES∼Dn [( 1\nn n∑ t=1 A(z1:t−1)\n) − inf\nh∈H LD(h) ] ≤ 1 n n∑ t=1 ES [LD(A(z1:t−1))]− inf h∈H 1 n n∑ t=1 LD(h)\n= 1\nn n∑ t=1 ES∼Dn [`(A(z1:t−1), zt)]− inf h∈H 1 n n∑ t=1 ES∼Dn [`(h, zt)]\n= ES∼Dn [ 1\nn n∑ t=1 `(A(z1:t−1), zt)\n] − inf\nh∈H ES\n[ 1\nn n∑ t=1 `(h, zt)\n]\n≤ ES∼Dn [ 1\nn n∑ t=1 `(A(z1:t−1), zt)\n] − ES [ inf h∈H 1 n n∑ t=1 `(h, zt) ] = ES∼Dn [Rn(A, z1, . . . , zn)]\n≤ sup z1,...,zn∈Z Rn(A, z1, . . . , zn)\nwhere the second step is because A(z1, . . . , zt−1) only depends on z1, . . . , zt−1 and so ES [`(A(z1, . . . , zt−1), zt)] = ES [LD(A(z1, . . . , zt−1))]. The second part of the proposition is from the fact that the above holds for any online learning algorithm.\nProposition 97. If (H?,X ) has type p with some constant C iidp then for q = p p−1 , the pair (H,X ?) has co-type q with constant 1 Ciidp\nProof. Given h1, . . . ,hn ∈ B? for any > 0 we can pick x1, . . . ,xn such that\nn∑ t=1 〈ht,xt〉 ≥ (1− ) ( n∑ t=1 ‖ht‖qX? )1/q ( n∑ t=1 ‖xt‖pX )1/p (8.5.2)\nOn the other hand we have that,\nn∑ t=1 〈hi,xi〉 = E [〈 n∑ t=1 tht, n∑ t=1 txt 〉]\n≤ ( E [∥∥∥∥∥ n∑ t=1 tht ∥∥∥∥∥ q\nH\n])1/q ( E [∥∥∥∥∥ n∑ t=1 txt ∥∥∥∥∥ p\nH?\n])1/p\n≤ C iidp\n( E [∥∥∥∥∥ n∑ t=1 tht ∥∥∥∥∥ q\nH\n])1/q ( n∑ t=1 ‖xt‖pX )1/p\nwhere the last step is by the type inequality for (H,X ). Combining the above and Equation 8.5.2 and taking limit of → 0 proves the statement.\nDefinition 37. For p ∈ [1, 2] the pair (H?,X ) is said to be p-convex with constantKp if for any x1, . . . ,xN ∈\n170\nX , ∥∥∥∥∥∥ ( N∑ t=1 |xt|p )1/p∥∥∥∥∥∥\nH?\n≤ Kp ( N∑ t=1 ‖xt‖pX )1/p\nDefinition 38. For q ∈ [2,∞) the pair (H,X ?) is said to be q-concave with constant Kq if for any K ≤ Kq(H,X ?) and any x1, . . . ,xN ∈ X ,∥∥∥∥∥∥ ( N∑ t=1 |xt|q )1/q∥∥∥∥∥∥\nH?\n≥ 1 K̃q ( N∑ t=1 ‖xt‖qX )1/q\nLemma 98. If the pair (H?,X ) is of type p with constant C iidp then, for any x1, . . . ,xn ∈ X ,\n√ 2C iidp ( n∑ t=1 ‖xt‖pX? )1/p ≥ ∥∥∥∥∥∥ ( n∑ t=1 |xt|p ) 1 p ∥∥∥∥∥∥ H?\nThat is it is p-convex with constant √\n2C iidp .\nProof. By type p with constant C iidp ,\nC iidp ( n∑ t=1 ‖xt‖pX? )1/p ≥ ( E [∥∥∥∥∥ n∑ t=1 txt ∥∥∥∥∥ p\nH?\n]) 1 p\n≥ E [∥∥∥∥∥ n∑ t=1 txt ∥∥∥∥∥ H? ] ≥ ∥∥∥∥∥E ∣∣∣∣∣ n∑ t=1 txt ∣∣∣∣∣ ∥∥∥∥∥ H?\n≥ 1√ 2 ∥∥∥∥∥∥ ( n∑ t=1 |xt|p ) 1 p ∥∥∥∥∥∥ H?\nwhere the last inequality is got by using scalar version of Kintchine’s inequality with functional calculus for\nBanach lattice (specifically Theorem 1.d.1 of [75] by noting that expectation w.r.t. can be written as finite\naverage). This concludes the proof.\nProposition 99. If (H?,X ) is p-convex with some constant Kp then for q = pp−1 , the pair (H,X ?) is q-concave with constant Kp. That is for any h1, . . . ,hn ∈ H∥∥∥∥∥∥ ( n∑ t=1 |ht|q )1/q∥∥∥∥∥∥\nH\n≥ 1 Kp ( n∑ t=1 ‖ht‖qX? )1/q\nProof. Given h1, . . . ,hn ∈ B? for any > 0 we can pick x1, . . . ,xn such that\nn∑ t=1 〈ht,xt〉 ≥ (1− ) ( n∑ t=1 ‖ht‖qX? )1/q ( n∑ t=1 ‖xt‖pX )1/p (8.5.3)\n171\nOn the other hand, using 1.d.2 (iii) [75] we can conclude that\nn∑ t=1 〈ht,xt〉 ≤ 〈( n∑ t=1 |ht|q )1/q , ( n∑ t=1 |xt|p )1/p〉\n≤ ∥∥∥∥∥∥ ( n∑ t=1 |ht|q )1/q∥∥∥∥∥∥\nH\n∥∥∥∥∥∥ ( n∑ t=1 |xt|p )1/p∥∥∥∥∥∥\nH?\n≤ Kp ∥∥∥∥∥∥ ( n∑ t=1 |ht|q )1/q∥∥∥∥∥∥\nH\n( n∑ t=1 ‖xt‖pX )1/p\nwhere the last step is by the p-convex inequality of (H?,X ). Combining the above and Equation ?? and taking limit of → 0 proves the statement.\nProposition 100. For any n ∈ N, any sequence real valued tree a of depth n and for any 1 ≤ p ≤ 2 ≤ r < ∞,\n183 √ 2 r3p 3 2\n√ p− 1 (r − 1)\n( E [∣∣∣∣∣ n∑ t=1 tat( ) ∣∣∣∣∣ p])1/p ≥ ( E [∣∣∣∣∣ n∑ t=1 tat( ) ∣∣∣∣∣ r])1/r\nProof.\n( E [ n∑ t=1 |at( )|p ]) 1 p ≥ E ( n∑ t=1 |at( )|2 ) p 2  1p ∀p ≤ 2, ‖·‖pp ≥ ‖·‖p2 ≥ √ p− 1\n18 p 3 2\n( E , ′ [∣∣∣∣∣ n∑ t=1 ′t tat( ) ∣∣∣∣∣ p])1/p\nBurkholder’s Inequality [86]\n≥ √ p− 1 18 √ 2(r − 1)p 32\n( E , ′ [∣∣∣∣∣ n∑ t=1 ′t tat( ) ∣∣∣∣∣ r])1/r\n≥ √ p− 1 (r − 1)\n183 √ 2 r3p 3 2\n( E [∣∣∣∣∣ n∑ t=1 tat( ) ∣∣∣∣∣ r])1/r\nThis concludes the proof.\nProof of Lemma 93. Consider any X valued tree x of infinite depth. We start by noting that type p with\n172\nconstant C iidp implies p-convexity with constant √ 2C iidp and so\n183 √\n8p 3 2 r3C iidp C̃r√\np− 1(r − 1)\n( E [ n∑ t=1 ‖xt( )‖pX ]) 1 p = 183 √ 8p 3 2 r3C iidp C̃r√ p− 1(r − 1)  1 2n ∑ ∈{±1}n n∑ t=1 ‖xt( )‖pX  1p\n≥ 18 3 √ 4p 3 2 r3C̃r√\np− 1(r − 1) ∥∥∥∥∥∥∥  1 2n ∑ ∈{±1}n n∑ t=1 |xt( )|p  1p ∥∥∥∥∥∥∥ H?\n= 183 √ 4p 3 2 r3C̃r√\np− 1(r − 1) ∥∥∥∥∥∥ ( E [ n∑ t=1 |xt( )|p ]) 1 p ∥∥∥∥∥∥ H?\n(8.5.4)\nNow note that for the real-valued case by Proposition 100 we have that, for any real valued tree a of depth n and for any 1 ≤ p ≤ 2 ≤ r <∞,\n183 √ 2 r3p 3 2\n√ p− 1 (r − 1)\n( E [∣∣∣∣∣ n∑ t=1 tat( ) ∣∣∣∣∣ p])1/p ≥ ( E [∣∣∣∣∣ n∑ t=1 tat( ) ∣∣∣∣∣ r])1/r\nSince both sides the expressions are degree one homogenous, applying Theorem 1.d.1 [75] (by expanding out the tree of depth n to its 2n − 1 elements and noting that expectation w.r.t. n Rademacher variables is in fact a finite average of 2n signs) we conclude that\n183 √ 2r3p 3 2\n√ p− 1(r − 1)\n( E [ n∑ t=1 |xt( )|p ]) 1 p ( E [∣∣∣∣∣ n∑ t=1 txt( ) ∣∣∣∣∣ r])1/r\nPlugging this back in Equation 8.5.4 and noting that co-type r with constant C̃r of the pair (H?,H?) implies its r-concavity with constant √ 2C̃r, we conclude that :\n183 √\n8p 3 2 r3C iidp C̃r√\np− 1(r − 1)\n( E [ n∑ t=1 ‖xt( )‖pX ]) 1 p ≥ √ 2 C̃r ∥∥∥∥∥∥ ( E [∣∣∣∣∣ n∑ t=1 txt( ) ∣∣∣∣∣ r]) 1r ∥∥∥∥∥∥\nH?\n≥ ( E [∥∥∥∥∥ n∑ t=1 txt( ) ∥∥∥∥∥ r\nH?\n]) 1 r\n≥ ( E [∥∥∥∥∥ n∑ t=1 txt( ) ∥∥∥∥∥ p\nH?\n])1/p\nNoting that r3/(r − 1) ≤ 2r2 and that p3/2 ≤ √ 8 and by definition of Cp we conclude the proof.\n173\nProof of Lemma 95. Consider any X valued infinite depth tree x. We have,\nE [∥∥∥∥∥ n∑ i=1 ixi( ) ∥∥∥∥∥ H? ] ≤ DE , ′ [∥∥∥∥∥ n∑ i=1 i ′ ixi( ) ∥∥∥∥∥ H? ]\n= DE [ E ′ [∥∥∥∥∥ n∑ i=1 i ′ ixi( ) ∥∥∥∥∥ H? ]]\n≤ DC iidr E ( n∑ i=1 ‖ ixi( )‖rX )1/r ≤ DC iidr sup\n∈{±1}n ( n∑ i=1 ‖xi( )‖rX )1/r\nNow this is effectively what we had in the proof of Lemma 67 and exactly as we did there applying Lemma’s\n79, 80 and 81 and repeating the steps in proof of Lemma 67 we can conclude that\nE ∥∥∥∥∥ n∑ i=1 ixi( ) ∥∥∥∥∥ p′\nH?\n ≤ 92DC iidr (r − p′) n∑ i=1 E [ ‖xi( )‖p ′ X ]\nThis concludes the proof."
    }, {
      "heading" : "8.6 Discussion",
      "text" : "The highlight of this chapter was that we showed that for most commonly occurring learning problems constant Cp can be almost bounded by C iidp which we could the use to conclude that mirror descent was near optimal in terms of both learning rate and oracle complexity for statistical convex problems too. In fact for the common examples we saw that the constant factor relating Cp and C iidp can be a fixed universal constant. We also saw that while many a times oracle complexity could be directly associated with time complexity of the algorithms this is not always true and for the very practical problem of matrix completion with max norm this issue arose leading to the open question of whether we can efficiently solve in the online setting, matrix completion with max-norm with optimal rates.\nWe also saw strong connection between oracle complexity of offline optimization of convex lipschitz class and statistical complexity of linear class and using this we also informally argued that for large dimensional problems in “reasonable space”, mirror descent algorithm is near optimal even for offline optimization. Overall the aim of this section is to show that mirror descent is near optimal ubiquitously for most practical convex statistical learning problems and for high dimensional offline optimization problems not just in the online learning setting as was shown in the previous chapter.\n174\nChapter 9\nOptimality of Mirror Descent for Offline Convex Optimization\nIn this chapter we consider the problem of offline convex optimization. To address the issue of efficiency of optimization methods for the convex optimization problems, we use the notion of oracle complexity introduced by Nemirovski and Yudin in [1]. We show interesting connections between convex optimization and statistical convex learning. Furthermore for several commonly encountered high dimensional convex optimization problems we also show that mirror descent algorithm is near optimal, in terms of oracle complexity, even for offline convex optimization. Based on the results we also show that for several statistical convex learning problems, mirror descent is optimal even when one has access to more powerful oracles that can account for parallel computational methods for optimization. In fact we show that for these problems, parallelization does not help (in improving efficiency of the learning procedure) and simply using single pass mirror descent algorithm is optimal both in terms of rates and efficiency.\nSection 9.1 introduces the oracle based offline optimization model of Nemirovski and Yudin [1]. Section 9.2 provides lower bound on oracle complexity of offline convex optimization problems in terms of fat shattering dimension of associated linear class and shows connections between convex optimization and statistical learning and specifically also shows that whenever a the convex optimization problem over function class associated with ZLip(X ) is efficiently optimizable, then the supervised learning problem is also efficiently learnable. Section 9.3 shows that for several high dimensional problems, mirror descent is also optimal for offline convex optimization. Section 9.4 deals with statistical learning with distributed oracles and shows that for several cases, mirror descent is near optimal even when compared to learning algorithms that have access to distributed oracles and thus show that in these cases parallelization does not help. Section 9.5 provides the detailed proofs of this chapter and we conclude with some discussions in Section 9.6.\n175"
    }, {
      "heading" : "9.1 Oracle-based Offline Convex Optimization",
      "text" : "A typical convex optimization algorithm initially picks some point in the feasible setH and iteratively updates these points based on some local information about the function it calculates around this point. Examples of these type of procedures are gradient descent methods that uses first order gradient information, newton’s method that uses second over hessian information, interior point methods and so on. In fact most procedures one can think of for optimization are based on iteratively updating based on some local information about the function at the point. In general computing the exact computational complexity of these methods is cumbersome and may not even be possible in full generality. To capture efficiency of optimization procedures in a general way, we consider the oracle based optimization problem and associated oracle complexity of the problem. To this end we first formally define an Oracle. Such models have been introduced and analyzed in [1]. In general, given an instance set Z indexing convex functions on hypothesis set H̄, we use the term oracle to refer to any mapping O : H̄ × Z 7→ I from hypothesis instance pairs to an answer I ∈ I, where set I is some arbitrary information set I. However at this generality note that the information set I could be all of Z and Oracle O could be the identity mapping. This would defeat the purpose of introducing oracle models and associated oracle complexity. To address this issue we use the notion of local oracle defined by Nemirovski and Yudin [1] and whenever we use the term oracle we mean local oracle. Before we formally define a (local) oracle we first define the neighborhood set of a point h ∈ H̄. Given δ > 0 and a point h ∈ H, the δ-neighborhood of the point h is the set\nBδ(h) = { h′ ∈ H̄ : ‖h− h′‖H ≤ δ } .\nDefinition 39. A (local) oracle O : H̄ × Z 7→ I is a mapping, which, given any point h ∈ H̄ and instances z, z′ ∈ Z such that\nlim δ→0 sup h′,h′′∈Bδ(h)\n|`(h′, z)− `(h′′, z′)| = 0 ,\noutputs answers that satisfy the equality O(z,h) = O(z′,h).\nThe definition basically says if two instances z, z′ ∈ Z correspond to convex losses `(·, z) and `(·, z′) respectively that are indistinguishable about some neighborhood of a point h ∈ H̄, then querying an oracle about the two instances at this point h leads to the same answer. For a query with instance h ∈ H̄ at instance z ∈ Z , the oracle only provides local information about the function `(·, z) at the point of query h. To make the concept clearer we provide the following examples of (local) oracles commonly used in practice.\nExample 13 (Zero’th-order Oracle). This is perhaps the simplest oracle that simply evaluates the given function at the query point and returns the value. That is O(h, z) = `(h, z). Clearly I ⊆ R.\nThe zero’th order oracle captures bandit learning problems.\nExample 14 (First-order Oracle). This is an Oracle that provides the sub-gradient of the function at query point. That is O(h, z) = ∇h`(h, z). Clearly I ⊂ B∗ the dual space of B (the banach space containing H̄).\n176\nExample 15 (Second-order Oracle). Consider the example where H̄ ⊆ Rd and each z ∈ Z corresponds to twice differentiable convex loss. In this case a second-order oracle is one that returns the hessian of the function at the query point. That is O(h, z) = ∇2h`(h, z).\nAs mentioned earlier since we are restricting ourselves to convex problems, we only need to consider deterministic algorithms. We now generically define Oracle Based Learning/Optimization Algorithm for a given oracle O.\nDefinition 40. For a given OracleO, an “Oracle Based Optimization/Learning Algorithm”, AO : ⋃ n∈N In → H̄ is a mapping from a sequences of oracle answers in I to an element of hypothesis set H̄.\nWe now describe the oracle-based offline convex optimization protocol. Given z ∈ Z corresponding to convex function `(·, z) (unknown to the learner) the optimization procedure is as follows :\nOracle-based Offline Optimization Protocol :\nfor t = 1 to m Pick hypothesis ht ∈ H̄ for query Oracle provides answer It = O(ht, z) end for\nThe goal is to solve the optimization problem\nargmin h∈H̄ `(h, z)\nand at the end of m steps the sub-optimality of the Learner is given by\n`(hm, z)− inf h∈H̄ `(h, z) .\nGiven an Oracle-based learning/optimization algorithm, AO we shall use the short-hand, I1 = O (h1, z) and further iteratively, use the notation It = O(ht, z) where of course each hypothesis ht = AO(I1, . . . , It−1) is picked using the Oracle-based learning/optimization algorithm. We now define what it means to be offline optimizable using an oracle O.\nDefinition 41. For a given class of convex functions corresponding to instance set Z and a given Oracle O, the offline oracle complexity of a given ”Oracle based optimization/learning algorithm”, AO, is defined as\nmoff( ,A O,Z) = inf { m ∈ N ∣∣∣∣ sup z∈Z { `(AO(I1, . . . , Im), z)− inf h∈H `(h, z) } ≤ }\nFurther, for the given oracle O, the O-offline oracle complexity of the given offline optimization problem is defined as moff( ,Z,O) = infAO moff( ,AO,Z).\nRoughly speaking, given > 0, the oracle complexity of an algorithm is the minimum number of oracle answers needed by the algorithm to guarantee sub-optimality smaller than against any instance z ∈ Z .\n177\nFurther, the above definition basically implies that for any , there exists an Oracle Based Optimization Algorithm, AO that provides an sub-optimality for any z ∈ Z within m( ,Z,O) steps. While the above definition of oracle complexity tells us what is the best (oracle) efficiency achievable for a problem for a given problem using oracle O, one might still wonder if using some other oracle one can improve efficiency. To address this issue we now define oracle complexity of a problem (independent of any particular oracle).\nDefinition 42. For a given class of convex functions corresponding to instance set Z , the offline oracle complexity of the given offline optimization problem is defined as\nmoff( ,Z) = inf O moff( ,Z,O) .\nFurther we say that a given problem is oracle-based offline optimizable if there exists some Oracle O such that ∀ > 0, moff( ,Z,O) <∞."
    }, {
      "heading" : "9.2 Lower Bounding Oracle Complexity: Connections to Statistical",
      "text" : "Convex Learning\nIn this section we establish some connections between offline optimization of convex Lipschitz instance class ZLip(X ) and fat-shattering dimension and Rademacher complexities of the linear function class. We also establish interesting connection between convex optimization and statistical convex learning.\nThe following lemma which lower bounds oracle complexity by fat-shattering dimension of the corresponding linear function class is based on the proof technique for lower bounds on oracle complexity for offline optimization of convex Lipschitz function classes in [1].\nLemma 101. For anyH ⊂ B and X ⊂ B? that are convex and centrally symmetric,,\nmoff(β,ZLip(X )) ≥ fatiid2β(Flin(H,X )) .\nBased on the above lemma and relationship between fat-shattering dimension and statistical Rademacher complexity, we prove the following theorem which bounds Rademacher complexity of the linear function class in terms using any polynomial upper bound on oracle complexity of offline optimization of the convex Lipschitz function class.\nTheorem 102. If there exists some V > 0 and q ∈ (0,∞) such that for any β > 0,\nmoff(β,ZLip(X )) ≤ ( V\nβ\n)q\n178\nthen using the shorthand r = max{2, q}, we have that :\n∀n ∈ N, Riidn (Flin(H,X )) ≤ 9V log1+ 1 r (n)\nn1/r .\nNotice that in the above theorem if q ≥ 2 then r = q and so like in Theorem ?? we get a tight relationship between Rademacher complexity and oracle complexity of offline optimization. However this is not always true. In d dimensional space where d is small, using the centroid methods once can guarantee oracle complexity upper bound of d log(1/ ). However it turns out that at least for the dual learning problems (when H = X ?) if the dimension is large enough, by the celebrated Dvoretzky-Roger’s theorem (see for instance [87]), we can infer that for all β < , fatiidβ (Flin(H,X )) is larger than order 1/β2. Later on in Section 8.3 as a side interest we can use this to conclude that for dual learning problems in high dimensional problems in appropriate spaces (most common ones), mirror descent is almost optimal even for offline optimization for the convex Lipschitz class!\nThe above theorem establishes connections between offline convex optimization and statistical convex learning especially for supervised learning problems. The following corollary implies that for anyH and X , if we can find an efficient optimization algorithm for optimizing functions in class ZLip(X ), then we can find an efficient algorithm for statistical learning of supervised convex learning problem.\nCorollary 103. If there exists some V > 0 and q ∈ (0,∞) such that for any β > 0,\nmoff(β,ZLip(X )) ≤ ( V\nβ )q then using the shorthand r = max{2, q}, we have that there exists a learning algorithm AaERM (that solves the empirical risk minimization problem approximately) which enjoys learning guarantee for the supervised\nconvex learning problem in the statistical framework:\nsup D∈∆(Zsup(X ))\nES∼Dn [ LD(AaERM(S))− inf\nh∈H LD(h)\n] ≤ 18V log 1+ 1r (n)\nn1/r .\nFurthermore, the number of oracle calls needed by this algorithm is bounded by n 1+q/r\n18q logq+ q r (n)\n, and so number\nof oracle calls is at most order n2, the sample size.\nProof. We provide a proof sketch of this simple corollary. We start by noting that by Proposition 85, learning\nrate for the empirical risk minimizer is bounded by Rademcher complexity of the associated linear class and by previous theorem we see that upper bound on oracle complexity for optimization of ZLip(X ) implies upper bound on Rademacher complexity. Thus combining these results we conclude the statement of the\ncorollary.\nFirst of all, while the above corollary we give for supervised convex learning problem with absolute loss, it can also be extended to logistic loss, hinge losses and basically any convex Lipschitz loss.\n179"
    }, {
      "heading" : "9.3 Main Result : Optimality of Mirror Descent for Offline Convex",
      "text" : "Optimization\nThe following theorem shows that for a large class of convex problems, mirror descent is near optimal even for offline convex optimization in terms of oracle complexity.\nTheorem 104. If there exists some constant G ≥ 1 such that for any 1 < p′ < r ≤ 2\nCp′ ≤ G C iidr\nand it is true that for some V > 0 and p ∈ (1,∞),\nmoff( ,ZLip(X )) ≤ ( V )q where q = p/(p− 1). Then there exists function Ψ and step size η using which the stochastic mirror descent algorithm enjoys the guarantee\nmoff( ,AMD,Z(X )) ≤\n( 54018 G V log5 ( 1 ) )r\nProof. First step we use Theorem 102 to bound Riidn (Flin(X )). Next we use proceed as in the proof of 86. Next using Corollary 89 we see that for any p′ < p, C iidp′ ≤ 60 √ 2V p−p′ . Using the assumption that Cp ≤ G C iid p we get bound on Cp′ . However Cp is an upper bound on Dp and so using Corollary 62 we conclude the proof.\nThe first thing we notice is that if q ≥ 2 then r = q. Hence in this case the guarantee on the oracle complexity of offline optimization using mirror descent algorithm matches the upper bound on oracle complexity for the problem. Hence we see that for such a case mirror descent algorithm is near optimal even for offline convex optimization. In particular, if for a problem no algorithm can guarantee a better oracle complexity than order 1/ 2 then from the above theorem we can conclude that mirror descent is near optimal (up to polynomials).\nNow let us consider the dual problem, that is the case when H = X ?. In this case, the celebrated Dvoretzky Roger’s theorem (see [87] for geometric interpretation we use here) implies that for any Banach space of dimension large enough (larger than 2n), there exists set of n points, x1, . . . ,xn ∈ X such that for all 1, . . . , n ∈ {±1}n,\n1√ n ≤ ∥∥∥∥∥ 1n n∑ t=1 txt ∥∥∥∥∥ . Hence we can conclude that for any β > 0, if dim(H)→∞, then\nfatβ(Flin(H,H?)) ≥ 1\nβ2 .\n180\nHence by Lemma 101 we can conclude that oracle complexity of offline optimization for the convex Lipschitz class in the dual case when the dimensionality is large enough is lower bounded by 1/2β2. Hence we can conclude that for large dimensional problems, mirror descent is near optimal even for offline optimization problem. Also as per the discussion in sub-section ?? we can also conclude that for statistical learning problem with any powerful oracle O, mirror descent is near optimal for convex Lipschitz problems in the dual case when dimension is large enough.\nThe Dvoretzky Roger’s theorem tells us very generally that for any large dimensional dual problem, mirror descent is always near optimal. However the dimension of the problem needs to be very large (of order exponential in 1/ or more) for this claim to be true. While at complete generality we can only ensure this much, for many commonly encountered problems one cash provide lower bounds for fat-shattering dimension of order 1/ 2 or worse even when dimensionality is only as large as some polynomial in 1/ . More specifically, for the case when the pair H/X are either `d2/`d2 or `d1/`d∞, fat-shattering dimension can be lower bound by 1/ 2 as long as dimension is larger than order 1/ 2. Hence we can conclude that for these cases, when we are dealing with high dimensional convex optimization problems when dimension is as large as order 1/ 2 where is desired sub-optimality, then mirror descent is near optimal even for offline optimization problem. This result can also be extended to `p/`q pair case when p ∈ [2,∞) and Schatten norm counterparts of all the `p cases. We would also like to point out that such high dimensional (relative to desired sub-optimality) problems are common in machine learning and high-dimensional statistics application for cases when dimension d is large than the sample size n. In all these cases mirror descent is also near optimal for offline convex optimization."
    }, {
      "heading" : "9.4 Statistical Learning With Distributed Oracles",
      "text" : "In the previous chapter, in section ?? we showed that for most reasonable statistical convex learning problems, stochastic mirror descent is near optimal not only in terms of learning rate but also in terms of number of gradient access (or even any other local oracle information) needed to guarantee excess risk smaller than some target value. Of course in the result we counted accessing gradient at a point of one point sample point as one oracle query. While we abstract off oracle as a black box, in reality the answer that the oracle computes, example gradient, is generally needs to be calculated by the learning algorithm itself. With the growth of cluster and grid computing and its influence in the design of machine learning and optimization algorithms one may wonder if having access to several machines can help speed up learning. For instance one could think of performing gradient descent or mirror descent on training sample but then calculate gradient of empirical average loss in parallel so that computation of in fact several gradients is done more or less in the time of computing one. In this section we try to formalize a more powerful oracle based statistical learning model that can capture such distributed computing scenarios. We show that for most reasonable cases in high dimension for statistical convex learning (of ZLip(X )) even having such more powerful oracles do not help and the single pass gradient based mirror descent is near optimal in terms of efficiency even when compared against learning algorithms that have access to these more powerful distributed oracles. In short we show that parallelization does not help for the case when instance set is ZLip(X ).\n181\nMore formally, in the oracle-based statistical learning model we consider, learner has access to a local distributed oracle O : H × ⋃ n∈NZn 7→ I that can provide answers when queried at a point about multiple instances simultaneously. Of course here local means the oracle is local w.r.t. the sequence of instances z1, . . . , zn it is queried on at any times. The corresponding learning protocol is given below.\nPowerful Oracle-based Statistical Learning Protocol : Sample S = (z1, . . . , zn) ∼ Dn for t = 1 to m Learner picks hypothesis ht ∈ H̄ Oracle provides answer It = O(ht, S) end for\nNow to study efficiency of statistical learning algorithms that have access to such distributed oracles, similar to moff , the oracle complexity of offline optimization procedures, we introduce oracle complexity mstat of oracle-based statistical learning algorithms that have access to distributed oracles.\nDefinition 43. For a given class of convex functions corresponding to instance set Z and a distributed Oracle O, the distributed statistical oracle complexity of a given ”Oracle based optimization/learning algorithm”, AO, is defined as\nmstat( ,A O,Z) = inf { m ∈ N ∣∣∣∣∣ supD∈∆(Z) { LD(A O(I1, . . . , Im))− inf h∈H LD(h) } ≤ }\nFurther, for the given distributed oracle O, the O-distributed statistical oracle complexity of the given statistical convex learning problem is defined as mstat( ,Z,O) = infAO mstat( ,AO,Z).\nFurther we can define oracle complexity of a distributed oracle-based statistical convex learning problem (independent of any particular oracle) as follows :\nDefinition 44. For a given class of convex functions corresponding to instance setZ , the distributed statistical oracle complexity of the given distributed oracle-based statistical convex learning problem is defined as\nmstat( ,Z) = inf O mstat( ,Z,O) .\nNow to provide lower bounds on oracle complexity under this more powerful oracle based statistical learning scenario, we start by noticing that lower bounds on oracle complexity of offline optimization problems also provide lower bounds on oracle complexity for learning under the powerful oracle based statistical learning protocol. The reasoning for this is simple : If distribution D deterministically picked a single function then the problem is identical to offline optimization in the function class and querying oracle on a sequence or just the one function is exactly the same. The following proposition captures exactly this.\n182\nProposition 105. For anyH and convex learning instance space Z , we have that\nmstat( ,Z) ≥ moff( ,Z)\nIn the previous section specifically Theorem 104 we showed that for several reasonable, high dimensional offline convex optimization problem over instance space ZLip(X ) mirror descent is in fact near optimal even when we consider distributed oracle-based statistical convex learning algorithm. That is we have the following corollary which is trivial given previous proposition and Theorem 104.\nCorollary 106. If there exists some constant G ≥ 1 such that for any 1 < p′ < r ≤ 2\nCp′ ≤ G C iidr\nand it is true that for some V > 0 and p ∈ (1,∞),\nmstat( ,ZLip(X )) ≤ ( V )q where q = p/(p− 1). Then there exists function Ψ and step size η using which the stochastic mirror descent algorithm enjoys the guarantee\nmstat( ,AMD,Z(X )) ≤\n( 54018 G V log5 ( 1 ) )r\nHowever note that mirror descent is a gradient-based single pass algorithm that at each iteration only queries gradient at a single sample point. Hence we see that for these convex learning problems over instance space ZLip(X ), stochastic mirror descent is near optimal and that having access to any kind of distributed local oracle does not help. That is in the worst case parallelization does not help."
    }, {
      "heading" : "9.5 Detailed Proofs",
      "text" : "Proof of Lemma 101. The proof is essentially a vdery simple modification of the one provided by Nemirovski\nand Yudin in Section 4.4.2 of [1]. We provide an abridged version here with the appropriate modifications\nneeded to deal with the non-dual case with a few minor alterations to relate to fat-shattering dimension. To prove the lower bound, we first start by picking x1, . . . ,xm ∈ X add s1, . . . , sm ∈ R. Now the functions we shall consider are of form\nz (h; (x1, s1), . . . , (xm, sm)) = max i∈[m]\ni(〈h,−xi〉+ si)\n183\nwhere ∈ {±1}m. Notice that each z ∈ ZLip(X ). Note also that for any ∈ {±1}m,\n− inf h∈H z (h; (x1, s1), . . . , (xm, sm)) = − inf h∈H max i∈[m] i(〈h,−xi〉+ si) = sup h∈H min i∈[m] i(〈h,xi〉 − si) .\n(9.5.1)\nRemember that we want to show that for any AO there exists a function that requires at least m calls to some Oracle O to ensure sub-optimality less than > 0. The first thing we notice is that the family of functions we consider are piece wise linear and so any local oracle can give no more information that function value and gradient at point of query. Now given an Optimization algorithm AO the exact function we shall use for the lower bound will be constructed in m steps based on the algorithm and the choosen x1, . . . ,xm ∈ X add s1, . . . , sm ∈ R. The procedure for constructing the function is given below :\nInitialize I1 = [m] for t = 1 to m\nAO picks ht ∈ H for query i(t) = argmax\ni∈It {|〈ht,−xi〉+ si|}\nt =\n{ +1 if ( 〈 ht,−xi(t) 〉 + si(t)) ≥ 0\n−1 otherwise It+1 = It \\ {i(t)} zt(h) = maxj∈[t] { j( 〈 h,−xi(j) 〉 + si(j))\n} Return answer to query as It = O(ht, zt).\nend for\nThe first thing we notice about zm is that it is of the form :\nzm(·) = z (·; (xi(1), si(1)), . . . , (xi(m), si(m))) (9.5.2)\nwhere 1, . . . , m are given by the procedure above. Next, zm is such that, for any i ∈ [m] and any local oracle O,\nO(hi, zm) = O(hi, zi)\nhence the h1, . . . ,hm returned by the algorithm when it is presented with function fm is the same as the corresponding ones in the above procedure. Finally, by the way the functions are constructed (specifically\nthe way t is picked),\nzm(hm) ≥ 0\n184\nHence we conclude that\nzm(hm)− inf h∈H zm(h) ≥ − inf h∈H zm(h)\n= − inf h∈H z (h; (xi(1), si(1)), . . . , (xi(m), si(m))) (by Eq. 9.5.2)\n= sup h∈H min i∈[m]\ni(〈h,xi〉 − si) (by Eq. 9.5.1)\n≥ inf ∈{±1}m sup h∈H min i∈[m] i(〈h,xi〉 − si) .\nFurthermore note that the choice of x1, . . . ,xm ∈ X and s1, . . . , sm are arbitrary. Hence we can conclude that for any β > 0, if for any m there exists x1, . . . ,xm ∈ X and s1, . . . , sm ∈ R such that\ninf ∈{±1}m sup h∈H min i∈[m]\ni(〈h,xi〉 − si) > β ,\nthen no oracle based algorithm can achieve sub-optimality smaller than β in m or less steps. However note\nthat this is exactly the definition of fat-shattering dimension at scale 2β (Definition 13) for the linear class\ngiven by\nF = {x 7→ 〈h,x〉 : h ∈ H} .\nHence we conclude the lemma statement.\nProof of Theorem 102. The first inequality is a direct consequence of Lemma 101. For the upper bound on the Rademacher complexity note that, for any n ∈ N, by the refined Dudley integral bound we have that :\nRiidn (Flin(H,X )) ≤ inf α>0 4α+ 10 ∫ 1 α √ fatiidβ (Flin(H,X )) log(n) n dβ  We now divide the analysis into two cases, first where q ∈ [2,∞) and next where q ∈ (0, 2). We start for the case when q ∈ [2,∞) and see that using the assumption of this theorem and Lemma 101 we see that for any\n185\nq ∈ [2,∞):\nRiidn (Flin(H,X )) ≤ inf α>0\n{ 4α+ √ V q log(n)\nn 10 ∫ 1 α 1 β q 2 dβ }\n≤ inf α>0\n{ 4α+ √ V q log(n)\nn 10 ∫ 1 α ( q 2 − 1 ) log(1/β) + 1 β q 2 dβ }\n≤ inf α>0\n{ 4α+ 10 √ V q log(n)\nn\n[ log(β)\nβ q 2−1 ]1 α }\n≤ inf α>0\n{ 4α+ 10 √ V q log(n)\nn\nlog(1/α)\nα q 2−1\n}\n≤ 9V log 1+ 1q (n)\nn1/q\nwhere in the last step above we used the value α = V log 1/q(n)\nn1/q . Now we turn our attention to case when q ∈ (0, 2). As for this case we simply note that ( V )q ≤ (V )2 and so using the case when q = 2 we conclude that\nRiidn (Flin(H,X )) ≤ 9V log3/2(n)\nn1/2\nThis concludes the proof."
    }, {
      "heading" : "9.6 Discussion",
      "text" : "The key result of this chapter is that for most reasonable cases, if the dimension of the vector space B is large enough, then stochastic mirror descent algorithm is near optimal even for offline convex optimization for instance space ZLip(X ). We further show that for statistical convex learning problems over instance space ZLip(X ) even when we consider learning algorithm that use distributed oracles, (ie. uses distributed computation of local oracle information like gradients etc.) mirror descent is still near optimal and parallelization does not help.\n186\nChapter 10\nConclusion and Future Work\nIn this section we delineate some important questions that are related to the work in this dissertation. We also discuss some further directions of research. Finally we summarize and give some concluding remarks."
    }, {
      "heading" : "10.1 Open Problems",
      "text" : ""
    }, {
      "heading" : "10.1.1 Online Optimization and Stability",
      "text" : "Another direction yet to be explored is the question of online learnability in the general learning setting. In the statistical paradigm we used the tool of stability and properties of asymptotic empirical minimizer of learning rule to determine learnability for the general setting. We would like to explore the problem of online learnability in the general setting on similar lines.\nQuestion 1. Are there properties analogous to stability and AERM property in the online paradigm that guarantee online learnability in the general setting for learning ?\nQuestion 2. Can we provide a generic strategy for Learner in the online learning framework that guarantees diminishing regret whenever the problem is learnable ?"
    }, {
      "heading" : "10.1.2 Upper Bounding Oracle Complexity in Terms of Fat-Shattering Dimension",
      "text" : "In the second part of the dissertation especially in Chapter ?? we showed that the oracle complexity of offline convex optimization problem, moff( ,ZLip(X )) is lower bounded by the fat shattering dimension of the associated linear class Flin(H,X ). Using this we showed that at least for supervised learning problems, if one can efficiently optimize convex function corresponding to the classZLip(X ) then one can also statistically learn and efficiently. In the same chapter we also showed that for most reasonable cases, if dimension is large enough, then mirror descent is near optimal even for offline optimization. Using the results in the thesis one\n187\ncan also conclude that for these large dimensional cases, moff( ,ZLip(X )) can also be upper bounded by Õ(fat ). Can this result be generalized and can we show the upper bound on oracle complexity in terms of fat shattering dimension always hold? We pose the question.\nQuestion 3. Is it always true that\nmoff( ,ZLip(X )) ≤ fatc (Flin(H,X ))\nwhere c is some universal constants? If it is true, then can one give an optimization algorithm that has oracle\ncomplexity bounded by fat-shattering dimension?"
    }, {
      "heading" : "10.2 Further Directions",
      "text" : "The thesis mainly covers the story of learning from the perspective of optimization and answers questions about learnability. However there were a few results that emerged out of results and techniques provided in this thesis and we delineate a few below.\nWe mainly considered two extreme scenarios while considering statistical and online learning framework. In statistical framework, instances were sampled iid and in the online learning framework instances picked adversarially. It is interesting to consider the intermediate scenarios where learner is not faced with a completely worst case adversary but is also not faced with iid sampling of instances. Maybe adversary might have some constraints on instances that can be chosen or choses instances in a stochastic way that is more complex than iid sampling. Such a scenario is analyzed in [88] based on techniques in Chapter 4. Another orthogonal way in which the results in the chapter were extended was to games beyond online learning to include games like Blackwell’s approachability, calibration etc. in [89, 90].\nResults in chapters ?? and ?? influenced and shaped the work in paper [91] where we showed how one can make appropriate changes to stochastic mirror descent and accelerated methods to include mini-batching (breaking sample into blocks and instead of updating in each step with single gradient update with average of the block of gradients). We showed that this helped in guaranteeing better time complexity with parallelization of these methods."
    }, {
      "heading" : "10.3 Summary",
      "text" : "An important question in the field of theoretical machine learning is that of learnability and learning rates. We have explored this question for various learning problems in both statistical and online learning frameworks. In the statistical learning framework we provide the first general characterization of learnability in the general setting using the notion of stability of learning algorithms. We also provided a generic algorithm for learning in the statistical learning framework. As for the problem of learnability in the online framework while we don’t yet have a complete picture we introduced various complexity measures analogous to the\n188\nones in statistical learning framework. We also provide characterization of online learnability for real valued supervised learning problem.\nAn integral part of machine learning is optimization. While the question of learnability and learning rates are central to machine learning theory, from a practical point of view one would like to consider problems that are efficiently learnable. To address this issue in a general way, we considered convex learning and optimization problems in both statistical and online learning framework. We used the notion of oracle complexity to address issue of efficiency. For the online learning problems, we showed mirror descent is universal and near optimal. That is whenever a convex problem is online learnable, it is learnable with near optimal rates using mirror descent. Since mirror descent is a first order method (sub-gradient based) we could infer that for online learning scenario mirror descent is near optimal in terms of both rates and oracle complexity. We also explored connections between learning in the various frameworks and oracle based optimization. For the statistical convex learning problem, unlike online setting, in general it is not true that mirror descent is universal. However we saw that for problems we would encounter in practical applications though, this was in fact the case. Mirror descent would indeed be near optimal. We also saw that for certain offline optimization problems in high enough dimensions, mirror descent can again shown to be near optimal.\nWe expect the work to provide a better understanding of learning algorithms especially from the perspective of optimization. While it is common that for machine learning practitioners optimization is often an after thought and is in a sense mainly a computational issue, through this work we would like to stress that learning can be seen as optimization and should in fact be seen as so. On the other hand, we also show some strong connections between optimization and showed how tools from learning theory can be used to prove results on optimization. Hence we would also like to stress overall the strong and inevitable connections between the two.\nIn this work we also used several concepts from the theory of Banach space geometry. It would certainly be interesting to see if more connections can be made and techniques from Banach space geometry be used to prove more results about learning and optimization.\n189"
    }, {
      "heading" : "Appendix A",
      "text" : "Relating Various Complexity Measures : Statistical Learning\nA.1 The Refined Dudley Integral: Bounding Rademacher Complexity\nwith L2 Covering Numbers\nWe shall find it simpler here to use the empirical Rademacher complexity for a given sample x1, . . . , xn [92]:\nR̂n(H) = Eσ∼Unif({±1}n) [ sup h∈H 1 n ∣∣∣∣∣ n∑ i=1 h(xi)σi ∣∣∣∣∣ ]\n(A.1.1)\nand theL2 covering number at scale > 0 specific to a sample x1, . . . , xn, denoted byN2 ( ,F , (x1, . . . , xn)) as the size of a minimal cover C such that\n∀f ∈ F ,∃f ∈ C s.t. √√√√ 1 n n∑ i=1 (f(zi)− f (zi))2 ≤ .\nWe will also denote Ê[f2] = 1n ∑n i=1 f 2(xi).\nWe state our bound in terms of the empirical Rademacher complexity and covering numbers. Taking a supremum over samples of size n, we get the same relationship between the worst-case Rademacher complexity and covering numbers, as is used in Section ??.\nLemma 107. For any function class F containing functions f : X 7→ R, we have that\nR̂n(F) ≤ inf α≥0 4α+ 10 ∫ supf∈F√Ê[f2] α √ logN ( ,F , (x1, . . . , xn)) n d  . 197\nProof. Let β0 = supf∈F √ Ê[f2] and for any j ∈ Z+ let βj = 2−j supf∈F √ Ê[f2]. The basic trick here is the idea of chaining. For each j let Ti be a (proper) L2-cover at scale βj of F for the given sample. For each f ∈ F and j, pick an f̂i ∈ Ti such that f̂i is an βi approximation of f . Now for any N , we express f by chaining as\nf = f − f̂N + N∑ i=1 ( f̂i − f̂i−1 ) where f̂0 = 0. Hence for any N we have that\nR̂n(F) = 1\nn Eσ sup f∈F n∑ i=1 σi f(xi)− f̂N (xi) + N∑ j=1 ( f̂j(xi)− f̂j−1(xi) ) ≤ 1 n Eσ [ sup f∈F n∑ i=1 σi ( f(xi)− f̂N (xi) )] + N∑ j=1 1 n Eσ [ sup f∈F n∑ i=1 σi ( f̂j(xi)− f̂j−1(xi) )]\n≤ 1 n √√√√ n∑ i=1 σ2i sup f∈F √√√√ n∑ i=1 (f(xi)− f̂N (xi)2 + N∑ j=1 1 n Eσ [ sup f∈F n∑ i=1 σi ( f̂j(xi)− f̂j−1(xi) )] ≤ βN + N∑ j=1 1 n Eσ [ sup f∈F n∑ i=1 σi ( f̂j(xi)− f̂j−1(xi) )] (A.1.2)\nwhere the step before last is due to Cauchy-Shwarz inequality and σ = [σ1, ..., σn] >. Now note that\n1 n n∑ i=1 (f̂j(xi)− f̂j−1(xi))2 = 1 n n∑ i=1 ( (f̂j(xi))− f(xi)) + (f(xi)− f̂j−1(xi)) )2 ≤ 2 n n∑ i=1 ( f̂j(xi))− f(xi) )2 + 2 n n∑ i=1 ( f(xi)− f̂j−1(xi)\n)2 ≤ 2β2j + 2β2j−1 = 6β2j .\nNow Massart’s finite class lemma [93] states that if for any function class G, supg∈G √ 1 n ∑n i=1 g(xi) 2 ≤ R,\nthen R̂n(G) ≤ √ 2R2 log(|G|) n . Applying this to function classes {f − f ′ : f ∈ Tj , f ′ ∈ Tj−1} (for each j)\n198\nwe get from Eq. (A.1.2) that for any N ,\nR̂n(F) ≤ βN + N∑ j=1 βj\n√ 12 log(|Tj | |Tj−1|)\nn\n≤ βN + N∑ j=1 βj\n√ 24 log |Tj |\nn\n≤ βN + 10 N∑ j=1 (βj − βj+1) √ log |Tj | n ≤ βN + 10 N∑ j=1 (βj − βj+1) √ log N (βj ,F , (x1, . . . , xn)) n\n≤ βN + 10 ∫ β0 βN+1 √ log N ( ,F , (x1, . . . , xn)) n d\nwhere the third step is because 2(βj − βj+1) = βj and we bounded √ 24 by 5. Now for any α > 0, pick N = sup{j : βj > 2α}. In this case we see that by our choice of N , βN+1 ≤ 2α and so βN = 2βN+1 ≤ 4 . Also note that since βN > 2α, βN+1 = βN2 > α. Hence we conclude that\nR̂n(F) ≤ 4α+ 10 ∫ supf∈F√Ê[f2] α √ log N ( ,F , (x1, . . . , xn)) n d .\nSince the choice of α was arbitrary we take an infimum over α.\nA.2 Bounding L∞ covering number by Fat-shattering Dimension\nThe following proposition and lemma are standard in statistical learning theory and their proof can be found for instance in [94]. We provide the statement and the proof of the Lemma for completeness and so that we can state it in the exact form it is used in, in this work.\nProposition 108. LetH ⊆ {0, . . . , k}X be a class of functions with fat2 = d. Then, we have,\nN∞(1/2,H, n) ≤ d∑ i=0 ( n i ) ki\nand specifically for n ≥ d this gives,\nN∞(1/2,H, n) ≤ ( ekn\nd\n)d .\n199\nLemma 109. For any function classH bounded by B and any α > 0 such that fatα < n, we have,\nN∞(α,H, n) ≤ ( 2eBn\nα fatα(H)\n)fatα(H) .\nProof. For any α > 0, define an α-discretization of the [−B,B] interval as Bα = {−B + α/2,−B + 3α/2, . . . ,−B + (2k + 1)α/2, . . .} for 0 ≤ k and (2k + 1)α ≤ 4B. Also for any a ∈ [−B,B], define bacα = argmin\nr∈Bα |r − a| with ties being broken by choosing the smaller discretization point. For a function\nh : X 7→ [−B,B] let the function bhcα be defined pointwise as bh(x)cα, and let bHcα = {bhcα : h ∈ H}. First, we prove that N∞(α,H, {xi}ni=1) ≤ N∞(α/2, bHcα, {xi}ni=1). Indeed, suppose the set V is a minimal α/2-cover of bHcα on {xi}ni=1. That is,\n∀hα ∈ bHcα, ∃v ∈ V s.t. |vi − hα(xi)| ≤ α/2 .\nPick any h ∈ H and let hα = bhcα. Then ‖h− hα‖∞ ≤ α/2 and for any i ∈ [n]\n|h(xi)− vi| ≤ |h(xi)− hα(xi)|+ |hα(xi)− vi| ≤ α,\nand so V also provides an L∞ cover at scale α.\nWe conclude that N∞(α,H, {xi}ni=1) ≤ N∞(α/2, bHcα, {xi}ni=1) = N∞(1/2,G, {xi}ni=1) where G = 1 αbHcα. The functions of G take on a discrete set of at most b2B/αc + 1 values. Obviously, by adding a constant to all the functions in G, we can make the set of values to be {0, . . . , b2B/αc}. We now apply Proposition 108 with an upper bound ∑d i=0 ( n i ) ki ≤ ( ekn d )d which holds for any n > d. This yields\nN∞(1/2,G, {xi}ni=1) ≤ ( 2eBn αfat2(G) )fat2(G) . It remains to prove fat2(G) ≤ fatα(H), or, equivalently (by scaling) fat2α(bHcα) ≤ fatα(H). To this end, suppose there exists a set {xni=1} of size d = fat2α(bHcα) such that there is an witness s1, . . . , sn with\n∀ ∈ {±1}d, ∃hα ∈ bHcα s.t. ∀i ∈ [d], i(hα(xi)− si) ≥ α .\nUsing the fact that for any h ∈ H and hα = bhcα we have ‖h− hα‖∞ ≤ α/2, it follows that\n∀ ∈ {±1}d, ∃h ∈ H s.t. ∀i ∈ [d], i(h(xi)− si) ≥ α/2 .\nThat is, s1, . . . , sn is a witness to α-shattering byH. Thus for any {xi}ni=1, as long as n > fatα\nN∞(α,H, {xi}ni=1}) ≤ N∞(α/2, bHcα, {xi}ni=1) ≤ (\n2eBn\nαfat2α(bHcα)\n)fat2α(bHcα) ≤ ( 2eBn\nαfatα\n)fatα(H) .\n200\nA.3 Relating Fat-shattering Dimension and Rademacher complexity\nThe following lemma upper bounds the fat-shattering dimension at scale ≥ Rn(H) in terms of the Rademacher complexity of the function class. The proof closely follows the arguments of Mendelson [95, discussion after Definition 4.2].\nLemma 110. For any hypothesis classH, any sample size n and any > Rn(H) we have that\nfat (H) ≤ 4 nRn(H)2\n2 .\nIn particular, ifRn(H) = √ R/n (the typical case), then fat (H) ≤ R/ 2.\nProof. Consider any ≥ Rn(H). Let x∗1, . . . , x∗fat be the set of fat shattered points. This means that there exists s1, . . . , sfat such that for any J ⊂ [fat ] there exists hJ ∈ H such that ∀i ∈ J, hJ(xi) ≥ si + and ∀i 6∈ J, hJ(xi) ≤ si − . Now consider a sample x1, . . . , xn′ of size n′ = d nfat efat , obtained by taking each x∗i and repeating it d nfat e times, i.e. xi = x ∗ b ifat c . Now, following Mendelson’s arguments:\nRn′(H) ≥ Eσ∼Unif{±1}n′  1 n′ sup h∈H ∣∣∣∣∣∣ n′∑ i=1 σih(xi) ∣∣∣∣∣∣ \n≥ 1 2 Eσ∼Unif{±1}n′  1 n′ sup h,h′∈H ∣∣∣∣∣∣ n′∑ i=1 σi(h(xi)− h′(xi)) ∣∣∣∣∣∣  (triangle inequality)\n= 1\n2 Eσ∼Unif{±1}n′  1 n′ sup h,h′∈H ∣∣∣∣∣∣ fat ∑ i=1 dn/fat e∑ j=1 σ(i−1)fat +j  (h(x∗i )− h′(x∗i )) ∣∣∣∣∣∣ \n≥ 1 2 Eσ∼Unif{±1}n′  1 n′ ∣∣∣∣∣∣ fat ∑ i=1 dn/fat e∑ j=1 σ(i−1)fat +j  (hR(x∗i )− hR(x∗i )) ∣∣∣∣∣∣ \n201\nwhere for each σ1, . . . , σn′ ,R ⊆ [fat ] is given byR = { i ∈ [fat ] ∣∣∣sign(∑dn/fat ej=1 σ(i−1)dn/fat e+j) ≥ 0}, hR is the function inH that -shatters the set R and hR be the function that shatters the complement of set R.\n≥ 1 2 Eσ∼Unif{±1}n′  1 n′ fat ∑ i=1 ∣∣∣∣∣∣ dn/fat e∑ j=1 σ(i−1)fat +j ∣∣∣∣∣∣ 2 \n≥ n′ fat ∑ i=1 Eσ∼Unif{±1}n′ ∣∣∣∣∣∣ dn/fat e∑ j=1 σ(i−1)fat +j ∣∣∣∣∣∣ \n≥ fat n′\n√ dn/fat e\n2 (Khintchine’s inequality)\n= √ 2 fat 2 n′ .\nWe can now conclude that:\nfat ≤ 2n′R2n′(H) 2 ≤ 4nR 2 n(H) 2\nwhere last inequality is because Rademacher complexity decreases with increase in number of samples and n ≤ n′ ≤ 2n (because ≥ Rn(H) which implies that fat < n).\n202"
    } ],
    "references" : [ {
      "title" : "Problem complexity and method efficiency in optimization",
      "author" : [ "A. Nemirovski", "D. Yudin" ],
      "venue" : "Nauka Publishers, Moscow,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1978
    }, {
      "title" : "Stochastic convex optimization",
      "author" : [ "S. Shalev-Shwartz", "O. Shamir", "N. Srebro", "K. Sridharan" ],
      "venue" : "Conference on Learning Theory,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Learnability and stability in the general learning setting",
      "author" : [ "S. Shalev-Shwartz", "O. Shamir", "N. Srebro", "K. Sridharan" ],
      "venue" : "Proceedings of the 22nd Annual Conference on Computational Learning Theory,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Learnability, stability and uniform convergence",
      "author" : [ "Shai Shalev-Shwartz", "Ohad Shamir", "Nathan Srebro", "Karthik Sridharan" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2010
    }, {
      "title" : "Online learning: Random averages, combinatorial parameters, and learnability",
      "author" : [ "A. Rakhlin", "K. Sridharan", "A. Tewari" ],
      "venue" : "Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "On the universality of online mirror descent. Arxiv",
      "author" : [ "Nathan Srebro", "Karthik Sridharan", "Ambuj Tewari" ],
      "venue" : null,
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2011
    }, {
      "title" : "Convex games in Banach spaces",
      "author" : [ "K. Sridharan", "A. Tewari" ],
      "venue" : "Proceedings of the 23nd Annual Conference on Learning Theory,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "The Nature of Statistical Learning Theory",
      "author" : [ "V.N. Vapnik" ],
      "venue" : "Springer,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Learnability and the Vapnik-Chervonenkis dimension",
      "author" : [ "Anselm Blumer", "Andrzej Ehrenfeucht", "David Haussler", "Manfred K. Warmuth" ],
      "venue" : "Journal of the Association for Computing Machinery,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1989
    }, {
      "title" : "Scale-sensitive dimensions, uniform convergence, and learnability",
      "author" : [ "N. Alon", "S. Ben-David", "N. Cesa-Bianchi", "D. Haussler" ],
      "venue" : "Journal of the ACM, 44(4):615–631,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Toward efficient agnostic learning",
      "author" : [ "M.J. Kearns", "R.E. Schapire", "L.M. Sellie" ],
      "venue" : "Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, pages 341–352, July",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "Decision theoretic generalizations of the PAC model for neural net and other learning applications",
      "author" : [ "D. Haussler" ],
      "venue" : "Information and Computation, 100(1):78–150,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "Chervonenkis",
      "author" : [ "V.N. Vapnik", "A. Ya" ],
      "venue" : "On the uniform convergence of relative frequencies of events to their probabilities. Theory of Probability and its applications, XVI(2):264–280,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 1971
    }, {
      "title" : "Neural Network Learning: Theoretical Foundations",
      "author" : [ "Martin Anthony", "Peter Bartlet" ],
      "venue" : null,
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1999
    }, {
      "title" : "Statistical Learning Theory",
      "author" : [ "V.N. Vapnik" ],
      "venue" : "Wiley,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "The sizes of compact subsets of Hilbert space and continuity of Gaussian processes",
      "author" : [ "R.M. Dudley" ],
      "venue" : "Journal of Functional Analysis, 1(3):290–330,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 1967
    }, {
      "title" : "Sur les problèmes aux dérivées partielles et leur signification physique",
      "author" : [ "J. Hadamard" ],
      "venue" : "Princeton University Bulletin, 13:49–52,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 1902
    }, {
      "title" : "On the stability of inverse problems",
      "author" : [ "A.N. Tikhonov" ],
      "venue" : "Dolk. Akad. Nauk SSSR, 39(5):195–198,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 1943
    }, {
      "title" : "A technique for the numerical solution of certain integral equations of the first kind",
      "author" : [ "D.L. Phillips" ],
      "venue" : "Journal of the ACM, 9(1):84–97,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 1962
    }, {
      "title" : "A finite sample distribution-free performance bound for local discrimination rules",
      "author" : [ "W. Rogers", "T. Wagner" ],
      "venue" : "Annals of Statistics, 6(3):506–514,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 1978
    }, {
      "title" : "A Probabilistic Theory of Pattern Recognition",
      "author" : [ "L. Devroye", "L. Györfi", "G. Lugosi" ],
      "venue" : "Springer,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Bias, variance, and arcing classifiers",
      "author" : [ "Leo Breiman" ],
      "venue" : "Technical Report 460,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 1996
    }, {
      "title" : "Algorithmic stability and sanity-check bounds for leave-one-out crossvalidation",
      "author" : [ "M. Kearns", "D. Ron" ],
      "venue" : "Neural Computation, 11(6):1427–1453,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Stability and generalization",
      "author" : [ "Olivier Bousquet", "André Elisseeff" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2002
    }, {
      "title" : "Almost-everywhere algorithmic stability and generalization error",
      "author" : [ "S. Kutin", "P. Niyogi" ],
      "venue" : "Proceedings of the 18th Conference in Uncertainty in Artificial Intelligence, pages 275–282,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Stability results in learning theory",
      "author" : [ "S. Rakhlin", "S. Mukherjee", "T. Poggio" ],
      "venue" : "Analysis and Applications, 3(4):397–419,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Learning theory: stability is sufficient for generalization and necessary and sufficient for consistency of empirical risk minimization",
      "author" : [ "S. Mukherjee", "P. Niyogi", "T. Poggio", "R. Rifkin" ],
      "venue" : "Advances in Computational Mathematics, 25(1-3):161–193,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Fast rates for regularized objectives",
      "author" : [ "K. Sridharan", "N. Srebro", "S. Shalev-Shwartz" ],
      "venue" : "Advances in Neural Information Processing Systems 22,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Online convex programming and generalized infinitesimal gradient ascent",
      "author" : [ "M. Zinkevich" ],
      "venue" : "ICML, pages 928–936,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Learnability and stability in the general learning setting",
      "author" : [ "S. Shalev-Shwartz", "O. Shamir", "N. Srebro", "K. Sridharan" ],
      "venue" : "Proceedings of the 22nd Annual Conference on Computational Learning Theory,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "On the convergence rate of good-turing estimators",
      "author" : [ "D.A. McAllester", "R.E. Schapire" ],
      "venue" : "Proceedings of the Thirteenth Annual Conference on Computational Learning Theory,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Multiclass learnability and the erm principle",
      "author" : [ "Amit Daniely", "Sivan Sabato", "Shai Ben-David", "Shai Shalev-Shwartz" ],
      "venue" : null,
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2011
    }, {
      "title" : "Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm",
      "author" : [ "N. Littlestone" ],
      "venue" : "Machine Learning, 2(4):285–318, 04",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 1988
    }, {
      "title" : "Agnostic online learning",
      "author" : [ "S. Ben-David", "D. Pal", "S. Shalev-Shwartz" ],
      "venue" : "Proceedings of the 22th Annual Conference on Learning Theory,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "A stochastic view of optimal regret through minimax duality",
      "author" : [ "J. Abernethy", "A. Agarwal", "P. Bartlett", "A. Rakhlin" ],
      "venue" : "Proceedings of the 22nd Annual Conference on Learning Theory,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Rademacher processes and bounding the risk of function learning",
      "author" : [ "V. Koltchinskii", "D. Panchenko" ],
      "venue" : "High Dimensional Probability II, 47:443–459,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Rademacher and gaussian complexities: risk bounds and structural results",
      "author" : [ "P.L. Bartlett", "S. Mendelson" ],
      "venue" : "J. Mach. Learn. Res., 3:463–482,",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "A few notes on statistical learning theory",
      "author" : [ "S. Mendelson" ],
      "venue" : "S. Mendelson and A. J. Smola, editors, Advanced Lectures in Machine Learning, LNCS 2600, Machine Learning Summer School 2002, Canberra, Australia, February 11-22, pages 1–40. Springer,",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Uniform Central Limit Theorems",
      "author" : [ "R.M. Dudley" ],
      "venue" : "Cambridge University Press,",
      "citeRegEx" : "40",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Probability in Banach Spaces",
      "author" : [ "M. Ledoux", "M. Talagrand" ],
      "venue" : "Springer-Verlag, New York,",
      "citeRegEx" : "41",
      "shortCiteRegEx" : null,
      "year" : 1991
    }, {
      "title" : "Chervonenkis",
      "author" : [ "V.N. Vapnik", "A. Ya" ],
      "venue" : "On the uniform convergence of relative frequencies of events to their probabilities. Theory of Probability and its Applications, 16(2):264–280,",
      "citeRegEx" : "42",
      "shortCiteRegEx" : null,
      "year" : 1971
    }, {
      "title" : "Scale-sensitive dimensions, uniform convergence, and learnability",
      "author" : [ "N. Alon", "S. Ben-David", "N. Cesa-Bianchi", "D. Haussler" ],
      "venue" : "Journal of the ACM, 44:615–631,",
      "citeRegEx" : "43",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Fat-shattering and the learnability of real-valued functions",
      "author" : [ "P.L. Bartlett", "P.M. Long", "R.C. Williamson" ],
      "venue" : "Journal of Computer and System Sciences, 52(3):434–452,",
      "citeRegEx" : "44",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "On the density of families of sets",
      "author" : [ "N. Sauer" ],
      "venue" : "J. Combinatorial Theory, 13:145–147,",
      "citeRegEx" : "45",
      "shortCiteRegEx" : null,
      "year" : 1972
    }, {
      "title" : "A combinatorial problem: Stability and order for models and theories in infinitary languages",
      "author" : [ "S. Shelah" ],
      "venue" : "Pac. J. Math, 4:247–261,",
      "citeRegEx" : "46",
      "shortCiteRegEx" : null,
      "year" : 1972
    }, {
      "title" : "The Probabilistic Method",
      "author" : [ "N. Alon", "J. Spencer" ],
      "venue" : "John Wiley & Sons, 2nd edition,",
      "citeRegEx" : "47",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Entropy and the combinatorial dimension",
      "author" : [ "S. Mendelson", "R. Vershynin" ],
      "venue" : "Inventiones mathematicae, 152:37–55,",
      "citeRegEx" : "48",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Some limit theorems for empirical processes",
      "author" : [ "E. Giné", "J. Zinn" ],
      "venue" : "Annals of Probability, 12(4):929– 989,",
      "citeRegEx" : "49",
      "shortCiteRegEx" : null,
      "year" : 1984
    }, {
      "title" : "Empirical Processes in M-Estimation",
      "author" : [ "S.A. van de Geer" ],
      "venue" : null,
      "citeRegEx" : "50",
      "shortCiteRegEx" : "50",
      "year" : 2000
    }, {
      "title" : "Some applications of concentration inequalities to statistics",
      "author" : [ "P. Massart" ],
      "venue" : "Annales de la Faculté des Sciences de Toulouse, IX(2):245–303,",
      "citeRegEx" : "51",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Prediction, Learning, and Games",
      "author" : [ "N. Cesa-Bianchi", "G. Lugosi" ],
      "venue" : "Cambridge University Press,",
      "citeRegEx" : "52",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Boosting the margin: A new explanation for the effectiveness of voting methods",
      "author" : [ "R.E. Schapire", "Y. Freund", "P. Bartlett", "W.S. Lee" ],
      "venue" : "The Annals of Statistics, pages 322–330,",
      "citeRegEx" : "53",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Empirical margin distributions and bounding the generalization error of combined classifiers",
      "author" : [ "V. Koltchinskii", "D. Panchenko" ],
      "venue" : "Annals of Statistics, 30(1):1–50,",
      "citeRegEx" : "54",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "From batch to transductive online learning",
      "author" : [ "S.M. Kakade", "A. Kalai" ],
      "venue" : "NIPS,",
      "citeRegEx" : "55",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "On prediction of individual sequences",
      "author" : [ "N. Cesa-Bianchi", "G. Lugosi" ],
      "venue" : "Annals of Statistics, pages 1865–1895,",
      "citeRegEx" : "56",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "The isotron algorithm: High-dimensional isotonic regression",
      "author" : [ "A. Tauman Kalai", "R. Sastry" ],
      "venue" : "Proceedings of the 22th Annual Conference on Learning Theory,",
      "citeRegEx" : "57",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "ε-entropy and ε-capacity of sets in function spaces",
      "author" : [ "A.N. Kolmogorov", "V.M. Tikhomirov" ],
      "venue" : "Uspekhi Matematicheskikh Nauk, 14(2):3–86,",
      "citeRegEx" : "58",
      "shortCiteRegEx" : null,
      "year" : 1959
    }, {
      "title" : "Local rademacher complexities",
      "author" : [ "P.L. Bartlett", "O. Bousquet", "S. Mendelson" ],
      "venue" : "Annals of Statistics, 33 (4):1497–1537,",
      "citeRegEx" : "59",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Smoothness, low-noise and fast rates",
      "author" : [ "Nathan Srebro", "Karthik Sridharan", "Ambuj Tewari" ],
      "venue" : null,
      "citeRegEx" : "60",
      "shortCiteRegEx" : "60",
      "year" : 2010
    }, {
      "title" : "On the generalization ability of on-line learning algorithms",
      "author" : [ "N. Cesa-Bianchi", "A. Conconi", "C. Gentile" ],
      "venue" : "IEEE Trans. on Information Theory, 50(9):2050–2057, September",
      "citeRegEx" : "62",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Logarithmic regret algorithms for online convex optimization",
      "author" : [ "E. Hazan", "A. Kalai", "S. Kale", "A. Agarwal" ],
      "venue" : "Proceedings of the Nineteenth Annual Conference on Computational Learning Theory,",
      "citeRegEx" : "63",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Logarithmic regret algorithms for strictly convex repeated games",
      "author" : [ "S. Shalev-Shwartz", "Y. Singer" ],
      "venue" : "Technical report, The Hebrew University,",
      "citeRegEx" : "64",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Martingales with values in uniformly convex spaces",
      "author" : [ "G. Pisier" ],
      "venue" : "Israel Journal of Mathematics, 20(3–4): 326–350,",
      "citeRegEx" : "65",
      "shortCiteRegEx" : null,
      "year" : 1975
    }, {
      "title" : "Martingales in banach spaces (in connection with type and cotype)",
      "author" : [ "G. Pisier" ],
      "venue" : "Winter School/IHP Graduate Course,",
      "citeRegEx" : "66",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Sharp uniform convexity and smoothness inequalities for trace norms",
      "author" : [ "Keith Ball", "Eric A. Carlen", "Elliott H. Lieb" ],
      "venue" : "Invent. Math.,",
      "citeRegEx" : "67",
      "shortCiteRegEx" : "67",
      "year" : 1994
    }, {
      "title" : "Maximum-margin matrix factorization",
      "author" : [ "Nathan Srebro", "Jason D.M. Rennie", "Tommi S. Jaakola" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "69",
      "shortCiteRegEx" : "69",
      "year" : 2005
    }, {
      "title" : "Rank, trace-norm and max-norm",
      "author" : [ "Nathan Srebro", "Adi Shraibman" ],
      "venue" : "In Proceedings of the 18th Annual Conference on Learning Theory,",
      "citeRegEx" : "70",
      "shortCiteRegEx" : "70",
      "year" : 2005
    }, {
      "title" : "Regularization and variable selection via the elastic net",
      "author" : [ "Hui Zou", "Trevor Hastie" ],
      "venue" : "Journal of the Royal Statistical Society, Series B,",
      "citeRegEx" : "71",
      "shortCiteRegEx" : "71",
      "year" : 2005
    }, {
      "title" : "Sparse and low-rank matrix decompositions",
      "author" : [ "V. Chandrasekaran", "S. Sanghavi", "P. Parrilo", "A. Willsky" ],
      "venue" : "IFAC Symposium on System Identification,",
      "citeRegEx" : "72",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "A Dirty Model for Multi-task Learning",
      "author" : [ "Ali Jalali", "Pradeep Ravikumar", "Sujay Sanghavi", "Chao Ruan" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "73",
      "shortCiteRegEx" : "73",
      "year" : 2010
    }, {
      "title" : "Type, cotype and k-convexity",
      "author" : [ "Bernard Maurey" ],
      "venue" : "VOLUME",
      "citeRegEx" : "74",
      "shortCiteRegEx" : "74",
      "year" : 2003
    }, {
      "title" : "Classical Banach Spaces I and II. Classics in Mathematics",
      "author" : [ "Joram Lindenstrauss", "Lior Tzafriri" ],
      "venue" : null,
      "citeRegEx" : "75",
      "shortCiteRegEx" : "75",
      "year" : 1996
    }, {
      "title" : "Theoremes de factorisation dans les espaces reticules",
      "author" : [ "J.L. Krivine" ],
      "venue" : "Seminaire MAurey-Schwartz 1973- 74: Exp. Nos. 22 and 23, Ecole Polytech, Paris,",
      "citeRegEx" : "76",
      "shortCiteRegEx" : null,
      "year" : 1974
    }, {
      "title" : "Series de variables aleatoires vectorielles independantes et proprietes geometriques des espaces de banach",
      "author" : [ "Bernard Maurey", "Gilles Pisier" ],
      "venue" : "Studia Math.,",
      "citeRegEx" : "77",
      "shortCiteRegEx" : "77",
      "year" : 1976
    }, {
      "title" : "Random martingale transform inequalities",
      "author" : [ "D.J.H. Garling" ],
      "venue" : "Probability in Banach spaces 6 (Sandbjerg, 1986), (20):pp. 101–119,",
      "citeRegEx" : "78",
      "shortCiteRegEx" : null,
      "year" : 1986
    }, {
      "title" : "Vector-valued decoupling and the burkholder-davis gundy inequality",
      "author" : [ "Sonja Cox", "Mark Veraar" ],
      "venue" : "Technical Report arXiv:1107.2218,",
      "citeRegEx" : "79",
      "shortCiteRegEx" : "79",
      "year" : 2011
    }, {
      "title" : "Some remarks on tangent martingale difference sequences in l1-spaces",
      "author" : [ "Sonja Cox", "Mark Veraar" ],
      "venue" : "Technical Report",
      "citeRegEx" : "80",
      "shortCiteRegEx" : "80",
      "year" : 2007
    }, {
      "title" : "On a domination of sums of random variables by sums of conditionally independent ones",
      "author" : [ "Pawe? Hitczenko", "Stephen J. Montgomery-Smith" ],
      "venue" : "Mathematical Proceedings of the Cambridge Philosophical Society,",
      "citeRegEx" : "81",
      "shortCiteRegEx" : "81",
      "year" : 1996
    }, {
      "title" : "On a domination of sums of random variables by sums of conditionally independent ones",
      "author" : [ "Pawel Hitczenko" ],
      "venue" : "The Annals of Probability,",
      "citeRegEx" : "82",
      "shortCiteRegEx" : "82",
      "year" : 1994
    }, {
      "title" : "Comparison of moments for tangent sequences of random variables",
      "author" : [ "Pawel Hitczenko" ],
      "venue" : "Probability Theory and Related Fields,",
      "citeRegEx" : "83",
      "shortCiteRegEx" : "83",
      "year" : 1988
    }, {
      "title" : "Efficient online learning via randomized rounding",
      "author" : [ "Nicol Cesa-Bianchi", "Ohad Shamir" ],
      "venue" : "Neural Information Processing System,",
      "citeRegEx" : "84",
      "shortCiteRegEx" : "84",
      "year" : 2011
    }, {
      "title" : "Some random series of functions",
      "author" : [ "J.-P. Kahane" ],
      "venue" : "2nd edition,",
      "citeRegEx" : "85",
      "shortCiteRegEx" : null,
      "year" : 1985
    }, {
      "title" : "Martingale transforms",
      "author" : [ "D.L. Burkholder" ],
      "venue" : "The Annals of Mathematical Statistics, 37(6):pp. 1494–1504,",
      "citeRegEx" : "86",
      "shortCiteRegEx" : null,
      "year" : 1966
    }, {
      "title" : "Chapter 1 basic concepts in the geometry of banach spaces. volume 1 of Handbook of the Geometry of Banach Spaces, pages 1 – 84",
      "author" : [ "William B. Johnson", "Joram Lindenstrauss" ],
      "venue" : "Elsevier Science B.V.,",
      "citeRegEx" : "87",
      "shortCiteRegEx" : "87",
      "year" : 2001
    }, {
      "title" : "Online learning: Stochastic and constrained adversaries",
      "author" : [ "Alexander Rakhlin", "Karthik Sridharan", "Ambuj Tewari" ],
      "venue" : null,
      "citeRegEx" : "88",
      "shortCiteRegEx" : "88",
      "year" : 2011
    }, {
      "title" : "Complexity-based approach to calibration with checking rules",
      "author" : [ "Dean Foster", "Alexander Rakhlin", "Karthik Sridharan", "Ambuj Tewari" ],
      "venue" : null,
      "citeRegEx" : "90",
      "shortCiteRegEx" : "90",
      "year" : 2011
    }, {
      "title" : "Better mini-batch algorithms via accelerated gradient methods",
      "author" : [ "Andrew Cotter", "Nathan Srebro", "Ohad Shamir", "Karthik Sridharan" ],
      "venue" : null,
      "citeRegEx" : "91",
      "shortCiteRegEx" : "91",
      "year" : 2011
    }, {
      "title" : "Rademacher and Gaussian complexities: Risk bounds and structural results",
      "author" : [ "P.L. Bartlett", "S. Mendelson" ],
      "venue" : "JMLR, 3:463–482,",
      "citeRegEx" : "92",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Some applications of concentration inequalities to statistics",
      "author" : [ "P. Massart" ],
      "venue" : "Annales de la Faculté des Sciences de Toulouse, IX(2):245–303,",
      "citeRegEx" : "93",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Scale-sensitive dimensions, uniform convergence, and learnability",
      "author" : [ "N. Alon", "S. Ben-David", "N. Cesa-Bianchi", "D. Haussler" ],
      "venue" : "FOCS, 0:292–301,",
      "citeRegEx" : "94",
      "shortCiteRegEx" : null,
      "year" : 1993
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Specifically in the second half of the dissertation, focusing on convex learning problems, we show that mirror descent method originally introduced for convex optimization problems by Nemirovski and Yudin [1], is always near optimal for online learning problems, near optimal for most reasonable statistical learning problems and even near optimal for several high dimensional offline convex optimization problems.",
      "startOffset" : 205,
      "endOffset" : 208
    }, {
      "referenceID" : 0,
      "context" : "The next chapter, Chapter 6 introduces the mirror descent methods (see [1]) for the statistical and online convex learning problems described in the previous chapter and provides upper bounds on learning rate for them.",
      "startOffset" : 71,
      "endOffset" : 74
    }, {
      "referenceID" : 1,
      "context" : "Early versions of the results can be found in [2, 3].",
      "startOffset" : 46,
      "endOffset" : 52
    }, {
      "referenceID" : 2,
      "context" : "Early versions of the results can be found in [2, 3].",
      "startOffset" : 46,
      "endOffset" : 52
    }, {
      "referenceID" : 3,
      "context" : "See [4] for later version which is closer to the one presented in the chapter.",
      "startOffset" : 4,
      "endOffset" : 7
    }, {
      "referenceID" : 4,
      "context" : "The results in Chapter 4 are from joint work with Alexander Rakhlin and Ambuj Tewari and can be found in [5].",
      "startOffset" : 105,
      "endOffset" : 108
    }, {
      "referenceID" : 5,
      "context" : "In the second part of the dissertation, few of the result from Chapter 6 can be found in [6].",
      "startOffset" : 89,
      "endOffset" : 92
    }, {
      "referenceID" : 6,
      "context" : "Relating basic concept of martingale type and certain online convex learning problems was first done in [7].",
      "startOffset" : 104,
      "endOffset" : 107
    }, {
      "referenceID" : 5,
      "context" : "In [6] the result of universality and near optimality of mirror descent method from Chapter 7 is provided.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 7,
      "context" : "In this chapter we consider the problem of statistical learning in the general learning problems introduced by [8] where we would like to minimize a population risk functional (stochastic objective)",
      "startOffset" : 111,
      "endOffset" : 114
    }, {
      "referenceID" : 8,
      "context" : "2) for all h ∈ H converge uniformly to the population risk ([9, 10]).",
      "startOffset" : 60,
      "endOffset" : 67
    }, {
      "referenceID" : 9,
      "context" : "2) for all h ∈ H converge uniformly to the population risk ([9, 10]).",
      "startOffset" : 60,
      "endOffset" : 67
    }, {
      "referenceID" : 10,
      "context" : "It is a direct generalization of agnostic PAC-learnability ([11]) to Vapnik”s General Setting of Learning as studied by [12] and others.",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 11,
      "context" : "It is a direct generalization of agnostic PAC-learnability ([11]) to Vapnik”s General Setting of Learning as studied by [12] and others.",
      "startOffset" : 120,
      "endOffset" : 124
    }, {
      "referenceID" : 12,
      "context" : "For binary classification problems (where Z = X × {0, 1}, each hypothesis is a mapping from X to {0, 1}, and `(h; (x, y)) = 1{h(x)6=y}), [13] showed that the finiteness of a simple combinatorial measure known as the VC-dimension implies uniform convergence.",
      "startOffset" : 137,
      "endOffset" : 141
    }, {
      "referenceID" : 9,
      "context" : "mension at all finite scales now replaces the property of having finite VC-dimension, but the basic equivalence still holds: a problem is learnable if and only if uniform convergence holds ([10], see also [14], Chapter 19).",
      "startOffset" : 190,
      "endOffset" : 194
    }, {
      "referenceID" : 13,
      "context" : "mension at all finite scales now replaces the property of having finite VC-dimension, but the basic equivalence still holds: a problem is learnable if and only if uniform convergence holds ([10], see also [14], Chapter 19).",
      "startOffset" : 205,
      "endOffset" : 209
    }, {
      "referenceID" : 14,
      "context" : "To justify the necessity of uniform convergence even in the General Learning Setting, Vapnik attempted to show that in this setting, learnability with the ERM learning rule is equivalent to uniform convergence ([15]).",
      "startOffset" : 211,
      "endOffset" : 215
    }, {
      "referenceID" : 15,
      "context" : "Pollard’s bound [16] and Dudley integral bound [17] can be used to bound rates of uniform convergence in terms of covering numbers.",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 19,
      "context" : "In the context of modern learning theory1, the use of stability can be traced back at least to the work of [21], which noted that the sensitivity of a learning algorithm with regard to small changes in the sample controls the variance of the leave-one-out estimate.",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 20,
      "context" : "These results were later extended to other “local” learning algorithms (see [22] and references therein).",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 21,
      "context" : "In addition, practical methods have been developed to introduce stability into learning algorithms, in particular the Bagging technique introduced by [23].",
      "startOffset" : 150,
      "endOffset" : 154
    }, {
      "referenceID" : 22,
      "context" : "[24] showed that an algorithm operating on a hypothesis class with finite VC dimension is also stable (under a certain definition of stability).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 23,
      "context" : "[25] introduced a strong notion of stability (denoted as uniform stability) and showed that it is a sufficient condition for learnability, satisfied by popular learning algorithms such as regularized linear classifiers and regressors in Hilbert spaces (including several variants of SVM).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 24,
      "context" : "[26] introduced several weaker variants of stability, and showed how they are sufficient to obtain generalization bounds for algorithms stable in their sense.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 25,
      "context" : "A more recent line of work ([27],[28]) studied stability as a necessary condition for learnability.",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 26,
      "context" : "A more recent line of work ([27],[28]) studied stability as a necessary condition for learnability.",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 16,
      "context" : "The necessity of stability for so-called inverse problems to be well posed was first recognized by [18].",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 17,
      "context" : "The idea of regularization (that is, introducing stability into ill-posed inverse problems) became widely known through the works of [19] and [20].",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 18,
      "context" : "The idea of regularization (that is, introducing stability into ill-posed inverse problems) became widely known through the works of [19] and [20].",
      "startOffset" : 142,
      "endOffset" : 146
    }, {
      "referenceID" : 27,
      "context" : "When the domainH and the mapping φ are bounded, we have uniform convergence, in the sense that |L(h) − L̂(h)| is uniformly bounded over all h ∈ H (see [29]).",
      "startOffset" : 151,
      "endOffset" : 155
    }, {
      "referenceID" : 0,
      "context" : "where for now we let H to be the d-dimensional unit sphere H = { h ∈ R : ‖h‖ ≤ 1 } , we let z = (x, α) with α ∈ [0, 1] and x ∈ H, and we define u ∗ v to be an element-wise product.",
      "startOffset" : 112,
      "endOffset" : 118
    }, {
      "referenceID" : 0,
      "context" : "The confidences α are now a mapping of each coordinate to [0, 1].",
      "startOffset" : 58,
      "endOffset" : 64
    }, {
      "referenceID" : 0,
      "context" : "That is, an infinite sequence of reals in [0, 1].",
      "startOffset" : 42,
      "endOffset" : 48
    }, {
      "referenceID" : 28,
      "context" : "Since the problem is convex and (1 + )-Lipschitz and sinceH is the unit ball in the Hilbert space it follows (for instance from [30] + online to batch conversion) that irrespective of which distribution D over instances we use, LD(hS)− inf h∈H LD(h) ≤ √ 2(1 + ) n Thus we can conclude that the problem is learnable and in fact enjoys a rate of order 1 √ n , yet as we already say both uniform convergence and ERM (SAA approach) fails.",
      "startOffset" : 128,
      "endOffset" : 132
    }, {
      "referenceID" : 17,
      "context" : "In particular, adding ‖h‖ is the so-called Tikhonov Regularization technique, which has been known for more than half a century (see [19]).",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 23,
      "context" : "To relate our stability definitions to the ones in the literature, we note that our definitions of uniform-RO stability and strongly-uniform-RO stability are somewhat similar to uniform stability ([25]), which in our 3RO is short for “replace-one”.",
      "startOffset" : 197,
      "endOffset" : 201
    }, {
      "referenceID" : 24,
      "context" : "Moreover, we show that uniform-RO stable AERM’s characterize learnability, while it is well known that uniformly stable AERM’s are not necessary for learnability (see [26]).",
      "startOffset" : 167,
      "endOffset" : 171
    }, {
      "referenceID" : 25,
      "context" : "Our definition of average-RO stable is similar to “average stability” defined in [27], which in our notation is defined as ES∼Dn,z′ 1 [ `(A(S); z1)− `(A(S); z1) ] .",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 24,
      "context" : "For an elaborate study on other stability notions and their relationships, see [26].",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 23,
      "context" : "For instance, the notion of “all-i-LOO” below has been studied by several authors under different names [25, 28, 27].",
      "startOffset" : 104,
      "endOffset" : 116
    }, {
      "referenceID" : 26,
      "context" : "For instance, the notion of “all-i-LOO” below has been studied by several authors under different names [25, 28, 27].",
      "startOffset" : 104,
      "endOffset" : 116
    }, {
      "referenceID" : 25,
      "context" : "For instance, the notion of “all-i-LOO” below has been studied by several authors under different names [25, 28, 27].",
      "startOffset" : 104,
      "endOffset" : 116
    }, {
      "referenceID" : 26,
      "context" : "While this is possible in the General Learning Setting, in supervised classification every such AERM has to be LOO stable (this is essentially proven in [28]).",
      "startOffset" : 153,
      "endOffset" : 157
    }, {
      "referenceID" : 24,
      "context" : "This example is taken from [26].",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 0,
      "context" : "Let the instance space be [0, 1], the hypothesis space [0, 1] ∪ 2, and the objective function `(h, z) = 1{h=z}.",
      "startOffset" : 26,
      "endOffset" : 32
    }, {
      "referenceID" : 0,
      "context" : "Let the instance space be [0, 1], the hypothesis space [0, 1] ∪ 2, and the objective function `(h, z) = 1{h=z}.",
      "startOffset" : 55,
      "endOffset" : 61
    }, {
      "referenceID" : 29,
      "context" : "In [31], we show a version of Theorem 5, which asserts that a problem is learnable if and only if there is an on-average-LOO stable AERM.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 26,
      "context" : ", [28] and [27]).",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 25,
      "context" : ", [28] and [27]).",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 0,
      "context" : "In other words, randomization implicitly replaces the arbitrary hypothesis class H by the space of probability distributions overH, M = { α : H → [0, 1] s.",
      "startOffset" : 146,
      "endOffset" : 152
    }, {
      "referenceID" : 23,
      "context" : "Allowing randomization allows us to obtain results with respect to the following very strong notion of stability4: 4This definition of stability is very similar to the so-called “uniform stability”, discussed in [25], although [25] consider deterministic learning rules.",
      "startOffset" : 212,
      "endOffset" : 216
    }, {
      "referenceID" : 23,
      "context" : "Allowing randomization allows us to obtain results with respect to the following very strong notion of stability4: 4This definition of stability is very similar to the so-called “uniform stability”, discussed in [25], although [25] consider deterministic learning rules.",
      "startOffset" : 227,
      "endOffset" : 231
    }, {
      "referenceID" : 0,
      "context" : "Let the instance space be [0, 1], the hypothesis space consist of all finite subsets of [0, 1], and define the objective function as `(h, z) = 1{z/ ∈h}).",
      "startOffset" : 26,
      "endOffset" : 32
    }, {
      "referenceID" : 0,
      "context" : "Let the instance space be [0, 1], the hypothesis space consist of all finite subsets of [0, 1], and define the objective function as `(h, z) = 1{z/ ∈h}).",
      "startOffset" : 88,
      "endOffset" : 94
    }, {
      "referenceID" : 0,
      "context" : "Let the instance space be [0, 1].",
      "startOffset" : 26,
      "endOffset" : 32
    }, {
      "referenceID" : 0,
      "context" : "Let the hypothesis space consist of all finite subsets of [0, 1], and the objective function be the indicator function `(h, z) = 1{z∈h}.",
      "startOffset" : 58,
      "endOffset" : 64
    }, {
      "referenceID" : 0,
      "context" : "Consider the following learning rule: given a sample S ⊆ [0, 1], the learning rule checks if there are any two identical instances in the sample.",
      "startOffset" : 57,
      "endOffset" : 63
    }, {
      "referenceID" : 0,
      "context" : "Consider any continuous distribution on [0, 1].",
      "startOffset" : 40,
      "endOffset" : 46
    }, {
      "referenceID" : 0,
      "context" : "If the underlying distribution is continuous on [0, 1], then the returned hypothesis is S, which is countable hence , L(S) = 0 = infh L(h).",
      "startOffset" : 48,
      "endOffset" : 54
    }, {
      "referenceID" : 31,
      "context" : "Subsequently in a very recent work [33] it was shows that for multi-class learning problems with large number of classes, problems could still be learnable while uniform convergence fails and ERM approach may not be successful (at least not all ERM’s are good).",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 32,
      "context" : "Littlestone [34] has shown that, in the setting of prediction of binary outcomes, a certain combinatorial property of the binaryvalued function class characterizes learnability in the realizable case (that is, when the outcomes presented by the adversary are given according to some function in the class F).",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 33,
      "context" : "The result has been extended to the non-realizable case by Shai Ben-David, Dávid Pál and Shai Shalev-Shwartz [35] who named this combinatorial quantity the Littlestone’s dimension.",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 33,
      "context" : "Coincident with [35], minimax analysis of online convex optimization yielded new insights into the value of the game, its minimax dual representation, as well as algorithm-independent upper and lower bounds [36, 7].",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 34,
      "context" : "Coincident with [35], minimax analysis of online convex optimization yielded new insights into the value of the game, its minimax dual representation, as well as algorithm-independent upper and lower bounds [36, 7].",
      "startOffset" : 207,
      "endOffset" : 214
    }, {
      "referenceID" : 6,
      "context" : "Coincident with [35], minimax analysis of online convex optimization yielded new insights into the value of the game, its minimax dual representation, as well as algorithm-independent upper and lower bounds [36, 7].",
      "startOffset" : 207,
      "endOffset" : 214
    }, {
      "referenceID" : 35,
      "context" : "A natural generalization of Rademacher complexity [37, 38, 39], the sequential analogue possesses many of the nice properties of its classical cousin.",
      "startOffset" : 50,
      "endOffset" : 62
    }, {
      "referenceID" : 36,
      "context" : "A natural generalization of Rademacher complexity [37, 38, 39], the sequential analogue possesses many of the nice properties of its classical cousin.",
      "startOffset" : 50,
      "endOffset" : 62
    }, {
      "referenceID" : 37,
      "context" : "A natural generalization of Rademacher complexity [37, 38, 39], the sequential analogue possesses many of the nice properties of its classical cousin.",
      "startOffset" : 50,
      "endOffset" : 62
    }, {
      "referenceID" : 38,
      "context" : "Its proof requires considerably more work than the classical symmetrization proof [40, 39] due to the non-i.",
      "startOffset" : 82,
      "endOffset" : 90
    }, {
      "referenceID" : 37,
      "context" : "Its proof requires considerably more work than the classical symmetrization proof [40, 39] due to the non-i.",
      "startOffset" : 82,
      "endOffset" : 90
    }, {
      "referenceID" : 37,
      "context" : "In statistical learning theory, such structural results are obtained through properties of Rademacher averages [39, 38].",
      "startOffset" : 111,
      "endOffset" : 119
    }, {
      "referenceID" : 36,
      "context" : "In statistical learning theory, such structural results are obtained through properties of Rademacher averages [39, 38].",
      "startOffset" : 111,
      "endOffset" : 119
    }, {
      "referenceID" : 39,
      "context" : "As a special case of the result, we get a sequential counterpart of the Ledoux-Talagrand [41] contraction inequality.",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 39,
      "context" : "We remark that the lemma above encompasses the case of a Lipschitz φ : R 7→ R, as stated in [41, 38].",
      "startOffset" : 92,
      "endOffset" : 100
    }, {
      "referenceID" : 36,
      "context" : "We remark that the lemma above encompasses the case of a Lipschitz φ : R 7→ R, as stated in [41, 38].",
      "startOffset" : 92,
      "endOffset" : 100
    }, {
      "referenceID" : 37,
      "context" : "In the next proposition, we summarize some useful properties of Sequential Rademacher complexity (see [39, 38] for the results in the i.",
      "startOffset" : 102,
      "endOffset" : 110
    }, {
      "referenceID" : 36,
      "context" : "In the next proposition, we summarize some useful properties of Sequential Rademacher complexity (see [39, 38] for the results in the i.",
      "startOffset" : 102,
      "endOffset" : 110
    }, {
      "referenceID" : 40,
      "context" : "In statistical learning theory, learnability for binary classes of functions is characterized by the VapnikChervonenkis combinatorial dimension [42].",
      "startOffset" : 144,
      "endOffset" : 148
    }, {
      "referenceID" : 41,
      "context" : "For real-valued function classes, the corresponding notions are the scale-sensitive dimensions, such as Pγ [43, 44].",
      "startOffset" : 107,
      "endOffset" : 115
    }, {
      "referenceID" : 42,
      "context" : "For real-valued function classes, the corresponding notions are the scale-sensitive dimensions, such as Pγ [43, 44].",
      "startOffset" : 107,
      "endOffset" : 115
    }, {
      "referenceID" : 32,
      "context" : "For online learning, the notion characterizing learnability for binary prediction in the realizable case has been introduced by Littlestone [34] and extended to the non-realizable case of binary prediction by Shai Ben-David, Dávid Pál and Shai Shalev-Shwartz [35].",
      "startOffset" : 140,
      "endOffset" : 144
    }, {
      "referenceID" : 33,
      "context" : "For online learning, the notion characterizing learnability for binary prediction in the realizable case has been introduced by Littlestone [34] and extended to the non-realizable case of binary prediction by Shai Ben-David, Dávid Pál and Shai Shalev-Shwartz [35].",
      "startOffset" : 259,
      "endOffset" : 263
    }, {
      "referenceID" : 32,
      "context" : "Next, we define the Littlestone’s dimension [34, 35] and propose its scale-sensitive versions for real-valued function classes.",
      "startOffset" : 44,
      "endOffset" : 52
    }, {
      "referenceID" : 33,
      "context" : "Next, we define the Littlestone’s dimension [34, 35] and propose its scale-sensitive versions for real-valued function classes.",
      "startOffset" : 44,
      "endOffset" : 52
    }, {
      "referenceID" : 43,
      "context" : "In the binary case (k = 1 below), a reader might notice a similarity of Theorems 31 and 33 to the classical results due to Sauer [45], Shelah [46] (also, Perles and Shelah), and Vapnik and Chervonenkis [42].",
      "startOffset" : 129,
      "endOffset" : 133
    }, {
      "referenceID" : 44,
      "context" : "In the binary case (k = 1 below), a reader might notice a similarity of Theorems 31 and 33 to the classical results due to Sauer [45], Shelah [46] (also, Perles and Shelah), and Vapnik and Chervonenkis [42].",
      "startOffset" : 142,
      "endOffset" : 146
    }, {
      "referenceID" : 40,
      "context" : "In the binary case (k = 1 below), a reader might notice a similarity of Theorems 31 and 33 to the classical results due to Sauer [45], Shelah [46] (also, Perles and Shelah), and Vapnik and Chervonenkis [42].",
      "startOffset" : 202,
      "endOffset" : 206
    }, {
      "referenceID" : 45,
      "context" : "Alon and Spencer [47]).",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 41,
      "context" : "For the classical case of a cover based on a set points, the discretization idea appears in [43, 48].",
      "startOffset" : 92,
      "endOffset" : 100
    }, {
      "referenceID" : 46,
      "context" : "For the classical case of a cover based on a set points, the discretization idea appears in [43, 48].",
      "startOffset" : 92,
      "endOffset" : 100
    }, {
      "referenceID" : 46,
      "context" : "This point can be seen in the proof of Corollary 32 (also see [48]): the discretization process can assign almost identical function values to discrete values which differ by 1.",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 47,
      "context" : "When bounding deviations of means from expectations uniformly over the function class, the usual approach proceeds by a symmetrization argument [49] followed by passing to a cover of the function class and a union bound (e.",
      "startOffset" : 144,
      "endOffset" : 148
    }, {
      "referenceID" : 37,
      "context" : "[39]).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 48,
      "context" : "[50]).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 0,
      "context" : "A simple consequence of the above lemma is that if F ⊆ [0, 1]Z is a finite class, then for any given tree z we have that E [ max f∈F 1 n n ∑",
      "startOffset" : 55,
      "endOffset" : 61
    }, {
      "referenceID" : 50,
      "context" : "by the exponential weighted average forecaster algorithm (see [52]).",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 15,
      "context" : "However, as we show next, Lemma 34 goes well beyond just finite classes and can be used to get an analog of Dudley entropy bound [17] for the online setting through a chaining argument.",
      "startOffset" : 129,
      "endOffset" : 133
    }, {
      "referenceID" : 32,
      "context" : "[34, 35]) is the class of step functions on a bounded interval, which has a VC dimension 1, but is not learnable in the online setting.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 33,
      "context" : "[34, 35]) is the class of step functions on a bounded interval, which has a VC dimension 1, but is not learnable in the online setting.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 33,
      "context" : "We achieve this by generating “experts” in a way similar to [35].",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 51,
      "context" : "These results form the basis of the theory of large margin classifiers (see [53, 54]).",
      "startOffset" : 76,
      "endOffset" : 84
    }, {
      "referenceID" : 52,
      "context" : "These results form the basis of the theory of large margin classifiers (see [53, 54]).",
      "startOffset" : 76,
      "endOffset" : 84
    }, {
      "referenceID" : 33,
      "context" : "Recently, in the online setting, margin bounds have been shown through the concept of margin via the Littlestone dimension [35].",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 52,
      "context" : "We use ideas from [54] to do this.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 36,
      "context" : "bounds we provide are analogous to the ones considered in the batch setting in [38].",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 36,
      "context" : "The proposition is analogous to the one in [38] considered in the batch (classical) setting.",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 53,
      "context" : "In the transductive setting considered by Kakade and Kalai [55], it is assumed that m ≤ n and F are binaryvalued.",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 53,
      "context" : "Using the previous argument with c = en, we obtain a bound of 4 √ dn log(en) for the value of the game, matching [55] up to a constant 2.",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 54,
      "context" : "In particular, in the case of binary prediction, Cesa-Bianchi and Lugosi [56] proved upper bounds on the value of the game in terms of the (classical) Rademacher complexity and the (classical) Dudley integral.",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 54,
      "context" : "The particular assumption made in [56] is that experts are static.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 54,
      "context" : "We mention that the upper bound in Theorem 4 in [56] is tighter by a log n factor if a sharper bound on the `2 cover is considered.",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 50,
      "context" : "Finally, for the case of a finite number of experts, clearly N̂∞ ≤ N which gives the classical O( √ n logN) bound on the value of the game [52].",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 55,
      "context" : "5 Example: Isotron Recently, Kalai and Sastry [57] introduced a method called Isotron for learning Single Index Models (SIM).",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 55,
      "context" : "For brevity, we only describe the Idealized SIM problem from [57].",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 55,
      "context" : "The elegant computationally efficient method presented in [57] is motivated by Perceptron, and a natural open question posed by the authors is whether there is an online variant of Isotron.",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 37,
      "context" : "The other items follow similarly to Theorem 15 in [39] and we provide the proofs for completeness.",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 37,
      "context" : "Note that, unlike Rademacher complexity defined in [39], Sequential Rademacher complexity does not have the absolute value around the sum.",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 33,
      "context" : "For the lower bound, we use a construction similar to [35].",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 0,
      "context" : "Here we also assume that F ⊂ [0, 1]Z contains only non-negative functions (corresponds to loss class).",
      "startOffset" : 29,
      "endOffset" : 35
    }, {
      "referenceID" : 56,
      "context" : "First, by the classical result of Kolmogorov and Tihomirov [58], the class G of all bounded Lipschitz functions has small metric entropy: log N̂∞(α,G) = Θ(1/α).",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 57,
      "context" : "In the statistical learning framework the notion of Localized Rademacher complexity introduced in [59] can often be used to obtain fast rates.",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 0,
      "context" : "The mirror descent algorithm [1] is a natural generalization of gradient descent method for general convex learning problems.",
      "startOffset" : 29,
      "endOffset" : 32
    }, {
      "referenceID" : 0,
      "context" : "‖ · ‖ if for any h,h′ ∈ B: ∀α∈[0,1] Ψ (αh + (1− α)h′) ≤ αΨ(h) + (1− α)Ψ(h′)− α(1−α) q ‖h− h ′‖",
      "startOffset" : 30,
      "endOffset" : 35
    }, {
      "referenceID" : 58,
      "context" : "In [60] we had made the observation that any non-negative smooth convex loss satisfies this above self bounding property.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 58,
      "context" : "reasoning as in[60, 61].",
      "startOffset" : 15,
      "endOffset" : 23
    }, {
      "referenceID" : 59,
      "context" : "Refer to [62] for more details about online to batch conversion.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 0,
      "context" : "Now we upper bound the summation term by replacing each infimum over decompositions of∇`(ht, zt) into any arbitrary vectors ut and vt to vectors of specific form, ut = (1− α)∇φ(ht, zt) and vt = α∇φ(ht, zt) for some α ∈ [0, 1].",
      "startOffset" : 219,
      "endOffset" : 225
    }, {
      "referenceID" : 0,
      "context" : "α ∈ [0, 1],",
      "startOffset" : 4,
      "endOffset" : 10
    }, {
      "referenceID" : 0,
      "context" : "The mirror descent algorithm with uniformly convex Ψ functions were introduced by Nemirovski and Yudin in [1] for offline convex optimization.",
      "startOffset" : 106,
      "endOffset" : 109
    }, {
      "referenceID" : 0,
      "context" : "Specific upper bounds for offline convex optimization of ZLip dual case when H is the unit `p ball and X is the dual of H are provided in [1].",
      "startOffset" : 138,
      "endOffset" : 141
    }, {
      "referenceID" : 28,
      "context" : "For online convex optimization problem, online gradient descent (Euclidean case) was proposed by Zinkevich in[30].",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 60,
      "context" : "Faster rates when the losses are strongly convex in the Euclidean case for online gradient descent was proposed in [63].",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 61,
      "context" : "Mirror descent for general strongly convex objectives with log n/n rates was proposed and analyzed in [64].",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 62,
      "context" : "3 we extend Pisier’s result [65] to show that martingale type of the problem can be used to ensure existence of an appropriate uniformly convex function.",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 0,
      "context" : "Owing to this, for any p ∈ [1, 2] we define constant : Vp := inf { V ∣∣∣ ∀n ∈ N,Vn(H,Zlin(X )) ≤ V n−(1− 1 p )} (7.",
      "startOffset" : 27,
      "endOffset" : 33
    }, {
      "referenceID" : 1,
      "context" : "Owing to this, for any p ∈ [1, 2] we define constant : Vp := inf { V ∣∣∣ ∀n ∈ N,Vn(H,Zlin(X )) ≤ V n−(1− 1 p )} (7.",
      "startOffset" : 27,
      "endOffset" : 33
    }, {
      "referenceID" : 0,
      "context" : "To this end, similar to Vp for each p ∈ [1, 2] we can define:",
      "startOffset" : 40,
      "endOffset" : 46
    }, {
      "referenceID" : 1,
      "context" : "To this end, similar to Vp for each p ∈ [1, 2] we can define:",
      "startOffset" : 40,
      "endOffset" : 46
    }, {
      "referenceID" : 0,
      "context" : "For any p ∈ [1, 2]: Vp ≤ MDp",
      "startOffset" : 12,
      "endOffset" : 18
    }, {
      "referenceID" : 1,
      "context" : "For any p ∈ [1, 2]: Vp ≤ MDp",
      "startOffset" : 12,
      "endOffset" : 18
    }, {
      "referenceID" : 6,
      "context" : "In [7], it was shown that the concept of the Martingale type (also sometimes called the Haar type) of a Banach space and optimal rates for online convex optimization problem, where X andH are duals of each other, are closely related.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 62,
      "context" : "In this section we extend the classic notion of Martingale type of a Banach space (see for instance [65]) to one that accounts for the pair (H,X ).",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 62,
      "context" : "We point the reader to [65, 66] for more details.",
      "startOffset" : 23,
      "endOffset" : 31
    }, {
      "referenceID" : 63,
      "context" : "We point the reader to [65, 66] for more details.",
      "startOffset" : 23,
      "endOffset" : 31
    }, {
      "referenceID" : 0,
      "context" : "Further, for any p ∈ [1, 2] we also define,",
      "startOffset" : 21,
      "endOffset" : 27
    }, {
      "referenceID" : 1,
      "context" : "Further, for any p ∈ [1, 2] we also define,",
      "startOffset" : 21,
      "endOffset" : 27
    }, {
      "referenceID" : 62,
      "context" : "We then extend the result of Pisier in [65] to the “non-matching” setting combining it with the above theorem to finally get : Lemma 67.",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 0,
      "context" : "For any p ∈ [1, 2] and any p′ < p : Cp′ ≤ 1104 Vp (p−p′)2",
      "startOffset" : 12,
      "endOffset" : 18
    }, {
      "referenceID" : 1,
      "context" : "For any p ∈ [1, 2] and any p′ < p : Cp′ ≤ 1104 Vp (p−p′)2",
      "startOffset" : 12,
      "endOffset" : 18
    }, {
      "referenceID" : 62,
      "context" : "In [65], it was shown that a Banach space has Martingale type p (the classical notion) if and only if uniformly convex functions with certain properties exist on that space (w.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 0,
      "context" : "For any p ∈ [1, 2], Dp ≤ Cp.",
      "startOffset" : 12,
      "endOffset" : 18
    }, {
      "referenceID" : 1,
      "context" : "For any p ∈ [1, 2], Dp ≤ Cp.",
      "startOffset" : 12,
      "endOffset" : 18
    }, {
      "referenceID" : 62,
      "context" : "p′ < p, Cp′ ≤ Vp ≤ MDp ≤ Dp ≤ Cp Lemma 67 (extending Pisier’s result [65]) Definition of Vp (Generalized MD guarantee) Lemma 55 Construction of Ψ, Lemma 77 (extending Pisier’s result [65])",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 62,
      "context" : "p′ < p, Cp′ ≤ Vp ≤ MDp ≤ Dp ≤ Cp Lemma 67 (extending Pisier’s result [65]) Definition of Vp (Generalized MD guarantee) Lemma 55 Construction of Ψ, Lemma 77 (extending Pisier’s result [65])",
      "startOffset" : 183,
      "endOffset" : 187
    }, {
      "referenceID" : 63,
      "context" : "1) need not be such that (qΨq(h)) 1/q is a norm, with a simple modification as noted in [66] we can make it a norm.",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 64,
      "context" : "Ball et al [67] tightly calculate the constants of strong convexity of squared `p norms, establishing the tightness ofD2 when p1 = p2.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 64,
      "context" : "These results again follow using similar arguments as `p case and tight constants for strong convexity parameters of the Schatten norm from [67].",
      "startOffset" : 140,
      "endOffset" : 144
    }, {
      "referenceID" : 65,
      "context" : "4 Example : Max Norm Max-norm has been proposed as a convex matrix regularizer for application such as matrix completion [69].",
      "startOffset" : 121,
      "endOffset" : 125
    }, {
      "referenceID" : 66,
      "context" : "As noted in [70] the max-norm ball is equivalent, up to a factor two, to the convex hull of all rank one sign matrices.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 66,
      "context" : "This matches the stochastic (PAC) learning guarantee [70], and is the first guarantee we are aware of for the max norm matrix completion problem in the online setting.",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 67,
      "context" : "For instance one might want sparsity along with grouping effect in the linear predictors for which elastic-net type regularization introduced by Zou and Hastie [71] (this is captured by interpolation of the first type between `1 and `2 norms).",
      "startOffset" : 160,
      "endOffset" : 164
    }, {
      "referenceID" : 68,
      "context" : "al [72] (here one can use the interpolation norm of second type to interpolate between trace norm and element wise `1 norm).",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 69,
      "context" : "al [73].",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 69,
      "context" : "Similarly for the [73] case X could be either matrices with bounded entries or some other natural assumption that suits the problem.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 62,
      "context" : "techniques as in [65].",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 63,
      "context" : "We restate below a proposition from Pisier’s note (in [66]) Proposition 82 (Proposition 8.",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 63,
      "context" : "53 of [66]).",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 4,
      "context" : "Now before we proceed we recall from [5] that the value of the online learning game is equal to :",
      "startOffset" : 37,
      "endOffset" : 40
    }, {
      "referenceID" : 4,
      "context" : "Recall from [5] that the value of the online learning game is equal to :",
      "startOffset" : 12,
      "endOffset" : 15
    }, {
      "referenceID" : 0,
      "context" : "We first note that using mirror descent with uniformly convex function (as opposed to strongly convex) is not new and has been used in optimization setting in [1].",
      "startOffset" : 159,
      "endOffset" : 162
    }, {
      "referenceID" : 62,
      "context" : "While the classic definition of martingale type and the associated results are for dual pairs, in this chapter we extended results by [65] to handle non-dual scenario.",
      "startOffset" : 134,
      "endOffset" : 138
    }, {
      "referenceID" : 0,
      "context" : "Owing to the fact that optimal learning rates for the linear class provide lower bounds for convex lipschitz and supervised convex learning problems, we define for each p ∈ [1, 2] the constant V iid p analogous to the definition of Vp in the previous chapter.",
      "startOffset" : 173,
      "endOffset" : 179
    }, {
      "referenceID" : 1,
      "context" : "Owing to the fact that optimal learning rates for the linear class provide lower bounds for convex lipschitz and supervised convex learning problems, we define for each p ∈ [1, 2] the constant V iid p analogous to the definition of Vp in the previous chapter.",
      "startOffset" : 173,
      "endOffset" : 179
    }, {
      "referenceID" : 36,
      "context" : "The following proposition which is a direct consequence of results from [38] show that the learning rates for linear and supervised convex learning problems are upper bounded by the statistical Rademacher complexity.",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 36,
      "context" : "[38] For any set X ∈ B if Z(X ) is one of either Zlin(X ) or Zsup(X ), then for any n ∈ N,",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 36,
      "context" : "The inequality of linear class is a direct consequence of symmetrization (see [38]).",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 36,
      "context" : "The inequality for the supervised learning class Zsup(X ) additionally uses the Lipschitz contraction property (Theorem 10 (4) of [38] ) since the absolute loss is 1-Lipschitz.",
      "startOffset" : 130,
      "endOffset" : 134
    }, {
      "referenceID" : 70,
      "context" : "In this section we extend the classic notion of Rademacher type of a Banach space (see for instance [74]) to one that accounts for the pair (H,X ).",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 63,
      "context" : "We point the reader to [66] for more details.",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 0,
      "context" : "Further, for any p ∈ [1, 2] we also define constant C iid p , analogous to the definition of Cp in previous chapter.",
      "startOffset" : 21,
      "endOffset" : 27
    }, {
      "referenceID" : 1,
      "context" : "Further, for any p ∈ [1, 2] we also define constant C iid p , analogous to the definition of Cp in previous chapter.",
      "startOffset" : 21,
      "endOffset" : 27
    }, {
      "referenceID" : 71,
      "context" : "Definition 34 (Banach Lattice [75]).",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 72,
      "context" : "The main technology behind proving results about Banach lattices arises from the so called functional calculus over banach lattices introduced by Krivine [76] (See [75]).",
      "startOffset" : 154,
      "endOffset" : 158
    }, {
      "referenceID" : 71,
      "context" : "The main technology behind proving results about Banach lattices arises from the so called functional calculus over banach lattices introduced by Krivine [76] (See [75]).",
      "startOffset" : 164,
      "endOffset" : 168
    }, {
      "referenceID" : 71,
      "context" : "1 in [75]) which roughly states that if we prove any inequality involving continuous degree 1, homogenous equations involving finite number of real valued variables, then the same inequality is true with of course appropriate changes like ≤ replaced by and absolute value replaced by the lattice version and so on.",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 73,
      "context" : "This is due to the celebrated result of Maurey and Pisier [77] which (in the dual case) assures that any space with non-trivial type also has finite co-type.",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 74,
      "context" : "2 Decoupling Inequalities Another way to guarantee that Cp ≤ GC iid p for some finiteG is by using the so called decoupling inequalities (see [78, 79] for more details).",
      "startOffset" : 142,
      "endOffset" : 150
    }, {
      "referenceID" : 75,
      "context" : "2 Decoupling Inequalities Another way to guarantee that Cp ≤ GC iid p for some finiteG is by using the so called decoupling inequalities (see [78, 79] for more details).",
      "startOffset" : 142,
      "endOffset" : 150
    }, {
      "referenceID" : 74,
      "context" : "We would like to point out that the above definition is not the same as the decoupling inequalities in [78, 79] where the above needs to be true for all martingale difference sequences where as above we only consider Walsh-Paley martingales.",
      "startOffset" : 103,
      "endOffset" : 111
    }, {
      "referenceID" : 75,
      "context" : "We would like to point out that the above definition is not the same as the decoupling inequalities in [78, 79] where the above needs to be true for all martingale difference sequences where as above we only consider Walsh-Paley martingales.",
      "startOffset" : 103,
      "endOffset" : 111
    }, {
      "referenceID" : 75,
      "context" : "1 of [79]).",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 75,
      "context" : "Using some of the results in [79, 80, 81, 82, 83] we can conclude that for many interesting spaces we commonly encounter, there in fact even exists a universal 1-decoupling constant, call itBR.",
      "startOffset" : 29,
      "endOffset" : 49
    }, {
      "referenceID" : 76,
      "context" : "Using some of the results in [79, 80, 81, 82, 83] we can conclude that for many interesting spaces we commonly encounter, there in fact even exists a universal 1-decoupling constant, call itBR.",
      "startOffset" : 29,
      "endOffset" : 49
    }, {
      "referenceID" : 77,
      "context" : "Using some of the results in [79, 80, 81, 82, 83] we can conclude that for many interesting spaces we commonly encounter, there in fact even exists a universal 1-decoupling constant, call itBR.",
      "startOffset" : 29,
      "endOffset" : 49
    }, {
      "referenceID" : 78,
      "context" : "Using some of the results in [79, 80, 81, 82, 83] we can conclude that for many interesting spaces we commonly encounter, there in fact even exists a universal 1-decoupling constant, call itBR.",
      "startOffset" : 29,
      "endOffset" : 49
    }, {
      "referenceID" : 79,
      "context" : "Using some of the results in [79, 80, 81, 82, 83] we can conclude that for many interesting spaces we commonly encounter, there in fact even exists a universal 1-decoupling constant, call itBR.",
      "startOffset" : 29,
      "endOffset" : 49
    }, {
      "referenceID" : 75,
      "context" : "(This constantBR is the one referred to asDR in [79]).",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 75,
      "context" : "The following proposition proved in [79] (see also [80]) is particularly useful especially to provide decoupling inequalities for group norms and interpolation norms.",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 76,
      "context" : "The following proposition proved in [79] (see also [80]) is particularly useful especially to provide decoupling inequalities for group norms and interpolation norms.",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 75,
      "context" : "6 [79]).",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 75,
      "context" : "11 of [79] it has been shown that for p ≥ log2(d), the p-decoupling constant of `∞ spaces if bounded by 2BR.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 77,
      "context" : "In [81] it has been shown that a large class of Orlicz and Rearrangement invariant function spaces satisfy 1-decoupling inequality with universal constant",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 0,
      "context" : "It turns out that for `p norms, when p ∈ [1, 2] we have a universal constant for 1-decoupling by Proposition 96.",
      "startOffset" : 41,
      "endOffset" : 47
    }, {
      "referenceID" : 1,
      "context" : "It turns out that for `p norms, when p ∈ [1, 2] we have a universal constant for 1-decoupling by Proposition 96.",
      "startOffset" : 41,
      "endOffset" : 47
    }, {
      "referenceID" : 80,
      "context" : "In [84], in the transductive online setting for max norm it is shown that once can again use the SDP to obtain a poly-time algorithm.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 81,
      "context" : "We conclude the proof by using Kahane Inequality (see [85]) which asserts that for any p ∈ [1, 2], ( E ∥∥∥∥ n ∑",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 0,
      "context" : "We conclude the proof by using Kahane Inequality (see [85]) which asserts that for any p ∈ [1, 2], ( E ∥∥∥∥ n ∑",
      "startOffset" : 91,
      "endOffset" : 97
    }, {
      "referenceID" : 1,
      "context" : "We conclude the proof by using Kahane Inequality (see [85]) which asserts that for any p ∈ [1, 2], ( E ∥∥∥∥ n ∑",
      "startOffset" : 91,
      "endOffset" : 97
    }, {
      "referenceID" : 0,
      "context" : "For p ∈ [1, 2] the pair (H,X ) is said to be p-convex with constantKp if for any x1, .",
      "startOffset" : 8,
      "endOffset" : 14
    }, {
      "referenceID" : 1,
      "context" : "For p ∈ [1, 2] the pair (H,X ) is said to be p-convex with constantKp if for any x1, .",
      "startOffset" : 8,
      "endOffset" : 14
    }, {
      "referenceID" : 71,
      "context" : "1 of [75] by noting that expectation w.",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 71,
      "context" : "2 (iii) [75] we can conclude that",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 82,
      "context" : "t=1 t tat( ) ∣∣∣∣ p])1/p Burkholder’s Inequality [86]",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 71,
      "context" : "1 [75] (by expanding out the tree of depth n to its 2 − 1 elements and noting that expectation w.",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 0,
      "context" : "To address the issue of efficiency of optimization methods for the convex optimization problems, we use the notion of oracle complexity introduced by Nemirovski and Yudin in [1].",
      "startOffset" : 174,
      "endOffset" : 177
    }, {
      "referenceID" : 0,
      "context" : "1 introduces the oracle based offline optimization model of Nemirovski and Yudin [1].",
      "startOffset" : 81,
      "endOffset" : 84
    }, {
      "referenceID" : 0,
      "context" : "Such models have been introduced and analyzed in [1].",
      "startOffset" : 49,
      "endOffset" : 52
    }, {
      "referenceID" : 0,
      "context" : "To address this issue we use the notion of local oracle defined by Nemirovski and Yudin [1] and whenever we use the term oracle we mean local oracle.",
      "startOffset" : 88,
      "endOffset" : 91
    }, {
      "referenceID" : 0,
      "context" : "The following lemma which lower bounds oracle complexity by fat-shattering dimension of the corresponding linear function class is based on the proof technique for lower bounds on oracle complexity for offline optimization of convex Lipschitz function classes in [1].",
      "startOffset" : 263,
      "endOffset" : 266
    }, {
      "referenceID" : 83,
      "context" : "However it turns out that at least for the dual learning problems (when H = X ) if the dimension is large enough, by the celebrated Dvoretzky-Roger’s theorem (see for instance [87]), we can infer that for all β < , fat β (Flin(H,X )) is larger than order 1/β.",
      "startOffset" : 176,
      "endOffset" : 180
    }, {
      "referenceID" : 83,
      "context" : "In this case, the celebrated Dvoretzky Roger’s theorem (see [87] for geometric interpretation we use here) implies that for any Banach space of dimension large enough (larger than 2), there exists set of n points, x1, .",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 0,
      "context" : "2 of [1].",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 84,
      "context" : "Such a scenario is analyzed in [88] based on techniques in Chapter 4.",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 85,
      "context" : "in [89, 90].",
      "startOffset" : 3,
      "endOffset" : 11
    }, {
      "referenceID" : 86,
      "context" : "Results in chapters ?? and ?? influenced and shaped the work in paper [91] where we showed how one can make appropriate changes to stochastic mirror descent and accelerated methods to include mini-batching (breaking sample into blocks and instead of updating in each step with single gradient update with average of the block of gradients).",
      "startOffset" : 70,
      "endOffset" : 74
    } ],
    "year" : 2012,
    "abstractText" : "Optimization has always played a central role in machine learning and advances in the field of optimization and mathematical programming have greatly influenced machine learning models. However the connection between optimization and learning is much deeper : one can phrase statistical and online learning problems directly as corresponding optimization problems. In this dissertation I take this viewpoint and analyze learning problems in both the statistical and online learning frameworks from an optimization perspective. In doing so we develop a deeper understanding of the connections between statistical and online learning and between learning and optimization. The dissertation can roughly be divided into two parts. In the first part we consider the question of learnability and possible learning rates for general statistical and online learning problems without regard to tractability issues. In the second part we restrict ourselves to convex learning problems and address the issue of tractability for both online and statistical learning problems by considering the oracle complexity of these learning problems. I. We first consider the question of learnability and possible learning rates for statistical learning problems under the general learning setting. The notion of learnability was first introduced by Valiant (1984) for the problem of binary classification in the realizable case. Vapnik (1995) introduced the general learning setting as a unifying framework for the general problem of statistical learning from empirical data. In this framework the learner is provided with a sample of instances drawn i.i.d. from some distribution unknown to the learner. The goal of the learner is to pick a hypothesis with low expected loss based on the sample received. The question of learnability is well studied and fully characterized for binary classification using the Vapnik Chervonenkis (VC) theory and for real valued supervised learning problems using the theory of uniform convergence with tools like Rademacher complexity, covering numbers and fat-shattering dimension etc. However we show that for the general learning setting the traditional approach of using uniform convergence theory to characterize learnability fails. Specifically we phrase the learning problem as a stochastic optimization problem and construct an example of a convex problem where Stochastic Approximation (SA) approach provides successful learning guarantee but Empirical Risk Minimization (ERM) (or equivalently Sample Average Approximation (SAA) approach) fails to give any meaningful learning guarantee. This example establishes that for general learning problems the concept of uniform convergence fails to capture learnability and ERM/SAA ap-",
    "creator" : "LaTeX with hyperref package"
  }
}