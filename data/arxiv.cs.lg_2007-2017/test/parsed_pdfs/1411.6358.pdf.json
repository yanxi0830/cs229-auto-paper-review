{
  "name" : "1411.6358.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "A hybrid solution to improve iteration efficiency in the distributed learning",
    "authors" : [ "Junxiong Wang" ],
    "emails" : [ "chuangzhetianxia@gmail.com", "wangzh@hit.edu.cn", "zhaochenxu001@gmail.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n41 1.\n63 58\nv1 [\ncs .D\nC ]\n2 4\nN ov\n1. Introduction\nWith an explosion in the number of massive-scale data, the cost time of traditional solutions is unacceptable. As a result of it, the demand for scalable disturbed platforms and frameworks is rising in many areas including machine learning, data mining and pattern recognition.\nHadoop is widely used to speed up many machine learning algorithms applied in academy and industry. However, the serve problem about it is the lack of support for iterative programs. Thus lots of popular-used platforms and distributed frameworks appeared to optimize the iteration process for higher efficiency and less cost. A framework named Spark[1], widely used with a library for machine\nlearning (MLlib), has improved the efficiency in many applications. Its modified solution, Resilient Distributed Datasets[2], speed up the data storage to an amazing step. In addition, Haloop[3] makes lots of improvement on iteration. It provides some rules to consult when developing the machine learning project based on map/reduce.\nWhen it comes to applications, some slave nodes, taking up only a small percentage of all, always cost much more time than others in one iteration because of communication fault or their low efficiency. Traditional solutions cannot handle it as they have to calculate it again when failure occurs.\nThis paper presents a novel algorithm which is able to make a suitable balance in performance and efficiency. The master node doesn’t have to wait all the slave nodes. After a certain percentage of slaves has finished calculated and communicated with the master node, the master one will start next iteration rather than waiting others. This process can not only improve the efficiency dramatically but also have a high fault tolerance because some nodes’ fault do not have influence on this system. This algorithm is developed to decrease the total time, combining the synchronous and asynchronous process, resulting in reasonable efficiency and accuracy.\nWe discuss the relationship between the abandon rate and the accuracy with statistic. In next section, this algorithm is proved to converge and the speed of convergence is Q-linear convergence with the mathematics proof. It is such an excepted result that balance the efficiency and performance.\nThis idea can be applied to a list of algorithms including iterations such as Stochastic Gradient Descent, Conjugate Gradient Descent, L-BFGS and so on. Many existing platforms and framework can be improved with\nthis approach.\n2. Algorithm\nWhen our approach is applied in Gradient Descent, we give the following algorithm.\nSuppose machine num is M and examples in each machine are ζ. For each iteration, the number of slaves that master should wait is γ. That is to say, only γζ examples can be calculated.\nEstimating the least number of slave nodes which need to communicate with master node is the first step.\nAlgorithm 1 Calculate The Least Number of Slave Nodes Input: The capacity of data : N Confidence coefficient : α Relative error ξ\nExamples in each machine : ζ Output: Estimated Machine Number : γ\nγ = Nu2α/2\n(ξ2N + u2α/2) ∗ ζ ;\nAlgorithm 2 Master’s algorithm 1: while IsConvergence == false do 2: if received γ slave nodes then 3: θi+1 = θi − ηt\nγ\n∑γ j=1 θ j i\n4: end if 5: end while\nAlgorithm 3 Slaves’ algorithm\n1: Receive(θt) 2: θt+1 = θt− { 1\nζ\n∑ζ i=1(θ T K[xi]− yi)K[xi] + λθt\n}\n3: Send(θt+1)\n3. The Algorithm’s Convergence\nIn this section we will prove the convergence of this algorithm with an example of the quadratic programming. A definition of quadratic programming problem will be given first and then we will prove the convergence to explain the algorithm’s correctness.At the end of this section, we will discuss the speed of convergence.\n3.1. Brief Introduction\nWe donate X ∈ Rm×n and Y ∈ Rm as the example input and output space respectively. And set θ∗ ∈ Rl as the\noptimal solution to a quadratic programming, then\nθ ∗ = arg min\nθ∈Rl\n1\nm\nm ∑\ni=1\n(fθ(xi)− yi)2 + λ ∥ ∥θ ∥ ∥ 2\nl2 (1)\nwhere λ > 0 is the regularization parameter.\nDefinition 3.1. Denote function K is a kernel function\nwhere K[xi] =\n\n                     \nx2i1 xi1xi2\n... x2i2\nxi2xi3 ...\nx2in xi1 xi2\n... xin 1\n\n                      ∈ Rl.\nAbove all, θ∗ can be written as\nθ ∗ = arg min\nθ∈Rl\n1\nm\nm ∑\ni=1\n(θTK[xi]− yi)2 + λ ∥ ∥θ ∥ ∥ 2\nl2 (2)\nIn the t th iteration,θt can be updated as\nθ t+1 = θt−ηt{\n1\nω\nω ∑\ni=1\n(θTK[xi]− yi)K[xi] + λθt} (3)\n3.2. The Proof of Convergence\nAccording to Taylor formula, (3) can turn into\nf(θt+1) = f(θt)−\nηt∇f(θt)T { 1\nω\nω ∑\ni=1\n(θTK[xi]− yi)K[xi] + λθt}\n+O(0)\n(4)\nIf f(θt+1) − f(θt) < 0, then\nηt{ 1\nm\nm ∑\ni=1\n(θTK[xi]− yi)K[xi] + λθt}T\n∗ { 1 ω\nω ∑\ni=1\n(θTK[xi]− yi)K[xi] + λθt} > 0 (5)\nwhere the step size ηt > 0. In order to prove the correctness of (5),we mention some useful lemma.\nLemma 3.1. Donate the overall variance is σ2,we choose a random sample of n (sampled without repeating) capacity from N elements. The variance of the sample mean is\nσn = σ√ n\n√\nN − n N − 1\nThen, we proof this by statics theory. We define Z and z stand for the overall collection and sample collection respectively. So,\nσn =\n√ √ √ √ 1\nCnN\ncn N\n∑\ni=1\n(zi − Z) (6)\nThen,\nσ2n = 1\nCnN\nCn N ∑\ni=1\n[ (zi1 + zi2 + · · ·+ zin)\nn − Z]2\n= 1\nCnNn 2\nCn N ∑\ni=1\n[(zi1 + zi2 + · · ·+ zin)− nZ]2\n= 1\nCnNn 2 [\nCn N ∑\ni=1\n(zi1 + zi2 + · · ·+ zin)2\n− 2nZ Cn N ∑\ni=1\n(zi1 + zi2 + · · ·+ zin) + n2CnNZ 2 ]\n= 1\nCnNn 2 [\nCn N ∑\ni=1\n(zi1 + zi2 + · · ·+ zin)2\n− 2nZCn−1N−1 N ∑\nS=1\nZS + n 2CnNZ 2 ]\n= 1\nCnNn 2\nCn N ∑\ni=1\n(zi1 + zi2 + · · ·+ zin)2 − Z 2\n(7)\nEvery sample has a ZS , then the expansion of the sample sum square has one Z2S , so there are C n−1 N−1 Z 2 S of ∑M i=1(zi1 + zi2 + · · ·+ zin), the total number of quadratic term is Cn−1N−1N = nC n N . There are (n 2−n)CnN ZiZj(i 6= j) and N(N − 1)\n2 kinds of ZiZj . So every kind has\n2n(n− 1)CnN N(N − 1) terms.\n1\nCnNn 2\nCn N ∑\ni=1\n(zi1 + zi2 + · · ·+ zin)2 − Z 2\n= 1\nCnNn 2 {Cn−1N−1\nN ∑\nS=1\nZ2S + 2n(n− 1)CnN N(N − 1) ∑\ni<j\nZiZj} − Z 2\n=( 1 nN − 1 N2 )\nM ∑\nS=1\nZ2S + 2( n− 1 N(N − 1) − 1 N2 ) ∑\ni<j\nZiZj\n=( N − n n(N − 1))( N − 1 N2\nN ∑\nS=1\nZ2S − 2\nN2\n∑\ni<j\nZiZj)\n=( N − n n(N − 1))[ 1 N\nN ∑\nS=1\nZ2S − 1\nN2 (\nN ∑\nS=1\n+2 ∑\ni<j\nZiZj)]\n= N − n N − 1 σ2\nn (8)\nAbove all, Lemma 2.1 is proved to be right.\nLemma 3.2. If the sample size n >= Nu2α/2s 2\n∆2N + u2α/2s 2\n,\nthe confidence coefficient of error under 1−∆ is α\nProof. If confidence coefficient is 1−∆. Then,\nP [| z − Z |< ∆] = 1− α (9)\nWhen n is large,we can utilize normal approximation to make a conclusion that\nP{| z − Z σz |< ∆z σz } = Φ(ua/2)− Φ(−ua/2) (10)\nThen, ∆z = ua/2σz (11)\nCombining Lemma 2.1 ,we know\n∆z = ua/2 s√ n\n√\n1− n N\n(12)\nn = Nu2a/2s 2\n∆2zN + u 2 a/2s 2\n(13)\nAbove all, the correctness of Lemma 2.2 can be proved.\nUsing these Lemma, we can debate some questions about (5).Denote a set named Z\nZ = {(θTK[x1]− y1)K[x1], (θTK[x2]− y2)K[x2], · · · , (θTK[xm]− ym)K[xm]}\n(14)\nTake ω elements from the collection Z , if we donate the average num of these elements is ω. When ∆ = ∣ ∣ ∣ ξZ ∣ ∣ ∣ is small, the correctness of (5) can\nbe guaranteed. Combining (14), we can get n = Nu2a/2s 2 (ξZ) 2 N + u2a/2s 2 ≤ Nu2a/2s 2 ξ2s2N + u2a/2s 2 = Nu2a/2 ξ2N + u2a/2\n3.3. Speed of Convergence\nDefinition 3.2. Suppose that this process produce a sequence of iterations(θt) converge to (θ∗), if there exist a real number β > 0 and a constant(q > 0) which has no relationship with the number of iterations(t) let\nlimt→∞\n∥ ∥θ t+1 − θ∗ ∥ ∥\nl2 ∥ ∥ ∥θ k − θ∗ ∥ ∥ ∥ β\nl2\n= q. Therefore, the sequence θt\nhas a convergence speed of Q-β -th.\nOur algorithm has a linear convergence of Q-th. That’s to say β = 1 and q > 0. We denote Bt = 1\nω\n∑ω i=1(θ T K[xi] − yi)K[xi] + λθt.\nThen\n∥ ∥θ t+1 − θ∗ ∥ ∥ 2\nl2\n= 〈 θ t − θ∗ − ηtBt, θt − θ∗ − ηtBt 〉\nl2\n= ∥ ∥θ t − θ∗ ∥ ∥ 2\nl2 + 2ηt\n〈\nθ ∗ − θt, Bt\n〉\nl2 + η2t\n∥ ∥Bt ∥ ∥ 2\nl2\n(15)\n〈\nθ ∗ − θt, Bt\n〉\n=\n〈\nθ ∗ − θt, 1\nω\nω ∑\ni=1\n(θTK[xi]− [yi])K[xi] 〉\n+ λ 〈 θ ∗ − θt, θt 〉\n=\n〈\nθ ∗ − θt, 1\nω\nω ∑\ni=1\n(θTK[xi]− [yi])K[xi] 〉\n+ λ 〈 θ ∗, θt 〉 − λ ∥ ∥θ t ∥ ∥ 2\nl2\n≤ 〈 θ ∗ − θt, 1\nω\nω ∑\ni=1\n(θTK[xi]− [yi])K[xi] 〉\n+ 1\n2 λ ∥ ∥θ ∗ ∥ ∥ 2 l2 +\n1 2 λ ∥ ∥θ t ∥ ∥ 2 l2 − λ ∥ ∥θ t ∥ ∥ 2 l2\n≤ 〈 θ ∗ − θt, 1\nω\nω ∑\ni=1\n(θTK[xi]− [yi])K[xi] 〉\n+ λ\n2\n∥ ∥θ ∗ ∥ ∥ 2 l2 − λ\n2\n∥ ∥θ t ∥ ∥ 2\nl2\n(16)\nBy the convexity, we get from\n〈\nθ ∗ − θt, 1\nω\nω ∑\ni=1\n(θTK[xi]− [yi])K[xi] 〉\n≤ 1 2ω [ 1 ω\nω ∑\ni=1\n(θ∗KT [xi]− yi)2\n− ω ∑\ni=1\n(θtKT [xi]− yi)2]\n(17)\nSo (16) can turn into\n〈\nθ ∗ − θt, Bt\n〉 l2 ≤ 1 2 [ 1 ω\nω ∑\ni=1\n(θ∗KT [xi]− yi)2\n+ λ ∥ ∥θ ∗ ∥ ∥ 2 l2 ]− 1 2 [ 1 ω\nω ∑\ni=1\n(θtKT [xi]− yi)2 + λ ∥ ∥θ t ∥ ∥ 2\nl2 ]\n(18)\nSo we need to prove the correctness of this function\nLemma 3.3. λ ∥ ∥θ − θ∗ ∥ ∥ 2\nl2\n≤ { 1 ω ∑ω i=1 (θK T [xi]− yi)2 + λ ∥ ∥θ ∥ ∥ 2 l2 }− { 1 ω ∑ω i=1 (θ ∗ K T [xi]− yi)2 + λ ∥ ∥θ ∗ ∥ ∥ 2 l2 }\nProof.\nθ ν = νθ + (1− ν)θ∗ (19)\nG(ν) = 1\nω\nω ∑\ni=1\n(θνTK[xi]− yi)2 + λ ∥ ∥θ ν ∥ ∥ 2\nl2 (20)\nG′(ν) =2λ(θ − θ∗)Tθν + 2 ω\nω ∑\ni=1\n(θTK[xi]− yi)(θ\n− θ∗)TK[xi] (21)\nBeacause f ′(θ∗) = 0 ,then\nλ(θ − θ∗)Tθ∗ + 1 ω\nω ∑\ni=1\n(θTK[xi]− yi)(θ−\nθ ∗)TK[xi] = 0\n(22)\nG′(ν) = 2λ(θ − θ∗)Tθν − 2λ(θ − θ∗)Tθ∗\n+ 2\nω\nω ∑\ni=1\n(θνTK[xi]− yi)(θ − θ∗)TK[xi]\n− 2 ω\nω ∑\ni=1\n(θ∗TK[xi]− yi)(θ − θ∗)TK[xi]\n= 2λν(θ − θ∗)T (θ − θ∗) + 2ν ω\nω ∑\ni=1\n((θ − θ∗)TK[xi])2\n≥ 2λν(θ − θ∗)T (θ − θ∗) (23)\nObserve that G(1)−G(0) = ∫ 1\n0 G′(ν) dν.Thus,\nG(1)−G(0) ≥ 2λ(θ − θ∗)T (θ − θ∗) ∫ 1\n0\nν dν\n= λ(θ − θ∗)T (θ − θ∗) (24)\nThe correctness of Lemma3.3 can be proved.\nCombining Lemma3.3 and (18),We can get\n〈\nθ ∗ − θt, Bt\n〉 ≤ −λ 2 ∥ ∥θ t − θ∗ ∥ ∥ 2 l2 (25)\nAssume that k is the largest element of K[xi] , y is the largest element of Y\nLemma 3.4. ∥ ∥θ t ∥ ∥ l2 ≤ yk\nλI\nProof. We utilize mathematical induction to prove it. When t = 1, the correctness of lemma 2.4 is obvious.\nWhen t = t1, we assume that ∥ ∥θ t1 ∥ ∥ l2 ≤ yk\nλl is correct.\nSo When t = t1 + 1,\n∥ ∥θ t1+1 ∥ ∥ l2 ≤ (1− ληt) ∥ ∥θ t ∥ ∥ l2 + ∥ ∥ ∥ ηt\nω\n∑ω i=1 yiK[xi]\n∥ ∥ ∥\nl2\n≤ yk λl − ληt yk λl + ηt yk l = yk\nλl (26)\nAbove all, Lemma 2.4 can be proved.\nLemma 3.5.\n∥ ∥ ∥(θtTK[xi]− yi)K[xi] ∥ ∥ ∥ l2 ≤ yk\n3 λ + √ lyk (27)\nProof. ∥\n∥ ∥(θtTK[xi]− yi)K[xi] ∥ ∥ ∥\nl2 ≤\n∥ ∥ ∥θ tT K[xi] ∥ ∥ ∥\n∞\n∥ ∥K[xi] ∥ ∥\nl2\n+ y ∥ ∥K[xi] ∥ ∥\nl2\n≤ lk2 ∥ ∥θ t ∥ ∥\nl2 + y\n√ lk\n≤ lk2 yk λl\n+ y √ lk\n= yk3 λ + √ lyk\n(28)\nTherefore,\n∥ ∥Bt ∥ ∥ 2 l2 ≤ ( 1\nω\nω ∑\ni=1\n∥ ∥ ∥(θTK[xi]− yi)K[xi] ∥ ∥ ∥\nl2 + λ\n∥ ∥θ t ∥ ∥\nl2 )2\n≤ (yk 3 λ + √ lyk + λ yk λl )2\n(29)\nSo,combing (15) (25) (29),we can get\n∥ ∥θ t+1 − θ∗ ∥ ∥ 2 l2 ≤ (1− ληt) ∥ ∥θ t − θ∗ ∥ ∥ 2 l2\n+ η2t ( yk3 λ + √ lyk + λ yk λl )2\n(30)\nSo this algorithm is a linear convergence of Q-th."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2014,
    "abstractText" : "Currently, many machine learning algorithms contain lots of iterations. When it comes to existing large-scale distributed systems, some slave nodes may break down or have lower efficiency. Therefore traditional machine learning algorithm may fail because of the instability of distributed system.We presents a hybrid approach which not only own a high fault-tolerant but also achieve a balance of performance and efficiency.For each iteration, the result of slow machines will be abandoned. First, we discuss the relationship between accuracy and abandon rate. Then we debate the convergence speed of this process. Finally, our experiments demonstrate our idea can dramatically reduce calculation time and be used in many platforms.",
    "creator" : "LaTeX with hyperref package"
  }
}